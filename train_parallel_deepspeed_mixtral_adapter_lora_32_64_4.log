[2024-06-29 23:47:14,162] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:20,788] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-06-29 23:47:20,788] [INFO] [runner.py:568:main] cmd = /home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train_parallel_deepspeed_mixtral_adapter.py --num_stages=8 --shared_adapter_num=4 --shared_adapter_type=LoRA --lora_r=32 --lora_alpha=64 --save_model_shard=20 --skip_shard=660 --checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint
[2024-06-29 23:47:24,305] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:26,002] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-06-29 23:47:26,002] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-06-29 23:47:26,002] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-06-29 23:47:26,002] [INFO] [launch.py:163:main] dist_world_size=8
[2024-06-29 23:47:26,002] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-06-29 23:47:26,016] [INFO] [launch.py:253:main] process 451381 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=0', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:26,027] [INFO] [launch.py:253:main] process 451382 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=1', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:26,044] [INFO] [launch.py:253:main] process 451383 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=2', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:26,061] [INFO] [launch.py:253:main] process 451384 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=3', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:26,076] [INFO] [launch.py:253:main] process 451386 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=4', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:26,092] [INFO] [launch.py:253:main] process 451388 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=5', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:26,103] [INFO] [launch.py:253:main] process 451389 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=6', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:26,113] [INFO] [launch.py:253:main] process 451391 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=7', '--num_stages=8', '--shared_adapter_num=4', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=660', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint']
[2024-06-29 23:47:33,904] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:35,829] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 23:47:38,412] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:38,949] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:39,015] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:39,129] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 23:47:40,712] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 23:47:40,795] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 23:47:45,344] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:45,855] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 23:47:46,018] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 23:47:46,554] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 23:47:46,555] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:24,  1.27s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:23,  1.25s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:23,  1.22s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:44,  2.50s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:21,  1.12s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:32,  1.73s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:05<00:50,  2.80s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:05<00:55,  3.07s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:26,  1.46s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:37,  2.07s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:07<00:45,  2.66s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:07<00:46,  2.71s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:04<00:27,  1.61s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:08<00:51,  3.01s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:33,  1.95s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:09<00:39,  2.48s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:06<00:26,  1.63s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:10<00:40,  2.51s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:10<00:40,  2.53s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:26,  1.66s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:12<00:35,  2.35s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:08<00:28,  1.93s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:28,  1.87s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:12<00:38,  2.58s/it][2024-06-29 23:48:04,418] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  25%|██▌       | 5/20 [00:12<00:38,  2.54s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:14<00:30,  2.21s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:11<00:27,  1.94s/it][2024-06-29 23:48:06,231] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  30%|███       | 6/20 [00:10<00:27,  1.99s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:14<00:33,  2.36s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:14<00:33,  2.43s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:12<00:26,  2.02s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:13<00:26,  2.00s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:16<00:29,  2.27s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:16<00:29,  2.30s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:17<00:31,  2.45s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:15<00:25,  2.16s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:26,  2.19s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:19<00:27,  2.31s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:19<00:29,  2.48s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:19<00:30,  2.53s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:17<00:22,  2.06s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:21<00:24,  2.18s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:21<00:24,  2.21s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:21<00:24,  2.23s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:25,  2.34s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:31,  1.68s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:23<00:21,  2.17s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:23<00:22,  2.28s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:22,  2.28s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:22,  2.23s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:24<00:24,  2.45s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:29,  1.66s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:25<00:19,  2.17s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:22<00:18,  2.11s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:21<00:19,  2.11s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:25<00:19,  2.19s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:25<00:20,  2.23s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:29,  1.76s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:16,  2.06s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:06<00:24,  1.52s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:17,  2.21s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:24<00:17,  2.17s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:25<00:18,  2.34s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:28<00:19,  2.46s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:08<00:25,  1.71s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:29<00:15,  2.17s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:26<00:14,  2.13s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:26<00:15,  2.16s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:30<00:15,  2.22s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:30<00:16,  2.30s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:10<00:25,  1.84s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:29<00:12,  2.15s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:32<00:13,  2.25s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:32<00:13,  2.20s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:28<00:13,  2.25s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:32<00:13,  2.26s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:12<00:22,  1.74s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:34<00:10,  2.19s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:31<00:10,  2.17s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:30<00:10,  2.16s/it][2024-06-29 23:48:26,221] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  75%|███████▌  | 15/20 [00:34<00:11,  2.21s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:35<00:11,  2.29s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:13<00:21,  1.78s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:33<00:08,  2.10s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:32<00:08,  2.12s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:36<00:08,  2.20s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:36<00:08,  2.14s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:15<00:17,  1.64s/it][2024-06-29 23:48:28,599] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  80%|████████  | 16/20 [00:37<00:08,  2.23s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:38<00:06,  2.19s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:38<00:06,  2.15s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:17<00:18,  1.84s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:36<00:07,  2.35s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:35<00:07,  2.41s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:40<00:07,  2.50s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:40<00:04,  2.17s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:37<00:04,  2.19s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:19<00:18,  2.02s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:38<00:04,  2.26s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:41<00:04,  2.30s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:42<00:04,  2.27s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:42<00:02,  2.10s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:21<00:14,  1.87s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:39<00:02,  2.06s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:39<00:02,  2.09s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:43<00:02,  2.21s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.17s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards: 100%|██████████| 20/20 [00:39<00:00,  1.60s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:39<00:00,  1.99s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  1.72s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.19s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:44<00:02,  2.17s/it]Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:20,  1.10s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:23,  1.22s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  1.85s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  2.06s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:  65%|██████▌   | 13/20 [00:22<00:12,  1.75s/it]Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
Loading checkpoint shards: 100%|██████████| 20/20 [00:44<00:00,  1.75s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:44<00:00,  2.24s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:18,  1.04s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:23<00:08,  1.49s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:24,  1.33s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:24<00:06,  1.35s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:03<00:18,  1.09s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:03<00:19,  1.15s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:25<00:05,  1.27s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:04<00:17,  1.08s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:04<00:19,  1.19s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:26<00:03,  1.20s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:05<00:16,  1.12s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:05<00:17,  1.15s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:28<00:02,  1.15s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:06<00:15,  1.12s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:07<00:16,  1.15s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:28<00:01,  1.06s/it]Rank 6 initialized with CUDA_MEM (60881108992, 85100068864)
Deepspeed engine initializing at --- RANK 6 --- ...
Loading checkpoint shards: 100%|██████████| 20/20 [00:29<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
Loading checkpoint shards:  35%|███▌      | 7/20 [00:07<00:14,  1.11s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:07<00:13,  1.00s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:08<00:12,  1.01s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:08<00:11,  1.06it/s]Rank 4 initialized with CUDA_MEM (60881108992, 85100068864)
Deepspeed engine initializing at --- RANK 4 --- ...
Rank 3 initialized with CUDA_MEM (60881108992, 85100068864)
Loading checkpoint shards:  45%|████▌     | 9/20 [00:09<00:10,  1.08it/s]Deepspeed engine initializing at --- RANK 3 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  45%|████▌     | 9/20 [00:09<00:09,  1.14it/s]Rank 7 initialized with CUDA_MEM (60354723840, 85100068864)
Deepspeed engine initializing at --- RANK 7 --- ...
Loading checkpoint shards:  50%|█████     | 10/20 [00:09<00:08,  1.19it/s]Loading checkpoint shards:  50%|█████     | 10/20 [00:10<00:08,  1.16it/s]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:10<00:07,  1.28it/s]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:10<00:07,  1.24it/s]Rank 5 initialized with CUDA_MEM (60881108992, 85100068864)
Deepspeed engine initializing at --- RANK 5 --- ...
Loading checkpoint shards:  60%|██████    | 12/20 [00:11<00:05,  1.36it/s]Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading checkpoint shards:  60%|██████    | 12/20 [00:11<00:06,  1.27it/s]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:11<00:05,  1.38it/s]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:12<00:05,  1.33it/s]Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  70%|███████   | 14/20 [00:12<00:04,  1.42it/s]Loading checkpoint shards:  70%|███████   | 14/20 [00:12<00:04,  1.41it/s]Rank 2 initialized with CUDA_MEM (60881108992, 85100068864)
Deepspeed engine initializing at --- RANK 2 --- ...
Loading checkpoint shards:  75%|███████▌  | 15/20 [00:13<00:03,  1.45it/s]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:13<00:03,  1.42it/s]Loading checkpoint shards:  80%|████████  | 16/20 [00:14<00:03,  1.33it/s]Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  80%|████████  | 16/20 [00:14<00:02,  1.42it/s]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:15<00:02,  1.21it/s]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:15<00:02,  1.28it/s]Loading checkpoint shards:  90%|█████████ | 18/20 [00:16<00:01,  1.09it/s]Loading checkpoint shards:  90%|█████████ | 18/20 [00:16<00:01,  1.13it/s]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:17<00:00,  1.07it/s]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:17<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1, ProcessCoord(pipe=2, data=0): 2, ProcessCoord(pipe=3, data=0): 3, ProcessCoord(pipe=4, data=0): 4, ProcessCoord(pipe=5, data=0): 5, ProcessCoord(pipe=6, data=0): 6, ProcessCoord(pipe=7, data=0): 7}
[2024-06-29 23:48:52,272] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:LanguageModelLayerWrapper
stage=0 layers=2
     0: LlavaMultiModalModuleWrapper
     1: LanguageModelLayerWrapper
stage=1 layers=1
     2: LanguageModelLayerWrapper
stage=2 layers=1
     3: LanguageModelLayerWrapper
stage=3 layers=1
     4: LanguageModelLayerWrapper
stage=4 layers=1
     5: LanguageModelLayerWrapper
stage=5 layers=1
     6: LanguageModelLayerWrapper
stage=6 layers=1
     7: LanguageModelLayerWrapper
stage=7 layers=2
     8: LanguageModelLayerWrapper
     9: LanguageModelFinalWrapper
  loss: loss_fn
Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.3.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.1.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.2.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.3.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 33554432 || all params: 47060850688 || trainable%: 0.07
Print trainable params: 54534144 || all params: 47060850688 || trainable%: 0.12
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 9.239184856414795 seconds
[2024-06-29 23:48:53,284] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 6.217909574508667 seconds
[2024-06-29 23:48:53,313] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 4.414160966873169 seconds
Time to load fused_adam op: 7.318392753601074 seconds
[2024-06-29 23:48:53,329] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-06-29 23:48:53,330] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 7.322081565856934 seconds
[2024-06-29 23:48:53,341] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 7.323277473449707 seconds
[2024-06-29 23:48:53,360] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Rank 0 initialized with CUDA_MEM (59041906688, 85100068864)
Deepspeed engine initializing at --- RANK 0 --- ...
[2024-06-29 23:48:55,237] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
Rank 1 initialized with CUDA_MEM (60881108992, 85100068864)
Deepspeed engine initializing at --- RANK 1 --- ...
[2024-06-29 23:48:55,528] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 5.794666290283203 seconds
[2024-06-29 23:49:02,466] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-06-29 23:49:02,466] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-29 23:49:02,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-06-29 23:49:02,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-06-29 23:49:02,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupCosineLR
[2024-06-29 23:49:02,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupCosineLR object at 0x1454c07224c0>
[2024-06-29 23:49:02,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-06-29 23:49:02,479] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-06-29 23:49:02,479] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-29 23:49:02,479] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1454c0954f70>
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-06-29 23:49:02,480] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 128
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=True, output_path='/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint', job_name='deepspeed_monitor_logs') enabled=True
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   optimizer_name ............... adam
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 2e-05}
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True, 'use_reentrant': False}
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupCosineLR
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 5718, 'warmup_min_ratio': 0.1, 'warmup_num_steps': 171.54}
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   steps_per_print .............. 1
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-06-29 23:49:02,481] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-06-29 23:49:02,482] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-06-29 23:49:02,482] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-06-29 23:49:02,482] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-06-29 23:49:02,482] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-29 23:49:02,482] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-06-29 23:49:02,482] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-29 23:49:02,482] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-06-29 23:49:02,482] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 128, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupCosineLR", 
        "params": {
            "total_num_steps": 5.718000e+03, 
            "warmup_min_ratio": 0.1, 
            "warmup_num_steps": 171.54
        }
    }, 
    "pipeline": {
        "use_reentrant": false
    }, 
    "csv_monitor": {
        "enabled": true, 
        "output_path": "/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint", 
        "job_name": "deepspeed_monitor_logs"
    }
}
[2024-06-29 23:49:02,482] [INFO] [engine.py:101:__init__] CONFIG: micro_batches=128 micro_batch_size=1
[2024-06-29 23:49:02,482] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 5.511980295181274 seconds
[2024-06-29 23:49:02,513] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-06-29 23:49:06,986] [INFO] [engine.py:160:__init__] RANK=1 STAGE=1 LAYERS=1 [2, 3) STAGE_PARAMS=4194304 (4.194M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
[2024-06-29 23:49:06,986] [INFO] [engine.py:160:__init__] RANK=3 STAGE=3 LAYERS=1 [4, 5) STAGE_PARAMS=4194304 (4.194M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
[2024-06-29 23:49:06,986] [INFO] [engine.py:160:__init__] RANK=5 STAGE=5 LAYERS=1 [6, 7) STAGE_PARAMS=4194304 (4.194M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
[2024-06-29 23:49:06,986] [INFO] [engine.py:160:__init__] RANK=2 STAGE=2 LAYERS=1 [3, 4) STAGE_PARAMS=4194304 (4.194M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
[2024-06-29 23:49:06,986] [INFO] [engine.py:160:__init__] RANK=7 STAGE=7 LAYERS=2 [8, 10) STAGE_PARAMS=4194304 (4.194M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
[2024-06-29 23:49:06,986] [INFO] [engine.py:160:__init__] RANK=6 STAGE=6 LAYERS=1 [7, 8) STAGE_PARAMS=4194304 (4.194M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
[2024-06-29 23:49:06,986] [INFO] [engine.py:160:__init__] RANK=4 STAGE=4 LAYERS=1 [5, 6) STAGE_PARAMS=4194304 (4.194M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
[2024-06-29 23:49:06,987] [INFO] [engine.py:160:__init__] RANK=0 STAGE=0 LAYERS=2 [0, 2) STAGE_PARAMS=25174016 (25.174M) TOTAL_PARAMS=54534144 (54.534M) UNIQUE_PARAMS=54534144 (54.534M)
Deepspeed engine successfully initialized at --- RANK 0 --- hosting 36 of 260 trainable parameters
Loading latest model checkpoint at shard 660
[2024-06-29 23:49:09,705] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:09,780] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:09,780] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:09,846] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:09,847] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_00-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 7 --- hosting 32 of 260 trainable parameters
[2024-06-29 23:49:10,135] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:10,178] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:10,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_07_model_states.pt...
[2024-06-29 23:49:10,192] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_07_model_states.pt.
[2024-06-29 23:49:10,193] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_08-model_states.pt...
[2024-06-29 23:49:10,451] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_00-model_states.pt.
[2024-06-29 23:49:10,453] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_00-model_states.pt...
[2024-06-29 23:49:10,973] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_00-model_states.pt.
[2024-06-29 23:49:11,147] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_01-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 2 --- hosting 32 of 260 trainable parameters
Deepspeed engine successfully initialized at --- RANK 1 --- hosting 32 of 260 trainable parameters
[2024-06-29 23:49:11,777] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:11,777] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:11,841] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:11,842] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_01_model_states.pt...
[2024-06-29 23:49:11,852] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_01_model_states.pt.
[2024-06-29 23:49:11,853] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_02-model_states.pt...
[2024-06-29 23:49:11,861] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:11,861] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_02_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 4 --- hosting 32 of 260 trainable parameters
[2024-06-29 23:49:11,868] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 3 --- hosting 32 of 260 trainable parameters
[2024-06-29 23:49:11,871] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:11,875] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_02_model_states.pt.
[2024-06-29 23:49:11,876] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_03-model_states.pt...
[2024-06-29 23:49:11,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:11,936] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_04_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 6 --- hosting 32 of 260 trainable parameters
Deepspeed engine successfully initialized at --- RANK 5 --- hosting 32 of 260 trainable parameters
[2024-06-29 23:49:11,942] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:11,942] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt...
[2024-06-29 23:49:11,947] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:11,947] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_03_model_states.pt...
[2024-06-29 23:49:11,947] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_04_model_states.pt.
[2024-06-29 23:49:11,948] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_05-model_states.pt...
[2024-06-29 23:49:11,959] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_03_model_states.pt.
[2024-06-29 23:49:11,959] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_04-model_states.pt...
[2024-06-29 23:49:11,996] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:11,996] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_06_model_states.pt...
[2024-06-29 23:49:12,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_06_model_states.pt.
[2024-06-29 23:49:12,009] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_07-model_states.pt...
[2024-06-29 23:49:12,014] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_00_model_states.pt.
[2024-06-29 23:49:12,014] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_05_model_states.pt...
[2024-06-29 23:49:12,025] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/mp_rank_05_model_states.pt.
[2024-06-29 23:49:12,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_06-model_states.pt...
[2024-06-29 23:49:19,383] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_08-model_states.pt.
[2024-06-29 23:49:20,078] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_08-model_states.pt...
[2024-06-29 23:49:20,319] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_05-model_states.pt.
[2024-06-29 23:49:20,960] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_05-model_states.pt...
[2024-06-29 23:49:23,912] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_06-model_states.pt.
[2024-06-29 23:49:25,123] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_06-model_states.pt...
[2024-06-29 23:49:25,648] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_07-model_states.pt.
[2024-06-29 23:49:26,669] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_07-model_states.pt...
[2024-06-29 23:49:28,830] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_02-model_states.pt.
[2024-06-29 23:49:28,927] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_02-model_states.pt...
[2024-06-29 23:49:33,049] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_05-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:49:38,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_07-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:49:50,391] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_08-model_states.pt.
[2024-06-29 23:49:51,783] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_09-model_states.pt...
[2024-06-29 23:49:52,056] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_09-model_states.pt.
[2024-06-29 23:49:52,083] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_09-model_states.pt...
[2024-06-29 23:49:52,254] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_06-model_states.pt.
[2024-06-29 23:49:52,299] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_09-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:49:55,864] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_01-model_states.pt.
[2024-06-29 23:49:56,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_01-model_states.pt...
[2024-06-29 23:50:12,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_01-model_states.pt.
[2024-06-29 23:50:14,719] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_02-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]Shard 0 / 660 skipped
Shard 1 / 660 skipped
Shard 2 / 660 skipped
Shard 3 / 660 skipped
Shard 4 / 660 skipped
Shard 5 / 660 skipped
Shard 6 / 660 skipped
Shard 7 / 660 skipped
Shard 8 / 660 skipped
Shard 9 / 660 skipped
Shard 10 / 660 skipped
Shard 11 / 660 skipped
Shard 12 / 660 skipped
Shard 13 / 660 skipped
Shard 14 / 660 skipped
Shard 15 / 660 skipped
Shard 16 / 660 skipped
Shard 17 / 660 skipped
Shard 18 / 660 skipped
Shard 19 / 660 skipped
Shard 20 / 660 skipped
Shard 21 / 660 skipped
Shard 22 / 660 skipped
Shard 23 / 660 skipped
Shard 24 / 660 skipped
Shard 25 / 660 skipped
Shard 26 / 660 skipped
Shard 27 / 660 skipped
Shard 28 / 660 skipped
Shard 29 / 660 skipped
Shard 30 / 660 skipped
Shard 31 / 660 skipped
Shard 32 / 660 skipped
Shard 33 / 660 skipped
Shard 34 / 660 skipped
Shard 35 / 660 skipped
Shard 36 / 660 skipped
Shard 37 / 660 skipped
Shard 38 / 660 skipped
Shard 39 / 660 skipped
Shard 40 / 660 skipped
Shard 41 / 660 skipped
Shard 42 / 660 skipped
Shard 43 / 660 skipped
Shard 44 / 660 skipped
Shard 45 / 660 skipped
Shard 46 / 660 skipped
Shard 47 / 660 skipped
Shard 48 / 660 skipped
Shard 49 / 660 skipped
Shard 50 / 660 skipped
Shard 51 / 660 skipped
Shard 52 / 660 skipped
Shard 53 / 660 skipped
Shard 54 / 660 skipped
Shard 55 / 660 skipped
Shard 56 / 660 skipped
Shard 57 / 660 skipped
Shard 58 / 660 skipped
Shard 59 / 660 skipped
Shard 60 / 660 skipped
Shard 61 / 660 skipped
Shard 62 / 660 skipped
Shard 63 / 660 skipped
Shard 64 / 660 skipped
Shard 65 / 660 skipped
Shard 66 / 660 skipped
Shard 67 / 660 skipped
Shard 68 / 660 skipped
Shard 69 / 660 skipped
Shard 70 / 660 skipped
Shard 71 / 660 skipped
Shard 72 / 660 skipped
Shard 73 / 660 skipped
Shard 74 / 660 skipped
Shard 75 / 660 skipped
Shard 76 / 660 skipped
Shard 77 / 660 skipped
Shard 78 / 660 skipped
Shard 79 / 660 skipped
Shard 80 / 660 skipped
Shard 81 / 660 skipped
Shard 82 / 660 skipped
Shard 83 / 660 skipped
Shard 84 / 660 skipped
Shard 85 / 660 skipped
Shard 86 / 660 skipped
Shard 87 / 660 skipped
Shard 88 / 660 skipped
Shard 89 / 660 skipped
Shard 90 / 660 skipped
Shard 91 / 660 skipped
Shard 92 / 660 skipped
Shard 93 / 660 skipped
Shard 94 / 660 skipped
Shard 95 / 660 skipped
Shard 96 / 660 skipped
Shard 97 / 660 skipped
Shard 98 / 660 skipped
Shard 99 / 660 skipped
Shard 100 / 660 skipped
Shard 101 / 660 skipped
Shard 102 / 660 skipped
Shard 103 / 660 skipped
Shard 104 / 660 skipped
Shard 105 / 660 skipped
Shard 106 / 660 skipped
Shard 107 / 660 skipped
Shard 108 / 660 skipped
Shard 109 / 660 skipped
Shard 110 / 660 skipped
Shard 111 / 660 skipped
Shard 112 / 660 skipped
Shard 113 / 660 skipped
Shard 114 / 660 skipped
Shard 115 / 660 skipped
Shard 116 / 660 skipped
Shard 117 / 660 skipped
Shard 118 / 660 skipped
Shard 119 / 660 skipped
Shard 120 / 660 skipped
Shard 121 / 660 skipped
Shard 122 / 660 skipped
Shard 123 / 660 skipped
Shard 124 / 660 skipped
Shard 125 / 660 skipped
Shard 126 / 660 skipped
Shard 127 / 660 skipped
Shard 128 / 660 skipped
Shard 129 / 660 skipped
Shard 130 / 660 skipped
Shard 131 / 660 skipped
Shard 132 / 660 skipped
Shard 133 / 660 skipped
Shard 134 / 660 skipped
Shard 135 / 660 skipped
Shard 136 / 660 skipped
Shard 137 / 660 skipped
Shard 138 / 660 skipped
Shard 139 / 660 skipped
Shard 140 / 660 skipped
Shard 141 / 660 skipped
Shard 142 / 660 skipped
Shard 143 / 660 skipped
Shard 144 / 660 skipped
Shard 145 / 660 skipped
Shard 146 / 660 skipped
Shard 147 / 660 skipped
Shard 148 / 660 skipped
Shard 149 / 660 skipped
Shard 150 / 660 skipped
Shard 151 / 660 skipped
Shard 152 / 660 skipped
Shard 153 / 660 skipped
Shard 154 / 660 skipped
Shard 155 / 660 skipped
Shard 156 / 660 skipped
Shard 157 / 660 skipped
Shard 158 / 660 skipped
Shard 159 / 660 skipped
Shard 160 / 660 skipped
Shard 161 / 660 skipped
Shard 162 / 660 skipped
Shard 163 / 660 skipped
Shard 164 / 660 skipped
Shard 165 / 660 skipped
Shard 166 / 660 skipped
Shard 167 / 660 skipped
Shard 168 / 660 skipped
Shard 169 / 660 skipped
Shard 170 / 660 skipped
Shard 171 / 660 skipped
Shard 172 / 660 skipped
Shard 173 / 660 skipped
Shard 174 / 660 skipped
Shard 175 / 660 skipped
Shard 176 / 660 skipped
Shard 177 / 660 skipped
Shard 178 / 660 skipped
Shard 179 / 660 skipped
Shard 180 / 660 skipped
Shard 181 / 660 skipped
Shard 182 / 660 skipped
Shard 183 / 660 skipped
Shard 184 / 660 skipped
Shard 185 / 660 skipped
Shard 186 / 660 skipped
Shard 187 / 660 skipped
Shard 188 / 660 skipped
Shard 189 / 660 skipped
Shard 190 / 660 skipped
Shard 191 / 660 skipped
Shard 192 / 660 skipped
Shard 193 / 660 skipped
Shard 194 / 660 skipped
Shard 195 / 660 skipped
Shard 196 / 660 skipped
Shard 197 / 660 skipped
Shard 198 / 660 skipped
Shard 199 / 660 skipped
Shard 200 / 660 skipped
Shard 201 / 660 skipped
Shard 202 / 660 skipped
Shard 203 / 660 skipped
Shard 204 / 660 skipped
Shard 205 / 660 skipped
Shard 206 / 660 skipped
Shard 207 / 660 skipped
Shard 208 / 660 skipped
Shard 209 / 660 skipped
Shard 210 / 660 skipped
Shard 211 / 660 skipped
Shard 212 / 660 skipped
Shard 213 / 660 skipped
Shard 214 / 660 skipped
Shard 215 / 660 skipped
Shard 216 / 660 skipped
Shard 217 / 660 skipped
Shard 218 / 660 skipped
Shard 219 / 660 skipped
Shard 220 / 660 skipped
Shard 221 / 660 skipped
Shard 222 / 660 skipped
Shard 223 / 660 skipped
Shard 224 / 660 skipped
Shard 225 / 660 skipped
Shard 226 / 660 skipped
Shard 227 / 660 skipped
Shard 228 / 660 skipped
Shard 229 / 660 skipped
Shard 230 / 660 skipped
Shard 231 / 660 skipped
Shard 232 / 660 skipped
Shard 233 / 660 skipped
Shard 234 / 660 skipped
Shard 235 / 660 skipped
Shard 236 / 660 skipped
Shard 237 / 660 skipped
Shard 238 / 660 skipped
Shard 239 / 660 skipped
Shard 240 / 660 skipped
Shard 241 / 660 skipped
Shard 242 / 660 skipped
Shard 243 / 660 skipped
Shard 244 / 660 skipped
Shard 245 / 660 skipped
Shard 246 / 660 skipped
Shard 247 / 660 skipped
Shard 248 / 660 skipped
Shard 249 / 660 skipped
Shard 250 / 660 skipped
Shard 251 / 660 skipped
Shard 252 / 660 skipped
Shard 253 / 660 skipped
Shard 254 / 660 skipped
Shard 255 / 660 skipped
Shard 256 / 660 skipped
Shard 257 / 660 skipped
Shard 258 / 660 skipped
Shard 259 / 660 skipped
Shard 260 / 660 skipped
Shard 261 / 660 skipped
Shard 262 / 660 skipped
Shard 263 / 660 skipped
Shard 264 / 660 skipped
Shard 265 / 660 skipped
Shard 266 / 660 skipped
Shard 267 / 660 skipped
Shard 268 / 660 skipped
Shard 269 / 660 skipped
Shard 270 / 660 skipped
Shard 271 / 660 skipped
Shard 272 / 660 skipped
Shard 273 / 660 skipped
Shard 274 / 660 skipped
Shard 275 / 660 skipped
Shard 276 / 660 skipped
Shard 277 / 660 skipped
Shard 278 / 660 skipped
Shard 279 / 660 skipped
Shard 280 / 660 skipped
Shard 281 / 660 skipped
Shard 282 / 660 skipped
Shard 283 / 660 skipped
Shard 284 / 660 skipped
Shard 285 / 660 skipped
Shard 286 / 660 skipped
Shard 287 / 660 skipped
Shard 288 / 660 skipped
Shard 289 / 660 skipped
Shard 290 / 660 skipped
Shard 291 / 660 skipped
Shard 292 / 660 skipped
Shard 293 / 660 skipped
Shard 294 / 660 skipped
Shard 295 / 660 skipped
Shard 296 / 660 skipped
Shard 297 / 660 skipped
Shard 298 / 660 skipped
Shard 299 / 660 skipped
Shard 300 / 660 skipped
Shard 301 / 660 skipped
Shard 302 / 660 skipped
Shard 303 / 660 skipped
Shard 304 / 660 skipped
Shard 305 / 660 skipped
Shard 306 / 660 skipped
Shard 307 / 660 skipped
Shard 308 / 660 skipped
Shard 309 / 660 skipped
Shard 310 / 660 skipped
Shard 311 / 660 skipped
Shard 312 / 660 skipped
Shard 313 / 660 skipped
Shard 314 / 660 skipped
Shard 315 / 660 skipped
Shard 316 / 660 skipped
Shard 317 / 660 skipped
Shard 318 / 660 skipped
Shard 319 / 660 skipped
Shard 320 / 660 skipped
Shard 321 / 660 skipped
Shard 322 / 660 skipped
Shard 323 / 660 skipped
Shard 324 / 660 skipped
Shard 325 / 660 skipped
Shard 326 / 660 skipped
Shard 327 / 660 skipped
Shard 328 / 660 skipped
Shard 329 / 660 skipped
Shard 330 / 660 skipped
Shard 331 / 660 skipped
Shard 332 / 660 skipped
Shard 333 / 660 skipped
Shard 334 / 660 skipped
Shard 335 / 660 skipped
Shard 336 / 660 skipped
Shard 337 / 660 skipped
Shard 338 / 660 skipped
Shard 339 / 660 skipped
Shard 340 / 660 skipped
Shard 341 / 660 skipped
Shard 342 / 660 skipped
Shard 343 / 660 skipped
Shard 344 / 660 skipped
Shard 345 / 660 skipped
Shard 346 / 660 skipped
Shard 347 / 660 skipped
Shard 348 / 660 skipped
Shard 349 / 660 skipped
Shard 350 / 660 skipped
Shard 351 / 660 skipped
Shard 352 / 660 skipped
Shard 353 / 660 skipped
Shard 354 / 660 skipped
Shard 355 / 660 skipped
Shard 356 / 660 skipped
Shard 357 / 660 skipped
Shard 358 / 660 skipped
Shard 359 / 660 skipped
Shard 360 / 660 skipped
Shard 361 / 660 skipped
Shard 362 / 660 skipped
Shard 363 / 660 skipped
Shard 364 / 660 skipped
Shard 365 / 660 skipped
Shard 366 / 660 skipped
Shard 367 / 660 skipped
Shard 368 / 660 skipped
Shard 369 / 660 skipped
Shard 370 / 660 skipped
Shard 371 / 660 skipped
Shard 372 / 660 skipped
Shard 373 / 660 skipped
Shard 374 / 660 skipped
Shard 375 / 660 skipped
Shard 376 / 660 skipped
Shard 377 / 660 skipped
Shard 378 / 660 skipped
Shard 379 / 660 skipped
Shard 380 / 660 skipped
Shard 381 / 660 skipped
Shard 382 / 660 skipped
Shard 383 / 660 skipped
Shard 384 / 660 skipped
Shard 385 / 660 skipped
Shard 386 / 660 skipped
Shard 387 / 660 skipped
Shard 388 / 660 skipped
Shard 389 / 660 skipped
Shard 390 / 660 skipped
Shard 391 / 660 skipped
Shard 392 / 660 skipped
Shard 393 / 660 skipped
Shard 394 / 660 skipped
Shard 395 / 660 skipped
Shard 396 / 660 skipped
Shard 397 / 660 skipped
Shard 398 / 660 skipped
Shard 399 / 660 skipped
Shard 400 / 660 skipped
Shard 401 / 660 skipped
Shard 402 / 660 skipped
Shard 403 / 660 skipped
Shard 404 / 660 skipped
Shard 405 / 660 skipped
Shard 406 / 660 skipped
Shard 407 / 660 skipped
Shard 408 / 660 skipped
Shard 409 / 660 skipped
Shard 410 / 660 skipped
Shard 411 / 660 skipped
Shard 412 / 660 skipped
Shard 413 / 660 skipped
Shard 414 / 660 skipped
Shard 415 / 660 skipped
Shard 416 / 660 skipped
Shard 417 / 660 skipped
Shard 418 / 660 skipped
Shard 419 / 660 skipped
Shard 420 / 660 skipped
Shard 421 / 660 skipped
Shard 422 / 660 skipped
Shard 423 / 660 skipped
Shard 424 / 660 skipped
Shard 425 / 660 skipped
Shard 426 / 660 skipped
Shard 427 / 660 skipped
Shard 428 / 660 skipped
Shard 429 / 660 skipped
Shard 430 / 660 skipped
Shard 431 / 660 skipped
Shard 432 / 660 skipped
Shard 433 / 660 skipped
Shard 434 / 660 skipped
Shard 435 / 660 skipped
Shard 436 / 660 skipped
Shard 437 / 660 skipped
Shard 438 / 660 skipped
Shard 439 / 660 skipped
Shard 440 / 660 skipped
Shard 441 / 660 skipped
Shard 442 / 660 skipped
Shard 443 / 660 skipped
Shard 444 / 660 skipped
Shard 445 / 660 skipped
Shard 446 / 660 skipped
Shard 447 / 660 skipped
Shard 448 / 660 skipped
Shard 449 / 660 skipped
Shard 450 / 660 skipped
Shard 451 / 660 skipped
Shard 452 / 660 skipped
Shard 453 / 660 skipped
Shard 454 / 660 skipped
Shard 455 / 660 skipped
Shard 456 / 660 skipped
Shard 457 / 660 skipped
Shard 458 / 660 skipped
Shard 459 / 660 skipped
Shard 460 / 660 skipped
Shard 461 / 660 skipped
Shard 462 / 660 skipped
Shard 463 / 660 skipped
Shard 464 / 660 skipped
Shard 465 / 660 skipped
Shard 466 / 660 skipped
Shard 467 / 660 skipped
Shard 468 / 660 skipped
Shard 469 / 660 skipped
Shard 470 / 660 skipped
Shard 471 / 660 skipped
Shard 472 / 660 skipped
Shard 473 / 660 skipped
Shard 474 / 660 skipped
Shard 475 / 660 skipped
Shard 476 / 660 skipped
Shard 477 / 660 skipped
Shard 478 / 660 skipped
Shard 479 / 660 skipped
Shard 480 / 660 skipped
Shard 481 / 660 skipped
Shard 482 / 660 skipped
Shard 483 / 660 skipped
Shard 484 / 660 skipped
Shard 485 / 660 skipped
Shard 486 / 660 skipped
Shard 487 / 660 skipped
Shard 488 / 660 skipped
Shard 489 / 660 skipped
Shard 490 / 660 skipped
Shard 491 / 660 skipped
Shard 492 / 660 skipped
Shard 493 / 660 skipped
Shard 494 / 660 skipped
Shard 495 / 660 skipped
Shard 496 / 660 skipped
Shard 497 / 660 skipped
Shard 498 / 660 skipped
Shard 499 / 660 skipped
Shard 500 / 660 skipped
Shard 501 / 660 skipped
Shard 502 / 660 skipped
Shard 503 / 660 skipped
Shard 504 / 660 skipped
Shard 505 / 660 skipped
Shard 506 / 660 skipped
Shard 507 / 660 skipped
Shard 508 / 660 skipped
Shard 509 / 660 skipped
Shard 510 / 660 skipped
Shard 511 / 660 skipped
Shard 512 / 660 skipped
Shard 513 / 660 skipped
Shard 514 / 660 skipped
Shard 515 / 660 skipped
Shard 516 / 660 skipped
Shard 517 / 660 skipped
Shard 518 / 660 skipped
Shard 519 / 660 skipped
Shard 520 / 660 skipped
Shard 521 / 660 skipped
Shard 522 / 660 skipped
Shard 523 / 660 skipped
Shard 524 / 660 skipped
Shard 525 / 660 skipped
Shard 526 / 660 skipped
Shard 527 / 660 skipped
Shard 528 / 660 skipped
Shard 529 / 660 skipped
Shard 530 / 660 skipped
Shard 531 / 660 skipped
Shard 532 / 660 skipped
Shard 533 / 660 skipped
Shard 534 / 660 skipped
Shard 535 / 660 skipped
Shard 536 / 660 skipped
Shard 537 / 660 skipped
Shard 538 / 660 skipped
Shard 539 / 660 skipped
Shard 540 / 660 skipped
Shard 541 / 660 skipped
Shard 542 / 660 skipped
Shard 543 / 660 skipped
Shard 544 / 660 skipped
Shard 545 / 660 skipped
Shard 546 / 660 skipped
Shard 547 / 660 skipped
Shard 548 / 660 skipped
Shard 549 / 660 skipped
Shard 550 / 660 skipped
Shard 551 / 660 skipped
Shard 552 / 660 skipped
Shard 553 / 660 skipped
Shard 554 / 660 skipped
Shard 555 / 660 skipped
Shard 556 / 660 skipped
Shard 557 / 660 skipped
Shard 558 / 660 skipped
Shard 559 / 660 skipped
Shard 560 / 660 skipped
Shard 561 / 660 skipped
Shard 562 / 660 skipped
Shard 563 / 660 skipped
Shard 564 / 660 skipped
Shard 565 / 660 skipped
Shard 566 / 660 skipped
Shard 567 / 660 skipped
Shard 568 / 660 skipped
Shard 569 / 660 skipped
Shard 570 / 660 skipped
Shard 571 / 660 skipped
Shard 572 / 660 skipped
Shard 573 / 660 skipped
Shard 574 / 660 skipped
Shard 575 / 660 skipped
Shard 576 / 660 skipped
Shard 577 / 660 skipped
Shard 578 / 660 skipped
Shard 579 / 660 skipped
Shard 580 / 660 skipped
Shard 581 / 660 skipped
Shard 582 / 660 skipped
Shard 583 / 660 skipped
Shard 584 / 660 skipped
Shard 585 / 660 skipped
Shard 586 / 660 skipped
Shard 587 / 660 skipped
Shard 588 / 660 skipped
Shard 589 / 660 skipped
Shard 590 / 660 skipped
Shard 591 / 660 skipped
Shard 592 / 660 skipped
Shard 593 / 660 skipped
Shard 594 / 660 skipped
Shard 595 / 660 skipped
Shard 596 / 660 skipped
Shard 597 / 660 skipped
Shard 598 / 660 skipped
Shard 599 / 660 skipped
Shard 600 / 660 skipped
Shard 601 / 660 skipped
Shard 602 / 660 skipped
Shard 603 / 660 skipped
Shard 604 / 660 skipped
Shard 605 / 660 skipped
Shard 606 / 660 skipped
Shard 607 / 660 skipped
Shard 608 / 660 skipped
Shard 609 / 660 skipped
Shard 610 / 660 skipped
Shard 611 / 660 skipped
Shard 612 / 660 skipped
Shard 613 / 660 skipped
Shard 614 / 660 skipped
Shard 615 / 660 skipped
Shard 616 / 660 skipped
Shard 617 / 660 skipped
Shard 618 / 660 skipped
Shard 619 / 660 skipped
Shard 620 / 660 skipped
Shard 621 / 660 skipped
Shard 622 / 660 skipped
Shard 623 / 660 skipped
Shard 624 / 660 skipped
Shard 625 / 660 skipped
Shard 626 / 660 skipped
Shard 627 / 660 skipped
Shard 628 / 660 skipped
Shard 629 / 660 skipped
Shard 630 / 660 skipped
Shard 631 / 660 skipped
Shard 632 / 660 skipped
Shard 633 / 660 skipped
Shard 634 / 660 skipped
Shard 635 / 660 skipped
Shard 636 / 660 skipped
Shard 637 / 660 skipped
Shard 638 / 660 skipped
Shard 639 / 660 skipped
Shard 640 / 660 skipped
Shard 641 / 660 skipped
Shard 642 / 660 skipped
Shard 643 / 660 skipped
Shard 644 / 660 skipped
Shard 645 / 660 skipped
Shard 646 / 660 skipped
Shard 647 / 660 skipped
Shard 648 / 660 skipped
Shard 649 / 660 skipped
Shard 650 / 660 skipped
Shard 651 / 660 skipped
Shard 652 / 660 skipped
Shard 653 / 660 skipped
Shard 654 / 660 skipped
Shard 655 / 660 skipped
Shard 656 / 660 skipped
Shard 657 / 660 skipped
Shard 658 / 660 skipped
Shard 659 / 660 skipped
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_619
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:50:36,217] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_03-model_states.pt.
[2024-06-29 23:50:37,079] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_03-model_states.pt...
[2024-06-29 23:50:45,446] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_03-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:51:00,964] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_04-model_states.pt.
[2024-06-29 23:51:01,361] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_04-model_states.pt...
[2024-06-29 23:51:09,072] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step652/layer_04-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:52:58,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=653, skipped=0, lr=[1.963049372447637e-05], mom=[(0.9, 0.999)]
steps: 653 loss: 0.5591 iter time (s): 168.169 samples/sec: 0.761

100%|██████████| 1/1 [03:10<00:00, 190.24s/it][A100%|██████████| 1/1 [03:10<00:00, 190.24s/it]
 13%|█▎        | 661/5198 [03:10<21:45,  3.47it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [03:22<00:00, 202.42s/it][A100%|██████████| 1/1 [03:22<00:00, 202.42s/it]
 13%|█▎        | 661/5198 [03:22<23:09,  3.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [03:08<00:00, 188.97s/it][A100%|██████████| 1/1 [03:08<00:00, 188.97s/it]
 13%|█▎        | 661/5198 [03:08<21:37,  3.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [03:28<00:00, 208.63s/it][A100%|██████████| 1/1 [03:28<00:00, 208.63s/it]
 13%|█▎        | 661/5198 [03:28<23:52,  3.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.70s/it][A100%|██████████| 1/1 [01:52<00:00, 112.70s/it]
 13%|█▎        | 661/5198 [01:52<12:53,  5.86it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.01s/it][A100%|██████████| 1/1 [02:16<00:00, 136.01s/it]
 13%|█▎        | 661/5198 [02:16<15:33,  4.86it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:49<00:00, 169.04s/it][A100%|██████████| 1/1 [02:49<00:00, 169.04s/it]
 13%|█▎        | 661/5198 [02:49<19:20,  3.91it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_620
Training on 128 of 128 sentences.

100%|██████████| 1/1 [02:47<00:00, 167.10s/it][A100%|██████████| 1/1 [02:47<00:00, 167.10s/it]
 13%|█▎        | 661/5198 [02:47<19:06,  3.96it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.50s/it][A100%|██████████| 1/1 [01:52<00:00, 112.50s/it]
 13%|█▎        | 662/5198 [05:02<40:03,  1.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:54:58,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=654, skipped=0, lr=[1.9628966763565235e-05], mom=[(0.9, 0.999)]
steps: 654 loss: 0.5705 iter time (s): 114.550 samples/sec: 1.117

100%|██████████| 1/1 [01:55<00:00, 115.44s/it][A100%|██████████| 1/1 [01:55<00:00, 115.44s/it]
 13%|█▎        | 662/5198 [05:17<41:55,  1.80it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.67s/it][A100%|██████████| 1/1 [01:55<00:00, 115.67s/it]
 13%|█▎        | 662/5198 [05:04<40:25,  1.87it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.44s/it][A100%|██████████| 1/1 [01:55<00:00, 115.44s/it]
 13%|█▎        | 662/5198 [05:24<42:38,  1.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.57s/it][A100%|██████████| 1/1 [01:55<00:00, 115.57s/it]
 13%|█▎        | 662/5198 [03:48<31:42,  2.38it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.51s/it][A100%|██████████| 1/1 [01:55<00:00, 115.51s/it]
 13%|█▎        | 662/5198 [04:11<34:21,  2.20it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.50s/it][A100%|██████████| 1/1 [01:55<00:00, 115.50s/it]
 13%|█▎        | 662/5198 [04:42<37:54,  1.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.52s/it][A100%|██████████| 1/1 [01:55<00:00, 115.52s/it]
 13%|█▎        | 662/5198 [04:44<38:07,  1.98it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_621
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.21s/it][A100%|██████████| 1/1 [01:19<00:00, 79.21s/it]
 13%|█▎        | 663/5198 [06:21<58:18,  1.30it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:56:16,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=655, skipped=0, lr=[1.962743671376337e-05], mom=[(0.9, 0.999)]
steps: 655 loss: 0.5634 iter time (s): 77.254 samples/sec: 1.657

100%|██████████| 1/1 [01:18<00:00, 78.08s/it][A100%|██████████| 1/1 [01:18<00:00, 78.08s/it]
 13%|█▎        | 663/5198 [06:35<59:55,  1.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.01s/it][A100%|██████████| 1/1 [01:18<00:00, 78.01s/it]
 13%|█▎        | 663/5198 [06:22<58:24,  1.29it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.17s/it][A100%|██████████| 1/1 [01:18<00:00, 78.17s/it]
 13%|█▎        | 663/5198 [06:42<1:00:38,  1.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.08s/it][A100%|██████████| 1/1 [01:18<00:00, 78.08s/it]
 13%|█▎        | 663/5198 [05:06<49:43,  1.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.11s/it][A100%|██████████| 1/1 [01:18<00:00, 78.11s/it]
 13%|█▎        | 663/5198 [05:29<52:22,  1.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.09s/it][A100%|██████████| 1/1 [01:18<00:00, 78.09s/it]
 13%|█▎        | 663/5198 [06:00<55:54,  1.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.08s/it][A100%|██████████| 1/1 [01:18<00:00, 78.08s/it]
 13%|█▎        | 663/5198 [06:02<56:07,  1.35it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_622
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.54s/it][A100%|██████████| 1/1 [01:48<00:00, 108.54s/it]
 13%|█▎        | 664/5198 [08:10<1:33:52,  1.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:58:06,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=656, skipped=0, lr=[1.9625903575561652e-05], mom=[(0.9, 0.999)]
steps: 656 loss: 0.5517 iter time (s): 108.557 samples/sec: 1.179

100%|██████████| 1/1 [01:49<00:00, 109.49s/it][A100%|██████████| 1/1 [01:49<00:00, 109.49s/it]
 13%|█▎        | 664/5198 [08:25<1:35:47,  1.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.42s/it][A100%|██████████| 1/1 [01:49<00:00, 109.42s/it]
 13%|█▎        | 664/5198 [08:12<1:34:15,  1.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.35s/it][A100%|██████████| 1/1 [01:49<00:00, 109.35s/it]
 13%|█▎        | 664/5198 [08:31<1:36:28,  1.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.35s/it][A100%|██████████| 1/1 [01:49<00:00, 109.35s/it]
 13%|█▎        | 664/5198 [06:55<1:25:36,  1.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.44s/it][A100%|██████████| 1/1 [01:49<00:00, 109.44s/it]
 13%|█▎        | 664/5198 [07:19<1:28:16,  1.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.45s/it][A100%|██████████| 1/1 [01:49<00:00, 109.45s/it]
 13%|█▎        | 664/5198 [07:50<1:31:47,  1.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.46s/it][A100%|██████████| 1/1 [01:49<00:00, 109.46s/it]
 13%|█▎        | 664/5198 [07:52<1:32:00,  1.22s/it]Shard 664 in [76, 158, 182, 242, 293, 363, 418, 421, 664, 752, 814, 842, 991, 1266, 1366, 1425, 1464, 1574, 1728, 2166, 2441, 2563, 2739, 2854, 2894, 3089, 3181, 3395, 3576, 3831, 4300, 4589, 4947, 4950]: file /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_623 skipped to avoid exceeding cuda memory
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_624
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.21s/it][A100%|██████████| 1/1 [01:24<00:00, 84.21s/it]
 13%|█▎        | 666/5198 [09:34<2:11:56,  1.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-29 23:59:29,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=657, skipped=0, lr=[1.9624367349451948e-05], mom=[(0.9, 0.999)]
steps: 657 loss: 0.5762 iter time (s): 82.687 samples/sec: 1.548

100%|██████████| 1/1 [01:23<00:00, 83.57s/it][A100%|██████████| 1/1 [01:23<00:00, 83.57s/it]
 13%|█▎        | 666/5198 [09:49<2:13:28,  1.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.60s/it][A100%|██████████| 1/1 [01:23<00:00, 83.60s/it]
 13%|█▎        | 666/5198 [09:35<2:11:59,  1.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.64s/it][A100%|██████████| 1/1 [01:23<00:00, 83.64s/it]
 13%|█▎        | 666/5198 [09:55<2:14:10,  1.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.63s/it][A100%|██████████| 1/1 [01:23<00:00, 83.64s/it]
 13%|█▎        | 666/5198 [08:19<2:03:27,  1.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.53s/it][A100%|██████████| 1/1 [01:23<00:00, 83.53s/it]
 13%|█▎        | 666/5198 [08:42<2:06:02,  1.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.55s/it][A100%|██████████| 1/1 [01:23<00:00, 83.55s/it]
 13%|█▎        | 666/5198 [09:13<2:09:30,  1.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.56s/it][A100%|██████████| 1/1 [01:23<00:00, 83.56s/it]
 13%|█▎        | 666/5198 [09:15<2:09:44,  1.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_625
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.73s/it][A100%|██████████| 1/1 [01:49<00:00, 109.73s/it]
 13%|█▎        | 667/5198 [11:24<3:23:11,  2.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:01:20,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=658, skipped=0, lr=[1.962282803592712e-05], mom=[(0.9, 0.999)]
steps: 658 loss: 0.5626 iter time (s): 109.759 samples/sec: 1.166

100%|██████████| 1/1 [01:50<00:00, 110.85s/it][A100%|██████████| 1/1 [01:50<00:00, 110.85s/it]
 13%|█▎        | 667/5198 [11:39<3:25:21,  2.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.74s/it][A100%|██████████| 1/1 [01:50<00:00, 110.74s/it]
 13%|█▎        | 667/5198 [11:26<3:23:48,  2.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.67s/it][A100%|██████████| 1/1 [01:50<00:00, 110.67s/it]
 13%|█▎        | 667/5198 [11:45<3:25:56,  2.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.65s/it][A100%|██████████| 1/1 [01:50<00:00, 110.65s/it]
 13%|█▎        | 667/5198 [10:10<3:15:17,  2.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.68s/it][A100%|██████████| 1/1 [01:50<00:00, 110.68s/it]
 13%|█▎        | 667/5198 [10:33<3:17:52,  2.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.66s/it][A100%|██████████| 1/1 [01:50<00:00, 110.66s/it]
 13%|█▎        | 667/5198 [11:04<3:21:18,  2.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.67s/it][A100%|██████████| 1/1 [01:50<00:00, 110.67s/it]
 13%|█▎        | 667/5198 [11:06<3:21:32,  2.67s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_626
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.93s/it][A100%|██████████| 1/1 [01:25<00:00, 85.93s/it]
 13%|█▎        | 668/5198 [12:50<4:40:40,  3.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:02:45,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=659, skipped=0, lr=[1.9621285635481014e-05], mom=[(0.9, 0.999)]
steps: 659 loss: 0.6014 iter time (s): 84.505 samples/sec: 1.515

100%|██████████| 1/1 [01:25<00:00, 85.18s/it][A100%|██████████| 1/1 [01:25<00:00, 85.18s/it]
 13%|█▎        | 668/5198 [13:05<4:42:00,  3.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.29s/it][A100%|██████████| 1/1 [01:25<00:00, 85.29s/it]
 13%|█▎        | 668/5198 [12:51<4:40:34,  3.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.38s/it][A100%|██████████| 1/1 [01:25<00:00, 85.38s/it]
 13%|█▎        | 668/5198 [13:11<4:42:44,  3.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.44s/it][A100%|██████████| 1/1 [01:25<00:00, 85.44s/it]
 13%|█▎        | 668/5198 [11:35<4:32:17,  3.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.42s/it][A100%|██████████| 1/1 [01:25<00:00, 85.42s/it]
 13%|█▎        | 668/5198 [12:29<4:38:12,  3.68s/it]
100%|██████████| 1/1 [01:25<00:00, 85.44s/it][A100%|██████████| 1/1 [01:25<00:00, 85.44s/it]
 13%|█▎        | 668/5198 [11:58<4:34:50,  3.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.41s/it][A100%|██████████| 1/1 [01:25<00:00, 85.41s/it]
 13%|█▎        | 668/5198 [12:31<4:38:26,  3.69s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_627
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.81s/it][A100%|██████████| 1/1 [01:37<00:00, 97.81s/it]
 13%|█▎        | 669/5198 [14:28<6:43:35,  5.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:04:24,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[1.961974014860848e-05], mom=[(0.9, 0.999)]
steps: 660 loss: 0.5522 iter time (s): 97.395 samples/sec: 1.314

100%|██████████| 1/1 [01:38<00:00, 98.41s/it][A100%|██████████| 1/1 [01:38<00:00, 98.41s/it]
 13%|█▎        | 669/5198 [14:43<6:45:30,  5.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.43s/it][A100%|██████████| 1/1 [01:38<00:00, 98.43s/it]
 13%|█▎        | 669/5198 [14:30<6:44:08,  5.35s/it]
100%|██████████| 1/1 [01:38<00:00, 98.26s/it][A100%|██████████| 1/1 [01:38<00:00, 98.26s/it]
 13%|█▎        | 669/5198 [14:49<6:46:03,  5.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.29s/it][A100%|██████████| 1/1 [01:38<00:00, 98.29s/it]
 13%|█▎        | 669/5198 [13:13<6:35:49,  5.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.26s/it][A100%|██████████| 1/1 [01:38<00:00, 98.26s/it]
 13%|█▎        | 669/5198 [13:37<6:38:17,  5.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.31s/it][A100%|██████████| 1/1 [01:38<00:00, 98.31s/it]
 13%|█▎        | 669/5198 [14:08<6:41:39,  5.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.30s/it][A100%|██████████| 1/1 [01:38<00:00, 98.30s/it]
 13%|█▎        | 669/5198 [14:10<6:41:51,  5.32s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_628
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.59s/it][A100%|██████████| 1/1 [01:39<00:00, 99.59s/it]
 13%|█▎        | 670/5198 [16:08<9:35:10,  7.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:06:04,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=661, skipped=0, lr=[1.9618191575805334e-05], mom=[(0.9, 0.999)]
steps: 661 loss: 0.5974 iter time (s): 98.808 samples/sec: 1.295

100%|██████████| 1/1 [01:39<00:00, 99.69s/it][A100%|██████████| 1/1 [01:39<00:00, 99.69s/it]
 13%|█▎        | 670/5198 [16:23<9:37:00,  7.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.60s/it][A100%|██████████| 1/1 [01:39<00:00, 99.61s/it]
 13%|█▎        | 670/5198 [16:09<9:35:30,  7.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.78s/it][A100%|██████████| 1/1 [01:39<00:00, 99.78s/it]
 13%|█▎        | 670/5198 [16:29<9:37:41,  7.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.71s/it][A100%|██████████| 1/1 [01:39<00:00, 99.71s/it]
 13%|█▎        | 670/5198 [14:53<9:27:35,  7.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.73s/it][A100%|██████████| 1/1 [01:39<00:00, 99.73s/it]
 13%|█▎        | 670/5198 [15:16<9:30:02,  7.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.72s/it][A100%|██████████| 1/1 [01:39<00:00, 99.72s/it]
 13%|█▎        | 670/5198 [15:47<9:33:17,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.73s/it][A100%|██████████| 1/1 [01:39<00:00, 99.73s/it]
 13%|█▎        | 670/5198 [15:49<9:33:31,  7.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_629
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.37s/it][A100%|██████████| 1/1 [01:28<00:00, 88.37s/it]
 13%|█▎        | 671/5198 [17:36<12:58:12, 10.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:07:32,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=662, skipped=0, lr=[1.9616639917568404e-05], mom=[(0.9, 0.999)]
steps: 662 loss: 0.5871 iter time (s): 87.229 samples/sec: 1.467

100%|██████████| 1/1 [01:28<00:00, 88.28s/it][A100%|██████████| 1/1 [01:28<00:00, 88.28s/it]
 13%|█▎        | 671/5198 [17:51<12:59:26, 10.33s/it]
100%|██████████| 1/1 [01:28<00:00, 88.22s/it][A100%|██████████| 1/1 [01:28<00:00, 88.22s/it]
 13%|█▎        | 671/5198 [17:38<12:57:49, 10.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.44s/it][A100%|██████████| 1/1 [01:28<00:00, 88.44s/it]
 13%|█▎        | 671/5198 [17:57<13:00:29, 10.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.39s/it][A100%|██████████| 1/1 [01:28<00:00, 88.39s/it]
 13%|█▎        | 671/5198 [16:21<12:50:35, 10.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.38s/it][A100%|██████████| 1/1 [01:28<00:00, 88.38s/it]
 13%|█▎        | 671/5198 [16:45<12:52:56, 10.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.40s/it][A100%|██████████| 1/1 [01:28<00:00, 88.40s/it]
 13%|█▎        | 671/5198 [17:16<12:56:07, 10.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.55s/it][A100%|██████████| 1/1 [01:28<00:00, 88.55s/it]
 13%|█▎        | 671/5198 [17:18<12:56:43, 10.29s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_41
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.85s/it][A100%|██████████| 1/1 [01:53<00:00, 113.85s/it]
 13%|█▎        | 672/5198 [19:30<18:53:02, 15.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:09:26,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=663, skipped=0, lr=[1.9615085174395502e-05], mom=[(0.9, 0.999)]
steps: 663 loss: 0.7167 iter time (s): 113.363 samples/sec: 1.129

100%|██████████| 1/1 [01:54<00:00, 114.75s/it][A100%|██████████| 1/1 [01:54<00:00, 114.75s/it]
 13%|█▎        | 672/5198 [19:46<18:56:56, 15.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.91s/it][A100%|██████████| 1/1 [01:54<00:00, 114.91s/it]
 13%|█▎        | 672/5198 [19:32<18:55:52, 15.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.71s/it][A100%|██████████| 1/1 [01:54<00:00, 114.71s/it]
 13%|█▎        | 672/5198 [19:52<18:57:44, 15.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.69s/it][A100%|██████████| 1/1 [01:54<00:00, 114.69s/it]
 13%|█▎        | 672/5198 [19:10<18:53:29, 15.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.62s/it][A100%|██████████| 1/1 [01:54<00:00, 114.62s/it]
 13%|█▎        | 672/5198 [19:13<18:53:49, 15.03s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_630

100%|██████████| 1/1 [01:54<00:00, 114.93s/it][A100%|██████████| 1/1 [01:54<00:00, 114.93s/it]
 13%|█▎        | 672/5198 [18:16<18:49:02, 14.97s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.86s/it][A100%|██████████| 1/1 [01:54<00:00, 114.86s/it]
 13%|█▎        | 672/5198 [18:40<18:51:03, 14.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.48s/it][A100%|██████████| 1/1 [01:55<00:00, 115.48s/it]
 13%|█▎        | 673/5198 [21:26<26:35:16, 21.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:11:22,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=664, skipped=0, lr=[1.9613527346785424e-05], mom=[(0.9, 0.999)]
steps: 664 loss: 0.5534 iter time (s): 114.549 samples/sec: 1.117

100%|██████████| 1/1 [01:55<00:00, 115.52s/it][A100%|██████████| 1/1 [01:55<00:00, 115.52s/it]
 13%|█▎        | 673/5198 [21:41<26:38:05, 21.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.54s/it][A100%|██████████| 1/1 [01:55<00:00, 115.54s/it]
 13%|█▎        | 673/5198 [21:28<26:37:11, 21.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.45s/it][A100%|██████████| 1/1 [01:55<00:00, 115.45s/it]
 13%|█▎        | 673/5198 [21:48<26:38:29, 21.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.25s/it][A100%|██████████| 1/1 [01:55<00:00, 115.25s/it]
 13%|█▎        | 673/5198 [20:12<26:29:26, 21.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.34s/it][A100%|██████████| 1/1 [01:55<00:00, 115.34s/it]
 13%|█▎        | 673/5198 [20:35<26:31:44, 21.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.51s/it][A100%|██████████| 1/1 [01:55<00:00, 115.51s/it]
 13%|█▎        | 673/5198 [21:06<26:34:50, 21.15s/it]
100%|██████████| 1/1 [01:55<00:00, 115.41s/it][A100%|██████████| 1/1 [01:55<00:00, 115.41s/it]
 13%|█▎        | 673/5198 [21:08<26:34:38, 21.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_631

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.37s/it][A100%|██████████| 1/1 [01:32<00:00, 92.37s/it]
 13%|█▎        | 674/5198 [22:59<33:45:47, 26.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:12:54,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=665, skipped=0, lr=[1.9611966435237965e-05], mom=[(0.9, 0.999)]
steps: 665 loss: 0.5211 iter time (s): 90.975 samples/sec: 1.407

100%|██████████| 1/1 [01:31<00:00, 91.86s/it][A100%|██████████| 1/1 [01:31<00:00, 91.86s/it]
 13%|█▎        | 674/5198 [23:13<33:44:19, 26.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.71s/it][A100%|██████████| 1/1 [01:31<00:00, 91.71s/it]
 13%|█▎        | 674/5198 [23:00<33:42:34, 26.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.83s/it][A100%|██████████| 1/1 [01:31<00:00, 91.83s/it]
 13%|█▎        | 674/5198 [23:19<33:44:30, 26.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.85s/it][A100%|██████████| 1/1 [01:31<00:00, 91.85s/it]
 13%|█▎        | 674/5198 [21:43<33:36:16, 26.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.83s/it][A100%|██████████| 1/1 [01:31<00:00, 91.83s/it]
 13%|█▎        | 674/5198 [22:38<33:41:04, 26.80s/it]
100%|██████████| 1/1 [01:31<00:00, 91.87s/it][A100%|██████████| 1/1 [01:31<00:00, 91.87s/it]
 13%|█▎        | 674/5198 [22:07<33:38:31, 26.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.84s/it][A100%|██████████| 1/1 [01:31<00:00, 91.84s/it]
 13%|█▎        | 674/5198 [22:40<33:40:58, 26.80s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_632
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.52s/it][A100%|██████████| 1/1 [01:43<00:00, 103.52s/it]
 13%|█▎        | 675/5198 [24:42<43:39:34, 34.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:14:38,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=666, skipped=0, lr=[1.9610402440253906e-05], mom=[(0.9, 0.999)]
steps: 666 loss: 0.6217 iter time (s): 103.145 samples/sec: 1.241

100%|██████████| 1/1 [01:44<00:00, 104.02s/it][A100%|██████████| 1/1 [01:44<00:00, 104.02s/it]
 13%|█▎        | 675/5198 [24:57<43:40:51, 34.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.12s/it][A100%|██████████| 1/1 [01:44<00:00, 104.12s/it]
 13%|█▎        | 675/5198 [24:44<43:40:05, 34.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.07s/it][A100%|██████████| 1/1 [01:44<00:00, 104.07s/it]
 13%|█▎        | 675/5198 [25:03<43:41:25, 34.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.10s/it][A100%|██████████| 1/1 [01:44<00:00, 104.10s/it]
 13%|█▎        | 675/5198 [23:28<43:34:16, 34.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.06s/it][A100%|██████████| 1/1 [01:44<00:00, 104.06s/it]
 13%|█▎        | 675/5198 [23:51<43:35:58, 34.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.10s/it][A100%|██████████| 1/1 [01:44<00:00, 104.10s/it]
 13%|█▎        | 675/5198 [24:22<43:38:36, 34.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.10s/it][A100%|██████████| 1/1 [01:44<00:00, 104.10s/it]
 13%|█▎        | 675/5198 [24:24<43:38:26, 34.74s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_633
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.70s/it][A100%|██████████| 1/1 [01:27<00:00, 87.70s/it]
 13%|█▎        | 676/5198 [26:10<52:11:03, 41.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:16:05,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=667, skipped=0, lr=[1.9608835362335003e-05], mom=[(0.9, 0.999)]
steps: 667 loss: 0.5674 iter time (s): 86.494 samples/sec: 1.480

100%|██████████| 1/1 [01:27<00:00, 87.41s/it][A100%|██████████| 1/1 [01:27<00:00, 87.42s/it]
 13%|█▎        | 676/5198 [26:25<52:07:38, 41.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.42s/it][A100%|██████████| 1/1 [01:27<00:00, 87.42s/it]
 13%|█▎        | 676/5198 [26:11<52:07:02, 41.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.39s/it][A100%|██████████| 1/1 [01:27<00:00, 87.39s/it]
 13%|█▎        | 676/5198 [26:31<52:07:52, 41.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.45s/it][A100%|██████████| 1/1 [01:27<00:00, 87.45s/it]
 13%|█▎        | 676/5198 [24:55<52:02:11, 41.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.42s/it][A100%|██████████| 1/1 [01:27<00:00, 87.42s/it]
 13%|█▎        | 676/5198 [25:18<52:03:26, 41.44s/it]
100%|██████████| 1/1 [01:27<00:00, 87.39s/it][A100%|██████████| 1/1 [01:27<00:00, 87.39s/it]
 13%|█▎        | 676/5198 [25:49<52:05:24, 41.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.42s/it][A100%|██████████| 1/1 [01:27<00:00, 87.42s/it]
 13%|█▎        | 676/5198 [25:51<52:05:34, 41.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_634
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.60s/it][A100%|██████████| 1/1 [01:20<00:00, 80.60s/it]
 13%|█▎        | 677/5198 [27:31<59:46:31, 47.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:17:26,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=668, skipped=0, lr=[1.9607265201984024e-05], mom=[(0.9, 0.999)]
steps: 668 loss: 0.5827 iter time (s): 79.616 samples/sec: 1.608

100%|██████████| 1/1 [01:20<00:00, 80.55s/it][A100%|██████████| 1/1 [01:20<00:00, 80.55s/it]
 13%|█▎        | 677/5198 [27:45<59:41:26, 47.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.51s/it][A100%|██████████| 1/1 [01:20<00:00, 80.51s/it]
 13%|█▎        | 677/5198 [27:32<59:40:31, 47.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.45s/it][A100%|██████████| 1/1 [01:20<00:00, 80.45s/it]
 13%|█▎        | 677/5198 [27:51<59:40:38, 47.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.46s/it][A100%|██████████| 1/1 [01:20<00:00, 80.46s/it]
 13%|█▎        | 677/5198 [26:16<59:35:46, 47.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.44s/it][A100%|██████████| 1/1 [01:20<00:00, 80.44s/it]
 13%|█▎        | 677/5198 [26:39<59:36:40, 47.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.48s/it][A100%|██████████| 1/1 [01:20<00:00, 80.48s/it]
 13%|█▎        | 677/5198 [27:10<59:38:42, 47.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.47s/it][A100%|██████████| 1/1 [01:20<00:00, 80.47s/it]
 13%|█▎        | 677/5198 [27:12<59:38:42, 47.49s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_635
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.57s/it][A100%|██████████| 1/1 [01:30<00:00, 90.57s/it]
 13%|█▎        | 678/5198 [29:02<69:33:22, 55.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:18:57,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=669, skipped=0, lr=[1.9605691959704714e-05], mom=[(0.9, 0.999)]
steps: 669 loss: 0.5675 iter time (s): 90.144 samples/sec: 1.420

100%|██████████| 1/1 [01:30<00:00, 90.99s/it][A100%|██████████| 1/1 [01:30<00:00, 90.99s/it]
 13%|█▎        | 678/5198 [29:16<69:32:30, 55.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.95s/it][A100%|██████████| 1/1 [01:30<00:00, 90.96s/it]
 13%|█▎        | 678/5198 [29:03<69:31:17, 55.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.00s/it][A100%|██████████| 1/1 [01:31<00:00, 91.00s/it]
 13%|█▎        | 678/5198 [29:22<69:31:59, 55.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 13%|█▎        | 678/5198 [27:46<69:27:37, 55.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.02s/it][A100%|██████████| 1/1 [01:31<00:00, 91.02s/it]
 13%|█▎        | 678/5198 [28:10<69:29:00, 55.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.02s/it][A100%|██████████| 1/1 [01:31<00:00, 91.02s/it]
 13%|█▎        | 678/5198 [28:41<69:30:37, 55.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.02s/it][A100%|██████████| 1/1 [01:31<00:00, 91.02s/it]
 13%|█▎        | 678/5198 [28:43<69:30:35, 55.36s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_636
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.55s/it][A100%|██████████| 1/1 [01:49<00:00, 109.55s/it]
 13%|█▎        | 679/5198 [30:52<83:32:59, 66.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:20:47,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[1.960411563600181e-05], mom=[(0.9, 0.999)]
steps: 670 loss: 0.5410 iter time (s): 109.381 samples/sec: 1.170

100%|██████████| 1/1 [01:50<00:00, 110.22s/it][A100%|██████████| 1/1 [01:50<00:00, 110.22s/it]
 13%|█▎        | 679/5198 [31:06<83:39:16, 66.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.28s/it][A100%|██████████| 1/1 [01:50<00:00, 110.28s/it]
 13%|█▎        | 679/5198 [30:53<83:39:10, 66.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.31s/it][A100%|██████████| 1/1 [01:50<00:00, 110.31s/it]
 13%|█▎        | 679/5198 [31:13<83:40:14, 66.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.35s/it][A100%|██████████| 1/1 [01:50<00:00, 110.35s/it]
 13%|█▎        | 679/5198 [29:37<83:37:16, 66.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.32s/it][A100%|██████████| 1/1 [01:50<00:00, 110.32s/it]
 13%|█▎        | 679/5198 [30:00<83:37:57, 66.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.32s/it][A100%|██████████| 1/1 [01:50<00:00, 110.32s/it]
 13%|█▎        | 679/5198 [30:31<83:39:12, 66.64s/it]
100%|██████████| 1/1 [01:50<00:00, 110.30s/it][A100%|██████████| 1/1 [01:50<00:00, 110.30s/it]
 13%|█▎        | 679/5198 [30:33<83:38:51, 66.64s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_637

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.35s/it][A100%|██████████| 1/1 [01:28<00:00, 88.35s/it]
[2024-06-30 00:22:15,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=671, skipped=0, lr=[1.9602536231381025e-05], mom=[(0.9, 0.999)]
steps: 671 loss: 0.6292 iter time (s): 87.037 samples/sec: 1.471

100%|██████████| 1/1 [01:27<00:00, 87.99s/it][A100%|██████████| 1/1 [01:27<00:00, 87.99s/it]

100%|██████████| 1/1 [01:28<00:00, 88.02s/it][A100%|██████████| 1/1 [01:28<00:00, 88.02s/it]

100%|██████████| 1/1 [01:27<00:00, 87.88s/it][A100%|██████████| 1/1 [01:27<00:00, 87.88s/it]

100%|██████████| 1/1 [01:27<00:00, 87.95s/it][A100%|██████████| 1/1 [01:27<00:00, 87.95s/it]

100%|██████████| 1/1 [01:27<00:00, 87.92s/it][A100%|██████████| 1/1 [01:27<00:00, 87.92s/it]

100%|██████████| 1/1 [01:27<00:00, 87.95s/it][A100%|██████████| 1/1 [01:27<00:00, 87.95s/it]

100%|██████████| 1/1 [01:27<00:00, 87.98s/it][A100%|██████████| 1/1 [01:27<00:00, 87.98s/it]
Checkpointing at shard 679
[2024-06-30 00:22:21,464] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step671 is about to be saved!
[2024-06-30 00:22:23,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_00-model_states.pt...
[2024-06-30 00:22:26,139] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_00-model_states.pt.
[2024-06-30 00:22:33,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_02-model_states.pt...
[2024-06-30 00:22:33,099] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_04-model_states.pt...
[2024-06-30 00:22:33,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_07-model_states.pt...
[2024-06-30 00:22:34,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_08-model_states.pt...
[2024-06-30 00:22:34,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_03-model_states.pt...
[2024-06-30 00:22:36,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_05-model_states.pt...
[2024-06-30 00:22:37,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_06-model_states.pt...
[2024-06-30 00:22:38,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_01-model_states.pt...
[2024-06-30 00:26:29,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_05-model_states.pt.
[2024-06-30 00:26:29,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_08-model_states.pt.
[2024-06-30 00:26:29,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_04_model_states.pt...
[2024-06-30 00:26:30,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_04_model_states.pt.
[2024-06-30 00:26:30,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
[2024-06-30 00:26:30,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_02-model_states.pt.
[2024-06-30 00:26:30,835] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_09-model_states.pt...
[2024-06-30 00:26:30,909] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_01_model_states.pt
[2024-06-30 00:26:30,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_01_model_states.pt...
[2024-06-30 00:26:30,915] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_07-model_states.pt.
[2024-06-30 00:26:30,931] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_06-model_states.pt.
[2024-06-30 00:26:31,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_01_model_states.pt.
[2024-06-30 00:26:31,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
[2024-06-30 00:26:31,273] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_03-model_states.pt.
[2024-06-30 00:26:31,378] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_01-model_states.pt.
[2024-06-30 00:26:31,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_04-model_states.pt.
[2024-06-30 00:26:31,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/layer_09-model_states.pt.
[2024-06-30 00:26:31,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_07_model_states.pt...
[2024-06-30 00:26:31,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_07_model_states.pt.
[2024-06-30 00:26:31,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
[2024-06-30 00:26:31,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_05_model_states.pt...
[2024-06-30 00:26:31,802] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_05_model_states.pt.
[2024-06-30 00:26:31,802] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
[2024-06-30 00:26:32,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_06_model_states.pt...
[2024-06-30 00:26:32,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_06_model_states.pt.
[2024-06-30 00:26:32,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
[2024-06-30 00:26:32,675] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_00_model_states.pt
[2024-06-30 00:26:32,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_00_model_states.pt...
[2024-06-30 00:26:32,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_03_model_states.pt...
[2024-06-30 00:26:32,788] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_02_model_states.pt...
[2024-06-30 00:26:32,788] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_03_model_states.pt.
[2024-06-30 00:26:32,789] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
[2024-06-30 00:26:32,844] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_02_model_states.pt.
[2024-06-30 00:26:32,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
[2024-06-30 00:26:32,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step671/mp_rank_00_model_states.pt.
[2024-06-30 00:26:32,975] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step671 is ready now!
Checkpoint saved using --- 256.3019971847534 seconds ---
 13%|█▎        | 680/5198 [36:51<162:46:47, 129.70s/it] 13%|█▎        | 680/5198 [36:17<162:37:24, 129.58s/it] 13%|█▎        | 680/5198 [35:44<162:37:20, 129.58s/it] 13%|█▎        | 680/5198 [35:21<162:37:53, 129.59s/it] 13%|█▎        | 680/5198 [36:39<163:31:12, 130.29s/it] 13%|█▎        | 680/5198 [36:57<162:42:37, 129.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_638
 13%|█▎        | 680/5198 [36:38<162:44:36, 129.68s/it] 13%|█▎        | 680/5198 [36:15<162:37:46, 129.59s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.37s/it][A100%|██████████| 1/1 [01:50<00:00, 110.37s/it]
 13%|█▎        | 681/5198 [38:30<157:26:21, 125.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:28:26,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=672, skipped=0, lr=[1.9600953746349084e-05], mom=[(0.9, 0.999)]
steps: 672 loss: 0.5847 iter time (s): 113.194 samples/sec: 1.131

100%|██████████| 1/1 [01:53<00:00, 113.55s/it][A100%|██████████| 1/1 [01:53<00:00, 113.55s/it]
 13%|█▎        | 681/5198 [38:45<157:51:28, 125.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.65s/it][A100%|██████████| 1/1 [01:53<00:00, 113.65s/it]
 13%|█▎        | 681/5198 [38:32<157:51:39, 125.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.83s/it][A100%|██████████| 1/1 [01:53<00:00, 113.83s/it]
 13%|█▎        | 681/5198 [38:51<157:53:29, 125.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.97s/it][A100%|██████████| 1/1 [01:53<00:00, 113.97s/it]
 13%|█▎        | 681/5198 [37:15<157:52:29, 125.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.02s/it][A100%|██████████| 1/1 [01:54<00:00, 114.02s/it]
 13%|█▎        | 681/5198 [37:39<157:53:04, 125.83s/it]
100%|██████████| 1/1 [01:54<00:00, 114.03s/it][A100%|██████████| 1/1 [01:54<00:00, 114.03s/it]
 13%|█▎        | 681/5198 [38:10<157:53:27, 125.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.08s/it][A100%|██████████| 1/1 [01:54<00:00, 114.08s/it]
 13%|█▎        | 681/5198 [38:12<157:54:05, 125.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_639
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.72s/it][A100%|██████████| 1/1 [01:29<00:00, 89.72s/it]
 13%|█▎        | 682/5198 [40:00<145:50:23, 116.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:29:55,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=673, skipped=0, lr=[1.959936818141368e-05], mom=[(0.9, 0.999)]
steps: 673 loss: 0.5552 iter time (s): 88.137 samples/sec: 1.452

100%|██████████| 1/1 [01:29<00:00, 89.10s/it][A100%|██████████| 1/1 [01:29<00:00, 89.10s/it]
 13%|█▎        | 682/5198 [40:14<145:53:57, 116.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.17s/it][A100%|██████████| 1/1 [01:29<00:00, 89.17s/it]
 13%|█▎        | 682/5198 [40:01<145:55:23, 116.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.06s/it][A100%|██████████| 1/1 [01:29<00:00, 89.06s/it]
 13%|█▎        | 682/5198 [40:20<145:54:49, 116.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.07s/it][A100%|██████████| 1/1 [01:29<00:00, 89.07s/it]
 13%|█▎        | 682/5198 [38:45<145:54:07, 116.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.04s/it][A100%|██████████| 1/1 [01:29<00:00, 89.04s/it]
 13%|█▎        | 682/5198 [39:08<145:53:54, 116.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.09s/it][A100%|██████████| 1/1 [01:29<00:00, 89.09s/it]
 13%|█▎        | 682/5198 [39:39<145:55:17, 116.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.07s/it][A100%|██████████| 1/1 [01:29<00:00, 89.07s/it]
 13%|█▎        | 682/5198 [39:41<145:55:22, 116.32s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_640
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.67s/it][A100%|██████████| 1/1 [01:26<00:00, 86.67s/it]
 13%|█▎        | 683/5198 [41:27<135:49:53, 108.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:31:22,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=674, skipped=0, lr=[1.9597779537083507e-05], mom=[(0.9, 0.999)]
steps: 674 loss: 0.5985 iter time (s): 85.770 samples/sec: 1.492

100%|██████████| 1/1 [01:26<00:00, 86.73s/it][A100%|██████████| 1/1 [01:26<00:00, 86.73s/it]
 13%|█▎        | 683/5198 [41:41<135:51:15, 108.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.64s/it][A100%|██████████| 1/1 [01:26<00:00, 86.64s/it]
 13%|█▎        | 683/5198 [41:27<135:50:28, 108.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.71s/it][A100%|██████████| 1/1 [01:26<00:00, 86.71s/it]
 13%|█▎        | 683/5198 [41:47<135:51:35, 108.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.64s/it][A100%|██████████| 1/1 [01:26<00:00, 86.64s/it]
 13%|█▎        | 683/5198 [40:11<135:49:32, 108.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.71s/it][A100%|██████████| 1/1 [01:26<00:00, 86.71s/it]
 13%|█▎        | 683/5198 [41:06<135:51:40, 108.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.77s/it][A100%|██████████| 1/1 [01:26<00:00, 86.78s/it]
 13%|█▎        | 683/5198 [40:35<135:52:03, 108.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.71s/it][A100%|██████████| 1/1 [01:26<00:00, 86.71s/it]
 13%|█▎        | 683/5198 [41:08<135:51:44, 108.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_641
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:38<00:00, 158.11s/it][A100%|██████████| 1/1 [02:38<00:00, 158.11s/it]
 13%|█▎        | 684/5198 [44:05<153:17:39, 122.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:34:01,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=675, skipped=0, lr=[1.9596187813868238e-05], mom=[(0.9, 0.999)]
steps: 675 loss: 0.5830 iter time (s): 158.372 samples/sec: 0.808

100%|██████████| 1/1 [02:39<00:00, 159.23s/it][A100%|██████████| 1/1 [02:39<00:00, 159.23s/it]
 13%|█▎        | 684/5198 [44:20<153:36:00, 122.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:39<00:00, 159.29s/it][A100%|██████████| 1/1 [02:39<00:00, 159.29s/it]
 13%|█▎        | 684/5198 [44:07<153:36:36, 122.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:39<00:00, 159.27s/it][A100%|██████████| 1/1 [02:39<00:00, 159.28s/it]
 13%|█▎        | 684/5198 [44:26<153:37:02, 122.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:39<00:00, 159.27s/it][A100%|██████████| 1/1 [02:39<00:00, 159.27s/it]
 13%|█▎        | 684/5198 [42:50<153:35:26, 122.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:39<00:00, 159.18s/it][A100%|██████████| 1/1 [02:39<00:00, 159.19s/it]
 13%|█▎        | 684/5198 [43:14<153:35:32, 122.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:39<00:00, 159.24s/it][A100%|██████████| 1/1 [02:39<00:00, 159.24s/it]
 13%|█▎        | 684/5198 [43:45<153:36:25, 122.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:39<00:00, 159.26s/it][A100%|██████████| 1/1 [02:39<00:00, 159.26s/it]
 13%|█▎        | 684/5198 [43:47<153:36:49, 122.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_642
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.17s/it][A100%|██████████| 1/1 [01:59<00:00, 119.17s/it]
 13%|█▎        | 685/5198 [46:04<152:15:11, 121.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:36:00,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=676, skipped=0, lr=[1.9594593012278537e-05], mom=[(0.9, 0.999)]
steps: 676 loss: 0.6023 iter time (s): 118.434 samples/sec: 1.081

100%|██████████| 1/1 [01:59<00:00, 119.27s/it][A100%|██████████| 1/1 [01:59<00:00, 119.27s/it]
 13%|█▎        | 685/5198 [46:19<152:25:14, 121.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.24s/it][A100%|██████████| 1/1 [01:59<00:00, 119.24s/it]
 13%|█▎        | 685/5198 [46:06<152:24:44, 121.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.30s/it][A100%|██████████| 1/1 [01:59<00:00, 119.30s/it]
 13%|█▎        | 685/5198 [46:26<152:26:25, 121.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.32s/it][A100%|██████████| 1/1 [01:59<00:00, 119.32s/it]
 13%|█▎        | 685/5198 [44:50<152:25:47, 121.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.36s/it][A100%|██████████| 1/1 [01:59<00:00, 119.36s/it]
 13%|█▎        | 685/5198 [45:13<152:26:36, 121.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.33s/it][A100%|██████████| 1/1 [01:59<00:00, 119.33s/it]
 13%|█▎        | 685/5198 [45:44<152:26:35, 121.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.34s/it][A100%|██████████| 1/1 [01:59<00:00, 119.34s/it]
 13%|█▎        | 685/5198 [45:46<152:27:11, 121.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_643
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.39s/it][A100%|██████████| 1/1 [02:18<00:00, 138.39s/it]
 13%|█▎        | 686/5198 [48:23<158:33:24, 126.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:38:20,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=677, skipped=0, lr=[1.959299513282606e-05], mom=[(0.9, 0.999)]
steps: 677 loss: 0.5570 iter time (s): 138.617 samples/sec: 0.923

100%|██████████| 1/1 [02:19<00:00, 139.59s/it][A100%|██████████| 1/1 [02:19<00:00, 139.59s/it]
 13%|█▎        | 686/5198 [48:39<158:54:36, 126.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.63s/it][A100%|██████████| 1/1 [02:19<00:00, 139.64s/it]
 13%|█▎        | 686/5198 [48:26<158:55:21, 126.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.57s/it][A100%|██████████| 1/1 [02:19<00:00, 139.57s/it]
 13%|█▎        | 686/5198 [48:45<158:55:08, 126.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.56s/it][A100%|██████████| 1/1 [02:19<00:00, 139.56s/it]
 13%|█▎        | 686/5198 [47:09<158:54:37, 126.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.58s/it][A100%|██████████| 1/1 [02:19<00:00, 139.58s/it]
 13%|█▎        | 686/5198 [47:33<158:55:25, 126.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.58s/it][A100%|██████████| 1/1 [02:19<00:00, 139.58s/it]
 13%|█▎        | 686/5198 [48:04<158:55:32, 126.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.54s/it][A100%|██████████| 1/1 [02:19<00:00, 139.54s/it]
 13%|█▎        | 686/5198 [48:06<158:54:53, 126.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_644
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.50s/it][A100%|██████████| 1/1 [02:01<00:00, 121.50s/it]
 13%|█▎        | 687/5198 [50:25<156:43:55, 125.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:40:21,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=678, skipped=0, lr=[1.959139417602344e-05], mom=[(0.9, 0.999)]
steps: 678 loss: 0.6035 iter time (s): 120.330 samples/sec: 1.064

100%|██████████| 1/1 [02:01<00:00, 121.21s/it][A100%|██████████| 1/1 [02:01<00:00, 121.21s/it]
 13%|█▎        | 687/5198 [50:40<156:50:07, 125.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.29s/it][A100%|██████████| 1/1 [02:01<00:00, 121.29s/it]
 13%|█▎        | 687/5198 [50:27<156:52:21, 125.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.27s/it][A100%|██████████| 1/1 [02:01<00:00, 121.27s/it]
 13%|█▎        | 687/5198 [50:47<156:51:45, 125.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.24s/it][A100%|██████████| 1/1 [02:01<00:00, 121.24s/it]
 13%|█▎        | 687/5198 [49:11<156:50:43, 125.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.23s/it][A100%|██████████| 1/1 [02:01<00:00, 121.23s/it]
 13%|█▎        | 687/5198 [49:34<156:51:10, 125.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.25s/it][A100%|██████████| 1/1 [02:01<00:00, 121.25s/it]
 13%|█▎        | 687/5198 [50:05<156:51:37, 125.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.25s/it][A100%|██████████| 1/1 [02:01<00:00, 121.25s/it]
 13%|█▎        | 687/5198 [50:07<156:51:09, 125.18s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_42
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.76s/it][A100%|██████████| 1/1 [01:59<00:00, 119.76s/it]
 13%|█▎        | 688/5198 [52:25<154:46:57, 123.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:42:21,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=679, skipped=0, lr=[1.9589790142384307e-05], mom=[(0.9, 0.999)]
steps: 679 loss: 0.8024 iter time (s): 118.905 samples/sec: 1.076

100%|██████████| 1/1 [01:59<00:00, 119.92s/it][A100%|██████████| 1/1 [01:59<00:00, 119.92s/it]
 13%|█▎        | 688/5198 [52:40<154:52:06, 123.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.82s/it][A100%|██████████| 1/1 [01:59<00:00, 119.82s/it]
 13%|█▎        | 688/5198 [52:27<154:51:33, 123.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.93s/it][A100%|██████████| 1/1 [01:59<00:00, 119.94s/it]
 13%|█▎        | 688/5198 [52:46<154:53:40, 123.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.92s/it][A100%|██████████| 1/1 [01:59<00:00, 119.92s/it]
 13%|█▎        | 688/5198 [51:11<154:52:31, 123.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.89s/it][A100%|██████████| 1/1 [01:59<00:00, 119.89s/it]
 13%|█▎        | 688/5198 [51:34<154:52:21, 123.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.89s/it][A100%|██████████| 1/1 [01:59<00:00, 119.89s/it]
 13%|█▎        | 688/5198 [52:05<154:52:33, 123.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.89s/it][A100%|██████████| 1/1 [01:59<00:00, 119.89s/it]
 13%|█▎        | 688/5198 [52:07<154:52:12, 123.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_645
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.51s/it][A100%|██████████| 1/1 [01:47<00:00, 107.51s/it]
 13%|█▎        | 689/5198 [54:13<148:52:52, 118.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:44:08,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[1.9588183032423273e-05], mom=[(0.9, 0.999)]
steps: 680 loss: 0.5994 iter time (s): 106.420 samples/sec: 1.203

100%|██████████| 1/1 [01:47<00:00, 107.33s/it][A100%|██████████| 1/1 [01:47<00:00, 107.33s/it]
 13%|█▎        | 689/5198 [54:28<148:47:57, 118.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.29s/it][A100%|██████████| 1/1 [01:47<00:00, 107.29s/it]
 13%|█▎        | 689/5198 [54:14<148:46:38, 118.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.26s/it][A100%|██████████| 1/1 [01:47<00:00, 107.26s/it]
 13%|█▎        | 689/5198 [54:34<148:47:22, 118.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.31s/it][A100%|██████████| 1/1 [01:47<00:00, 107.31s/it]
 13%|█▎        | 689/5198 [53:21<148:47:23, 118.79s/it]
100%|██████████| 1/1 [01:47<00:00, 107.39s/it][A100%|██████████| 1/1 [01:47<00:00, 107.40s/it]
 13%|█▎        | 689/5198 [52:58<148:49:29, 118.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.29s/it][A100%|██████████| 1/1 [01:47<00:00, 107.30s/it]
 13%|█▎        | 689/5198 [53:52<148:47:13, 118.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.31s/it][A100%|██████████| 1/1 [01:47<00:00, 107.31s/it]
 13%|█▎        | 689/5198 [53:54<148:47:21, 118.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_646
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.73s/it][A100%|██████████| 1/1 [01:43<00:00, 103.73s/it]
 13%|█▎        | 690/5198 [55:57<143:16:24, 114.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:45:52,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=681, skipped=0, lr=[1.9586572846655943e-05], mom=[(0.9, 0.999)]
steps: 681 loss: 0.5811 iter time (s): 102.894 samples/sec: 1.244

100%|██████████| 1/1 [01:43<00:00, 103.78s/it][A100%|██████████| 1/1 [01:43<00:00, 103.79s/it]
 13%|█▎        | 690/5198 [56:11<143:10:58, 114.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.76s/it][A100%|██████████| 1/1 [01:43<00:00, 103.76s/it]
 13%|█▎        | 690/5198 [55:58<143:09:29, 114.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.78s/it][A100%|██████████| 1/1 [01:43<00:00, 103.78s/it]
 13%|█▎        | 690/5198 [56:18<143:10:14, 114.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.66s/it][A100%|██████████| 1/1 [01:43<00:00, 103.66s/it]
 13%|█▎        | 690/5198 [54:42<143:08:54, 114.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.76s/it][A100%|██████████| 1/1 [01:43<00:00, 103.76s/it]
 13%|█▎        | 690/5198 [55:05<143:09:54, 114.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.79s/it][A100%|██████████| 1/1 [01:43<00:00, 103.79s/it]
 13%|█▎        | 690/5198 [55:36<143:10:22, 114.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.79s/it][A100%|██████████| 1/1 [01:43<00:00, 103.79s/it]
 13%|█▎        | 690/5198 [55:38<143:10:26, 114.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_647
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.93s/it][A100%|██████████| 1/1 [01:46<00:00, 106.93s/it]
 13%|█▎        | 691/5198 [57:44<140:35:29, 112.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:47:39,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=682, skipped=0, lr=[1.9584959585598902e-05], mom=[(0.9, 0.999)]
steps: 682 loss: 0.5866 iter time (s): 106.124 samples/sec: 1.206

100%|██████████| 1/1 [01:47<00:00, 107.10s/it][A100%|██████████| 1/1 [01:47<00:00, 107.10s/it]
 13%|█▎        | 691/5198 [57:58<140:27:01, 112.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.18s/it][A100%|██████████| 1/1 [01:47<00:00, 107.18s/it]
 13%|█▎        | 691/5198 [57:45<140:27:54, 112.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.06s/it][A100%|██████████| 1/1 [01:47<00:00, 107.06s/it]
 13%|█▎        | 691/5198 [58:05<140:25:46, 112.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.15s/it][A100%|██████████| 1/1 [01:47<00:00, 107.15s/it]
 13%|█▎        | 691/5198 [56:29<140:26:54, 112.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.09s/it][A100%|██████████| 1/1 [01:47<00:00, 107.09s/it]
 13%|█▎        | 691/5198 [56:52<140:26:01, 112.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.09s/it][A100%|██████████| 1/1 [01:47<00:00, 107.09s/it]
 13%|█▎        | 691/5198 [57:23<140:26:24, 112.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.10s/it][A100%|██████████| 1/1 [01:47<00:00, 107.11s/it]
 13%|█▎        | 691/5198 [57:25<140:26:49, 112.18s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_648
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.75s/it][A100%|██████████| 1/1 [01:22<00:00, 82.75s/it]
 13%|█▎        | 692/5198 [59:07<129:34:56, 103.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:49:02,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=683, skipped=0, lr=[1.9583343249769725e-05], mom=[(0.9, 0.999)]
steps: 683 loss: 0.5416 iter time (s): 81.682 samples/sec: 1.567

100%|██████████| 1/1 [01:22<00:00, 82.59s/it][A100%|██████████| 1/1 [01:22<00:00, 82.59s/it]
 13%|█▎        | 692/5198 [59:21<129:21:31, 103.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.51s/it][A100%|██████████| 1/1 [01:22<00:00, 82.51s/it]
 13%|█▎        | 692/5198 [59:08<129:20:23, 103.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.64s/it][A100%|██████████| 1/1 [01:22<00:00, 82.64s/it]
 13%|█▎        | 692/5198 [59:27<129:21:45, 103.35s/it]
100%|██████████| 1/1 [01:22<00:00, 82.52s/it][A100%|██████████| 1/1 [01:22<00:00, 82.52s/it]
 13%|█▎        | 692/5198 [57:51<129:19:54, 103.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.58s/it][A100%|██████████| 1/1 [01:22<00:00, 82.58s/it]
 13%|█▎        | 692/5198 [58:15<129:20:45, 103.34s/it]
100%|██████████| 1/1 [01:22<00:00, 82.55s/it][A100%|██████████| 1/1 [01:22<00:00, 82.55s/it]
 13%|█▎        | 692/5198 [58:46<129:20:08, 103.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.55s/it][A100%|██████████| 1/1 [01:22<00:00, 82.55s/it]
 13%|█▎        | 692/5198 [58:48<129:20:25, 103.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_649
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.99s/it][A100%|██████████| 1/1 [01:52<00:00, 112.99s/it]
 13%|█▎        | 693/5198 [1:01:00<133:09:55, 106.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:50:55,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=684, skipped=0, lr=[1.958172383968697e-05], mom=[(0.9, 0.999)]
steps: 684 loss: 0.5753 iter time (s): 112.435 samples/sec: 1.138

100%|██████████| 1/1 [01:53<00:00, 113.24s/it][A100%|██████████| 1/1 [01:53<00:00, 113.24s/it]
 13%|█▎        | 693/5198 [1:01:14<133:02:12, 106.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.32s/it][A100%|██████████| 1/1 [01:53<00:00, 113.32s/it]
 13%|█▎        | 693/5198 [1:01:01<133:03:07, 106.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.23s/it][A100%|██████████| 1/1 [01:53<00:00, 113.23s/it]
 13%|█▎        | 693/5198 [1:01:20<133:02:11, 106.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.32s/it][A100%|██████████| 1/1 [01:53<00:00, 113.32s/it]
 13%|█▎        | 693/5198 [59:45<133:02:46, 106.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.30s/it][A100%|██████████| 1/1 [01:53<00:00, 113.31s/it]
 13%|█▎        | 693/5198 [1:00:08<133:03:02, 106.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.34s/it][A100%|██████████| 1/1 [01:53<00:00, 113.34s/it]
 13%|█▎        | 693/5198 [1:00:39<133:03:14, 106.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.34s/it][A100%|██████████| 1/1 [01:53<00:00, 113.34s/it]
 13%|█▎        | 693/5198 [1:00:41<133:03:26, 106.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_650
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.58s/it][A100%|██████████| 1/1 [01:35<00:00, 95.58s/it]
 13%|█▎        | 694/5198 [1:02:36<129:13:38, 103.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:52:31,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=685, skipped=0, lr=[1.9580101355870188e-05], mom=[(0.9, 0.999)]
steps: 685 loss: 0.6180 iter time (s): 95.184 samples/sec: 1.345

100%|██████████| 1/1 [01:36<00:00, 96.36s/it][A100%|██████████| 1/1 [01:36<00:00, 96.37s/it]
 13%|█▎        | 694/5198 [1:02:51<129:17:25, 103.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.31s/it][A100%|██████████| 1/1 [01:36<00:00, 96.31s/it]
 13%|█▎        | 694/5198 [1:02:37<129:16:27, 103.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.39s/it][A100%|██████████| 1/1 [01:36<00:00, 96.39s/it]
 13%|█▎        | 694/5198 [1:02:57<129:17:36, 103.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.35s/it][A100%|██████████| 1/1 [01:36<00:00, 96.35s/it]
 13%|█▎        | 694/5198 [1:01:21<129:17:09, 103.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.34s/it][A100%|██████████| 1/1 [01:36<00:00, 96.34s/it]
 13%|█▎        | 694/5198 [1:02:17<129:17:22, 103.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_651
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.47s/it][A100%|██████████| 1/1 [01:36<00:00, 96.47s/it]
 13%|█▎        | 694/5198 [1:02:15<129:20:10, 103.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.66s/it][A100%|██████████| 1/1 [01:36<00:00, 96.66s/it]
 13%|█▎        | 694/5198 [1:01:45<129:24:18, 103.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.41s/it][A100%|██████████| 1/1 [01:40<00:00, 100.41s/it]
 13%|█▎        | 695/5198 [1:04:17<128:10:50, 102.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:54:12,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=686, skipped=0, lr=[1.957847579883991e-05], mom=[(0.9, 0.999)]
steps: 686 loss: 0.5854 iter time (s): 99.610 samples/sec: 1.285

100%|██████████| 1/1 [01:40<00:00, 100.44s/it][A100%|██████████| 1/1 [01:40<00:00, 100.44s/it]
 13%|█▎        | 695/5198 [1:04:31<128:11:09, 102.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.56s/it][A100%|██████████| 1/1 [01:40<00:00, 100.56s/it]
 13%|█▎        | 695/5198 [1:04:18<128:12:35, 102.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.44s/it][A100%|██████████| 1/1 [01:40<00:00, 100.44s/it]
 13%|█▎        | 695/5198 [1:04:37<128:10:56, 102.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.45s/it][A100%|██████████| 1/1 [01:40<00:00, 100.45s/it]
 13%|█▎        | 695/5198 [1:03:01<128:10:48, 102.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.13s/it][A100%|██████████| 1/1 [01:40<00:00, 100.13s/it]
 13%|█▎        | 695/5198 [1:03:25<128:08:34, 102.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.44s/it][A100%|██████████| 1/1 [01:40<00:00, 100.44s/it]
 13%|█▎        | 695/5198 [1:03:58<128:10:38, 102.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_652
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.36s/it][A100%|██████████| 1/1 [01:40<00:00, 100.36s/it]
 13%|█▎        | 695/5198 [1:03:56<128:10:48, 102.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.66s/it][A100%|██████████| 1/1 [01:53<00:00, 113.66s/it]
 13%|█▎        | 696/5198 [1:06:10<132:23:46, 105.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:56:06,728] [INFO] [logging.py:96:log_dist] [Rank 0] step=687, skipped=0, lr=[1.9576847169117654e-05], mom=[(0.9, 0.999)]
steps: 687 loss: 0.5971 iter time (s): 113.376 samples/sec: 1.129

100%|██████████| 1/1 [01:54<00:00, 114.25s/it][A100%|██████████| 1/1 [01:54<00:00, 114.25s/it]
 13%|█▎        | 696/5198 [1:06:25<132:34:27, 106.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.19s/it][A100%|██████████| 1/1 [01:54<00:00, 114.19s/it]
 13%|█▎        | 696/5198 [1:06:12<132:34:01, 106.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.20s/it][A100%|██████████| 1/1 [01:54<00:00, 114.20s/it]
 13%|█▎        | 696/5198 [1:06:32<132:33:11, 106.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.21s/it][A100%|██████████| 1/1 [01:54<00:00, 114.21s/it]
 13%|█▎        | 696/5198 [1:04:56<132:33:13, 106.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.21s/it][A100%|██████████| 1/1 [01:54<00:00, 114.21s/it]
 13%|█▎        | 696/5198 [1:05:19<132:31:35, 105.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.20s/it][A100%|██████████| 1/1 [01:54<00:00, 114.20s/it]
 13%|█▎        | 696/5198 [1:05:50<132:32:57, 105.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.22s/it][A100%|██████████| 1/1 [01:54<00:00, 114.22s/it]
 13%|█▎        | 696/5198 [1:05:52<132:33:12, 106.00s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_653
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.00s/it][A100%|██████████| 1/1 [02:02<00:00, 122.00s/it]
 13%|█▎        | 697/5198 [1:08:12<138:28:04, 110.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:58:09,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=688, skipped=0, lr=[1.9575215467225925e-05], mom=[(0.9, 0.999)]
steps: 688 loss: 0.5544 iter time (s): 121.519 samples/sec: 1.053

100%|██████████| 1/1 [02:02<00:00, 122.29s/it][A100%|██████████| 1/1 [02:02<00:00, 122.29s/it]
 13%|█▎        | 697/5198 [1:08:28<138:39:00, 110.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.39s/it][A100%|██████████| 1/1 [02:02<00:00, 122.39s/it]
 13%|█▎        | 697/5198 [1:08:14<138:40:47, 110.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.46s/it][A100%|██████████| 1/1 [02:02<00:00, 122.46s/it]
 13%|█▎        | 697/5198 [1:08:34<138:41:53, 110.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.45s/it][A100%|██████████| 1/1 [02:02<00:00, 122.45s/it]
 13%|█▎        | 697/5198 [1:06:58<138:41:49, 110.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.46s/it][A100%|██████████| 1/1 [02:02<00:00, 122.46s/it]
 13%|█▎        | 697/5198 [1:07:21<138:40:46, 110.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.40s/it][A100%|██████████| 1/1 [02:02<00:00, 122.40s/it]
 13%|█▎        | 697/5198 [1:07:52<138:40:25, 110.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.44s/it][A100%|██████████| 1/1 [02:02<00:00, 122.44s/it]
 13%|█▎        | 697/5198 [1:07:54<138:41:23, 110.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_654
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.02s/it][A100%|██████████| 1/1 [01:42<00:00, 102.02s/it]
 13%|█▎        | 698/5198 [1:09:55<135:12:30, 108.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 00:59:50,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=689, skipped=0, lr=[1.9573580693688217e-05], mom=[(0.9, 0.999)]
steps: 689 loss: 0.6002 iter time (s): 100.601 samples/sec: 1.272

100%|██████████| 1/1 [01:41<00:00, 101.70s/it][A100%|██████████| 1/1 [01:41<00:00, 101.70s/it]
 13%|█▎        | 698/5198 [1:10:09<135:10:34, 108.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.53s/it][A100%|██████████| 1/1 [01:41<00:00, 101.53s/it]
 13%|█▎        | 698/5198 [1:09:56<135:07:53, 108.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.50s/it][A100%|██████████| 1/1 [01:41<00:00, 101.50s/it]
 13%|█▎        | 698/5198 [1:10:16<135:08:10, 108.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.48s/it][A100%|██████████| 1/1 [01:41<00:00, 101.48s/it]
 13%|█▎        | 698/5198 [1:08:40<135:07:30, 108.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.58s/it][A100%|██████████| 1/1 [01:41<00:00, 101.58s/it]
 13%|█▎        | 698/5198 [1:09:03<135:09:04, 108.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.54s/it][A100%|██████████| 1/1 [01:41<00:00, 101.54s/it]
 13%|█▎        | 698/5198 [1:09:36<135:08:43, 108.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_655
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.60s/it][A100%|██████████| 1/1 [01:41<00:00, 101.60s/it]
 13%|█▎        | 698/5198 [1:09:34<135:09:22, 108.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.63s/it][A100%|██████████| 1/1 [01:35<00:00, 95.63s/it]
 13%|█▎        | 699/5198 [1:11:30<130:32:46, 104.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:01:26,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[1.9571942849029e-05], mom=[(0.9, 0.999)]
steps: 690 loss: 0.5893 iter time (s): 94.710 samples/sec: 1.351

100%|██████████| 1/1 [01:35<00:00, 95.54s/it][A100%|██████████| 1/1 [01:35<00:00, 95.54s/it]
 13%|█▎        | 699/5198 [1:11:45<130:25:31, 104.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.70s/it][A100%|██████████| 1/1 [01:35<00:00, 95.70s/it]
 13%|█▎        | 699/5198 [1:11:32<130:27:24, 104.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.71s/it][A100%|██████████| 1/1 [01:35<00:00, 95.71s/it]
 13%|█▎        | 699/5198 [1:11:51<130:27:47, 104.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.74s/it][A100%|██████████| 1/1 [01:35<00:00, 95.74s/it]
 13%|█▎        | 699/5198 [1:10:15<130:28:08, 104.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.60s/it][A100%|██████████| 1/1 [01:35<00:00, 95.60s/it]
 13%|█▎        | 699/5198 [1:11:10<130:26:08, 104.37s/it]
100%|██████████| 1/1 [01:35<00:00, 95.63s/it][A100%|██████████| 1/1 [01:35<00:00, 95.63s/it]
 13%|█▎        | 699/5198 [1:10:39<130:26:44, 104.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.65s/it][A100%|██████████| 1/1 [01:35<00:00, 95.65s/it]
 13%|█▎        | 699/5198 [1:11:12<130:26:39, 104.38s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_656
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.59s/it][A100%|██████████| 1/1 [01:27<00:00, 87.60s/it]
[2024-06-30 01:02:53,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=691, skipped=0, lr=[1.957030193377374e-05], mom=[(0.9, 0.999)]
steps: 691 loss: 0.5781 iter time (s): 86.547 samples/sec: 1.479

100%|██████████| 1/1 [01:27<00:00, 87.43s/it][A100%|██████████| 1/1 [01:27<00:00, 87.44s/it]

100%|██████████| 1/1 [01:27<00:00, 87.46s/it][A100%|██████████| 1/1 [01:27<00:00, 87.46s/it]

100%|██████████| 1/1 [01:27<00:00, 87.36s/it][A100%|██████████| 1/1 [01:27<00:00, 87.36s/it]

100%|██████████| 1/1 [01:27<00:00, 87.40s/it][A100%|██████████| 1/1 [01:27<00:00, 87.40s/it]

100%|██████████| 1/1 [01:27<00:00, 87.43s/it][A100%|██████████| 1/1 [01:27<00:00, 87.43s/it]

100%|██████████| 1/1 [01:27<00:00, 87.44s/it][A100%|██████████| 1/1 [01:27<00:00, 87.44s/it]

100%|██████████| 1/1 [01:27<00:00, 87.44s/it][A100%|██████████| 1/1 [01:27<00:00, 87.44s/it]
Checkpointing at shard 699
[2024-06-30 01:02:54,659] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step691 is about to be saved!
[2024-06-30 01:02:56,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_00-model_states.pt...
[2024-06-30 01:03:00,713] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_00-model_states.pt.
[2024-06-30 01:03:02,241] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_02-model_states.pt...
[2024-06-30 01:03:05,559] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_08-model_states.pt...
[2024-06-30 01:03:06,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_07-model_states.pt...
[2024-06-30 01:03:07,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_05-model_states.pt...
[2024-06-30 01:03:07,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_03-model_states.pt...
[2024-06-30 01:03:07,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_04-model_states.pt...
[2024-06-30 01:03:08,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_06-model_states.pt...
[2024-06-30 01:03:10,051] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_01-model_states.pt...
[2024-06-30 01:06:13,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_02-model_states.pt.
[2024-06-30 01:06:13,310] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_01_model_states.pt
[2024-06-30 01:06:13,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_01_model_states.pt...
[2024-06-30 01:06:13,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_01_model_states.pt.
[2024-06-30 01:06:13,448] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[2024-06-30 01:06:13,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_08-model_states.pt.
[2024-06-30 01:06:14,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_09-model_states.pt...
[2024-06-30 01:06:16,072] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_09-model_states.pt.
[2024-06-30 01:06:16,101] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_07_model_states.pt...
[2024-06-30 01:06:16,202] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_07_model_states.pt.
[2024-06-30 01:06:16,202] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[2024-06-30 01:06:19,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_03-model_states.pt.
[2024-06-30 01:06:20,422] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_02_model_states.pt...
[2024-06-30 01:06:20,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_02_model_states.pt.
[2024-06-30 01:06:20,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[2024-06-30 01:06:20,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_05-model_states.pt.
[2024-06-30 01:06:21,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_04_model_states.pt...
[2024-06-30 01:06:21,668] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_04_model_states.pt.
[2024-06-30 01:06:21,668] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[2024-06-30 01:06:28,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_07-model_states.pt.
[2024-06-30 01:06:28,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_06-model_states.pt.
[2024-06-30 01:06:28,340] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_04-model_states.pt.
[2024-06-30 01:06:28,368] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/layer_01-model_states.pt.
[2024-06-30 01:06:29,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_03_model_states.pt...
[2024-06-30 01:06:29,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_03_model_states.pt.
[2024-06-30 01:06:29,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[2024-06-30 01:06:29,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_06_model_states.pt...
[2024-06-30 01:06:29,405] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_00_model_states.pt
[2024-06-30 01:06:29,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_00_model_states.pt...
[2024-06-30 01:06:29,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_06_model_states.pt.
[2024-06-30 01:06:29,405] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[2024-06-30 01:06:29,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_05_model_states.pt...
[2024-06-30 01:06:29,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_05_model_states.pt.
[2024-06-30 01:06:29,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
[2024-06-30 01:06:29,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step691/mp_rank_00_model_states.pt.
[2024-06-30 01:06:29,688] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step691 is ready now!
Checkpoint saved using --- 215.0326600074768 seconds ---
 13%|█▎        | 700/5198 [1:16:14<204:39:04, 163.79s/it] 13%|█▎        | 700/5198 [1:15:18<204:42:13, 163.84s/it] 13%|█▎        | 700/5198 [1:16:12<204:39:18, 163.80s/it] 13%|█▎        | 700/5198 [1:16:34<204:47:29, 163.91s/it] 13%|█▎        | 700/5198 [1:16:54<204:43:40, 163.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_657 13%|█▎        | 700/5198 [1:15:41<204:39:40, 163.80s/it]
 13%|█▎        | 700/5198 [1:16:36<205:54:08, 164.80s/it] 13%|█▎        | 700/5198 [1:16:48<204:50:13, 163.94s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.35s/it][A100%|██████████| 1/1 [01:37<00:00, 97.35s/it]
 13%|█▎        | 701/5198 [1:18:14<180:38:49, 144.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:08:09,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=692, skipped=0, lr=[1.956865794844888e-05], mom=[(0.9, 0.999)]
steps: 692 loss: 0.5279 iter time (s): 99.718 samples/sec: 1.284

100%|██████████| 1/1 [01:40<00:00, 100.13s/it][A100%|██████████| 1/1 [01:40<00:00, 100.13s/it]
 13%|█▎        | 701/5198 [1:18:28<180:56:33, 144.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.30s/it][A100%|██████████| 1/1 [01:40<00:00, 100.30s/it]
 13%|█▎        | 701/5198 [1:18:15<180:58:24, 144.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.45s/it][A100%|██████████| 1/1 [01:40<00:00, 100.46s/it]
 13%|█▎        | 701/5198 [1:18:35<180:59:11, 144.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.52s/it][A100%|██████████| 1/1 [01:40<00:00, 100.52s/it]
 13%|█▎        | 701/5198 [1:16:59<180:59:36, 144.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.54s/it][A100%|██████████| 1/1 [01:40<00:00, 100.55s/it]
 13%|█▎        | 701/5198 [1:17:22<180:58:26, 144.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.61s/it][A100%|██████████| 1/1 [01:40<00:00, 100.61s/it]
 13%|█▎        | 701/5198 [1:17:53<180:59:41, 144.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.65s/it][A100%|██████████| 1/1 [01:40<00:00, 100.65s/it]
 13%|█▎        | 701/5198 [1:17:55<181:00:19, 144.90s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_658
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.43s/it][A100%|██████████| 1/1 [01:43<00:00, 103.43s/it]
 14%|█▎        | 702/5198 [1:19:57<165:13:36, 132.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:09:53,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=693, skipped=0, lr=[1.9567010893581858e-05], mom=[(0.9, 0.999)]
steps: 693 loss: 0.5849 iter time (s): 102.688 samples/sec: 1.246

100%|██████████| 1/1 [01:43<00:00, 103.62s/it][A100%|██████████| 1/1 [01:43<00:00, 103.62s/it]
 14%|█▎        | 702/5198 [1:20:12<165:27:33, 132.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.55s/it][A100%|██████████| 1/1 [01:43<00:00, 103.55s/it]
 14%|█▎        | 702/5198 [1:19:59<165:27:31, 132.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.65s/it][A100%|██████████| 1/1 [01:43<00:00, 103.65s/it]
 14%|█▎        | 702/5198 [1:20:18<165:30:19, 132.52s/it]
100%|██████████| 1/1 [01:43<00:00, 103.64s/it][A100%|██████████| 1/1 [01:43<00:00, 103.64s/it]
 14%|█▎        | 702/5198 [1:18:42<165:30:18, 132.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.70s/it][A100%|██████████| 1/1 [01:43<00:00, 103.70s/it]
 14%|█▎        | 702/5198 [1:19:06<165:30:44, 132.53s/it]
100%|██████████| 1/1 [01:43<00:00, 103.67s/it][A100%|██████████| 1/1 [01:43<00:00, 103.67s/it]
 14%|█▎        | 702/5198 [1:19:37<165:30:48, 132.53s/it]
100%|██████████| 1/1 [01:43<00:00, 103.68s/it][A100%|██████████| 1/1 [01:43<00:00, 103.68s/it]
 14%|█▎        | 702/5198 [1:19:39<165:31:32, 132.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_659

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s]
[A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.01s/it][A100%|██████████| 1/1 [01:38<00:00, 98.01s/it]
 14%|█▎        | 703/5198 [1:21:35<152:28:09, 122.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:11:31,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=694, skipped=0, lr=[1.956536076970108e-05], mom=[(0.9, 0.999)]
steps: 694 loss: 0.5781 iter time (s): 97.099 samples/sec: 1.318

100%|██████████| 1/1 [01:38<00:00, 98.17s/it][A100%|██████████| 1/1 [01:38<00:00, 98.18s/it]
 14%|█▎        | 703/5198 [1:21:50<152:35:06, 122.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.13s/it][A100%|██████████| 1/1 [01:38<00:00, 98.13s/it]
 14%|█▎        | 703/5198 [1:21:37<152:34:43, 122.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.36s/it][A100%|██████████| 1/1 [01:38<00:00, 98.36s/it]
 14%|█▎        | 703/5198 [1:20:21<152:40:39, 122.28s/it]
100%|██████████| 1/1 [01:38<00:00, 98.41s/it][A100%|██████████| 1/1 [01:38<00:00, 98.41s/it]
 14%|█▎        | 703/5198 [1:21:57<152:42:47, 122.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.29s/it][A100%|██████████| 1/1 [01:38<00:00, 98.29s/it]
 14%|█▎        | 703/5198 [1:21:15<152:41:14, 122.29s/it]
100%|██████████| 1/1 [01:38<00:00, 98.33s/it][A100%|██████████| 1/1 [01:38<00:00, 98.33s/it]
 14%|█▎        | 703/5198 [1:21:17<152:41:32, 122.29s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_43
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.67s/it][A100%|██████████| 1/1 [01:38<00:00, 98.67s/it]
 14%|█▎        | 703/5198 [1:20:44<152:49:25, 122.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.97s/it][A100%|██████████| 1/1 [01:59<00:00, 119.97s/it]
 14%|█▎        | 704/5198 [1:23:36<151:44:42, 121.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:13:31,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=695, skipped=0, lr=[1.9563707577335952e-05], mom=[(0.9, 0.999)]
steps: 695 loss: 0.7790 iter time (s): 119.601 samples/sec: 1.070

100%|██████████| 1/1 [02:01<00:00, 121.15s/it][A100%|██████████| 1/1 [02:01<00:00, 121.15s/it]
 14%|█▎        | 704/5198 [1:23:51<152:09:40, 121.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.22s/it][A100%|██████████| 1/1 [02:01<00:00, 121.22s/it]
 14%|█▎        | 704/5198 [1:23:38<152:12:01, 121.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.04s/it][A100%|██████████| 1/1 [02:01<00:00, 121.04s/it]
 14%|█▎        | 704/5198 [1:22:22<152:11:44, 121.92s/it]
100%|██████████| 1/1 [02:01<00:00, 121.06s/it][A100%|██████████| 1/1 [02:01<00:00, 121.06s/it]
 14%|█▎        | 704/5198 [1:23:58<152:12:52, 121.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.05s/it][A100%|██████████| 1/1 [02:01<00:00, 121.05s/it]
 14%|█▎        | 704/5198 [1:23:16<152:13:14, 121.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.17s/it][A100%|██████████| 1/1 [02:01<00:00, 121.17s/it]
 14%|█▎        | 704/5198 [1:23:18<152:15:17, 121.97s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_660
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.96s/it][A100%|██████████| 1/1 [02:00<00:00, 120.96s/it]
 14%|█▎        | 704/5198 [1:22:45<152:15:18, 121.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.24s/it][A100%|██████████| 1/1 [01:41<00:00, 101.24s/it]
 14%|█▎        | 705/5198 [1:25:17<144:11:38, 115.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:15:13,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=696, skipped=0, lr=[1.9562051317016863e-05], mom=[(0.9, 0.999)]
steps: 696 loss: 0.5609 iter time (s): 99.275 samples/sec: 1.289

100%|██████████| 1/1 [01:40<00:00, 100.50s/it][A100%|██████████| 1/1 [01:40<00:00, 100.50s/it]
 14%|█▎        | 705/5198 [1:25:32<144:07:12, 115.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.46s/it][A100%|██████████| 1/1 [01:40<00:00, 100.46s/it]
 14%|█▎        | 705/5198 [1:25:18<144:08:09, 115.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.17s/it][A100%|██████████| 1/1 [01:40<00:00, 100.17s/it]
 14%|█▎        | 705/5198 [1:25:38<144:03:41, 115.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.32s/it][A100%|██████████| 1/1 [01:40<00:00, 100.33s/it]
 14%|█▎        | 705/5198 [1:24:02<144:06:23, 115.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.13s/it][A100%|██████████| 1/1 [01:40<00:00, 100.13s/it]
 14%|█▎        | 705/5198 [1:24:25<144:03:00, 115.42s/it]
100%|██████████| 1/1 [01:40<00:00, 100.24s/it][A100%|██████████| 1/1 [01:40<00:00, 100.24s/it]
 14%|█▎        | 705/5198 [1:24:56<144:06:27, 115.47s/it]
100%|██████████| 1/1 [01:40<00:00, 100.24s/it][A100%|██████████| 1/1 [01:40<00:00, 100.24s/it]
 14%|█▎        | 705/5198 [1:24:58<144:05:39, 115.46s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_661
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.35s/it][A100%|██████████| 1/1 [01:32<00:00, 92.35s/it]
 14%|█▎        | 706/5198 [1:26:50<135:34:51, 108.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:16:45,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=697, skipped=0, lr=[1.9560391989275175e-05], mom=[(0.9, 0.999)]
steps: 697 loss: 0.5228 iter time (s): 91.434 samples/sec: 1.400

100%|██████████| 1/1 [01:32<00:00, 92.46s/it][A100%|██████████| 1/1 [01:32<00:00, 92.46s/it]
 14%|█▎        | 706/5198 [1:27:04<135:28:55, 108.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.37s/it][A100%|██████████| 1/1 [01:32<00:00, 92.37s/it]
 14%|█▎        | 706/5198 [1:26:51<135:27:13, 108.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.36s/it][A100%|██████████| 1/1 [01:32<00:00, 92.36s/it]
 14%|█▎        | 706/5198 [1:27:10<135:25:29, 108.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 14%|█▎        | 706/5198 [1:25:35<135:26:58, 108.55s/it]
100%|██████████| 1/1 [01:32<00:00, 92.36s/it][A100%|██████████| 1/1 [01:32<00:00, 92.36s/it]
 14%|█▎        | 706/5198 [1:26:29<135:26:10, 108.54s/it]
100%|██████████| 1/1 [01:32<00:00, 92.36s/it][A100%|██████████| 1/1 [01:32<00:00, 92.36s/it]
 14%|█▎        | 706/5198 [1:25:58<135:24:48, 108.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.39s/it][A100%|██████████| 1/1 [01:32<00:00, 92.39s/it]
 14%|█▎        | 706/5198 [1:26:31<135:25:49, 108.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_662
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.31s/it][A100%|██████████| 1/1 [01:25<00:00, 85.31s/it]
 14%|█▎        | 707/5198 [1:28:15<126:53:22, 101.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:18:10,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=698, skipped=0, lr=[1.9558729594643247e-05], mom=[(0.9, 0.999)]
steps: 698 loss: 0.5602 iter time (s): 84.458 samples/sec: 1.516

100%|██████████| 1/1 [01:25<00:00, 85.17s/it][A100%|██████████| 1/1 [01:25<00:00, 85.17s/it]
 14%|█▎        | 707/5198 [1:28:29<126:42:23, 101.57s/it]
100%|██████████| 1/1 [01:25<00:00, 85.27s/it][A100%|██████████| 1/1 [01:25<00:00, 85.27s/it]
 14%|█▎        | 707/5198 [1:28:16<126:42:45, 101.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.26s/it][A100%|██████████| 1/1 [01:25<00:00, 85.26s/it]
 14%|█▎        | 707/5198 [1:28:36<126:41:35, 101.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.25s/it][A100%|██████████| 1/1 [01:25<00:00, 85.25s/it]
 14%|█▎        | 707/5198 [1:27:00<126:44:09, 101.59s/it]
100%|██████████| 1/1 [01:25<00:00, 85.27s/it][A100%|██████████| 1/1 [01:25<00:00, 85.27s/it]
 14%|█▎        | 707/5198 [1:27:23<126:41:14, 101.55s/it]
100%|██████████| 1/1 [01:25<00:00, 85.30s/it][A100%|██████████| 1/1 [01:25<00:00, 85.30s/it]
 14%|█▎        | 707/5198 [1:27:54<126:43:03, 101.58s/it]
100%|██████████| 1/1 [01:25<00:00, 85.30s/it][A100%|██████████| 1/1 [01:25<00:00, 85.30s/it]
 14%|█▎        | 707/5198 [1:27:56<126:42:21, 101.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_663

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.56s/it][A100%|██████████| 1/1 [01:25<00:00, 85.56s/it]
 14%|█▎        | 708/5198 [1:29:41<120:58:03, 96.99s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:19:36,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=699, skipped=0, lr=[1.9557064133654417e-05], mom=[(0.9, 0.999)]
steps: 699 loss: 0.5748 iter time (s): 85.042 samples/sec: 1.505

100%|██████████| 1/1 [01:25<00:00, 85.87s/it][A100%|██████████| 1/1 [01:25<00:00, 85.87s/it]
 14%|█▎        | 708/5198 [1:29:56<120:52:04, 96.91s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.93s/it][A100%|██████████| 1/1 [01:25<00:00, 85.93s/it]
 14%|█▎        | 708/5198 [1:29:42<120:50:44, 96.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.97s/it][A100%|██████████| 1/1 [01:25<00:00, 85.97s/it]
 14%|█▎        | 708/5198 [1:30:02<120:51:31, 96.90s/it] 
100%|██████████| 1/1 [01:25<00:00, 85.82s/it][A100%|██████████| 1/1 [01:25<00:00, 85.82s/it]
 14%|█▎        | 708/5198 [1:28:26<120:50:18, 96.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.92s/it][A100%|██████████| 1/1 [01:25<00:00, 85.92s/it]
 14%|█▎        | 708/5198 [1:28:49<120:49:44, 96.88s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.90s/it][A100%|██████████| 1/1 [01:25<00:00, 85.91s/it]
 14%|█▎        | 708/5198 [1:29:20<120:50:27, 96.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.94s/it][A100%|██████████| 1/1 [01:25<00:00, 85.94s/it]
 14%|█▎        | 708/5198 [1:29:22<120:50:30, 96.89s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_664
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.89s/it][A100%|██████████| 1/1 [01:41<00:00, 101.89s/it]
 14%|█▎        | 709/5198 [1:31:23<122:52:17, 98.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:21:19,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[1.9555395606842998e-05], mom=[(0.9, 0.999)]
steps: 700 loss: 0.5923 iter time (s): 101.554 samples/sec: 1.260

100%|██████████| 1/1 [01:42<00:00, 102.59s/it][A100%|██████████| 1/1 [01:42<00:00, 102.59s/it]
 14%|█▎        | 709/5198 [1:31:38<122:58:30, 98.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.52s/it][A100%|██████████| 1/1 [01:42<00:00, 102.52s/it]
 14%|█▎        | 709/5198 [1:31:25<122:58:01, 98.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.56s/it][A100%|██████████| 1/1 [01:42<00:00, 102.56s/it]
 14%|█▎        | 709/5198 [1:31:44<122:58:56, 98.63s/it]
100%|██████████| 1/1 [01:42<00:00, 102.58s/it][A100%|██████████| 1/1 [01:42<00:00, 102.58s/it]
 14%|█▎        | 709/5198 [1:30:08<122:58:03, 98.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.59s/it][A100%|██████████| 1/1 [01:42<00:00, 102.59s/it]
 14%|█▎        | 709/5198 [1:30:32<122:56:34, 98.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.65s/it][A100%|██████████| 1/1 [01:42<00:00, 102.65s/it]
 14%|█▎        | 709/5198 [1:31:03<122:58:18, 98.62s/it]
100%|██████████| 1/1 [01:42<00:00, 102.50s/it][A100%|██████████| 1/1 [01:42<00:00, 102.50s/it]
 14%|█▎        | 709/5198 [1:31:05<122:57:51, 98.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_665

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.21s/it][A100%|██████████| 1/1 [02:05<00:00, 125.21s/it]
 14%|█▎        | 710/5198 [1:33:29<132:57:01, 106.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:23:25,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=701, skipped=0, lr=[1.9553724014744304e-05], mom=[(0.9, 0.999)]
steps: 701 loss: 0.6275 iter time (s): 125.227 samples/sec: 1.022

100%|██████████| 1/1 [02:06<00:00, 126.32s/it][A100%|██████████| 1/1 [02:06<00:00, 126.32s/it]
 14%|█▎        | 710/5198 [1:33:44<133:18:44, 106.93s/it]
100%|██████████| 1/1 [02:06<00:00, 126.24s/it][A100%|██████████| 1/1 [02:06<00:00, 126.24s/it]
 14%|█▎        | 710/5198 [1:33:31<133:18:22, 106.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.12s/it][A100%|██████████| 1/1 [02:06<00:00, 126.12s/it]
 14%|█▎        | 710/5198 [1:33:51<133:15:46, 106.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.23s/it][A100%|██████████| 1/1 [02:06<00:00, 126.23s/it]
 14%|█▎        | 710/5198 [1:32:15<133:16:53, 106.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.28s/it][A100%|██████████| 1/1 [02:06<00:00, 126.28s/it]
 14%|█▎        | 710/5198 [1:32:38<133:16:21, 106.90s/it]
100%|██████████| 1/1 [02:06<00:00, 126.17s/it][A100%|██████████| 1/1 [02:06<00:00, 126.17s/it]
 14%|█▎        | 710/5198 [1:33:09<133:16:26, 106.90s/it]
100%|██████████| 1/1 [02:06<00:00, 126.20s/it][A100%|██████████| 1/1 [02:06<00:00, 126.20s/it]
 14%|█▎        | 710/5198 [1:33:11<133:17:01, 106.91s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_666

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.41s/it][A100%|██████████| 1/1 [01:28<00:00, 88.41s/it]
 14%|█▎        | 711/5198 [1:34:58<126:14:12, 101.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:24:53,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=702, skipped=0, lr=[1.955204935789462e-05], mom=[(0.9, 0.999)]
steps: 702 loss: 0.5750 iter time (s): 86.613 samples/sec: 1.478

100%|██████████| 1/1 [01:27<00:00, 87.38s/it][A100%|██████████| 1/1 [01:27<00:00, 87.38s/it]
 14%|█▎        | 711/5198 [1:35:12<126:00:17, 101.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.52s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 14%|█▎        | 711/5198 [1:34:59<126:01:34, 101.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.57s/it][A100%|██████████| 1/1 [01:27<00:00, 87.57s/it]
 14%|█▎        | 711/5198 [1:35:18<126:02:02, 101.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.48s/it][A100%|██████████| 1/1 [01:27<00:00, 87.48s/it]
 14%|█▎        | 711/5198 [1:34:06<126:01:15, 101.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.52s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 14%|█▎        | 711/5198 [1:34:37<126:01:32, 101.11s/it]
100%|██████████| 1/1 [01:27<00:00, 87.69s/it][A100%|██████████| 1/1 [01:27<00:00, 87.69s/it]
 14%|█▎        | 711/5198 [1:33:42<126:04:02, 101.15s/it]
100%|██████████| 1/1 [01:27<00:00, 87.51s/it][A100%|██████████| 1/1 [01:27<00:00, 87.51s/it]
 14%|█▎        | 711/5198 [1:34:39<126:01:22, 101.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_667

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.36s/it][A100%|██████████| 1/1 [01:39<00:00, 99.36s/it]
 14%|█▎        | 712/5198 [1:36:37<125:36:33, 100.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:26:33,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=703, skipped=0, lr=[1.9550371636831217e-05], mom=[(0.9, 0.999)]
steps: 703 loss: 0.5976 iter time (s): 99.019 samples/sec: 1.293

100%|██████████| 1/1 [01:40<00:00, 100.02s/it][A100%|██████████| 1/1 [01:40<00:00, 100.02s/it]
 14%|█▎        | 712/5198 [1:36:52<125:35:39, 100.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.98s/it][A100%|██████████| 1/1 [01:39<00:00, 99.98s/it]
 14%|█▎        | 712/5198 [1:36:39<125:36:13, 100.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.05s/it][A100%|██████████| 1/1 [01:40<00:00, 100.05s/it]
 14%|█▎        | 712/5198 [1:36:58<125:37:45, 100.82s/it]
100%|██████████| 1/1 [01:39<00:00, 99.95s/it][A100%|██████████| 1/1 [01:39<00:00, 99.95s/it]
 14%|█▎        | 712/5198 [1:35:22<125:37:14, 100.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.11s/it][A100%|██████████| 1/1 [01:40<00:00, 100.11s/it]
 14%|█▎        | 712/5198 [1:35:46<125:37:15, 100.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.01s/it][A100%|██████████| 1/1 [01:40<00:00, 100.01s/it]
 14%|█▎        | 712/5198 [1:36:17<125:37:08, 100.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.97s/it][A100%|██████████| 1/1 [01:39<00:00, 99.97s/it]
 14%|█▎        | 712/5198 [1:36:19<125:37:09, 100.81s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_668
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.01s/it][A100%|██████████| 1/1 [01:33<00:00, 93.01s/it]
 14%|█▎        | 713/5198 [1:38:11<122:46:24, 98.55s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:28:06,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=704, skipped=0, lr=[1.9548690852092347e-05], mom=[(0.9, 0.999)]
steps: 704 loss: 0.5564 iter time (s): 92.063 samples/sec: 1.390

100%|██████████| 1/1 [01:33<00:00, 93.01s/it][A100%|██████████| 1/1 [01:33<00:00, 93.01s/it]
 14%|█▎        | 713/5198 [1:38:25<122:41:34, 98.48s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.11s/it][A100%|██████████| 1/1 [01:33<00:00, 93.11s/it]
 14%|█▎        | 713/5198 [1:38:12<122:42:22, 98.49s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 93.00s/it][A100%|██████████| 1/1 [01:32<00:00, 93.00s/it]
 14%|█▎        | 713/5198 [1:38:31<122:42:43, 98.50s/it] 
100%|██████████| 1/1 [01:33<00:00, 93.00s/it][A100%|██████████| 1/1 [01:33<00:00, 93.00s/it]
 14%|█▎        | 713/5198 [1:36:55<122:41:26, 98.48s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.06s/it][A100%|██████████| 1/1 [01:33<00:00, 93.06s/it]
 14%|█▎        | 713/5198 [1:37:19<122:41:52, 98.49s/it] 
100%|██████████| 1/1 [01:33<00:00, 93.06s/it][A100%|██████████| 1/1 [01:33<00:00, 93.06s/it]
 14%|█▎        | 713/5198 [1:37:50<122:41:49, 98.49s/it] 
100%|██████████| 1/1 [01:32<00:00, 92.97s/it][A100%|██████████| 1/1 [01:32<00:00, 92.97s/it]
 14%|█▎        | 713/5198 [1:37:52<122:41:22, 98.48s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_669

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.84s/it][A100%|██████████| 1/1 [01:54<00:00, 114.84s/it]
 14%|█▎        | 714/5198 [1:40:06<128:55:36, 103.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:30:01,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=705, skipped=0, lr=[1.9547007004217252e-05], mom=[(0.9, 0.999)]
steps: 705 loss: 0.5534 iter time (s): 114.494 samples/sec: 1.118

100%|██████████| 1/1 [01:55<00:00, 115.53s/it][A100%|██████████| 1/1 [01:55<00:00, 115.53s/it]
 14%|█▎        | 714/5198 [1:40:21<129:03:02, 103.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.56s/it][A100%|██████████| 1/1 [01:55<00:00, 115.56s/it]
 14%|█▎        | 714/5198 [1:40:07<129:04:52, 103.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.52s/it][A100%|██████████| 1/1 [01:55<00:00, 115.52s/it]
 14%|█▎        | 714/5198 [1:40:27<129:04:05, 103.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.57s/it][A100%|██████████| 1/1 [01:55<00:00, 115.57s/it]
 14%|█▎        | 714/5198 [1:38:51<129:03:20, 103.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.48s/it][A100%|██████████| 1/1 [01:55<00:00, 115.48s/it]
 14%|█▎        | 714/5198 [1:39:14<129:02:47, 103.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.54s/it][A100%|██████████| 1/1 [01:55<00:00, 115.54s/it]
 14%|█▎        | 714/5198 [1:39:45<129:04:00, 103.62s/it]
100%|██████████| 1/1 [01:55<00:00, 115.48s/it][A100%|██████████| 1/1 [01:55<00:00, 115.48s/it]
 14%|█▎        | 714/5198 [1:39:47<129:03:35, 103.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_670

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.55s/it][A100%|██████████| 1/1 [01:58<00:00, 118.55s/it]
 14%|█▍        | 715/5198 [1:42:05<134:38:23, 108.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:32:00,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=706, skipped=0, lr=[1.9545320093746153e-05], mom=[(0.9, 0.999)]
steps: 706 loss: 0.5601 iter time (s): 117.612 samples/sec: 1.088

100%|██████████| 1/1 [01:58<00:00, 118.61s/it][A100%|██████████| 1/1 [01:58<00:00, 118.61s/it]
 14%|█▍        | 715/5198 [1:42:19<134:38:00, 108.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.46s/it][A100%|██████████| 1/1 [01:58<00:00, 118.46s/it]
 14%|█▍        | 715/5198 [1:42:06<134:35:55, 108.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.50s/it][A100%|██████████| 1/1 [01:58<00:00, 118.50s/it]
 14%|█▍        | 715/5198 [1:42:26<134:36:53, 108.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.53s/it][A100%|██████████| 1/1 [01:58<00:00, 118.54s/it]
 14%|█▍        | 715/5198 [1:40:50<134:36:23, 108.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.57s/it][A100%|██████████| 1/1 [01:58<00:00, 118.57s/it]
 14%|█▍        | 715/5198 [1:41:13<134:36:53, 108.10s/it]
100%|██████████| 1/1 [01:58<00:00, 118.48s/it][A100%|██████████| 1/1 [01:58<00:00, 118.48s/it]
 14%|█▍        | 715/5198 [1:41:44<134:36:24, 108.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.52s/it][A100%|██████████| 1/1 [01:58<00:00, 118.52s/it]
 14%|█▍        | 715/5198 [1:41:46<134:36:55, 108.10s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_671
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.58s/it][A100%|██████████| 1/1 [01:33<00:00, 93.58s/it]
 14%|█▍        | 716/5198 [1:43:39<129:21:26, 103.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:33:33,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=707, skipped=0, lr=[1.9543630121220246e-05], mom=[(0.9, 0.999)]
steps: 707 loss: 0.5888 iter time (s): 92.400 samples/sec: 1.385

100%|██████████| 1/1 [01:33<00:00, 93.21s/it][A100%|██████████| 1/1 [01:33<00:00, 93.21s/it]
 14%|█▍        | 716/5198 [1:43:53<129:02:52, 103.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.33s/it][A100%|██████████| 1/1 [01:33<00:00, 93.33s/it]
 14%|█▍        | 716/5198 [1:43:39<129:03:50, 103.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.24s/it][A100%|██████████| 1/1 [01:33<00:00, 93.24s/it]
 14%|█▍        | 716/5198 [1:43:59<129:02:44, 103.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.28s/it][A100%|██████████| 1/1 [01:33<00:00, 93.28s/it]
 14%|█▍        | 716/5198 [1:42:23<129:03:10, 103.66s/it]
100%|██████████| 1/1 [01:33<00:00, 93.23s/it][A100%|██████████| 1/1 [01:33<00:00, 93.23s/it]
 14%|█▍        | 716/5198 [1:42:46<129:02:23, 103.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.31s/it][A100%|██████████| 1/1 [01:33<00:00, 93.31s/it]
 14%|█▍        | 716/5198 [1:43:17<129:03:40, 103.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.29s/it][A100%|██████████| 1/1 [01:33<00:00, 93.29s/it]
 14%|█▍        | 716/5198 [1:43:19<129:03:34, 103.66s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_672

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.35s/it][A100%|██████████| 1/1 [01:40<00:00, 100.35s/it]
 14%|█▍        | 717/5198 [1:45:19<128:08:23, 102.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:35:15,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=708, skipped=0, lr=[1.954193708718172e-05], mom=[(0.9, 0.999)]
steps: 708 loss: 0.6201 iter time (s): 100.291 samples/sec: 1.276

100%|██████████| 1/1 [01:41<00:00, 101.20s/it][A100%|██████████| 1/1 [01:41<00:00, 101.20s/it]
 14%|█▍        | 717/5198 [1:45:34<128:07:40, 102.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.26s/it][A100%|██████████| 1/1 [01:41<00:00, 101.26s/it]
 14%|█▍        | 717/5198 [1:45:21<128:08:38, 102.95s/it]
100%|██████████| 1/1 [01:41<00:00, 101.14s/it][A100%|██████████| 1/1 [01:41<00:00, 101.14s/it]
 14%|█▍        | 717/5198 [1:45:40<128:06:23, 102.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.20s/it][A100%|██████████| 1/1 [01:41<00:00, 101.20s/it]
 14%|█▍        | 717/5198 [1:44:04<128:07:59, 102.94s/it]
100%|██████████| 1/1 [01:41<00:00, 101.26s/it][A100%|██████████| 1/1 [01:41<00:00, 101.26s/it]
 14%|█▍        | 717/5198 [1:44:27<128:09:09, 102.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.29s/it][A100%|██████████| 1/1 [01:41<00:00, 101.29s/it]
 14%|█▍        | 717/5198 [1:44:59<128:09:18, 102.96s/it]
100%|██████████| 1/1 [01:41<00:00, 101.27s/it][A100%|██████████| 1/1 [01:41<00:00, 101.27s/it]
 14%|█▍        | 717/5198 [1:45:01<128:08:52, 102.95s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_673

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.42s/it][A100%|██████████| 1/1 [01:35<00:00, 95.42s/it]
 14%|█▍        | 718/5198 [1:46:55<125:28:57, 100.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:36:50,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=709, skipped=0, lr=[1.9540240992173746e-05], mom=[(0.9, 0.999)]
steps: 709 loss: 0.5181 iter time (s): 94.820 samples/sec: 1.350

100%|██████████| 1/1 [01:35<00:00, 95.77s/it][A100%|██████████| 1/1 [01:35<00:00, 95.77s/it]
 14%|█▍        | 718/5198 [1:47:10<125:26:31, 100.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.53s/it][A100%|██████████| 1/1 [01:35<00:00, 95.53s/it]
 14%|█▍        | 718/5198 [1:46:56<125:24:34, 100.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.66s/it][A100%|██████████| 1/1 [01:35<00:00, 95.66s/it]
 14%|█▍        | 718/5198 [1:47:16<125:23:49, 100.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.60s/it][A100%|██████████| 1/1 [01:35<00:00, 95.60s/it]
 14%|█▍        | 718/5198 [1:45:40<125:24:22, 100.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.70s/it][A100%|██████████| 1/1 [01:35<00:00, 95.70s/it]
 14%|█▍        | 718/5198 [1:46:03<125:25:38, 100.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.69s/it][A100%|██████████| 1/1 [01:35<00:00, 95.69s/it]
 14%|█▍        | 718/5198 [1:46:34<125:25:02, 100.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.72s/it][A100%|██████████| 1/1 [01:35<00:00, 95.72s/it]
 14%|█▍        | 718/5198 [1:46:36<125:25:19, 100.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_674
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.32s/it][A100%|██████████| 1/1 [01:43<00:00, 103.32s/it]
 14%|█▍        | 719/5198 [1:48:39<126:34:12, 101.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:38:34,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[1.953854183674047e-05], mom=[(0.9, 0.999)]
steps: 710 loss: 0.5993 iter time (s): 102.698 samples/sec: 1.246

100%|██████████| 1/1 [01:43<00:00, 103.68s/it][A100%|██████████| 1/1 [01:43<00:00, 103.68s/it]
 14%|█▍        | 719/5198 [1:48:53<126:30:10, 101.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.82s/it][A100%|██████████| 1/1 [01:43<00:00, 103.82s/it]
 14%|█▍        | 719/5198 [1:48:40<126:31:20, 101.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.79s/it][A100%|██████████| 1/1 [01:43<00:00, 103.79s/it]
 14%|█▍        | 719/5198 [1:49:00<126:30:41, 101.68s/it]
100%|██████████| 1/1 [01:43<00:00, 103.69s/it][A100%|██████████| 1/1 [01:43<00:00, 103.69s/it]
 14%|█▍        | 719/5198 [1:47:24<126:30:13, 101.68s/it]
100%|██████████| 1/1 [01:43<00:00, 103.69s/it][A100%|██████████| 1/1 [01:43<00:00, 103.69s/it]
 14%|█▍        | 719/5198 [1:47:47<126:29:12, 101.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.74s/it][A100%|██████████| 1/1 [01:43<00:00, 103.74s/it]
 14%|█▍        | 719/5198 [1:48:18<126:29:53, 101.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.60s/it][A100%|██████████| 1/1 [01:43<00:00, 103.60s/it]
 14%|█▍        | 719/5198 [1:48:20<126:30:07, 101.68s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_44
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.37s/it][A100%|██████████| 1/1 [02:02<00:00, 122.37s/it]
[2024-06-30 01:40:38,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=711, skipped=0, lr=[1.9536839621427022e-05], mom=[(0.9, 0.999)]
steps: 711 loss: 0.7623 iter time (s): 122.840 samples/sec: 1.042

100%|██████████| 1/1 [02:03<00:00, 123.85s/it][A100%|██████████| 1/1 [02:03<00:00, 123.85s/it]

100%|██████████| 1/1 [02:03<00:00, 123.74s/it][A100%|██████████| 1/1 [02:03<00:00, 123.74s/it]

100%|██████████| 1/1 [02:03<00:00, 123.70s/it][A100%|██████████| 1/1 [02:03<00:00, 123.70s/it]

100%|██████████| 1/1 [02:03<00:00, 123.93s/it][A100%|██████████| 1/1 [02:03<00:00, 123.93s/it]

100%|██████████| 1/1 [02:03<00:00, 123.94s/it][A100%|██████████| 1/1 [02:03<00:00, 123.94s/it]

100%|██████████| 1/1 [02:03<00:00, 123.95s/it][A100%|██████████| 1/1 [02:03<00:00, 123.95s/it]

100%|██████████| 1/1 [02:03<00:00, 123.84s/it][A100%|██████████| 1/1 [02:03<00:00, 123.84s/it]
Checkpointing at shard 719
[2024-06-30 01:40:39,528] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step711 is about to be saved!
[2024-06-30 01:40:41,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_00-model_states.pt...
[2024-06-30 01:40:44,655] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_02-model_states.pt...
[2024-06-30 01:40:52,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_00-model_states.pt.
[2024-06-30 01:40:52,832] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_03-model_states.pt...
[2024-06-30 01:40:53,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_04-model_states.pt...
[2024-06-30 01:40:53,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_08-model_states.pt...
[2024-06-30 01:40:53,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_06-model_states.pt...
[2024-06-30 01:40:53,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_05-model_states.pt...
[2024-06-30 01:40:54,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_07-model_states.pt...
[2024-06-30 01:41:01,546] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_01-model_states.pt...
[2024-06-30 01:47:56,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_01-model_states.pt.
[2024-06-30 01:47:57,707] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_00_model_states.pt
[2024-06-30 01:47:57,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_00_model_states.pt...
[2024-06-30 01:47:59,127] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_00_model_states.pt.
[2024-06-30 01:47:59,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
[2024-06-30 01:47:59,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_02-model_states.pt.
[2024-06-30 01:47:59,651] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_01_model_states.pt
[2024-06-30 01:47:59,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_01_model_states.pt...
[2024-06-30 01:48:00,194] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_01_model_states.pt.
[2024-06-30 01:48:00,194] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
[2024-06-30 01:51:29,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_04-model_states.pt.
[2024-06-30 01:51:29,857] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_05-model_states.pt.
[2024-06-30 01:51:29,868] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_03-model_states.pt.
[2024-06-30 01:51:29,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_08-model_states.pt.
[2024-06-30 01:51:29,948] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_03_model_states.pt...
[2024-06-30 01:51:29,954] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_06-model_states.pt.
[2024-06-30 01:51:29,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_07-model_states.pt.
[2024-06-30 01:51:30,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_03_model_states.pt.
[2024-06-30 01:51:30,219] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
[2024-06-30 01:51:30,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_06_model_states.pt...
[2024-06-30 01:51:30,439] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_06_model_states.pt.
[2024-06-30 01:51:30,440] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
[2024-06-30 01:51:30,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_05_model_states.pt...
[2024-06-30 01:51:30,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_05_model_states.pt.
[2024-06-30 01:51:30,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
[2024-06-30 01:51:30,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_02_model_states.pt...
[2024-06-30 01:51:30,752] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_04_model_states.pt...
[2024-06-30 01:51:30,777] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_09-model_states.pt...
[2024-06-30 01:51:30,855] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_02_model_states.pt.
[2024-06-30 01:51:30,855] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
[2024-06-30 01:51:31,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_04_model_states.pt.
[2024-06-30 01:51:31,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
[2024-06-30 01:51:31,912] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/layer_09-model_states.pt.
[2024-06-30 01:51:31,933] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_07_model_states.pt...
[2024-06-30 01:51:32,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step711/mp_rank_07_model_states.pt.
[2024-06-30 01:51:32,046] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step711 is ready now!
Checkpoint saved using --- 652.5201954841614 seconds ---
 14%|█▍        | 720/5198 [2:01:56<378:21:59, 304.18s/it] 14%|█▍        | 720/5198 [2:01:37<378:25:16, 304.22s/it] 14%|█▍        | 720/5198 [2:00:20<378:19:49, 304.15s/it] 14%|█▍        | 720/5198 [2:00:43<378:18:23, 304.13s/it] 14%|█▍        | 720/5198 [2:01:38<379:20:51, 304.97s/it] 14%|█▍        | 720/5198 [2:01:50<378:28:36, 304.27s/it] 14%|█▍        | 720/5198 [2:01:16<378:16:44, 304.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_675
 14%|█▍        | 720/5198 [2:01:15<378:17:22, 304.12s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.94s/it][A100%|██████████| 1/1 [01:24<00:00, 84.94s/it]
 14%|█▍        | 721/5198 [2:03:04<297:17:38, 239.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:52:59,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=712, skipped=0, lr=[1.9535134346779515e-05], mom=[(0.9, 0.999)]
steps: 712 loss: 0.5663 iter time (s): 86.930 samples/sec: 1.472

100%|██████████| 1/1 [01:27<00:00, 87.34s/it][A100%|██████████| 1/1 [01:27<00:00, 87.34s/it]
 14%|█▍        | 721/5198 [2:03:18<297:34:59, 239.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.45s/it][A100%|██████████| 1/1 [01:27<00:00, 87.46s/it]
 14%|█▍        | 721/5198 [2:03:05<297:35:07, 239.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.59s/it][A100%|██████████| 1/1 [01:27<00:00, 87.59s/it]
 14%|█▍        | 721/5198 [2:03:24<297:35:40, 239.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.68s/it][A100%|██████████| 1/1 [01:27<00:00, 87.68s/it]
 14%|█▍        | 721/5198 [2:01:48<297:36:08, 239.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.73s/it][A100%|██████████| 1/1 [01:27<00:00, 87.73s/it]
 14%|█▍        | 721/5198 [2:02:12<297:36:21, 239.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.78s/it][A100%|██████████| 1/1 [01:27<00:00, 87.78s/it]
 14%|█▍        | 721/5198 [2:02:43<297:36:43, 239.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.80s/it][A100%|██████████| 1/1 [01:27<00:00, 87.80s/it]
 14%|█▍        | 721/5198 [2:02:45<297:36:44, 239.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_676
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.79s/it][A100%|██████████| 1/1 [02:03<00:00, 123.79s/it]
 14%|█▍        | 722/5198 [2:05:08<254:22:00, 204.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:55:04,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=713, skipped=0, lr=[1.9533426013345048e-05], mom=[(0.9, 0.999)]
steps: 713 loss: 0.5500 iter time (s): 124.203 samples/sec: 1.031

100%|██████████| 1/1 [02:05<00:00, 125.02s/it][A100%|██████████| 1/1 [02:05<00:00, 125.02s/it]
 14%|█▍        | 722/5198 [2:05:23<254:54:47, 205.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.06s/it][A100%|██████████| 1/1 [02:05<00:00, 125.06s/it]
 14%|█▍        | 722/5198 [2:05:10<254:55:38, 205.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.12s/it][A100%|██████████| 1/1 [02:05<00:00, 125.12s/it]
 14%|█▍        | 722/5198 [2:05:29<254:57:01, 205.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.11s/it][A100%|██████████| 1/1 [02:05<00:00, 125.11s/it]
 14%|█▍        | 722/5198 [2:04:17<254:57:09, 205.06s/it]
100%|██████████| 1/1 [02:05<00:00, 125.16s/it][A100%|██████████| 1/1 [02:05<00:00, 125.16s/it]
 14%|█▍        | 722/5198 [2:03:53<254:58:24, 205.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.14s/it][A100%|██████████| 1/1 [02:05<00:00, 125.14s/it]
 14%|█▍        | 722/5198 [2:04:48<254:57:47, 205.06s/it]
100%|██████████| 1/1 [02:05<00:00, 125.13s/it][A100%|██████████| 1/1 [02:05<00:00, 125.13s/it]
 14%|█▍        | 722/5198 [2:04:50<254:57:49, 205.06s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_677
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.68s/it][A100%|██████████| 1/1 [01:30<00:00, 90.68s/it]
 14%|█▍        | 723/5198 [2:06:39<211:58:24, 170.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:56:34,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=714, skipped=0, lr=[1.9531714621671693e-05], mom=[(0.9, 0.999)]
steps: 714 loss: 0.5425 iter time (s): 88.861 samples/sec: 1.440

100%|██████████| 1/1 [01:29<00:00, 89.88s/it][A100%|██████████| 1/1 [01:29<00:00, 89.88s/it]
 14%|█▍        | 723/5198 [2:06:53<211:55:54, 170.49s/it]
100%|██████████| 1/1 [01:29<00:00, 89.76s/it][A100%|██████████| 1/1 [01:29<00:00, 89.76s/it]
 14%|█▍        | 723/5198 [2:06:40<211:54:39, 170.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.81s/it][A100%|██████████| 1/1 [01:29<00:00, 89.81s/it]
 14%|█▍        | 723/5198 [2:06:59<211:57:26, 170.51s/it]
100%|██████████| 1/1 [01:29<00:00, 89.79s/it][A100%|██████████| 1/1 [01:29<00:00, 89.79s/it]
 14%|█▍        | 723/5198 [2:05:23<211:55:42, 170.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.72s/it][A100%|██████████| 1/1 [01:29<00:00, 89.72s/it]
 14%|█▍        | 723/5198 [2:06:18<211:56:06, 170.50s/it]
100%|██████████| 1/1 [01:29<00:00, 89.90s/it][A100%|██████████| 1/1 [01:29<00:00, 89.90s/it]
 14%|█▍        | 723/5198 [2:05:47<211:57:23, 170.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.78s/it][A100%|██████████| 1/1 [01:29<00:00, 89.78s/it]
 14%|█▍        | 723/5198 [2:06:20<211:56:27, 170.50s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_678
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.71s/it][A100%|██████████| 1/1 [01:30<00:00, 90.71s/it]
 14%|█▍        | 724/5198 [2:08:10<182:15:58, 146.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:58:05,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=715, skipped=0, lr=[1.953000017230851e-05], mom=[(0.9, 0.999)]
steps: 715 loss: 0.5363 iter time (s): 90.290 samples/sec: 1.418

100%|██████████| 1/1 [01:31<00:00, 91.19s/it][A100%|██████████| 1/1 [01:31<00:00, 91.19s/it]
 14%|█▍        | 724/5198 [2:08:11<182:18:32, 146.69s/it]
100%|██████████| 1/1 [01:31<00:00, 91.20s/it][A100%|██████████| 1/1 [01:31<00:00, 91.20s/it]
 14%|█▍        | 724/5198 [2:08:24<182:20:50, 146.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.06s/it][A100%|██████████| 1/1 [01:31<00:00, 91.06s/it]
 14%|█▍        | 724/5198 [2:08:30<182:19:09, 146.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.19s/it][A100%|██████████| 1/1 [01:31<00:00, 91.19s/it]
 14%|█▍        | 724/5198 [2:06:55<182:20:39, 146.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.18s/it][A100%|██████████| 1/1 [01:31<00:00, 91.18s/it]
 14%|█▍        | 724/5198 [2:07:18<182:20:19, 146.72s/it]
100%|██████████| 1/1 [01:31<00:00, 91.24s/it][A100%|██████████| 1/1 [01:31<00:00, 91.24s/it]
 14%|█▍        | 724/5198 [2:07:49<182:20:52, 146.73s/it]
100%|██████████| 1/1 [01:31<00:00, 91.23s/it][A100%|██████████| 1/1 [01:31<00:00, 91.23s/it]
 14%|█▍        | 724/5198 [2:07:51<182:20:39, 146.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_679

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.47s/it][A100%|██████████| 1/1 [01:47<00:00, 107.47s/it]
 14%|█▍        | 725/5198 [2:09:58<167:45:40, 135.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 01:59:53,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=716, skipped=0, lr=[1.9528282665805523e-05], mom=[(0.9, 0.999)]
steps: 716 loss: 0.5539 iter time (s): 107.396 samples/sec: 1.192

100%|██████████| 1/1 [01:48<00:00, 108.18s/it][A100%|██████████| 1/1 [01:48<00:00, 108.18s/it]
 14%|█▍        | 725/5198 [2:10:13<167:58:15, 135.19s/it]
100%|██████████| 1/1 [01:48<00:00, 108.24s/it][A100%|██████████| 1/1 [01:48<00:00, 108.24s/it]
 14%|█▍        | 725/5198 [2:09:59<167:57:41, 135.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.31s/it][A100%|██████████| 1/1 [01:48<00:00, 108.31s/it]
 14%|█▍        | 725/5198 [2:10:19<167:59:56, 135.21s/it]
100%|██████████| 1/1 [01:48<00:00, 108.30s/it][A100%|██████████| 1/1 [01:48<00:00, 108.30s/it]
 14%|█▍        | 725/5198 [2:08:43<167:59:03, 135.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.27s/it][A100%|██████████| 1/1 [01:48<00:00, 108.27s/it]
 14%|█▍        | 725/5198 [2:09:06<168:00:03, 135.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.28s/it][A100%|██████████| 1/1 [01:48<00:00, 108.28s/it]
 14%|█▍        | 725/5198 [2:09:37<168:00:09, 135.21s/it]
100%|██████████| 1/1 [01:48<00:00, 108.29s/it][A100%|██████████| 1/1 [01:48<00:00, 108.29s/it]
 14%|█▍        | 725/5198 [2:09:39<167:59:49, 135.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_680

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.12s/it][A100%|██████████| 1/1 [01:31<00:00, 91.12s/it]
 14%|█▍        | 726/5198 [2:11:29<151:30:18, 121.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:01:24,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=717, skipped=0, lr=[1.9526562102713776e-05], mom=[(0.9, 0.999)]
steps: 717 loss: 0.5577 iter time (s): 90.033 samples/sec: 1.422

100%|██████████| 1/1 [01:30<00:00, 90.98s/it][A100%|██████████| 1/1 [01:30<00:00, 90.98s/it]
 14%|█▍        | 726/5198 [2:11:44<151:28:39, 121.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.23s/it][A100%|██████████| 1/1 [01:31<00:00, 91.23s/it]
 14%|█▍        | 726/5198 [2:11:30<151:33:10, 122.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 91.00s/it][A100%|██████████| 1/1 [01:30<00:00, 91.00s/it]
 14%|█▍        | 726/5198 [2:11:50<151:32:02, 121.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.11s/it][A100%|██████████| 1/1 [01:31<00:00, 91.11s/it]
 14%|█▍        | 726/5198 [2:10:14<151:32:51, 122.00s/it]
100%|██████████| 1/1 [01:31<00:00, 91.12s/it][A100%|██████████| 1/1 [01:31<00:00, 91.12s/it]
 14%|█▍        | 726/5198 [2:10:37<151:32:09, 121.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.14s/it][A100%|██████████| 1/1 [01:31<00:00, 91.14s/it]
 14%|█▍        | 726/5198 [2:11:08<151:32:33, 121.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.09s/it][A100%|██████████| 1/1 [01:31<00:00, 91.09s/it]
 14%|█▍        | 726/5198 [2:11:10<151:32:54, 122.00s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_681

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.26s/it][A100%|██████████| 1/1 [01:35<00:00, 95.26s/it]
 14%|█▍        | 727/5198 [2:13:05<141:41:33, 114.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:03:00,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=718, skipped=0, lr=[1.9524838483585245e-05], mom=[(0.9, 0.999)]
steps: 718 loss: 0.5451 iter time (s): 94.703 samples/sec: 1.352

100%|██████████| 1/1 [01:35<00:00, 95.66s/it][A100%|██████████| 1/1 [01:35<00:00, 95.67s/it]
 14%|█▍        | 727/5198 [2:13:19<141:40:18, 114.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.66s/it][A100%|██████████| 1/1 [01:35<00:00, 95.66s/it]
 14%|█▍        | 727/5198 [2:13:06<141:42:26, 114.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.55s/it][A100%|██████████| 1/1 [01:35<00:00, 95.55s/it]
 14%|█▍        | 727/5198 [2:13:25<141:39:58, 114.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.59s/it][A100%|██████████| 1/1 [01:35<00:00, 95.59s/it]
 14%|█▍        | 727/5198 [2:11:50<141:41:39, 114.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.64s/it][A100%|██████████| 1/1 [01:35<00:00, 95.64s/it]
 14%|█▍        | 727/5198 [2:12:13<141:42:49, 114.11s/it]
100%|██████████| 1/1 [01:35<00:00, 95.65s/it][A100%|██████████| 1/1 [01:35<00:00, 95.65s/it]
 14%|█▍        | 727/5198 [2:12:44<141:42:24, 114.10s/it]
100%|██████████| 1/1 [01:35<00:00, 95.66s/it][A100%|██████████| 1/1 [01:35<00:00, 95.66s/it]
 14%|█▍        | 727/5198 [2:12:46<141:42:14, 114.10s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_682
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.35s/it][A100%|██████████| 1/1 [01:45<00:00, 105.35s/it]
 14%|█▍        | 728/5198 [2:14:51<138:31:29, 111.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:04:46,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=719, skipped=0, lr=[1.9523111808972924e-05], mom=[(0.9, 0.999)]
steps: 719 loss: 0.5492 iter time (s): 105.003 samples/sec: 1.219

100%|██████████| 1/1 [01:45<00:00, 105.96s/it][A100%|██████████| 1/1 [01:45<00:00, 105.96s/it]
 14%|█▍        | 728/5198 [2:15:05<138:39:18, 111.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.88s/it][A100%|██████████| 1/1 [01:45<00:00, 105.88s/it]
 14%|█▍        | 728/5198 [2:14:52<138:38:25, 111.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.96s/it][A100%|██████████| 1/1 [01:45<00:00, 105.96s/it]
 14%|█▍        | 728/5198 [2:15:11<138:37:04, 111.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.74s/it][A100%|██████████| 1/1 [01:45<00:00, 105.74s/it]
 14%|█▍        | 728/5198 [2:13:59<138:36:18, 111.63s/it]
100%|██████████| 1/1 [01:45<00:00, 105.94s/it][A100%|██████████| 1/1 [01:45<00:00, 105.94s/it]
 14%|█▍        | 728/5198 [2:13:36<138:38:20, 111.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.85s/it][A100%|██████████| 1/1 [01:45<00:00, 105.85s/it]
 14%|█▍        | 728/5198 [2:14:30<138:37:52, 111.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.89s/it][A100%|██████████| 1/1 [01:45<00:00, 105.89s/it]
 14%|█▍        | 728/5198 [2:14:32<138:37:52, 111.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_683
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.23s/it][A100%|██████████| 1/1 [01:37<00:00, 97.23s/it]
 14%|█▍        | 729/5198 [2:16:28<133:14:59, 107.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:06:23,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[1.9521382079430765e-05], mom=[(0.9, 0.999)]
steps: 720 loss: 0.5264 iter time (s): 96.323 samples/sec: 1.329

100%|██████████| 1/1 [01:37<00:00, 97.23s/it][A100%|██████████| 1/1 [01:37<00:00, 97.23s/it]
 14%|█▍        | 729/5198 [2:16:43<133:15:29, 107.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.18s/it][A100%|██████████| 1/1 [01:37<00:00, 97.18s/it]
 14%|█▍        | 729/5198 [2:16:29<133:15:00, 107.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.32s/it][A100%|██████████| 1/1 [01:37<00:00, 97.32s/it]
 14%|█▍        | 729/5198 [2:16:49<133:16:06, 107.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.24s/it][A100%|██████████| 1/1 [01:37<00:00, 97.24s/it]
 14%|█▍        | 729/5198 [2:15:36<133:14:18, 107.33s/it]
100%|██████████| 1/1 [01:37<00:00, 97.23s/it][A100%|██████████| 1/1 [01:37<00:00, 97.23s/it]
 14%|█▍        | 729/5198 [2:15:13<133:15:19, 107.34s/it]
100%|██████████| 1/1 [01:37<00:00, 97.25s/it][A100%|██████████| 1/1 [01:37<00:00, 97.25s/it]
 14%|█▍        | 729/5198 [2:16:07<133:14:31, 107.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.27s/it][A100%|██████████| 1/1 [01:37<00:00, 97.27s/it]
 14%|█▍        | 729/5198 [2:16:09<133:15:12, 107.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_684
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.06s/it][A100%|██████████| 1/1 [02:04<00:00, 124.06s/it]
 14%|█▍        | 730/5198 [2:18:33<139:38:31, 112.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:08:28,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=721, skipped=0, lr=[1.9519649295513715e-05], mom=[(0.9, 0.999)]
steps: 721 loss: 0.5563 iter time (s): 123.833 samples/sec: 1.034

100%|██████████| 1/1 [02:04<00:00, 124.64s/it][A100%|██████████| 1/1 [02:04<00:00, 124.64s/it]
 14%|█▍        | 730/5198 [2:18:47<139:41:56, 112.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.77s/it][A100%|██████████| 1/1 [02:04<00:00, 124.77s/it]
 14%|█▍        | 730/5198 [2:18:34<139:42:48, 112.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.78s/it][A100%|██████████| 1/1 [02:04<00:00, 124.78s/it]
 14%|█▍        | 730/5198 [2:18:54<139:43:47, 112.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.68s/it][A100%|██████████| 1/1 [02:04<00:00, 124.68s/it]
 14%|█▍        | 730/5198 [2:17:18<139:42:46, 112.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.73s/it][A100%|██████████| 1/1 [02:04<00:00, 124.73s/it]
 14%|█▍        | 730/5198 [2:18:12<139:42:09, 112.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.78s/it][A100%|██████████| 1/1 [02:04<00:00, 124.78s/it]
 14%|█▍        | 730/5198 [2:17:41<139:44:24, 112.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.78s/it][A100%|██████████| 1/1 [02:04<00:00, 124.79s/it]
 14%|█▍        | 730/5198 [2:18:14<139:43:15, 112.58s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_685
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.30s/it][A100%|██████████| 1/1 [01:39<00:00, 99.30s/it]
 14%|█▍        | 731/5198 [2:20:12<134:48:53, 108.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:10:07,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=722, skipped=0, lr=[1.9517913457777686e-05], mom=[(0.9, 0.999)]
steps: 722 loss: 0.5945 iter time (s): 98.268 samples/sec: 1.303

100%|██████████| 1/1 [01:39<00:00, 99.28s/it][A100%|██████████| 1/1 [01:39<00:00, 99.28s/it]
 14%|█▍        | 731/5198 [2:20:27<134:44:03, 108.58s/it]
100%|██████████| 1/1 [01:39<00:00, 99.15s/it][A100%|██████████| 1/1 [01:39<00:00, 99.15s/it]
 14%|█▍        | 731/5198 [2:20:13<134:41:49, 108.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.16s/it][A100%|██████████| 1/1 [01:39<00:00, 99.16s/it]
 14%|█▍        | 731/5198 [2:20:33<134:42:45, 108.57s/it]
100%|██████████| 1/1 [01:39<00:00, 99.18s/it][A100%|██████████| 1/1 [01:39<00:00, 99.18s/it]
 14%|█▍        | 731/5198 [2:18:57<134:42:23, 108.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.17s/it][A100%|██████████| 1/1 [01:39<00:00, 99.17s/it]
 14%|█▍        | 731/5198 [2:19:51<134:41:39, 108.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.14s/it][A100%|██████████| 1/1 [01:39<00:00, 99.14s/it]
 14%|█▍        | 731/5198 [2:19:20<134:42:48, 108.57s/it]
100%|██████████| 1/1 [01:39<00:00, 99.14s/it][A100%|██████████| 1/1 [01:39<00:00, 99.14s/it]
 14%|█▍        | 731/5198 [2:19:53<134:41:50, 108.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_686
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.60s/it][A100%|██████████| 1/1 [01:50<00:00, 110.60s/it]
 14%|█▍        | 732/5198 [2:22:03<135:37:48, 109.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:11:59,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=723, skipped=0, lr=[1.9516174566779588e-05], mom=[(0.9, 0.999)]
steps: 723 loss: 0.6041 iter time (s): 110.360 samples/sec: 1.160

100%|██████████| 1/1 [01:51<00:00, 111.10s/it][A100%|██████████| 1/1 [01:51<00:00, 111.10s/it]
 14%|█▍        | 732/5198 [2:22:18<135:41:56, 109.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.22s/it][A100%|██████████| 1/1 [01:51<00:00, 111.22s/it]
 14%|█▍        | 732/5198 [2:22:05<135:41:52, 109.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.17s/it][A100%|██████████| 1/1 [01:51<00:00, 111.17s/it]
 14%|█▍        | 732/5198 [2:22:24<135:41:16, 109.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.24s/it][A100%|██████████| 1/1 [01:51<00:00, 111.24s/it]
 14%|█▍        | 732/5198 [2:20:48<135:41:02, 109.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.18s/it][A100%|██████████| 1/1 [01:51<00:00, 111.18s/it]
 14%|█▍        | 732/5198 [2:21:11<135:41:44, 109.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.34s/it][A100%|██████████| 1/1 [01:51<00:00, 111.34s/it]
 14%|█▍        | 732/5198 [2:21:43<135:42:16, 109.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.24s/it][A100%|██████████| 1/1 [01:51<00:00, 111.25s/it]
 14%|█▍        | 732/5198 [2:21:45<135:41:46, 109.38s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_687
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.17s/it][A100%|██████████| 1/1 [02:06<00:00, 126.17s/it]
 14%|█▍        | 733/5198 [2:24:10<142:01:15, 114.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:14:06,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=724, skipped=0, lr=[1.951443262307729e-05], mom=[(0.9, 0.999)]
steps: 724 loss: 0.5663 iter time (s): 126.289 samples/sec: 1.014

100%|██████████| 1/1 [02:07<00:00, 127.11s/it][A100%|██████████| 1/1 [02:07<00:00, 127.11s/it]
 14%|█▍        | 733/5198 [2:24:25<142:16:57, 114.72s/it]
100%|██████████| 1/1 [02:07<00:00, 127.00s/it][A100%|██████████| 1/1 [02:07<00:00, 127.00s/it]
 14%|█▍        | 733/5198 [2:24:12<142:15:42, 114.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.23s/it][A100%|██████████| 1/1 [02:07<00:00, 127.23s/it]
 14%|█▍        | 733/5198 [2:24:31<142:18:10, 114.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.19s/it][A100%|██████████| 1/1 [02:07<00:00, 127.19s/it]
 14%|█▍        | 733/5198 [2:22:55<142:18:08, 114.73s/it]
100%|██████████| 1/1 [02:07<00:00, 127.15s/it][A100%|██████████| 1/1 [02:07<00:00, 127.15s/it]
 14%|█▍        | 733/5198 [2:23:19<142:17:21, 114.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.17s/it][A100%|██████████| 1/1 [02:07<00:00, 127.17s/it]
 14%|█▍        | 733/5198 [2:23:52<142:17:08, 114.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_688
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.21s/it][A100%|██████████| 1/1 [02:07<00:00, 127.21s/it]
 14%|█▍        | 733/5198 [2:23:50<142:18:23, 114.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.19s/it][A100%|██████████| 1/1 [02:06<00:00, 126.19s/it]
 14%|█▍        | 734/5198 [2:26:16<146:25:06, 118.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:16:12,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=725, skipped=0, lr=[1.9512687627229657e-05], mom=[(0.9, 0.999)]
steps: 725 loss: 0.6337 iter time (s): 125.571 samples/sec: 1.019

100%|██████████| 1/1 [02:06<00:00, 126.37s/it][A100%|██████████| 1/1 [02:06<00:00, 126.37s/it]
 14%|█▍        | 734/5198 [2:26:32<146:37:10, 118.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.36s/it][A100%|██████████| 1/1 [02:06<00:00, 126.36s/it]
 14%|█▍        | 734/5198 [2:26:38<146:35:58, 118.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.64s/it][A100%|██████████| 1/1 [02:06<00:00, 126.64s/it]
 14%|█▍        | 734/5198 [2:26:18<146:41:05, 118.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.43s/it][A100%|██████████| 1/1 [02:06<00:00, 126.43s/it]
 14%|█▍        | 734/5198 [2:25:02<146:38:33, 118.26s/it]
100%|██████████| 1/1 [02:06<00:00, 126.43s/it][A100%|██████████| 1/1 [02:06<00:00, 126.43s/it]
 14%|█▍        | 734/5198 [2:25:25<146:37:25, 118.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.45s/it][A100%|██████████| 1/1 [02:06<00:00, 126.45s/it]
 14%|█▍        | 734/5198 [2:25:56<146:38:06, 118.25s/it]
100%|██████████| 1/1 [02:06<00:00, 126.48s/it][A100%|██████████| 1/1 [02:06<00:00, 126.48s/it]
 14%|█▍        | 734/5198 [2:25:58<146:37:46, 118.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_689
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.97s/it][A100%|██████████| 1/1 [01:47<00:00, 107.97s/it]
 14%|█▍        | 735/5198 [2:28:04<142:42:31, 115.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:18:00,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=726, skipped=0, lr=[1.9510939579796526e-05], mom=[(0.9, 0.999)]
steps: 726 loss: 0.5817 iter time (s): 106.640 samples/sec: 1.200

100%|██████████| 1/1 [01:47<00:00, 107.70s/it][A100%|██████████| 1/1 [01:47<00:00, 107.70s/it]
 14%|█▍        | 735/5198 [2:28:19<142:41:17, 115.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.68s/it][A100%|██████████| 1/1 [01:47<00:00, 107.68s/it]
 14%|█▍        | 735/5198 [2:28:25<142:39:02, 115.07s/it]
100%|██████████| 1/1 [01:47<00:00, 107.67s/it][A100%|██████████| 1/1 [01:47<00:00, 107.67s/it]
 14%|█▍        | 735/5198 [2:28:06<142:42:11, 115.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.63s/it][A100%|██████████| 1/1 [01:47<00:00, 107.63s/it]
 14%|█▍        | 735/5198 [2:26:50<142:39:45, 115.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.68s/it][A100%|██████████| 1/1 [01:47<00:00, 107.68s/it]
 14%|█▍        | 735/5198 [2:27:13<142:39:50, 115.08s/it]
100%|██████████| 1/1 [01:47<00:00, 107.55s/it][A100%|██████████| 1/1 [01:47<00:00, 107.55s/it]
 14%|█▍        | 735/5198 [2:27:44<142:40:03, 115.08s/it]
100%|██████████| 1/1 [01:47<00:00, 107.61s/it][A100%|██████████| 1/1 [01:47<00:00, 107.61s/it]
 14%|█▍        | 735/5198 [2:27:46<142:40:20, 115.08s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_45

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.17s/it][A100%|██████████| 1/1 [02:05<00:00, 125.17s/it]
 14%|█▍        | 736/5198 [2:30:10<146:29:25, 118.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:20:06,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=727, skipped=0, lr=[1.9509188481338714e-05], mom=[(0.9, 0.999)]
steps: 727 loss: 0.7652 iter time (s): 124.960 samples/sec: 1.024

100%|██████████| 1/1 [02:05<00:00, 125.76s/it][A100%|██████████| 1/1 [02:05<00:00, 125.77s/it]
 14%|█▍        | 736/5198 [2:30:25<146:38:27, 118.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.70s/it][A100%|██████████| 1/1 [02:05<00:00, 125.70s/it]
 14%|█▍        | 736/5198 [2:30:12<146:38:31, 118.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.78s/it][A100%|██████████| 1/1 [02:05<00:00, 125.78s/it]
 14%|█▍        | 736/5198 [2:30:31<146:38:11, 118.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.69s/it][A100%|██████████| 1/1 [02:05<00:00, 125.69s/it]
 14%|█▍        | 736/5198 [2:29:19<146:37:33, 118.30s/it]
100%|██████████| 1/1 [02:05<00:00, 125.87s/it][A100%|██████████| 1/1 [02:05<00:00, 125.87s/it]
 14%|█▍        | 736/5198 [2:28:55<146:38:55, 118.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.77s/it][A100%|██████████| 1/1 [02:05<00:00, 125.77s/it]
 14%|█▍        | 736/5198 [2:29:50<146:38:19, 118.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.79s/it][A100%|██████████| 1/1 [02:05<00:00, 125.79s/it]
 14%|█▍        | 736/5198 [2:29:52<146:38:28, 118.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_690
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.56s/it][A100%|██████████| 1/1 [01:26<00:00, 86.56s/it]
 14%|█▍        | 737/5198 [2:31:37<134:47:23, 108.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:21:32,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=728, skipped=0, lr=[1.9507434332418014e-05], mom=[(0.9, 0.999)]
steps: 728 loss: 0.5956 iter time (s): 84.867 samples/sec: 1.508

100%|██████████| 1/1 [01:25<00:00, 85.83s/it][A100%|██████████| 1/1 [01:25<00:00, 85.84s/it]
 14%|█▍        | 737/5198 [2:31:51<134:33:33, 108.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.87s/it][A100%|██████████| 1/1 [01:25<00:00, 85.87s/it]
 14%|█▍        | 737/5198 [2:31:38<134:34:24, 108.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.92s/it][A100%|██████████| 1/1 [01:25<00:00, 85.92s/it]
 14%|█▍        | 737/5198 [2:31:57<134:34:46, 108.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.81s/it][A100%|██████████| 1/1 [01:25<00:00, 85.81s/it]
 14%|█▍        | 737/5198 [2:30:21<134:33:11, 108.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.89s/it][A100%|██████████| 1/1 [01:25<00:00, 85.89s/it]
 14%|█▍        | 737/5198 [2:31:16<134:33:26, 108.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.17s/it][A100%|██████████| 1/1 [01:26<00:00, 86.17s/it]
 14%|█▍        | 737/5198 [2:30:45<134:40:44, 108.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.17s/it][A100%|██████████| 1/1 [01:26<00:00, 86.17s/it]
 14%|█▍        | 737/5198 [2:31:18<134:39:50, 108.67s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_691
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.04s/it][A100%|██████████| 1/1 [01:32<00:00, 92.04s/it]
 14%|█▍        | 738/5198 [2:33:09<128:38:03, 103.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:23:04,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=729, skipped=0, lr=[1.9505677133597203e-05], mom=[(0.9, 0.999)]
steps: 729 loss: 0.6113 iter time (s): 91.055 samples/sec: 1.406

100%|██████████| 1/1 [01:32<00:00, 92.07s/it][A100%|██████████| 1/1 [01:32<00:00, 92.08s/it]
 14%|█▍        | 738/5198 [2:33:23<128:26:40, 103.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.11s/it][A100%|██████████| 1/1 [01:32<00:00, 92.11s/it]
 14%|█▍        | 738/5198 [2:33:10<128:26:06, 103.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.16s/it][A100%|██████████| 1/1 [01:32<00:00, 92.16s/it]
 14%|█▍        | 738/5198 [2:33:29<128:27:16, 103.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.19s/it][A100%|██████████| 1/1 [01:32<00:00, 92.19s/it]
 14%|█▍        | 738/5198 [2:31:54<128:26:03, 103.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.91s/it][A100%|██████████| 1/1 [01:31<00:00, 91.91s/it]
 14%|█▍        | 738/5198 [2:32:17<128:25:14, 103.66s/it]
100%|██████████| 1/1 [01:32<00:00, 92.14s/it][A100%|██████████| 1/1 [01:32<00:00, 92.14s/it]
 14%|█▍        | 738/5198 [2:32:48<128:26:57, 103.68s/it]
100%|██████████| 1/1 [01:31<00:00, 91.95s/it][A100%|██████████| 1/1 [01:31<00:00, 91.95s/it]
 14%|█▍        | 738/5198 [2:32:50<128:25:16, 103.66s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_692
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.15s/it][A100%|██████████| 1/1 [01:41<00:00, 101.15s/it]
 14%|█▍        | 739/5198 [2:34:50<127:43:50, 103.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:24:46,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[1.9503916885440035e-05], mom=[(0.9, 0.999)]
steps: 730 loss: 0.5984 iter time (s): 100.738 samples/sec: 1.271

100%|██████████| 1/1 [01:41<00:00, 101.95s/it][A100%|██████████| 1/1 [01:41<00:00, 101.95s/it]
 14%|█▍        | 739/5198 [2:35:05<127:46:35, 103.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.85s/it][A100%|██████████| 1/1 [01:41<00:00, 101.85s/it]
 14%|█▍        | 739/5198 [2:34:52<127:45:10, 103.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.89s/it][A100%|██████████| 1/1 [01:41<00:00, 101.89s/it]
 14%|█▍        | 739/5198 [2:35:11<127:47:12, 103.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.99s/it][A100%|██████████| 1/1 [01:41<00:00, 102.00s/it]
 14%|█▍        | 739/5198 [2:33:36<127:47:41, 103.18s/it]
100%|██████████| 1/1 [01:41<00:00, 101.84s/it][A100%|██████████| 1/1 [01:41<00:00, 101.84s/it]
 14%|█▍        | 739/5198 [2:33:59<127:46:01, 103.15s/it]
100%|██████████| 1/1 [01:41<00:00, 101.85s/it][A100%|██████████| 1/1 [01:41<00:00, 101.86s/it]
 14%|█▍        | 739/5198 [2:34:30<127:46:48, 103.16s/it]
100%|██████████| 1/1 [01:41<00:00, 101.88s/it][A100%|██████████| 1/1 [01:41<00:00, 101.88s/it]
 14%|█▍        | 739/5198 [2:34:32<127:45:23, 103.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_693

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s]
[A  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.66s/it][A100%|██████████| 1/1 [01:59<00:00, 119.66s/it]
[2024-06-30 02:26:46,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=731, skipped=0, lr=[1.950215358851124e-05], mom=[(0.9, 0.999)]
steps: 731 loss: 0.5244 iter time (s): 119.238 samples/sec: 1.073

100%|██████████| 1/1 [02:00<00:00, 120.20s/it][A100%|██████████| 1/1 [02:00<00:00, 120.20s/it]

100%|██████████| 1/1 [02:00<00:00, 120.20s/it][A100%|██████████| 1/1 [02:00<00:00, 120.21s/it]

100%|██████████| 1/1 [02:00<00:00, 120.23s/it][A100%|██████████| 1/1 [02:00<00:00, 120.23s/it]

100%|██████████| 1/1 [02:00<00:00, 120.09s/it][A100%|██████████| 1/1 [02:00<00:00, 120.09s/it]

100%|██████████| 1/1 [02:00<00:00, 120.23s/it][A100%|██████████| 1/1 [02:00<00:00, 120.23s/it]

100%|██████████| 1/1 [02:00<00:00, 120.23s/it][A100%|██████████| 1/1 [02:00<00:00, 120.23s/it]
Checkpointing at shard 739

100%|██████████| 1/1 [02:00<00:00, 120.25s/it][A100%|██████████| 1/1 [02:00<00:00, 120.25s/it]
[2024-06-30 02:26:47,674] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step731 is about to be saved!
[2024-06-30 02:26:49,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_00-model_states.pt...
[2024-06-30 02:26:52,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_02-model_states.pt...
[2024-06-30 02:27:00,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_00-model_states.pt.
[2024-06-30 02:27:00,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_04-model_states.pt...
[2024-06-30 02:27:00,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_03-model_states.pt...
[2024-06-30 02:27:01,765] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_05-model_states.pt...
[2024-06-30 02:27:02,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_06-model_states.pt...
[2024-06-30 02:27:02,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_08-model_states.pt...
[2024-06-30 02:27:02,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_07-model_states.pt...
[2024-06-30 02:27:09,805] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_01-model_states.pt...
[2024-06-30 02:34:24,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_03-model_states.pt.
[2024-06-30 02:34:25,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_02_model_states.pt...
[2024-06-30 02:34:25,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_02_model_states.pt.
[2024-06-30 02:34:25,428] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
[2024-06-30 02:34:31,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_04-model_states.pt.
[2024-06-30 02:34:31,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_06-model_states.pt.
[2024-06-30 02:34:31,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_05-model_states.pt.
[2024-06-30 02:34:31,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_08-model_states.pt.
[2024-06-30 02:34:31,091] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_07-model_states.pt.
[2024-06-30 02:34:31,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_01-model_states.pt.
[2024-06-30 02:34:31,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_02-model_states.pt.
[2024-06-30 02:34:31,277] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_01_model_states.pt
[2024-06-30 02:34:31,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_01_model_states.pt...
[2024-06-30 02:34:31,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_01_model_states.pt.
[2024-06-30 02:34:31,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
[2024-06-30 02:34:31,509] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_06_model_states.pt...
[2024-06-30 02:34:31,677] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_06_model_states.pt.
[2024-06-30 02:34:31,677] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
[2024-06-30 02:34:31,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_03_model_states.pt...
[2024-06-30 02:34:31,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_05_model_states.pt...
[2024-06-30 02:34:31,879] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_09-model_states.pt...
[2024-06-30 02:34:31,992] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_04_model_states.pt...
[2024-06-30 02:34:32,055] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_03_model_states.pt.
[2024-06-30 02:34:32,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
[2024-06-30 02:34:32,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_05_model_states.pt.
[2024-06-30 02:34:32,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
[2024-06-30 02:34:32,253] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_00_model_states.pt
[2024-06-30 02:34:32,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_00_model_states.pt...
[2024-06-30 02:34:32,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_04_model_states.pt.
[2024-06-30 02:34:32,294] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
[2024-06-30 02:34:33,034] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_00_model_states.pt.
[2024-06-30 02:34:33,035] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
[2024-06-30 02:34:33,284] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/layer_09-model_states.pt.
[2024-06-30 02:34:33,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_07_model_states.pt...
[2024-06-30 02:34:33,449] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step731/mp_rank_07_model_states.pt.
[2024-06-30 02:34:33,449] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step731 is ready now!
Checkpoint saved using --- 465.7804090976715 seconds ---
 14%|█▍        | 740/5198 [2:44:40<308:22:33, 249.02s/it] 14%|█▍        | 740/5198 [2:44:38<307:17:03, 248.14s/it] 14%|█▍        | 740/5198 [2:43:22<307:11:23, 248.07s/it] 14%|█▍        | 740/5198 [2:44:16<307:09:31, 248.04s/it] 14%|█▍        | 740/5198 [2:44:18<307:07:58, 248.02s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_694
 14%|█▍        | 740/5198 [2:44:52<307:20:20, 248.19s/it] 14%|█▍        | 740/5198 [2:43:45<307:09:22, 248.04s/it] 14%|█▍        | 740/5198 [2:44:58<307:14:03, 248.10s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s]
[A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.72s/it][A100%|██████████| 1/1 [01:51<00:00, 111.72s/it]
 14%|█▍        | 741/5198 [2:46:32<257:27:46, 207.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:36:28,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=732, skipped=0, lr=[1.9500387243376528e-05], mom=[(0.9, 0.999)]
steps: 732 loss: 0.5551 iter time (s): 114.496 samples/sec: 1.118

100%|██████████| 1/1 [01:55<00:00, 115.13s/it][A100%|██████████| 1/1 [01:55<00:00, 115.13s/it]
 14%|█▍        | 741/5198 [2:46:47<258:00:07, 208.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.31s/it][A100%|██████████| 1/1 [01:55<00:00, 115.31s/it]
 14%|█▍        | 741/5198 [2:46:34<258:01:46, 208.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.44s/it][A100%|██████████| 1/1 [01:55<00:00, 115.44s/it]
 14%|█▍        | 741/5198 [2:46:54<258:02:37, 208.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.53s/it][A100%|██████████| 1/1 [01:55<00:00, 115.53s/it]
 14%|█▍        | 741/5198 [2:45:18<258:02:46, 208.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.59s/it][A100%|██████████| 1/1 [01:55<00:00, 115.59s/it]
 14%|█▍        | 741/5198 [2:45:41<258:02:40, 208.43s/it]
100%|██████████| 1/1 [01:55<00:00, 115.60s/it][A100%|██████████| 1/1 [01:55<00:00, 115.60s/it]
 14%|█▍        | 741/5198 [2:46:12<258:03:03, 208.43s/it]
100%|██████████| 1/1 [01:55<00:00, 115.62s/it][A100%|██████████| 1/1 [01:55<00:00, 115.62s/it]
 14%|█▍        | 741/5198 [2:46:14<258:02:31, 208.43s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_695
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:09<00:00, 129.77s/it][A100%|██████████| 1/1 [02:09<00:00, 129.77s/it]
 14%|█▍        | 742/5198 [2:48:42<228:29:46, 184.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:38:38,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=733, skipped=0, lr=[1.9498617850602584e-05], mom=[(0.9, 0.999)]
steps: 733 loss: 0.5916 iter time (s): 129.236 samples/sec: 0.990

100%|██████████| 1/1 [02:10<00:00, 130.38s/it][A100%|██████████| 1/1 [02:10<00:00, 130.38s/it]
 14%|█▍        | 742/5198 [2:48:58<228:58:51, 184.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.24s/it][A100%|██████████| 1/1 [02:10<00:00, 130.24s/it]
 14%|█▍        | 742/5198 [2:48:44<228:56:45, 184.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.16s/it][A100%|██████████| 1/1 [02:10<00:00, 130.17s/it]
 14%|█▍        | 742/5198 [2:49:04<228:56:44, 184.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.25s/it][A100%|██████████| 1/1 [02:10<00:00, 130.25s/it]
 14%|█▍        | 742/5198 [2:47:28<228:57:43, 184.98s/it]
100%|██████████| 1/1 [02:10<00:00, 130.19s/it][A100%|██████████| 1/1 [02:10<00:00, 130.19s/it]
 14%|█▍        | 742/5198 [2:48:22<228:58:01, 184.98s/it]
100%|██████████| 1/1 [02:10<00:00, 130.20s/it][A100%|██████████| 1/1 [02:10<00:00, 130.20s/it]
 14%|█▍        | 742/5198 [2:47:51<228:58:11, 184.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.24s/it][A100%|██████████| 1/1 [02:10<00:00, 130.24s/it]
 14%|█▍        | 742/5198 [2:48:24<228:58:02, 184.98s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_696
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.66s/it][A100%|██████████| 1/1 [01:47<00:00, 107.66s/it]
 14%|█▍        | 743/5198 [2:50:30<199:58:45, 161.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:40:26,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=734, skipped=0, lr=[1.949684541075708e-05], mom=[(0.9, 0.999)]
steps: 734 loss: 0.5433 iter time (s): 106.288 samples/sec: 1.204

100%|██████████| 1/1 [01:47<00:00, 107.08s/it][A100%|██████████| 1/1 [01:47<00:00, 107.08s/it]
 14%|█▍        | 743/5198 [2:50:45<200:00:44, 161.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.16s/it][A100%|██████████| 1/1 [01:47<00:00, 107.16s/it]
 14%|█▍        | 743/5198 [2:50:31<200:00:38, 161.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.17s/it][A100%|██████████| 1/1 [01:47<00:00, 107.17s/it]
 14%|█▍        | 743/5198 [2:50:51<200:02:39, 161.65s/it]
100%|██████████| 1/1 [01:47<00:00, 107.06s/it][A100%|██████████| 1/1 [01:47<00:00, 107.06s/it]
 14%|█▍        | 743/5198 [2:49:15<200:01:00, 161.63s/it]
100%|██████████| 1/1 [01:47<00:00, 107.10s/it][A100%|██████████| 1/1 [01:47<00:00, 107.10s/it]
 14%|█▍        | 743/5198 [2:49:38<200:01:38, 161.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.18s/it][A100%|██████████| 1/1 [01:47<00:00, 107.18s/it]
 14%|█▍        | 743/5198 [2:50:09<200:03:29, 161.66s/it]
100%|██████████| 1/1 [01:47<00:00, 107.19s/it][A100%|██████████| 1/1 [01:47<00:00, 107.19s/it]
 14%|█▍        | 743/5198 [2:50:11<200:02:52, 161.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_697
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.22s/it][A100%|██████████| 1/1 [01:47<00:00, 107.22s/it]
 14%|█▍        | 744/5198 [2:52:18<179:53:31, 145.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:42:13,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=735, skipped=0, lr=[1.9495069924408652e-05], mom=[(0.9, 0.999)]
steps: 735 loss: 0.5735 iter time (s): 106.659 samples/sec: 1.200

100%|██████████| 1/1 [01:47<00:00, 107.59s/it][A100%|██████████| 1/1 [01:47<00:00, 107.59s/it]
 14%|█▍        | 744/5198 [2:52:32<179:55:32, 145.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.50s/it][A100%|██████████| 1/1 [01:47<00:00, 107.50s/it]
 14%|█▍        | 744/5198 [2:52:19<179:55:46, 145.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.50s/it][A100%|██████████| 1/1 [01:47<00:00, 107.51s/it]
 14%|█▍        | 744/5198 [2:52:39<179:56:55, 145.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.60s/it][A100%|██████████| 1/1 [01:47<00:00, 107.60s/it]
 14%|█▍        | 744/5198 [2:51:03<179:57:16, 145.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.67s/it][A100%|██████████| 1/1 [01:47<00:00, 107.67s/it]
 14%|█▍        | 744/5198 [2:51:26<179:57:36, 145.45s/it]
100%|██████████| 1/1 [01:47<00:00, 107.55s/it][A100%|██████████| 1/1 [01:47<00:00, 107.55s/it]
 14%|█▍        | 744/5198 [2:51:57<179:57:26, 145.45s/it]
100%|██████████| 1/1 [01:47<00:00, 107.59s/it][A100%|██████████| 1/1 [01:47<00:00, 107.59s/it]
 14%|█▍        | 744/5198 [2:51:59<179:57:32, 145.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_698

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.48s/it][A100%|██████████| 1/1 [01:32<00:00, 92.48s/it]
 14%|█▍        | 745/5198 [2:53:50<160:22:51, 129.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:43:46,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=736, skipped=0, lr=[1.949329139212692e-05], mom=[(0.9, 0.999)]
steps: 736 loss: 0.5650 iter time (s): 91.354 samples/sec: 1.401

100%|██████████| 1/1 [01:32<00:00, 92.38s/it][A100%|██████████| 1/1 [01:32<00:00, 92.38s/it]
 14%|█▍        | 745/5198 [2:54:05<160:12:15, 129.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.29s/it][A100%|██████████| 1/1 [01:32<00:00, 92.29s/it]
 14%|█▍        | 745/5198 [2:53:51<160:12:27, 129.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.28s/it][A100%|██████████| 1/1 [01:32<00:00, 92.28s/it]
 14%|█▍        | 745/5198 [2:54:11<160:12:17, 129.52s/it]
100%|██████████| 1/1 [01:32<00:00, 92.23s/it][A100%|██████████| 1/1 [01:32<00:00, 92.23s/it]
 14%|█▍        | 745/5198 [2:52:35<160:10:33, 129.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.17s/it][A100%|██████████| 1/1 [01:32<00:00, 92.17s/it]
 14%|█▍        | 745/5198 [2:52:58<160:11:29, 129.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.23s/it][A100%|██████████| 1/1 [01:32<00:00, 92.23s/it]
 14%|█▍        | 745/5198 [2:53:29<160:12:13, 129.52s/it]
100%|██████████| 1/1 [01:32<00:00, 92.23s/it][A100%|██████████| 1/1 [01:32<00:00, 92.23s/it]
 14%|█▍        | 745/5198 [2:53:31<160:12:04, 129.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_699
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.47s/it][A100%|██████████| 1/1 [01:47<00:00, 107.47s/it]
 14%|█▍        | 746/5198 [2:55:38<152:18:22, 123.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:45:34,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=737, skipped=0, lr=[1.9491509814482483e-05], mom=[(0.9, 0.999)]
steps: 737 loss: 0.5679 iter time (s): 107.391 samples/sec: 1.192

100%|██████████| 1/1 [01:48<00:00, 108.31s/it][A100%|██████████| 1/1 [01:48<00:00, 108.31s/it]
 14%|█▍        | 746/5198 [2:55:53<152:18:24, 123.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.33s/it][A100%|██████████| 1/1 [01:48<00:00, 108.33s/it]
 14%|█▍        | 746/5198 [2:55:40<152:19:54, 123.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.26s/it][A100%|██████████| 1/1 [01:48<00:00, 108.26s/it]
 14%|█▍        | 746/5198 [2:55:59<152:18:53, 123.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.49s/it][A100%|██████████| 1/1 [01:48<00:00, 108.49s/it]
 14%|█▍        | 746/5198 [2:54:24<152:22:13, 123.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.44s/it][A100%|██████████| 1/1 [01:48<00:00, 108.44s/it]
 14%|█▍        | 746/5198 [2:54:47<152:20:38, 123.19s/it]
100%|██████████| 1/1 [01:48<00:00, 108.28s/it][A100%|██████████| 1/1 [01:48<00:00, 108.28s/it]
 14%|█▍        | 746/5198 [2:55:18<152:19:58, 123.18s/it]
100%|██████████| 1/1 [01:48<00:00, 108.32s/it][A100%|██████████| 1/1 [01:48<00:00, 108.32s/it]
 14%|█▍        | 746/5198 [2:55:20<152:20:03, 123.18s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_700
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.33s/it][A100%|██████████| 1/1 [01:46<00:00, 106.33s/it]
 14%|█▍        | 747/5198 [2:57:25<146:12:07, 118.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:47:21,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=738, skipped=0, lr=[1.948972519204692e-05], mom=[(0.9, 0.999)]
steps: 738 loss: 0.5849 iter time (s): 105.979 samples/sec: 1.208

100%|██████████| 1/1 [01:47<00:00, 107.11s/it][A100%|██████████| 1/1 [01:47<00:00, 107.11s/it]
 14%|█▍        | 747/5198 [2:57:40<146:20:23, 118.36s/it]
100%|██████████| 1/1 [01:46<00:00, 106.95s/it][A100%|██████████| 1/1 [01:46<00:00, 106.95s/it]
 14%|█▍        | 747/5198 [2:57:27<146:18:06, 118.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.14s/it][A100%|██████████| 1/1 [01:47<00:00, 107.14s/it]
 14%|█▍        | 747/5198 [2:57:46<146:20:24, 118.36s/it]
100%|██████████| 1/1 [01:46<00:00, 106.97s/it][A100%|██████████| 1/1 [01:46<00:00, 106.97s/it]
 14%|█▍        | 747/5198 [2:56:11<146:19:00, 118.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.86s/it][A100%|██████████| 1/1 [01:46<00:00, 106.86s/it]
 14%|█▍        | 747/5198 [2:56:34<146:17:48, 118.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.92s/it][A100%|██████████| 1/1 [01:46<00:00, 106.92s/it]
 14%|█▍        | 747/5198 [2:57:05<146:18:35, 118.34s/it]
100%|██████████| 1/1 [01:46<00:00, 106.97s/it][A100%|██████████| 1/1 [01:46<00:00, 106.97s/it]
 14%|█▍        | 747/5198 [2:57:07<146:18:49, 118.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_701
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.08s/it][A100%|██████████| 1/1 [01:26<00:00, 86.08s/it]
 14%|█▍        | 748/5198 [2:58:52<134:22:17, 108.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:48:47,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=739, skipped=0, lr=[1.9487937525392773e-05], mom=[(0.9, 0.999)]
steps: 739 loss: 0.6021 iter time (s): 84.682 samples/sec: 1.512

100%|██████████| 1/1 [01:25<00:00, 85.56s/it][A100%|██████████| 1/1 [01:25<00:00, 85.56s/it]
 14%|█▍        | 748/5198 [2:59:06<134:10:33, 108.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.74s/it][A100%|██████████| 1/1 [01:25<00:00, 85.74s/it]
 14%|█▍        | 748/5198 [2:58:53<134:12:09, 108.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.61s/it][A100%|██████████| 1/1 [01:25<00:00, 85.61s/it]
 14%|█▍        | 748/5198 [2:59:12<134:10:49, 108.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.66s/it][A100%|██████████| 1/1 [01:25<00:00, 85.67s/it]
 14%|█▍        | 748/5198 [2:57:36<134:10:10, 108.54s/it]
100%|██████████| 1/1 [01:25<00:00, 85.69s/it][A100%|██████████| 1/1 [01:25<00:00, 85.69s/it]
 14%|█▍        | 748/5198 [2:57:59<134:09:56, 108.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.58s/it][A100%|██████████| 1/1 [01:25<00:00, 85.58s/it]
 14%|█▍        | 748/5198 [2:58:31<134:10:32, 108.55s/it]
100%|██████████| 1/1 [01:25<00:00, 85.60s/it][A100%|██████████| 1/1 [01:25<00:00, 85.60s/it]
 14%|█▍        | 748/5198 [2:58:33<134:10:16, 108.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_702

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.13s/it][A100%|██████████| 1/1 [01:49<00:00, 109.13s/it]
 14%|█▍        | 749/5198 [3:00:41<134:35:26, 108.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:50:37,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[1.9486146815093573e-05], mom=[(0.9, 0.999)]
steps: 740 loss: 0.5875 iter time (s): 109.152 samples/sec: 1.173

100%|██████████| 1/1 [01:50<00:00, 110.03s/it][A100%|██████████| 1/1 [01:50<00:00, 110.03s/it]
 14%|█▍        | 749/5198 [3:00:56<134:43:34, 109.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.02s/it][A100%|██████████| 1/1 [01:50<00:00, 110.02s/it]
 14%|█▍        | 749/5198 [3:00:43<134:43:26, 109.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.07s/it][A100%|██████████| 1/1 [01:50<00:00, 110.07s/it]
 14%|█▍        | 749/5198 [3:01:02<134:43:17, 109.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.00s/it][A100%|██████████| 1/1 [01:50<00:00, 110.00s/it]
 14%|█▍        | 749/5198 [2:59:26<134:43:08, 109.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.09s/it][A100%|██████████| 1/1 [01:50<00:00, 110.09s/it]
 14%|█▍        | 749/5198 [2:59:50<134:43:49, 109.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.10s/it][A100%|██████████| 1/1 [01:50<00:00, 110.10s/it]
 14%|█▍        | 749/5198 [3:00:21<134:44:06, 109.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.11s/it][A100%|██████████| 1/1 [01:50<00:00, 110.11s/it]
 14%|█▍        | 749/5198 [3:00:23<134:43:40, 109.02s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_703
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.20s/it][A100%|██████████| 1/1 [01:31<00:00, 91.20s/it]
 14%|█▍        | 750/5198 [3:02:13<128:06:07, 103.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:52:08,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=741, skipped=0, lr=[1.948435306172383e-05], mom=[(0.9, 0.999)]
steps: 741 loss: 0.6027 iter time (s): 90.049 samples/sec: 1.421

100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
 14%|█▍        | 750/5198 [3:02:27<127:59:54, 103.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.86s/it][A100%|██████████| 1/1 [01:30<00:00, 90.86s/it]
 14%|█▍        | 750/5198 [3:02:14<127:58:12, 103.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
 14%|█▍        | 750/5198 [3:02:33<127:59:38, 103.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.98s/it][A100%|██████████| 1/1 [01:30<00:00, 90.98s/it]
 14%|█▍        | 750/5198 [3:00:57<128:00:30, 103.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.96s/it][A100%|██████████| 1/1 [01:30<00:00, 90.96s/it]
 14%|█▍        | 750/5198 [3:01:21<128:00:39, 103.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.96s/it][A100%|██████████| 1/1 [01:30<00:00, 90.96s/it]
 14%|█▍        | 750/5198 [3:01:52<128:00:37, 103.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 14%|█▍        | 750/5198 [3:01:54<128:00:32, 103.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_704
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.08s/it][A100%|██████████| 1/1 [01:19<00:00, 79.08s/it]
 14%|█▍        | 751/5198 [3:03:32<119:05:08, 96.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:53:27,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=742, skipped=0, lr=[1.948255626585902e-05], mom=[(0.9, 0.999)]
steps: 742 loss: 0.5914 iter time (s): 78.187 samples/sec: 1.637

100%|██████████| 1/1 [01:19<00:00, 79.08s/it][A100%|██████████| 1/1 [01:19<00:00, 79.08s/it]
 14%|█▍        | 751/5198 [3:03:46<118:53:16, 96.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.07s/it][A100%|██████████| 1/1 [01:19<00:00, 79.07s/it]
 14%|█▍        | 751/5198 [3:03:33<118:51:56, 96.23s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.11s/it][A100%|██████████| 1/1 [01:19<00:00, 79.11s/it]
 14%|█▍        | 751/5198 [3:03:52<118:53:38, 96.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.07s/it][A100%|██████████| 1/1 [01:19<00:00, 79.07s/it]
 14%|█▍        | 751/5198 [3:02:16<118:53:31, 96.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.05s/it][A100%|██████████| 1/1 [01:19<00:00, 79.05s/it]
 14%|█▍        | 751/5198 [3:02:40<118:53:05, 96.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.03s/it][A100%|██████████| 1/1 [01:19<00:00, 79.03s/it]
 14%|█▍        | 751/5198 [3:03:11<118:52:32, 96.23s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.03s/it][A100%|██████████| 1/1 [01:19<00:00, 79.04s/it]
 14%|█▍        | 751/5198 [3:03:13<118:52:41, 96.24s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_46
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:08<00:00, 128.88s/it][A100%|██████████| 1/1 [02:08<00:00, 128.88s/it]
 14%|█▍        | 752/5198 [3:05:41<131:08:13, 106.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:55:37,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=743, skipped=0, lr=[1.94807564280756e-05], mom=[(0.9, 0.999)]
steps: 743 loss: 0.7669 iter time (s): 129.511 samples/sec: 0.988

100%|██████████| 1/1 [02:10<00:00, 130.47s/it][A100%|██████████| 1/1 [02:10<00:00, 130.47s/it]
 14%|█▍        | 752/5198 [3:05:57<131:32:47, 106.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.64s/it][A100%|██████████| 1/1 [02:10<00:00, 130.64s/it]
 14%|█▍        | 752/5198 [3:05:43<131:35:26, 106.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.49s/it][A100%|██████████| 1/1 [02:10<00:00, 130.49s/it]
 14%|█▍        | 752/5198 [3:04:27<131:33:24, 106.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.58s/it][A100%|██████████| 1/1 [02:10<00:00, 130.58s/it]
 14%|█▍        | 752/5198 [3:06:03<131:35:25, 106.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.54s/it][A100%|██████████| 1/1 [02:10<00:00, 130.54s/it]
 14%|█▍        | 752/5198 [3:05:21<131:33:41, 106.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.51s/it][A100%|██████████| 1/1 [02:10<00:00, 130.51s/it]
 14%|█▍        | 752/5198 [3:05:23<131:33:18, 106.52s/it]Shard 752 in [76, 158, 182, 242, 293, 363, 418, 421, 664, 752, 814, 842, 991, 1266, 1366, 1425, 1464, 1574, 1728, 2166, 2441, 2563, 2739, 2854, 2894, 3089, 3181, 3395, 3576, 3831, 4300, 4589, 4947, 4950]: file /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_705 skipped to avoid exceeding cuda memory
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_706
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.58s/it][A100%|██████████| 1/1 [02:10<00:00, 130.58s/it]
 14%|█▍        | 752/5198 [3:04:50<131:34:58, 106.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.44s/it][A100%|██████████| 1/1 [01:51<00:00, 111.44s/it]
 15%|█▍        | 754/5198 [3:07:33<102:22:02, 82.93s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:57:28,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=744, skipped=0, lr=[1.9478953548951004e-05], mom=[(0.9, 0.999)]
steps: 744 loss: 0.5803 iter time (s): 110.140 samples/sec: 1.162

100%|██████████| 1/1 [01:51<00:00, 111.01s/it][A100%|██████████| 1/1 [01:51<00:00, 111.01s/it]
 15%|█▍        | 754/5198 [3:07:48<102:25:38, 82.97s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.92s/it][A100%|██████████| 1/1 [01:50<00:00, 110.92s/it]
 15%|█▍        | 754/5198 [3:07:34<102:25:30, 82.97s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.88s/it][A100%|██████████| 1/1 [01:50<00:00, 110.88s/it]
 15%|█▍        | 754/5198 [3:07:54<102:24:53, 82.96s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.05s/it][A100%|██████████| 1/1 [01:51<00:00, 111.05s/it]
 15%|█▍        | 754/5198 [3:06:18<102:26:33, 82.99s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.95s/it][A100%|██████████| 1/1 [01:50<00:00, 110.95s/it]
 15%|█▍        | 754/5198 [3:06:41<102:25:50, 82.98s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.04s/it][A100%|██████████| 1/1 [01:51<00:00, 111.04s/it]
 15%|█▍        | 754/5198 [3:07:12<102:26:41, 82.99s/it] 
100%|██████████| 1/1 [01:51<00:00, 111.04s/it][A100%|██████████| 1/1 [01:51<00:00, 111.04s/it]
 15%|█▍        | 754/5198 [3:07:14<102:26:20, 82.98s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_707

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.79s/it][A100%|██████████| 1/1 [01:31<00:00, 91.79s/it]
 15%|█▍        | 755/5198 [3:09:05<105:06:32, 85.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 02:59:00,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=745, skipped=0, lr=[1.9477147629063637e-05], mom=[(0.9, 0.999)]
steps: 745 loss: 0.5260 iter time (s): 90.450 samples/sec: 1.415

100%|██████████| 1/1 [01:31<00:00, 91.39s/it][A100%|██████████| 1/1 [01:31<00:00, 91.39s/it]
 15%|█▍        | 755/5198 [3:09:19<104:59:05, 85.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.39s/it][A100%|██████████| 1/1 [01:31<00:00, 91.39s/it]
 15%|█▍        | 755/5198 [3:09:06<104:58:55, 85.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.44s/it][A100%|██████████| 1/1 [01:31<00:00, 91.44s/it]
 15%|█▍        | 755/5198 [3:09:25<104:59:15, 85.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.35s/it][A100%|██████████| 1/1 [01:31<00:00, 91.35s/it]
 15%|█▍        | 755/5198 [3:07:49<104:58:48, 85.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.33s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 15%|█▍        | 755/5198 [3:08:13<104:57:54, 85.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.33s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 15%|█▍        | 755/5198 [3:08:44<104:58:36, 85.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.36s/it][A100%|██████████| 1/1 [01:31<00:00, 91.36s/it]
 15%|█▍        | 755/5198 [3:08:46<104:58:52, 85.06s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_708
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.81s/it][A100%|██████████| 1/1 [01:26<00:00, 86.81s/it]
 15%|█▍        | 756/5198 [3:10:31<105:39:13, 85.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:00:27,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=746, skipped=0, lr=[1.9475338668992883e-05], mom=[(0.9, 0.999)]
steps: 746 loss: 0.5713 iter time (s): 85.881 samples/sec: 1.490

100%|██████████| 1/1 [01:26<00:00, 86.76s/it][A100%|██████████| 1/1 [01:26<00:00, 86.76s/it]
 15%|█▍        | 756/5198 [3:10:46<105:30:39, 85.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.77s/it][A100%|██████████| 1/1 [01:26<00:00, 86.77s/it]
 15%|█▍        | 756/5198 [3:10:32<105:30:50, 85.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.71s/it][A100%|██████████| 1/1 [01:26<00:00, 86.71s/it]
 15%|█▍        | 756/5198 [3:10:52<105:29:54, 85.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.77s/it][A100%|██████████| 1/1 [01:26<00:00, 86.77s/it]
 15%|█▍        | 756/5198 [3:09:16<105:30:42, 85.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.82s/it][A100%|██████████| 1/1 [01:26<00:00, 86.82s/it]
 15%|█▍        | 756/5198 [3:09:39<105:30:55, 85.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.80s/it][A100%|██████████| 1/1 [01:26<00:00, 86.80s/it]
 15%|█▍        | 756/5198 [3:10:10<105:30:59, 85.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.79s/it][A100%|██████████| 1/1 [01:26<00:00, 86.79s/it]
 15%|█▍        | 756/5198 [3:10:12<105:31:04, 85.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_709
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 87.00s/it][A100%|██████████| 1/1 [01:27<00:00, 87.00s/it]
 15%|█▍        | 757/5198 [3:11:59<106:10:09, 86.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:01:54,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=747, skipped=0, lr=[1.947352666931911e-05], mom=[(0.9, 0.999)]
steps: 747 loss: 0.6526 iter time (s): 86.334 samples/sec: 1.483

100%|██████████| 1/1 [01:27<00:00, 87.20s/it][A100%|██████████| 1/1 [01:27<00:00, 87.20s/it]
 15%|█▍        | 757/5198 [3:12:13<106:03:26, 85.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.27s/it][A100%|██████████| 1/1 [01:27<00:00, 87.27s/it]
 15%|█▍        | 757/5198 [3:12:00<106:04:56, 85.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.32s/it][A100%|██████████| 1/1 [01:27<00:00, 87.32s/it]
 15%|█▍        | 757/5198 [3:12:19<106:05:25, 86.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.35s/it][A100%|██████████| 1/1 [01:27<00:00, 87.35s/it]
 15%|█▍        | 757/5198 [3:10:43<106:06:27, 86.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.38s/it][A100%|██████████| 1/1 [01:27<00:00, 87.38s/it]
 15%|█▍        | 757/5198 [3:11:07<106:07:12, 86.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.39s/it][A100%|██████████| 1/1 [01:27<00:00, 87.39s/it]
 15%|█▍        | 757/5198 [3:11:38<106:07:30, 86.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.41s/it][A100%|██████████| 1/1 [01:27<00:00, 87.41s/it]
 15%|█▍        | 757/5198 [3:11:40<106:07:51, 86.03s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_710
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.80s/it][A100%|██████████| 1/1 [01:41<00:00, 101.80s/it]
 15%|█▍        | 758/5198 [3:13:41<111:37:15, 90.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:03:36,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=748, skipped=0, lr=[1.947171163062364e-05], mom=[(0.9, 0.999)]
steps: 748 loss: 0.6035 iter time (s): 101.250 samples/sec: 1.264

100%|██████████| 1/1 [01:42<00:00, 102.31s/it][A100%|██████████| 1/1 [01:42<00:00, 102.31s/it]
 15%|█▍        | 758/5198 [3:13:55<111:40:24, 90.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.23s/it][A100%|██████████| 1/1 [01:42<00:00, 102.23s/it]
 15%|█▍        | 758/5198 [3:13:42<111:39:57, 90.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.29s/it][A100%|██████████| 1/1 [01:42<00:00, 102.29s/it]
 15%|█▍        | 758/5198 [3:14:02<111:41:30, 90.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.18s/it][A100%|██████████| 1/1 [01:42<00:00, 102.18s/it]
 15%|█▍        | 758/5198 [3:12:26<111:40:02, 90.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.21s/it][A100%|██████████| 1/1 [01:42<00:00, 102.21s/it]
 15%|█▍        | 758/5198 [3:12:49<111:41:12, 90.56s/it]
100%|██████████| 1/1 [01:42<00:00, 102.16s/it][A100%|██████████| 1/1 [01:42<00:00, 102.16s/it]
 15%|█▍        | 758/5198 [3:13:20<111:40:14, 90.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.15s/it][A100%|██████████| 1/1 [01:42<00:00, 102.15s/it]
 15%|█▍        | 758/5198 [3:13:22<111:40:18, 90.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_711
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.95s/it][A100%|██████████| 1/1 [01:22<00:00, 82.95s/it]
 15%|█▍        | 759/5198 [3:15:04<108:59:08, 88.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:04:59,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=749, skipped=0, lr=[1.9469893553488793e-05], mom=[(0.9, 0.999)]
steps: 749 loss: 0.6086 iter time (s): 81.659 samples/sec: 1.567

100%|██████████| 1/1 [01:22<00:00, 82.60s/it][A100%|██████████| 1/1 [01:22<00:00, 82.60s/it]
 15%|█▍        | 759/5198 [3:15:18<108:51:16, 88.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.67s/it][A100%|██████████| 1/1 [01:22<00:00, 82.67s/it]
 15%|█▍        | 759/5198 [3:15:05<108:52:20, 88.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.52s/it][A100%|██████████| 1/1 [01:22<00:00, 82.52s/it]
 15%|█▍        | 759/5198 [3:15:24<108:50:16, 88.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.57s/it][A100%|██████████| 1/1 [01:22<00:00, 82.57s/it]
 15%|█▍        | 759/5198 [3:13:48<108:50:20, 88.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.49s/it][A100%|██████████| 1/1 [01:22<00:00, 82.50s/it]
 15%|█▍        | 759/5198 [3:14:11<108:49:31, 88.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.53s/it][A100%|██████████| 1/1 [01:22<00:00, 82.53s/it]
 15%|█▍        | 759/5198 [3:14:43<108:49:35, 88.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.53s/it][A100%|██████████| 1/1 [01:22<00:00, 82.53s/it]
 15%|█▍        | 759/5198 [3:14:45<108:49:31, 88.26s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_712
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.99s/it][A100%|██████████| 1/1 [01:40<00:00, 100.99s/it]
[2024-06-30 03:06:40,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[1.946807243849784e-05], mom=[(0.9, 0.999)]
steps: 750 loss: 0.5668 iter time (s): 100.850 samples/sec: 1.269

100%|██████████| 1/1 [01:41<00:00, 101.69s/it][A100%|██████████| 1/1 [01:41<00:00, 101.69s/it]

100%|██████████| 1/1 [01:41<00:00, 101.75s/it][A100%|██████████| 1/1 [01:41<00:00, 101.75s/it]

100%|██████████| 1/1 [01:41<00:00, 101.77s/it][A100%|██████████| 1/1 [01:41<00:00, 101.77s/it]

100%|██████████| 1/1 [01:41<00:00, 101.79s/it][A100%|██████████| 1/1 [01:41<00:00, 101.79s/it]

100%|██████████| 1/1 [01:41<00:00, 101.81s/it][A100%|██████████| 1/1 [01:41<00:00, 101.81s/it]

100%|██████████| 1/1 [01:41<00:00, 101.83s/it][A100%|██████████| 1/1 [01:41<00:00, 101.83s/it]

100%|██████████| 1/1 [01:41<00:00, 101.81s/it][A100%|██████████| 1/1 [01:41<00:00, 101.81s/it]
Checkpointing at shard 759
[2024-06-30 03:06:41,890] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step750 is about to be saved!
[2024-06-30 03:06:43,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_00-model_states.pt...
[2024-06-30 03:06:48,103] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_02-model_states.pt...
[2024-06-30 03:06:48,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_00-model_states.pt.
[2024-06-30 03:06:55,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_04-model_states.pt...
[2024-06-30 03:06:55,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_06-model_states.pt...
[2024-06-30 03:06:55,237] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_03-model_states.pt...
[2024-06-30 03:06:55,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_08-model_states.pt...
[2024-06-30 03:06:56,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_07-model_states.pt...
[2024-06-30 03:06:56,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_05-model_states.pt...
[2024-06-30 03:06:57,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_01-model_states.pt...
[2024-06-30 03:12:12,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_07-model_states.pt.
[2024-06-30 03:12:13,110] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_05-model_states.pt.
[2024-06-30 03:12:13,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_06_model_states.pt...
[2024-06-30 03:12:14,023] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_04_model_states.pt...
[2024-06-30 03:12:14,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_06_model_states.pt.
[2024-06-30 03:12:14,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[2024-06-30 03:12:15,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_04_model_states.pt.
[2024-06-30 03:12:15,499] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[2024-06-30 03:12:15,757] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_01-model_states.pt.
[2024-06-30 03:12:16,599] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_00_model_states.pt
[2024-06-30 03:12:16,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_00_model_states.pt...
[2024-06-30 03:12:16,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_00_model_states.pt.
[2024-06-30 03:12:16,970] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[2024-06-30 03:12:21,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_04-model_states.pt.
[2024-06-30 03:12:21,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_03_model_states.pt...
[2024-06-30 03:12:21,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_03_model_states.pt.
[2024-06-30 03:12:21,881] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[2024-06-30 03:12:25,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_08-model_states.pt.
[2024-06-30 03:12:25,851] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_09-model_states.pt...
[2024-06-30 03:12:26,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_03-model_states.pt.
[2024-06-30 03:12:26,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_02_model_states.pt...
[2024-06-30 03:12:27,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_09-model_states.pt.
[2024-06-30 03:12:27,032] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_07_model_states.pt...
[2024-06-30 03:12:27,104] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_02_model_states.pt.
[2024-06-30 03:12:27,104] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[2024-06-30 03:12:27,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_07_model_states.pt.
[2024-06-30 03:12:27,196] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[2024-06-30 03:12:28,884] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_02-model_states.pt.
[2024-06-30 03:12:29,208] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_01_model_states.pt
[2024-06-30 03:12:29,208] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_01_model_states.pt...
[2024-06-30 03:12:29,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_01_model_states.pt.
[2024-06-30 03:12:29,319] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[2024-06-30 03:12:36,106] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/layer_06-model_states.pt.
[2024-06-30 03:12:36,920] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_05_model_states.pt...
[2024-06-30 03:12:36,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step750/mp_rank_05_model_states.pt.
[2024-06-30 03:12:36,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
Checkpoint saved using --- 355.0842635631561 seconds ---
 15%|█▍        | 760/5198 [3:22:43<241:33:49, 195.95s/it] 15%|█▍        | 760/5198 [3:21:25<240:32:17, 195.12s/it] 15%|█▍        | 760/5198 [3:22:55<240:40:33, 195.23s/it] 15%|█▍        | 760/5198 [3:22:19<240:29:56, 195.09s/it] 15%|█▍        | 760/5198 [3:22:21<240:29:20, 195.08s/it] 15%|█▍        | 760/5198 [3:21:48<240:30:44, 195.10s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_713
 15%|█▍        | 760/5198 [3:22:42<240:37:40, 195.19s/it] 15%|█▍        | 760/5198 [3:23:01<240:34:06, 195.14s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.39s/it][A100%|██████████| 1/1 [01:40<00:00, 100.39s/it]
 15%|█▍        | 761/5198 [3:24:24<207:06:33, 168.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:14:20,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=751, skipped=0, lr=[1.946624828623506e-05], mom=[(0.9, 0.999)]
steps: 751 loss: 0.5635 iter time (s): 102.871 samples/sec: 1.244

100%|██████████| 1/1 [01:43<00:00, 103.24s/it][A100%|██████████| 1/1 [01:43<00:00, 103.24s/it]
 15%|█▍        | 761/5198 [3:24:39<207:30:38, 168.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.36s/it][A100%|██████████| 1/1 [01:43<00:00, 103.36s/it]
 15%|█▍        | 761/5198 [3:24:25<207:31:16, 168.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.53s/it][A100%|██████████| 1/1 [01:43<00:00, 103.53s/it]
 15%|█▍        | 761/5198 [3:24:45<207:32:23, 168.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.63s/it][A100%|██████████| 1/1 [01:43<00:00, 103.63s/it]
 15%|█▍        | 761/5198 [3:23:09<207:33:24, 168.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.69s/it][A100%|██████████| 1/1 [01:43<00:00, 103.69s/it]
 15%|█▍        | 761/5198 [3:23:32<207:33:33, 168.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.74s/it][A100%|██████████| 1/1 [01:43<00:00, 103.74s/it]
 15%|█▍        | 761/5198 [3:24:03<207:33:58, 168.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.77s/it][A100%|██████████| 1/1 [01:43<00:00, 103.77s/it]
 15%|█▍        | 761/5198 [3:24:05<207:34:12, 168.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_714
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.57s/it][A100%|██████████| 1/1 [01:44<00:00, 104.57s/it]
 15%|█▍        | 762/5198 [3:26:09<184:03:08, 149.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:16:04,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=752, skipped=0, lr=[1.9464421097285668e-05], mom=[(0.9, 0.999)]
steps: 752 loss: 0.6002 iter time (s): 103.752 samples/sec: 1.234

100%|██████████| 1/1 [01:44<00:00, 104.65s/it][A100%|██████████| 1/1 [01:44<00:00, 104.65s/it]
 15%|█▍        | 762/5198 [3:26:23<184:18:51, 149.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.65s/it][A100%|██████████| 1/1 [01:44<00:00, 104.65s/it]
 15%|█▍        | 762/5198 [3:26:10<184:19:20, 149.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.61s/it][A100%|██████████| 1/1 [01:44<00:00, 104.61s/it]
 15%|█▍        | 762/5198 [3:26:30<184:19:13, 149.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.70s/it][A100%|██████████| 1/1 [01:44<00:00, 104.70s/it]
 15%|█▍        | 762/5198 [3:24:54<184:21:51, 149.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.66s/it][A100%|██████████| 1/1 [01:44<00:00, 104.66s/it]
 15%|█▍        | 762/5198 [3:25:17<184:21:04, 149.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.66s/it][A100%|██████████| 1/1 [01:44<00:00, 104.67s/it]
 15%|█▍        | 762/5198 [3:25:48<184:21:29, 149.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.66s/it][A100%|██████████| 1/1 [01:44<00:00, 104.66s/it]
 15%|█▍        | 762/5198 [3:25:50<184:21:27, 149.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_715
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.95s/it][A100%|██████████| 1/1 [01:32<00:00, 92.95s/it]
 15%|█▍        | 763/5198 [3:27:42<163:27:49, 132.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:17:37,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=753, skipped=0, lr=[1.946259087223588e-05], mom=[(0.9, 0.999)]
steps: 753 loss: 0.5690 iter time (s): 91.888 samples/sec: 1.393

100%|██████████| 1/1 [01:32<00:00, 92.89s/it][A100%|██████████| 1/1 [01:32<00:00, 92.89s/it]
 15%|█▍        | 763/5198 [3:27:56<163:34:32, 132.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.79s/it][A100%|██████████| 1/1 [01:32<00:00, 92.79s/it]
 15%|█▍        | 763/5198 [3:27:43<163:32:53, 132.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.85s/it][A100%|██████████| 1/1 [01:32<00:00, 92.85s/it]
 15%|█▍        | 763/5198 [3:28:02<163:33:54, 132.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.72s/it][A100%|██████████| 1/1 [01:32<00:00, 92.72s/it]
 15%|█▍        | 763/5198 [3:26:27<163:33:01, 132.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.78s/it][A100%|██████████| 1/1 [01:32<00:00, 92.78s/it]
 15%|█▍        | 763/5198 [3:26:50<163:33:37, 132.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.74s/it][A100%|██████████| 1/1 [01:32<00:00, 92.74s/it]
 15%|█▍        | 763/5198 [3:27:21<163:33:09, 132.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.77s/it][A100%|██████████| 1/1 [01:32<00:00, 92.78s/it]
 15%|█▍        | 763/5198 [3:27:23<163:33:48, 132.77s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_716
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.52s/it][A100%|██████████| 1/1 [01:59<00:00, 119.52s/it]
 15%|█▍        | 764/5198 [3:29:41<158:39:10, 128.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:19:37,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=754, skipped=0, lr=[1.9460757611672877e-05], mom=[(0.9, 0.999)]
steps: 754 loss: 0.5865 iter time (s): 119.249 samples/sec: 1.073

100%|██████████| 1/1 [02:00<00:00, 120.11s/it][A100%|██████████| 1/1 [02:00<00:00, 120.11s/it]
 15%|█▍        | 764/5198 [3:29:56<158:53:56, 129.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.14s/it][A100%|██████████| 1/1 [02:00<00:00, 120.14s/it]
 15%|█▍        | 764/5198 [3:29:43<158:53:27, 129.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.09s/it][A100%|██████████| 1/1 [02:00<00:00, 120.09s/it]
 15%|█▍        | 764/5198 [3:28:27<158:52:29, 128.99s/it]
100%|██████████| 1/1 [02:00<00:00, 120.16s/it][A100%|██████████| 1/1 [02:00<00:00, 120.16s/it]
 15%|█▍        | 764/5198 [3:30:03<158:54:44, 129.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.14s/it][A100%|██████████| 1/1 [02:00<00:00, 120.14s/it]
 15%|█▍        | 764/5198 [3:28:50<158:54:01, 129.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.15s/it][A100%|██████████| 1/1 [02:00<00:00, 120.15s/it]
 15%|█▍        | 764/5198 [3:29:21<158:53:55, 129.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.12s/it][A100%|██████████| 1/1 [02:00<00:00, 120.13s/it]
 15%|█▍        | 764/5198 [3:29:23<158:53:47, 129.01s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_717
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.38s/it][A100%|██████████| 1/1 [02:02<00:00, 122.38s/it]
 15%|█▍        | 765/5198 [3:31:45<156:31:48, 127.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:21:40,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=755, skipped=0, lr=[1.945892131618481e-05], mom=[(0.9, 0.999)]
steps: 755 loss: 0.5844 iter time (s): 121.681 samples/sec: 1.052

100%|██████████| 1/1 [02:02<00:00, 122.63s/it][A100%|██████████| 1/1 [02:02<00:00, 122.63s/it]
 15%|█▍        | 765/5198 [3:31:59<156:31:28, 127.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.69s/it][A100%|██████████| 1/1 [02:02<00:00, 122.69s/it]
 15%|█▍        | 765/5198 [3:31:46<156:32:18, 127.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.66s/it][A100%|██████████| 1/1 [02:02<00:00, 122.66s/it]
 15%|█▍        | 765/5198 [3:32:05<156:32:39, 127.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.76s/it][A100%|██████████| 1/1 [02:02<00:00, 122.76s/it]
 15%|█▍        | 765/5198 [3:30:29<156:33:14, 127.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.75s/it][A100%|██████████| 1/1 [02:02<00:00, 122.75s/it]
 15%|█▍        | 765/5198 [3:30:53<156:34:00, 127.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [02:02<00:00, 122.78s/it][A100%|██████████| 1/1 [02:02<00:00, 122.81s/it][A100%|██████████| 1/1 [02:02<00:00, 122.78s/it]
 15%|█▍        | 765/5198 [3:31:26<156:34:33, 127.15s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_718
100%|██████████| 1/1 [02:02<00:00, 122.81s/it]
 15%|█▍        | 765/5198 [3:31:24<156:35:12, 127.16s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.64s/it][A100%|██████████| 1/1 [01:40<00:00, 100.64s/it]
 15%|█▍        | 766/5198 [3:33:25<146:48:48, 119.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:23:21,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=756, skipped=0, lr=[1.9457081986360816e-05], mom=[(0.9, 0.999)]
steps: 756 loss: 0.5777 iter time (s): 99.736 samples/sec: 1.283

100%|██████████| 1/1 [01:40<00:00, 100.89s/it][A100%|██████████| 1/1 [01:40<00:00, 100.89s/it]
 15%|█▍        | 766/5198 [3:33:40<146:50:58, 119.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.79s/it][A100%|██████████| 1/1 [01:40<00:00, 100.79s/it]
 15%|█▍        | 766/5198 [3:33:46<146:49:38, 119.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.99s/it][A100%|██████████| 1/1 [01:40<00:00, 100.99s/it]
 15%|█▍        | 766/5198 [3:33:27<146:53:38, 119.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.86s/it][A100%|██████████| 1/1 [01:40<00:00, 100.87s/it]
 15%|█▍        | 766/5198 [3:32:10<146:51:31, 119.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.83s/it][A100%|██████████| 1/1 [01:40<00:00, 100.83s/it]
 15%|█▍        | 766/5198 [3:33:05<146:52:21, 119.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.94s/it][A100%|██████████| 1/1 [01:40<00:00, 100.94s/it]
 15%|█▍        | 766/5198 [3:32:34<146:53:39, 119.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.87s/it][A100%|██████████| 1/1 [01:40<00:00, 100.87s/it]
 15%|█▍        | 766/5198 [3:33:07<146:52:42, 119.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_719
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.53s/it][A100%|██████████| 1/1 [01:32<00:00, 92.53s/it]
 15%|█▍        | 767/5198 [3:34:58<136:59:22, 111.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:24:53,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=757, skipped=0, lr=[1.945523962279099e-05], mom=[(0.9, 0.999)]
steps: 757 loss: 0.6089 iter time (s): 91.546 samples/sec: 1.398

100%|██████████| 1/1 [01:32<00:00, 92.54s/it][A100%|██████████| 1/1 [01:32<00:00, 92.54s/it]
 15%|█▍        | 767/5198 [3:35:13<136:58:25, 111.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 15%|█▍        | 767/5198 [3:34:59<136:57:24, 111.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.57s/it][A100%|██████████| 1/1 [01:32<00:00, 92.57s/it]
 15%|█▍        | 767/5198 [3:35:19<136:58:06, 111.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.47s/it][A100%|██████████| 1/1 [01:32<00:00, 92.47s/it]
 15%|█▍        | 767/5198 [3:33:43<136:57:19, 111.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.39s/it][A100%|██████████| 1/1 [01:32<00:00, 92.39s/it]
 15%|█▍        | 767/5198 [3:34:06<136:56:55, 111.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.44s/it][A100%|██████████| 1/1 [01:32<00:00, 92.44s/it]
 15%|█▍        | 767/5198 [3:34:37<136:57:14, 111.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.44s/it][A100%|██████████| 1/1 [01:32<00:00, 92.44s/it]
 15%|█▍        | 767/5198 [3:34:39<136:57:25, 111.27s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_47
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.12s/it][A100%|██████████| 1/1 [02:01<00:00, 121.12s/it]
 15%|█▍        | 768/5198 [3:36:59<140:37:40, 114.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:26:55,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=758, skipped=0, lr=[1.945339422606641e-05], mom=[(0.9, 0.999)]
steps: 758 loss: 0.8333 iter time (s): 120.893 samples/sec: 1.059

100%|██████████| 1/1 [02:01<00:00, 121.86s/it][A100%|██████████| 1/1 [02:01<00:00, 121.86s/it]
 15%|█▍        | 768/5198 [3:37:14<140:50:26, 114.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.88s/it][A100%|██████████| 1/1 [02:01<00:00, 121.88s/it]
 15%|█▍        | 768/5198 [3:37:01<140:50:27, 114.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.86s/it][A100%|██████████| 1/1 [02:01<00:00, 121.86s/it]
 15%|█▍        | 768/5198 [3:37:21<140:50:25, 114.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.97s/it][A100%|██████████| 1/1 [02:01<00:00, 121.97s/it]
 15%|█▍        | 768/5198 [3:35:45<140:52:08, 114.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.90s/it][A100%|██████████| 1/1 [02:01<00:00, 121.90s/it]
 15%|█▍        | 768/5198 [3:36:41<140:50:33, 114.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_720


100%|██████████| 1/1 [02:01<00:00, 121.98s/it][A100%|██████████| 1/1 [02:01<00:00, 121.93s/it][A100%|██████████| 1/1 [02:01<00:00, 121.98s/it]
100%|██████████| 1/1 [02:01<00:00, 121.93s/it]
 15%|█▍        | 768/5198 [3:36:08<140:52:01, 114.47s/it] 15%|█▍        | 768/5198 [3:36:39<140:51:17, 114.46s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.89s/it][A100%|██████████| 1/1 [01:37<00:00, 97.89s/it]
 15%|█▍        | 769/5198 [3:38:37<134:38:05, 109.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:28:33,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=759, skipped=0, lr=[1.9451545796779134e-05], mom=[(0.9, 0.999)]
steps: 759 loss: 0.5914 iter time (s): 96.707 samples/sec: 1.324

100%|██████████| 1/1 [01:37<00:00, 97.55s/it][A100%|██████████| 1/1 [01:37<00:00, 97.55s/it]
 15%|█▍        | 769/5198 [3:38:52<134:35:08, 109.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.56s/it][A100%|██████████| 1/1 [01:37<00:00, 97.56s/it]
 15%|█▍        | 769/5198 [3:38:39<134:35:12, 109.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.58s/it][A100%|██████████| 1/1 [01:37<00:00, 97.58s/it]
 15%|█▍        | 769/5198 [3:38:58<134:35:45, 109.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.59s/it][A100%|██████████| 1/1 [01:37<00:00, 97.59s/it]
 15%|█▍        | 769/5198 [3:37:22<134:37:01, 109.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.55s/it][A
100%|██████████| 1/1 [01:37<00:00, 97.55s/it]
 15%|█▍        | 769/5198 [3:38:17<134:35:25, 109.40s/it]100%|██████████| 1/1 [01:37<00:00, 97.55s/it][A100%|██████████| 1/1 [01:37<00:00, 97.55s/it]
 15%|█▍        | 769/5198 [3:37:46<134:35:56, 109.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.59s/it][A100%|██████████| 1/1 [01:37<00:00, 97.59s/it]
 15%|█▍        | 769/5198 [3:38:19<134:35:55, 109.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_721
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.35s/it][A100%|██████████| 1/1 [01:40<00:00, 100.35s/it]
 15%|█▍        | 770/5198 [3:40:18<131:18:52, 106.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:30:13,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[1.9449694335522173e-05], mom=[(0.9, 0.999)]
steps: 760 loss: 0.5819 iter time (s): 99.706 samples/sec: 1.284

100%|██████████| 1/1 [01:40<00:00, 100.64s/it][A100%|██████████| 1/1 [01:40<00:00, 100.64s/it]
 15%|█▍        | 770/5198 [3:40:33<131:19:52, 106.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.60s/it][A100%|██████████| 1/1 [01:40<00:00, 100.60s/it]
 15%|█▍        | 770/5198 [3:40:19<131:19:02, 106.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.59s/it][A100%|██████████| 1/1 [01:40<00:00, 100.60s/it]
 15%|█▍        | 770/5198 [3:40:39<131:19:18, 106.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.58s/it][A100%|██████████| 1/1 [01:40<00:00, 100.58s/it]
 15%|█▍        | 770/5198 [3:39:03<131:19:56, 106.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.55s/it][A100%|██████████| 1/1 [01:40<00:00, 100.55s/it]
 15%|█▍        | 770/5198 [3:39:26<131:18:23, 106.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.61s/it][A100%|██████████| 1/1 [01:40<00:00, 100.61s/it]
 15%|█▍        | 770/5198 [3:39:57<131:19:20, 106.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.58s/it][A100%|██████████| 1/1 [01:40<00:00, 100.58s/it]
 15%|█▍        | 770/5198 [3:39:59<131:19:06, 106.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_722
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.59s/it][A100%|██████████| 1/1 [01:26<00:00, 86.59s/it]
 15%|█▍        | 771/5198 [3:41:45<123:53:44, 100.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:31:40,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=761, skipped=0, lr=[1.9447839842889523e-05], mom=[(0.9, 0.999)]
steps: 761 loss: 0.6285 iter time (s): 85.445 samples/sec: 1.498

100%|██████████| 1/1 [01:26<00:00, 86.38s/it][A100%|██████████| 1/1 [01:26<00:00, 86.38s/it]
 15%|█▍        | 771/5198 [3:41:59<123:47:12, 100.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.39s/it][A100%|██████████| 1/1 [01:26<00:00, 86.39s/it]
 15%|█▍        | 771/5198 [3:41:46<123:46:51, 100.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.36s/it][A100%|██████████| 1/1 [01:26<00:00, 86.36s/it]
 15%|█▍        | 771/5198 [3:42:05<123:46:21, 100.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.34s/it][A100%|██████████| 1/1 [01:26<00:00, 86.34s/it]
 15%|█▍        | 771/5198 [3:40:29<123:46:20, 100.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.34s/it][A100%|██████████| 1/1 [01:26<00:00, 86.34s/it]
 15%|█▍        | 771/5198 [3:40:52<123:45:10, 100.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.33s/it][A100%|██████████| 1/1 [01:26<00:00, 86.33s/it]
 15%|█▍        | 771/5198 [3:41:24<123:45:37, 100.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.35s/it][A100%|██████████| 1/1 [01:26<00:00, 86.35s/it]
 15%|█▍        | 771/5198 [3:41:26<123:45:56, 100.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_723
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.67s/it][A100%|██████████| 1/1 [01:29<00:00, 89.67s/it]
 15%|█▍        | 772/5198 [3:43:14<119:49:49, 97.47s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:33:10,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=762, skipped=0, lr=[1.9445982319476153e-05], mom=[(0.9, 0.999)]
steps: 762 loss: 0.5480 iter time (s): 88.932 samples/sec: 1.439

100%|██████████| 1/1 [01:29<00:00, 89.81s/it][A100%|██████████| 1/1 [01:29<00:00, 89.81s/it]
 15%|█▍        | 772/5198 [3:43:29<119:45:41, 97.41s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.94s/it][A100%|██████████| 1/1 [01:29<00:00, 89.94s/it]
 15%|█▍        | 772/5198 [3:43:16<119:48:10, 97.44s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.89s/it][A100%|██████████| 1/1 [01:29<00:00, 89.89s/it]
 15%|█▍        | 772/5198 [3:43:35<119:46:59, 97.43s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.85s/it][A100%|██████████| 1/1 [01:29<00:00, 89.85s/it]
 15%|█▍        | 772/5198 [3:41:59<119:46:10, 97.42s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.91s/it][A100%|██████████| 1/1 [01:29<00:00, 89.91s/it]
 15%|█▍        | 772/5198 [3:42:22<119:46:24, 97.42s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.89s/it][A100%|██████████| 1/1 [01:29<00:00, 89.89s/it]
 15%|█▍        | 772/5198 [3:42:53<119:46:13, 97.42s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.88s/it][A100%|██████████| 1/1 [01:29<00:00, 89.88s/it]
 15%|█▍        | 772/5198 [3:42:55<119:46:23, 97.42s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_724
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.19s/it][A100%|██████████| 1/1 [01:29<00:00, 89.19s/it]
 15%|█▍        | 773/5198 [3:44:44<116:48:16, 95.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:34:39,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=763, skipped=0, lr=[1.9444121765878002e-05], mom=[(0.9, 0.999)]
steps: 763 loss: 0.5490 iter time (s): 88.367 samples/sec: 1.448

100%|██████████| 1/1 [01:29<00:00, 89.22s/it][A100%|██████████| 1/1 [01:29<00:00, 89.22s/it]
 15%|█▍        | 773/5198 [3:44:58<116:42:59, 94.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.14s/it][A100%|██████████| 1/1 [01:29<00:00, 89.14s/it]
 15%|█▍        | 773/5198 [3:44:45<116:43:01, 94.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.22s/it][A100%|██████████| 1/1 [01:29<00:00, 89.22s/it]
 15%|█▍        | 773/5198 [3:45:04<116:43:55, 94.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.24s/it][A100%|██████████| 1/1 [01:29<00:00, 89.24s/it]
 15%|█▍        | 773/5198 [3:43:28<116:43:48, 94.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.25s/it][A100%|██████████| 1/1 [01:29<00:00, 89.25s/it]
 15%|█▍        | 773/5198 [3:43:52<116:44:10, 94.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.27s/it][A100%|██████████| 1/1 [01:29<00:00, 89.27s/it]
 15%|█▍        | 773/5198 [3:44:23<116:44:33, 94.98s/it]
100%|██████████| 1/1 [01:29<00:00, 89.24s/it][A100%|██████████| 1/1 [01:29<00:00, 89.24s/it]
 15%|█▍        | 773/5198 [3:44:25<116:43:59, 94.97s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_725

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.52s/it][A100%|██████████| 1/1 [02:01<00:00, 121.52s/it]
 15%|█▍        | 774/5198 [3:46:45<126:35:36, 103.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:36:41,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=764, skipped=0, lr=[1.944225818269199e-05], mom=[(0.9, 0.999)]
steps: 764 loss: 0.5724 iter time (s): 121.656 samples/sec: 1.052

100%|██████████| 1/1 [02:02<00:00, 122.65s/it][A100%|██████████| 1/1 [02:02<00:00, 122.65s/it]
 15%|█▍        | 774/5198 [3:47:01<126:54:09, 103.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.60s/it][A100%|██████████| 1/1 [02:02<00:00, 122.60s/it]
 15%|█▍        | 774/5198 [3:46:47<126:52:58, 103.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.70s/it][A100%|██████████| 1/1 [02:02<00:00, 122.70s/it]
 15%|█▍        | 774/5198 [3:47:07<126:55:49, 103.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.70s/it][A100%|██████████| 1/1 [02:02<00:00, 122.70s/it]
 15%|█▍        | 774/5198 [3:45:31<126:55:45, 103.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.65s/it][A100%|██████████| 1/1 [02:02<00:00, 122.65s/it]
 15%|█▍        | 774/5198 [3:45:54<126:54:55, 103.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.64s/it][A100%|██████████| 1/1 [02:02<00:00, 122.64s/it]
 15%|█▍        | 774/5198 [3:46:25<126:54:50, 103.28s/it]
100%|██████████| 1/1 [02:02<00:00, 122.64s/it][A100%|██████████| 1/1 [02:02<00:00, 122.64s/it]
 15%|█▍        | 774/5198 [3:46:27<126:54:29, 103.27s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_726

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.15s/it][A100%|██████████| 1/1 [01:33<00:00, 93.15s/it]
 15%|█▍        | 775/5198 [3:48:19<122:59:14, 100.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:38:14,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=765, skipped=0, lr=[1.9440391570515985e-05], mom=[(0.9, 0.999)]
steps: 765 loss: 0.6124 iter time (s): 91.531 samples/sec: 1.398

100%|██████████| 1/1 [01:32<00:00, 92.46s/it][A100%|██████████| 1/1 [01:32<00:00, 92.46s/it]
 15%|█▍        | 775/5198 [3:48:33<122:53:52, 100.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.58s/it][A100%|██████████| 1/1 [01:32<00:00, 92.58s/it]
 15%|█▍        | 775/5198 [3:48:20<122:55:25, 100.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.50s/it][A100%|██████████| 1/1 [01:32<00:00, 92.50s/it]
 15%|█▍        | 775/5198 [3:48:39<122:55:41, 100.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.44s/it][A100%|██████████| 1/1 [01:32<00:00, 92.44s/it]
 15%|█▍        | 775/5198 [3:47:04<122:54:17, 100.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.47s/it][A100%|██████████| 1/1 [01:32<00:00, 92.47s/it]
 15%|█▍        | 775/5198 [3:47:27<122:54:20, 100.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.52s/it][A100%|██████████| 1/1 [01:32<00:00, 92.52s/it]
 15%|█▍        | 775/5198 [3:47:58<122:55:24, 100.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.54s/it][A100%|██████████| 1/1 [01:32<00:00, 92.54s/it]
 15%|█▍        | 775/5198 [3:48:00<122:55:42, 100.05s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_727
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.01s/it][A100%|██████████| 1/1 [01:34<00:00, 94.01s/it]
 15%|█▍        | 776/5198 [3:49:53<120:47:34, 98.34s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:39:48,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=766, skipped=0, lr=[1.9438521929948858e-05], mom=[(0.9, 0.999)]
steps: 766 loss: 0.5767 iter time (s): 93.247 samples/sec: 1.373

100%|██████████| 1/1 [01:34<00:00, 94.24s/it][A100%|██████████| 1/1 [01:34<00:00, 94.24s/it]
 15%|█▍        | 776/5198 [3:50:07<120:44:35, 98.30s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.13s/it][A100%|██████████| 1/1 [01:34<00:00, 94.13s/it]
 15%|█▍        | 776/5198 [3:49:54<120:43:00, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.12s/it][A100%|██████████| 1/1 [01:34<00:00, 94.12s/it]
 15%|█▍        | 776/5198 [3:50:14<120:42:54, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.17s/it][A100%|██████████| 1/1 [01:34<00:00, 94.17s/it]
 15%|█▍        | 776/5198 [3:48:38<120:43:19, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.18s/it][A100%|██████████| 1/1 [01:34<00:00, 94.18s/it]
 15%|█▍        | 776/5198 [3:49:01<120:43:32, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.15s/it][A100%|██████████| 1/1 [01:34<00:00, 94.15s/it]
 15%|█▍        | 776/5198 [3:49:32<120:43:24, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.14s/it][A100%|██████████| 1/1 [01:34<00:00, 94.14s/it]
 15%|█▍        | 776/5198 [3:49:34<120:43:24, 98.28s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_728
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.56s/it][A100%|██████████| 1/1 [01:35<00:00, 95.56s/it]
 15%|█▍        | 777/5198 [3:51:29<119:46:58, 97.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:41:24,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=767, skipped=0, lr=[1.9436649261590425e-05], mom=[(0.9, 0.999)]
steps: 767 loss: 0.5746 iter time (s): 94.850 samples/sec: 1.350

100%|██████████| 1/1 [01:35<00:00, 95.66s/it][A100%|██████████| 1/1 [01:35<00:00, 95.66s/it]
 15%|█▍        | 777/5198 [3:51:43<119:45:01, 97.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.77s/it][A100%|██████████| 1/1 [01:35<00:00, 95.77s/it]
 15%|█▍        | 777/5198 [3:51:30<119:46:06, 97.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.77s/it][A100%|██████████| 1/1 [01:35<00:00, 95.77s/it]
 15%|█▍        | 777/5198 [3:51:49<119:46:08, 97.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.70s/it][A100%|██████████| 1/1 [01:35<00:00, 95.70s/it]
 15%|█▍        | 777/5198 [3:50:13<119:44:51, 97.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.74s/it][A100%|██████████| 1/1 [01:35<00:00, 95.74s/it]
 15%|█▍        | 777/5198 [3:50:37<119:45:46, 97.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.72s/it][A100%|██████████| 1/1 [01:35<00:00, 95.72s/it]
 15%|█▍        | 777/5198 [3:51:08<119:45:22, 97.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.74s/it][A100%|██████████| 1/1 [01:35<00:00, 95.74s/it]
 15%|█▍        | 777/5198 [3:51:10<119:45:40, 97.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_729
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.93s/it][A100%|██████████| 1/1 [01:26<00:00, 86.93s/it]
 15%|█▍        | 778/5198 [3:52:56<115:54:39, 94.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:42:51,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=768, skipped=0, lr=[1.9434773566041492e-05], mom=[(0.9, 0.999)]
steps: 768 loss: 0.5942 iter time (s): 85.945 samples/sec: 1.489

100%|██████████| 1/1 [01:26<00:00, 86.91s/it][A100%|██████████| 1/1 [01:26<00:00, 86.91s/it]
 15%|█▍        | 778/5198 [3:53:10<115:49:13, 94.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.83s/it][A100%|██████████| 1/1 [01:26<00:00, 86.83s/it]
 15%|█▍        | 778/5198 [3:52:57<115:48:11, 94.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.82s/it][A100%|██████████| 1/1 [01:26<00:00, 86.82s/it]
 15%|█▍        | 778/5198 [3:53:16<115:48:02, 94.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.84s/it][A100%|██████████| 1/1 [01:26<00:00, 86.84s/it]
 15%|█▍        | 778/5198 [3:51:40<115:47:37, 94.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.88s/it][A100%|██████████| 1/1 [01:26<00:00, 86.88s/it]
 15%|█▍        | 778/5198 [3:52:04<115:49:14, 94.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.87s/it][A100%|██████████| 1/1 [01:26<00:00, 86.87s/it]
 15%|█▍        | 778/5198 [3:52:35<115:48:40, 94.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.85s/it][A100%|██████████| 1/1 [01:26<00:00, 86.85s/it]
 15%|█▍        | 778/5198 [3:52:37<115:48:26, 94.32s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_730
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.42s/it][A100%|██████████| 1/1 [01:35<00:00, 95.42s/it]
 15%|█▍        | 779/5198 [3:54:31<116:18:02, 94.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:44:26,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=769, skipped=0, lr=[1.9432894843903823e-05], mom=[(0.9, 0.999)]
steps: 769 loss: 0.6099 iter time (s): 94.746 samples/sec: 1.351

100%|██████████| 1/1 [01:35<00:00, 95.62s/it][A100%|██████████| 1/1 [01:35<00:00, 95.62s/it]
 15%|█▍        | 779/5198 [3:54:46<116:16:27, 94.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.68s/it][A100%|██████████| 1/1 [01:35<00:00, 95.68s/it]
 15%|█▍        | 779/5198 [3:54:32<116:17:03, 94.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.64s/it][A100%|██████████| 1/1 [01:35<00:00, 95.64s/it]
 15%|█▍        | 779/5198 [3:54:52<116:15:49, 94.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.71s/it][A100%|██████████| 1/1 [01:35<00:00, 95.71s/it]
 15%|█▍        | 779/5198 [3:53:16<116:17:18, 94.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.61s/it][A100%|██████████| 1/1 [01:35<00:00, 95.61s/it]
 15%|█▍        | 779/5198 [3:53:39<116:16:10, 94.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.64s/it][A100%|██████████| 1/1 [01:35<00:00, 95.64s/it]
 15%|█▍        | 779/5198 [3:54:10<116:16:21, 94.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.66s/it][A100%|██████████| 1/1 [01:35<00:00, 95.67s/it]
 15%|█▍        | 779/5198 [3:54:12<116:16:42, 94.73s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_731
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.87s/it][A100%|██████████| 1/1 [01:59<00:00, 119.87s/it]
[2024-06-30 03:46:27,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[1.943101309578016e-05], mom=[(0.9, 0.999)]
steps: 770 loss: 0.5648 iter time (s): 119.939 samples/sec: 1.067

100%|██████████| 1/1 [02:00<00:00, 120.74s/it][A100%|██████████| 1/1 [02:00<00:00, 120.74s/it]

100%|██████████| 1/1 [02:00<00:00, 120.82s/it][A100%|██████████| 1/1 [02:00<00:00, 120.82s/it]

100%|██████████| 1/1 [02:00<00:00, 120.85s/it][A100%|██████████| 1/1 [02:00<00:00, 120.85s/it]

100%|██████████| 1/1 [02:00<00:00, 120.85s/it][A100%|██████████| 1/1 [02:00<00:00, 120.85s/it]

100%|██████████| 1/1 [02:00<00:00, 120.82s/it][A100%|██████████| 1/1 [02:00<00:00, 120.82s/it]

100%|██████████| 1/1 [02:00<00:00, 120.88s/it][A100%|██████████| 1/1 [02:00<00:00, 120.88s/it]

100%|██████████| 1/1 [02:00<00:00, 120.82s/it][A100%|██████████| 1/1 [02:00<00:00, 120.82s/it]
Checkpointing at shard 779
[2024-06-30 03:46:28,703] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step770 is about to be saved!
[2024-06-30 03:46:30,240] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_00-model_states.pt...
[2024-06-30 03:46:35,250] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_02-model_states.pt...
[2024-06-30 03:46:35,258] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_00-model_states.pt.
[2024-06-30 03:46:38,841] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_08-model_states.pt...
[2024-06-30 03:46:39,551] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_07-model_states.pt...
[2024-06-30 03:46:41,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_03-model_states.pt...
[2024-06-30 03:46:41,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_04-model_states.pt...
[2024-06-30 03:46:42,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_06-model_states.pt...
[2024-06-30 03:46:44,111] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_05-model_states.pt...
[2024-06-30 03:46:45,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_01-model_states.pt...
[2024-06-30 03:51:09,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_01-model_states.pt.
[2024-06-30 03:51:10,135] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_00_model_states.pt
[2024-06-30 03:51:10,135] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_00_model_states.pt...
[2024-06-30 03:51:10,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_00_model_states.pt.
[2024-06-30 03:51:10,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
[2024-06-30 03:51:11,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_05-model_states.pt.
[2024-06-30 03:51:12,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_04_model_states.pt...
[2024-06-30 03:51:12,510] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_04_model_states.pt.
[2024-06-30 03:51:12,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
[2024-06-30 03:51:17,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_02-model_states.pt.
[2024-06-30 03:51:17,581] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_06-model_states.pt.
[2024-06-30 03:51:17,907] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_01_model_states.pt
[2024-06-30 03:51:17,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_01_model_states.pt...
[2024-06-30 03:51:18,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_01_model_states.pt.
[2024-06-30 03:51:18,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
[2024-06-30 03:51:18,249] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_05_model_states.pt...
[2024-06-30 03:51:18,465] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_05_model_states.pt.
[2024-06-30 03:51:18,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
[2024-06-30 03:51:22,172] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_04-model_states.pt.
[2024-06-30 03:51:22,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_03_model_states.pt...
[2024-06-30 03:51:22,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_03_model_states.pt.
[2024-06-30 03:51:22,882] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
[2024-06-30 03:51:23,216] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_03-model_states.pt.
[2024-06-30 03:51:23,919] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_02_model_states.pt...
[2024-06-30 03:51:24,033] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_02_model_states.pt.
[2024-06-30 03:51:24,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
[2024-06-30 03:51:27,098] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_07-model_states.pt.
[2024-06-30 03:51:28,478] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_06_model_states.pt...
[2024-06-30 03:51:28,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_06_model_states.pt.
[2024-06-30 03:51:28,587] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
[2024-06-30 03:51:28,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_08-model_states.pt.
[2024-06-30 03:51:29,223] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_09-model_states.pt...
[2024-06-30 03:51:29,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/layer_09-model_states.pt.
[2024-06-30 03:51:29,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_07_model_states.pt...
[2024-06-30 03:51:30,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step770/mp_rank_07_model_states.pt.
[2024-06-30 03:51:30,040] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step770 is ready now!
Checkpoint saved using --- 301.337495803833 seconds ---
 15%|█▌        | 780/5198 [4:01:13<236:48:31, 192.96s/it] 15%|█▌        | 780/5198 [4:01:48<236:59:20, 193.11s/it] 15%|█▌        | 780/5198 [4:00:18<236:50:58, 193.00s/it] 15%|█▌        | 780/5198 [4:01:36<237:55:29, 193.87s/it] 15%|█▌        | 780/5198 [4:01:35<236:56:33, 193.07s/it] 15%|█▌        | 780/5198 [4:00:41<236:49:23, 192.98s/it] 15%|█▌        | 780/5198 [4:01:14<236:47:58, 192.96s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_732
 15%|█▌        | 780/5198 [4:01:54<236:53:22, 193.03s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.88s/it][A100%|██████████| 1/1 [01:31<00:00, 91.88s/it]
 15%|█▌        | 781/5198 [4:03:08<200:24:11, 163.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:53:04,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=771, skipped=0, lr=[1.9429128322274217e-05], mom=[(0.9, 0.999)]
steps: 771 loss: 0.5673 iter time (s): 94.066 samples/sec: 1.361

100%|██████████| 1/1 [01:34<00:00, 94.45s/it][A100%|██████████| 1/1 [01:34<00:00, 94.45s/it]
 15%|█▌        | 781/5198 [4:03:23<200:41:36, 163.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.61s/it][A100%|██████████| 1/1 [01:34<00:00, 94.61s/it]
 15%|█▌        | 781/5198 [4:03:10<200:42:59, 163.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.72s/it][A100%|██████████| 1/1 [01:34<00:00, 94.72s/it]
 15%|█▌        | 781/5198 [4:03:29<200:43:27, 163.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.88s/it][A100%|██████████| 1/1 [01:34<00:00, 94.88s/it]
 15%|█▌        | 781/5198 [4:01:53<200:45:18, 163.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.93s/it][A100%|██████████| 1/1 [01:34<00:00, 94.93s/it]
 15%|█▌        | 781/5198 [4:02:17<200:45:15, 163.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.95s/it][A100%|██████████| 1/1 [01:34<00:00, 94.95s/it]
 15%|█▌        | 781/5198 [4:02:48<200:45:12, 163.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.00s/it][A100%|██████████| 1/1 [01:35<00:00, 95.00s/it]
 15%|█▌        | 781/5198 [4:02:50<200:45:54, 163.63s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_733
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.93s/it][A100%|██████████| 1/1 [01:20<00:00, 80.93s/it]
 15%|█▌        | 782/5198 [4:04:30<170:06:32, 138.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:54:25,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=772, skipped=0, lr=[1.9427240523990677e-05], mom=[(0.9, 0.999)]
steps: 772 loss: 0.6015 iter time (s): 79.809 samples/sec: 1.604

100%|██████████| 1/1 [01:20<00:00, 80.68s/it][A100%|██████████| 1/1 [01:20<00:00, 80.68s/it]
 15%|█▌        | 782/5198 [4:04:44<170:08:45, 138.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.72s/it][A100%|██████████| 1/1 [01:20<00:00, 80.72s/it]
 15%|█▌        | 782/5198 [4:04:30<170:10:40, 138.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.69s/it][A100%|██████████| 1/1 [01:20<00:00, 80.69s/it]
 15%|█▌        | 782/5198 [4:04:50<170:10:20, 138.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.65s/it][A100%|██████████| 1/1 [01:20<00:00, 80.65s/it]
 15%|█▌        | 782/5198 [4:03:14<170:10:54, 138.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.67s/it][A100%|██████████| 1/1 [01:20<00:00, 80.67s/it]
 15%|█▌        | 782/5198 [4:03:37<170:11:12, 138.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.67s/it][A100%|██████████| 1/1 [01:20<00:00, 80.67s/it]
 15%|█▌        | 782/5198 [4:04:08<170:11:09, 138.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.68s/it][A100%|██████████| 1/1 [01:20<00:00, 80.68s/it]
 15%|█▌        | 782/5198 [4:04:10<170:11:45, 138.75s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_734
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.39s/it][A100%|██████████| 1/1 [01:41<00:00, 101.40s/it]
 15%|█▌        | 783/5198 [4:06:11<156:29:06, 127.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:56:07,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=773, skipped=0, lr=[1.9425349701535187e-05], mom=[(0.9, 0.999)]
steps: 773 loss: 0.5492 iter time (s): 101.427 samples/sec: 1.262

100%|██████████| 1/1 [01:42<00:00, 102.40s/it][A100%|██████████| 1/1 [01:42<00:00, 102.40s/it]
 15%|█▌        | 783/5198 [4:06:26<156:45:13, 127.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.40s/it][A100%|██████████| 1/1 [01:42<00:00, 102.40s/it]
 15%|█▌        | 783/5198 [4:06:13<156:46:26, 127.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.35s/it][A100%|██████████| 1/1 [01:42<00:00, 102.35s/it]
 15%|█▌        | 783/5198 [4:06:32<156:45:21, 127.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.43s/it][A100%|██████████| 1/1 [01:42<00:00, 102.43s/it]
 15%|█▌        | 783/5198 [4:04:56<156:47:22, 127.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.42s/it][A100%|██████████| 1/1 [01:42<00:00, 102.42s/it]
 15%|█▌        | 783/5198 [4:05:20<156:47:29, 127.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.46s/it][A100%|██████████| 1/1 [01:42<00:00, 102.46s/it]
 15%|█▌        | 783/5198 [4:05:51<156:48:10, 127.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.42s/it][A100%|██████████| 1/1 [01:42<00:00, 102.42s/it]
 15%|█▌        | 783/5198 [4:05:53<156:47:40, 127.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_48
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.68s/it][A100%|██████████| 1/1 [02:00<00:00, 120.68s/it]
 15%|█▌        | 784/5198 [4:08:12<153:56:39, 125.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:58:08,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=774, skipped=0, lr=[1.942345585551437e-05], mom=[(0.9, 0.999)]
steps: 774 loss: 0.8096 iter time (s): 120.368 samples/sec: 1.063

100%|██████████| 1/1 [02:01<00:00, 121.49s/it][A100%|██████████| 1/1 [02:01<00:00, 121.49s/it]
 15%|█▌        | 784/5198 [4:08:28<154:23:34, 125.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.36s/it][A100%|██████████| 1/1 [02:01<00:00, 121.37s/it]
 15%|█▌        | 784/5198 [4:08:14<154:22:02, 125.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.51s/it][A100%|██████████| 1/1 [02:01<00:00, 121.51s/it]
 15%|█▌        | 784/5198 [4:08:34<154:24:36, 125.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.45s/it][A100%|██████████| 1/1 [02:01<00:00, 121.45s/it]
 15%|█▌        | 784/5198 [4:06:58<154:24:14, 125.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.48s/it][A100%|██████████| 1/1 [02:01<00:00, 121.48s/it]
 15%|█▌        | 784/5198 [4:07:21<154:25:05, 125.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.47s/it][A100%|██████████| 1/1 [02:01<00:00, 121.47s/it]
 15%|█▌        | 784/5198 [4:07:52<154:25:18, 125.94s/it]
100%|██████████| 1/1 [02:01<00:00, 121.47s/it][A100%|██████████| 1/1 [02:01<00:00, 121.47s/it]
 15%|█▌        | 784/5198 [4:07:54<154:24:49, 125.94s/it]
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_735
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.88s/it][A100%|██████████| 1/1 [01:45<00:00, 105.88s/it]
 15%|█▌        | 785/5198 [4:09:58<146:43:30, 119.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 03:59:54,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=775, skipped=0, lr=[1.942155898653583e-05], mom=[(0.9, 0.999)]
steps: 775 loss: 0.6038 iter time (s): 104.470 samples/sec: 1.225

100%|██████████| 1/1 [01:45<00:00, 105.38s/it][A100%|██████████| 1/1 [01:45<00:00, 105.38s/it]
 15%|█▌        | 785/5198 [4:10:13<146:48:26, 119.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.38s/it][A100%|██████████| 1/1 [01:45<00:00, 105.38s/it]
 15%|█▌        | 785/5198 [4:10:00<146:47:24, 119.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.30s/it][A100%|██████████| 1/1 [01:45<00:00, 105.30s/it]
 15%|█▌        | 785/5198 [4:10:19<146:47:30, 119.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.35s/it][A100%|██████████| 1/1 [01:45<00:00, 105.35s/it]
 15%|█▌        | 785/5198 [4:08:43<146:48:12, 119.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.29s/it][A100%|██████████| 1/1 [01:45<00:00, 105.29s/it]
 15%|█▌        | 785/5198 [4:09:38<146:47:41, 119.75s/it]
100%|██████████| 1/1 [01:45<00:00, 105.34s/it][A100%|██████████| 1/1 [01:45<00:00, 105.34s/it]
 15%|█▌        | 785/5198 [4:09:07<146:48:41, 119.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.31s/it][A100%|██████████| 1/1 [01:45<00:00, 105.31s/it]
 15%|█▌        | 785/5198 [4:09:40<146:47:49, 119.75s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_736
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.10s/it][A100%|██████████| 1/1 [01:30<00:00, 90.10s/it]
 15%|█▌        | 786/5198 [4:11:28<135:51:41, 110.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:01:24,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=776, skipped=0, lr=[1.9419659095208117e-05], mom=[(0.9, 0.999)]
steps: 776 loss: 0.5492 iter time (s): 88.941 samples/sec: 1.439

100%|██████████| 1/1 [01:29<00:00, 89.83s/it][A100%|██████████| 1/1 [01:29<00:00, 89.83s/it]
 15%|█▌        | 786/5198 [4:11:43<135:46:28, 110.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.83s/it][A100%|██████████| 1/1 [01:29<00:00, 89.83s/it]
 15%|█▌        | 786/5198 [4:11:29<135:45:41, 110.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.89s/it][A100%|██████████| 1/1 [01:29<00:00, 89.89s/it]
 15%|█▌        | 786/5198 [4:11:49<135:47:01, 110.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.88s/it][A100%|██████████| 1/1 [01:29<00:00, 89.88s/it]
 15%|█▌        | 786/5198 [4:10:13<135:47:10, 110.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.83s/it][A100%|██████████| 1/1 [01:29<00:00, 89.84s/it]
 15%|█▌        | 786/5198 [4:10:36<135:46:37, 110.79s/it]
100%|██████████| 1/1 [01:29<00:00, 89.85s/it][A100%|██████████| 1/1 [01:29<00:00, 89.85s/it]
 15%|█▌        | 786/5198 [4:11:07<135:46:12, 110.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.85s/it][A100%|██████████| 1/1 [01:29<00:00, 89.85s/it]
 15%|█▌        | 786/5198 [4:11:09<135:46:17, 110.78s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_737
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.26s/it][A100%|██████████| 1/1 [01:27<00:00, 87.26s/it]
 15%|█▌        | 787/5198 [4:12:56<127:13:39, 103.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:02:51,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=777, skipped=0, lr=[1.941775618214077e-05], mom=[(0.9, 0.999)]
steps: 777 loss: 0.6247 iter time (s): 86.482 samples/sec: 1.480

100%|██████████| 1/1 [01:27<00:00, 87.34s/it][A100%|██████████| 1/1 [01:27<00:00, 87.34s/it]
 15%|█▌        | 787/5198 [4:13:10<127:08:00, 103.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.39s/it][A100%|██████████| 1/1 [01:27<00:00, 87.39s/it]
 15%|█▌        | 787/5198 [4:12:57<127:08:21, 103.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.26s/it][A100%|██████████| 1/1 [01:27<00:00, 87.27s/it]
 15%|█▌        | 787/5198 [4:11:40<127:06:31, 103.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.44s/it][A100%|██████████| 1/1 [01:27<00:00, 87.44s/it]
 15%|█▌        | 787/5198 [4:13:16<127:10:19, 103.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.35s/it][A100%|██████████| 1/1 [01:27<00:00, 87.35s/it]
 15%|█▌        | 787/5198 [4:12:04<127:08:00, 103.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.38s/it][A100%|██████████| 1/1 [01:27<00:00, 87.38s/it]
 15%|█▌        | 787/5198 [4:12:35<127:08:28, 103.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.39s/it][A100%|██████████| 1/1 [01:27<00:00, 87.39s/it]
 15%|█▌        | 787/5198 [4:12:37<127:08:36, 103.77s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_738
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.38s/it][A100%|██████████| 1/1 [01:28<00:00, 88.38s/it]
 15%|█▌        | 788/5198 [4:14:24<121:33:59, 99.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:04:20,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=778, skipped=0, lr=[1.9415850247944285e-05], mom=[(0.9, 0.999)]
steps: 778 loss: 0.5380 iter time (s): 87.597 samples/sec: 1.461

100%|██████████| 1/1 [01:28<00:00, 88.70s/it][A100%|██████████| 1/1 [01:28<00:00, 88.70s/it]
 15%|█▌        | 788/5198 [4:14:39<121:34:20, 99.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.81s/it][A100%|██████████| 1/1 [01:28<00:00, 88.81s/it]
 15%|█▌        | 788/5198 [4:14:26<121:36:59, 99.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.68s/it][A100%|██████████| 1/1 [01:28<00:00, 88.68s/it]
 15%|█▌        | 788/5198 [4:14:45<121:35:34, 99.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.79s/it][A100%|██████████| 1/1 [01:28<00:00, 88.79s/it]
 15%|█▌        | 788/5198 [4:13:09<121:35:21, 99.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.74s/it][A100%|██████████| 1/1 [01:28<00:00, 88.74s/it]
 15%|█▌        | 788/5198 [4:13:33<121:35:19, 99.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.73s/it][A100%|██████████| 1/1 [01:28<00:00, 88.73s/it]
 15%|█▌        | 788/5198 [4:14:06<121:35:25, 99.26s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_739
Training on 128 of 128 sentences.


  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [01:28<00:00, 88.76s/it][A100%|██████████| 1/1 [01:28<00:00, 88.77s/it]
 15%|█▌        | 788/5198 [4:14:04<121:36:10, 99.27s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.04s/it][A100%|██████████| 1/1 [01:38<00:00, 98.04s/it]
 15%|█▌        | 789/5198 [4:16:03<121:09:08, 98.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:05:58,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=779, skipped=0, lr=[1.941394129323014e-05], mom=[(0.9, 0.999)]
steps: 779 loss: 0.5743 iter time (s): 97.318 samples/sec: 1.315

100%|██████████| 1/1 [01:38<00:00, 98.30s/it][A100%|██████████| 1/1 [01:38<00:00, 98.30s/it]
 15%|█▌        | 789/5198 [4:16:17<121:12:12, 98.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.15s/it][A100%|██████████| 1/1 [01:38<00:00, 98.15s/it]
 15%|█▌        | 789/5198 [4:16:04<121:10:36, 98.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.24s/it][A100%|██████████| 1/1 [01:38<00:00, 98.24s/it]
 15%|█▌        | 789/5198 [4:16:23<121:11:34, 98.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.28s/it][A100%|██████████| 1/1 [01:38<00:00, 98.28s/it]
 15%|█▌        | 789/5198 [4:14:48<121:12:16, 98.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.25s/it][A100%|██████████| 1/1 [01:38<00:00, 98.25s/it]
 15%|█▌        | 789/5198 [4:15:11<121:11:41, 98.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.21s/it][A100%|██████████| 1/1 [01:38<00:00, 98.21s/it]
 15%|█▌        | 789/5198 [4:15:42<121:11:19, 98.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.25s/it][A100%|██████████| 1/1 [01:38<00:00, 98.25s/it]
 15%|█▌        | 789/5198 [4:15:44<121:11:48, 98.96s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_740
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.07s/it][A100%|██████████| 1/1 [01:26<00:00, 86.07s/it]
 15%|█▌        | 790/5198 [4:17:29<116:28:23, 95.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:07:24,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[1.9412029318610768e-05], mom=[(0.9, 0.999)]
steps: 780 loss: 0.5638 iter time (s): 84.988 samples/sec: 1.506

100%|██████████| 1/1 [01:25<00:00, 85.91s/it][A100%|██████████| 1/1 [01:25<00:00, 85.91s/it]
 15%|█▌        | 790/5198 [4:17:43<116:23:06, 95.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.01s/it][A100%|██████████| 1/1 [01:26<00:00, 86.01s/it]
 15%|█▌        | 790/5198 [4:17:30<116:24:05, 95.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.94s/it][A100%|██████████| 1/1 [01:25<00:00, 85.94s/it]
 15%|█▌        | 790/5198 [4:17:49<116:23:16, 95.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.83s/it][A100%|██████████| 1/1 [01:25<00:00, 85.83s/it]
 15%|█▌        | 790/5198 [4:16:13<116:21:21, 95.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.89s/it][A100%|██████████| 1/1 [01:25<00:00, 85.89s/it]
 15%|█▌        | 790/5198 [4:16:37<116:22:19, 95.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.91s/it][A100%|██████████| 1/1 [01:25<00:00, 85.91s/it]
 15%|█▌        | 790/5198 [4:17:08<116:22:18, 95.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.88s/it][A100%|██████████| 1/1 [01:25<00:00, 85.88s/it]
 15%|█▌        | 790/5198 [4:17:10<116:22:05, 95.04s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_741
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.61s/it][A100%|██████████| 1/1 [01:43<00:00, 103.61s/it]
 15%|█▌        | 791/5198 [4:19:13<119:37:17, 97.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:09:08,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=781, skipped=0, lr=[1.9410114324699585e-05], mom=[(0.9, 0.999)]
steps: 781 loss: 0.5672 iter time (s): 103.400 samples/sec: 1.238

100%|██████████| 1/1 [01:44<00:00, 104.27s/it][A100%|██████████| 1/1 [01:44<00:00, 104.27s/it]
 15%|█▌        | 791/5198 [4:19:27<119:44:54, 97.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.20s/it][A100%|██████████| 1/1 [01:44<00:00, 104.20s/it]
 15%|█▌        | 791/5198 [4:19:14<119:44:03, 97.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.30s/it][A100%|██████████| 1/1 [01:44<00:00, 104.30s/it]
 15%|█▌        | 791/5198 [4:19:34<119:45:32, 97.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.40s/it][A100%|██████████| 1/1 [01:44<00:00, 104.40s/it]
 15%|█▌        | 791/5198 [4:17:58<119:46:24, 97.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.31s/it][A100%|██████████| 1/1 [01:44<00:00, 104.31s/it]
 15%|█▌        | 791/5198 [4:18:21<119:45:07, 97.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.34s/it][A100%|██████████| 1/1 [01:44<00:00, 104.34s/it]
 15%|█▌        | 791/5198 [4:18:54<119:45:41, 97.83s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_742

100%|██████████| 1/1 [01:44<00:00, 104.36s/it][A100%|██████████| 1/1 [01:44<00:00, 104.36s/it]
 15%|█▌        | 791/5198 [4:18:52<119:46:10, 97.84s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.54s/it][A100%|██████████| 1/1 [02:06<00:00, 126.54s/it]
 15%|█▌        | 792/5198 [4:21:19<130:13:57, 106.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:11:16,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=782, skipped=0, lr=[1.9408196312110964e-05], mom=[(0.9, 0.999)]
steps: 782 loss: 0.5777 iter time (s): 126.351 samples/sec: 1.013

100%|██████████| 1/1 [02:07<00:00, 127.32s/it][A100%|██████████| 1/1 [02:07<00:00, 127.32s/it]
 15%|█▌        | 792/5198 [4:21:35<130:33:22, 106.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.29s/it][A100%|██████████| 1/1 [02:07<00:00, 127.29s/it]
 15%|█▌        | 792/5198 [4:21:21<130:32:08, 106.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.28s/it][A100%|██████████| 1/1 [02:07<00:00, 127.28s/it]
 15%|█▌        | 792/5198 [4:21:41<130:33:04, 106.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.25s/it][A100%|██████████| 1/1 [02:07<00:00, 127.25s/it]
 15%|█▌        | 792/5198 [4:20:05<130:32:49, 106.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.27s/it][A100%|██████████| 1/1 [02:07<00:00, 127.27s/it]
 15%|█▌        | 792/5198 [4:20:28<130:32:29, 106.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.28s/it][A100%|██████████| 1/1 [02:07<00:00, 127.28s/it]
 15%|█▌        | 792/5198 [4:20:59<130:33:11, 106.67s/it]
100%|██████████| 1/1 [02:07<00:00, 127.28s/it][A100%|██████████| 1/1 [02:07<00:00, 127.28s/it]
 15%|█▌        | 792/5198 [4:21:01<130:32:57, 106.67s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_743

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.57s/it][A100%|██████████| 1/1 [01:42<00:00, 102.57s/it]
 15%|█▌        | 793/5198 [4:23:02<128:51:41, 105.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:12:58,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=783, skipped=0, lr=[1.9406275281460255e-05], mom=[(0.9, 0.999)]
steps: 783 loss: 0.5433 iter time (s): 101.101 samples/sec: 1.266

100%|██████████| 1/1 [01:41<00:00, 101.94s/it][A100%|██████████| 1/1 [01:41<00:00, 101.94s/it]
 15%|█▌        | 793/5198 [4:23:17<128:47:33, 105.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.01s/it][A100%|██████████| 1/1 [01:42<00:00, 102.01s/it]
 15%|█▌        | 793/5198 [4:23:03<128:48:16, 105.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.98s/it][A100%|██████████| 1/1 [01:41<00:00, 101.98s/it]
 15%|█▌        | 793/5198 [4:23:23<128:48:22, 105.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.97s/it][A100%|██████████| 1/1 [01:41<00:00, 101.97s/it]
 15%|█▌        | 793/5198 [4:21:47<128:47:45, 105.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.00s/it][A100%|██████████| 1/1 [01:42<00:00, 102.00s/it]
 15%|█▌        | 793/5198 [4:22:10<128:48:16, 105.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.95s/it][A100%|██████████| 1/1 [01:41<00:00, 101.95s/it]
 15%|█▌        | 793/5198 [4:22:41<128:47:40, 105.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.98s/it][A100%|██████████| 1/1 [01:41<00:00, 101.98s/it]
 15%|█▌        | 793/5198 [4:22:43<128:48:05, 105.26s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_744
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.72s/it][A100%|██████████| 1/1 [02:16<00:00, 136.72s/it]
 15%|█▌        | 794/5198 [4:25:19<140:25:20, 114.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:15:15,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=784, skipped=0, lr=[1.9404351233363766e-05], mom=[(0.9, 0.999)]
steps: 784 loss: 0.5420 iter time (s): 137.023 samples/sec: 0.934

100%|██████████| 1/1 [02:17<00:00, 137.95s/it][A100%|██████████| 1/1 [02:17<00:00, 137.95s/it]
 15%|█▌        | 794/5198 [4:25:35<140:46:01, 115.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.98s/it][A100%|██████████| 1/1 [02:17<00:00, 137.98s/it]
 15%|█▌        | 794/5198 [4:25:21<140:47:11, 115.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.88s/it][A100%|██████████| 1/1 [02:17<00:00, 137.88s/it]
 15%|█▌        | 794/5198 [4:25:41<140:45:06, 115.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.97s/it][A100%|██████████| 1/1 [02:17<00:00, 137.97s/it]
 15%|█▌        | 794/5198 [4:24:05<140:46:26, 115.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.99s/it][A100%|██████████| 1/1 [02:17<00:00, 137.99s/it]
 15%|█▌        | 794/5198 [4:24:28<140:47:23, 115.09s/it]
100%|██████████| 1/1 [02:17<00:00, 137.97s/it][A100%|██████████| 1/1 [02:17<00:00, 137.97s/it]
 15%|█▌        | 794/5198 [4:24:59<140:46:24, 115.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.95s/it][A100%|██████████| 1/1 [02:17<00:00, 137.95s/it]
 15%|█▌        | 794/5198 [4:25:01<140:46:12, 115.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_745

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.15s/it][A100%|██████████| 1/1 [01:32<00:00, 92.15s/it]
 15%|█▌        | 795/5198 [4:26:51<132:09:06, 108.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:16:46,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=785, skipped=0, lr=[1.940242416843878e-05], mom=[(0.9, 0.999)]
steps: 785 loss: 0.5877 iter time (s): 90.106 samples/sec: 1.421

100%|██████████| 1/1 [01:31<00:00, 91.06s/it][A100%|██████████| 1/1 [01:31<00:00, 91.06s/it]
 15%|█▌        | 795/5198 [4:27:06<131:55:51, 107.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.00s/it][A100%|██████████| 1/1 [01:31<00:00, 91.00s/it]
 15%|█▌        | 795/5198 [4:26:52<131:55:15, 107.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.05s/it][A100%|██████████| 1/1 [01:31<00:00, 91.05s/it]
 15%|█▌        | 795/5198 [4:27:12<131:55:05, 107.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.98s/it][A100%|██████████| 1/1 [01:30<00:00, 90.98s/it]
 15%|█▌        | 795/5198 [4:25:36<131:54:20, 107.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 15%|█▌        | 795/5198 [4:25:59<131:54:42, 107.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.99s/it][A100%|██████████| 1/1 [01:30<00:00, 90.99s/it]
 15%|█▌        | 795/5198 [4:26:30<131:54:22, 107.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.01s/it][A100%|██████████| 1/1 [01:31<00:00, 91.01s/it]
 15%|█▌        | 795/5198 [4:26:32<131:54:45, 107.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_746
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.44s/it][A100%|██████████| 1/1 [01:35<00:00, 95.44s/it]
 15%|█▌        | 796/5198 [4:28:27<127:32:54, 104.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:18:22,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=786, skipped=0, lr=[1.9400494087303556e-05], mom=[(0.9, 0.999)]
steps: 786 loss: 0.5898 iter time (s): 94.745 samples/sec: 1.351

100%|██████████| 1/1 [01:35<00:00, 95.57s/it][A100%|██████████| 1/1 [01:35<00:00, 95.57s/it]
 15%|█▌        | 796/5198 [4:28:41<127:23:33, 104.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.58s/it][A100%|██████████| 1/1 [01:35<00:00, 95.58s/it]
 15%|█▌        | 796/5198 [4:28:28<127:23:26, 104.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.55s/it][A100%|██████████| 1/1 [01:35<00:00, 95.55s/it]
 15%|█▌        | 796/5198 [4:28:47<127:22:44, 104.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.68s/it][A100%|██████████| 1/1 [01:35<00:00, 95.68s/it]
 15%|█▌        | 796/5198 [4:27:12<127:24:44, 104.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.59s/it][A100%|██████████| 1/1 [01:35<00:00, 95.60s/it]
 15%|█▌        | 796/5198 [4:27:35<127:23:16, 104.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.59s/it][A100%|██████████| 1/1 [01:35<00:00, 95.59s/it]
 15%|█▌        | 796/5198 [4:28:06<127:23:00, 104.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.59s/it][A100%|██████████| 1/1 [01:35<00:00, 95.59s/it]
 15%|█▌        | 796/5198 [4:28:08<127:23:04, 104.18s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_747
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.29s/it][A100%|██████████| 1/1 [01:51<00:00, 111.29s/it]
 15%|█▌        | 797/5198 [4:30:18<130:07:51, 106.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:20:14,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=787, skipped=0, lr=[1.9398560990577305e-05], mom=[(0.9, 0.999)]
steps: 787 loss: 0.5397 iter time (s): 111.065 samples/sec: 1.152

100%|██████████| 1/1 [01:51<00:00, 111.93s/it][A100%|██████████| 1/1 [01:51<00:00, 111.93s/it]
 15%|█▌        | 797/5198 [4:30:33<130:12:29, 106.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.94s/it][A100%|██████████| 1/1 [01:51<00:00, 111.94s/it]
 15%|█▌        | 797/5198 [4:30:20<130:12:42, 106.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.94s/it][A100%|██████████| 1/1 [01:51<00:00, 111.94s/it]
 15%|█▌        | 797/5198 [4:30:39<130:12:08, 106.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.92s/it][A100%|██████████| 1/1 [01:51<00:00, 111.92s/it]
 15%|█▌        | 797/5198 [4:29:27<130:12:07, 106.50s/it]
100%|██████████| 1/1 [01:51<00:00, 111.95s/it][A100%|██████████| 1/1 [01:51<00:00, 111.95s/it]
 15%|█▌        | 797/5198 [4:29:04<130:13:46, 106.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.96s/it][A100%|██████████| 1/1 [01:51<00:00, 111.97s/it]
 15%|█▌        | 797/5198 [4:29:58<130:12:50, 106.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.95s/it][A100%|██████████| 1/1 [01:51<00:00, 111.95s/it]
 15%|█▌        | 797/5198 [4:30:00<130:12:35, 106.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_748
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.50s/it][A100%|██████████| 1/1 [01:50<00:00, 110.50s/it]
 15%|█▌        | 798/5198 [4:32:09<131:38:24, 107.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:22:05,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=788, skipped=0, lr=[1.939662487888021e-05], mom=[(0.9, 0.999)]
steps: 788 loss: 0.5859 iter time (s): 109.707 samples/sec: 1.167

100%|██████████| 1/1 [01:50<00:00, 110.64s/it][A100%|██████████| 1/1 [01:50<00:00, 110.64s/it]
 15%|█▌        | 798/5198 [4:32:24<131:41:46, 107.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.60s/it][A100%|██████████| 1/1 [01:50<00:00, 110.60s/it]
 15%|█▌        | 798/5198 [4:32:10<131:41:06, 107.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.72s/it][A100%|██████████| 1/1 [01:50<00:00, 110.72s/it]
 15%|█▌        | 798/5198 [4:32:30<131:43:19, 107.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.60s/it][A100%|██████████| 1/1 [01:50<00:00, 110.60s/it]
 15%|█▌        | 798/5198 [4:30:54<131:41:47, 107.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.65s/it][A100%|██████████| 1/1 [01:50<00:00, 110.65s/it]
 15%|█▌        | 798/5198 [4:31:17<131:41:42, 107.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.63s/it][A100%|██████████| 1/1 [01:50<00:00, 110.63s/it]
 15%|█▌        | 798/5198 [4:31:49<131:41:50, 107.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.64s/it][A100%|██████████| 1/1 [01:50<00:00, 110.64s/it]
 15%|█▌        | 798/5198 [4:31:51<131:41:51, 107.75s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_749
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.93s/it][A100%|██████████| 1/1 [02:17<00:00, 137.93s/it]
 15%|█▌        | 799/5198 [4:34:27<142:46:08, 116.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:24:23,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=789, skipped=0, lr=[1.939468575283343e-05], mom=[(0.9, 0.999)]
steps: 789 loss: 0.6099 iter time (s): 137.013 samples/sec: 0.934

100%|██████████| 1/1 [02:17<00:00, 137.86s/it][A100%|██████████| 1/1 [02:17<00:00, 137.86s/it]
 15%|█▌        | 799/5198 [4:34:42<142:42:26, 116.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.96s/it][A100%|██████████| 1/1 [02:17<00:00, 137.96s/it]
 15%|█▌        | 799/5198 [4:34:28<142:44:05, 116.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.84s/it][A100%|██████████| 1/1 [02:17<00:00, 137.84s/it]
 15%|█▌        | 799/5198 [4:34:48<142:43:06, 116.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.89s/it][A100%|██████████| 1/1 [02:17<00:00, 137.89s/it]
 15%|█▌        | 799/5198 [4:33:12<142:43:07, 116.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.87s/it][A100%|██████████| 1/1 [02:17<00:00, 137.87s/it]
 15%|█▌        | 799/5198 [4:33:35<142:42:39, 116.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.89s/it][A100%|██████████| 1/1 [02:17<00:00, 137.89s/it]
 15%|█▌        | 799/5198 [4:34:06<142:43:04, 116.80s/it]
100%|██████████| 1/1 [02:17<00:00, 137.88s/it][A100%|██████████| 1/1 [02:17<00:00, 137.88s/it]
 15%|█▌        | 799/5198 [4:34:08<142:42:48, 116.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_49

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.87s/it][A100%|██████████| 1/1 [02:01<00:00, 121.87s/it]
[2024-06-30 04:26:26,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[1.9392743613059082e-05], mom=[(0.9, 0.999)]
steps: 790 loss: 0.8316 iter time (s): 122.347 samples/sec: 1.046

100%|██████████| 1/1 [02:03<00:00, 123.39s/it][A100%|██████████| 1/1 [02:03<00:00, 123.39s/it]

100%|██████████| 1/1 [02:03<00:00, 123.32s/it][A100%|██████████| 1/1 [02:03<00:00, 123.32s/it]

100%|██████████| 1/1 [02:03<00:00, 123.33s/it][A100%|██████████| 1/1 [02:03<00:00, 123.33s/it]

100%|██████████| 1/1 [02:03<00:00, 123.44s/it][A100%|██████████| 1/1 [02:03<00:00, 123.44s/it]

100%|██████████| 1/1 [02:03<00:00, 123.43s/it][A100%|██████████| 1/1 [02:03<00:00, 123.43s/it]

100%|██████████| 1/1 [02:03<00:00, 123.39s/it][A100%|██████████| 1/1 [02:03<00:00, 123.39s/it]
Checkpointing at shard 799

100%|██████████| 1/1 [02:03<00:00, 123.43s/it][A100%|██████████| 1/1 [02:03<00:00, 123.43s/it]
[2024-06-30 04:26:27,398] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step790 is about to be saved!
[2024-06-30 04:26:28,928] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_00-model_states.pt...
[2024-06-30 04:26:33,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_02-model_states.pt...
[2024-06-30 04:26:34,600] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_00-model_states.pt.
[2024-06-30 04:26:40,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_03-model_states.pt...
[2024-06-30 04:26:40,922] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_08-model_states.pt...
[2024-06-30 04:26:40,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_04-model_states.pt...
[2024-06-30 04:26:41,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_06-model_states.pt...
[2024-06-30 04:26:42,016] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_07-model_states.pt...
[2024-06-30 04:26:42,501] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_05-model_states.pt...
[2024-06-30 04:26:43,416] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_01-model_states.pt...
[2024-06-30 04:31:11,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_05-model_states.pt.
[2024-06-30 04:31:12,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_04_model_states.pt...
[2024-06-30 04:31:13,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_04_model_states.pt.
[2024-06-30 04:31:13,016] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
[2024-06-30 04:31:18,271] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_01-model_states.pt.
[2024-06-30 04:31:19,146] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_00_model_states.pt
[2024-06-30 04:31:19,146] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_00_model_states.pt...
[2024-06-30 04:31:19,699] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_00_model_states.pt.
[2024-06-30 04:31:19,699] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
[2024-06-30 04:31:21,605] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_07-model_states.pt.
[2024-06-30 04:31:22,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_06_model_states.pt...
[2024-06-30 04:31:22,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_06_model_states.pt.
[2024-06-30 04:31:22,191] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
[2024-06-30 04:31:25,430] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_08-model_states.pt.
[2024-06-30 04:31:25,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_04-model_states.pt.
[2024-06-30 04:31:25,844] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_06-model_states.pt.
[2024-06-30 04:31:25,863] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_03-model_states.pt.
[2024-06-30 04:31:26,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_02-model_states.pt.
[2024-06-30 04:31:26,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_09-model_states.pt...
[2024-06-30 04:31:26,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_03_model_states.pt...
[2024-06-30 04:31:26,308] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_03_model_states.pt.
[2024-06-30 04:31:26,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
[2024-06-30 04:31:26,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_02_model_states.pt...
[2024-06-30 04:31:26,629] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_05_model_states.pt...
[2024-06-30 04:31:26,638] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_01_model_states.pt
[2024-06-30 04:31:26,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_01_model_states.pt...
[2024-06-30 04:31:26,693] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_02_model_states.pt.
[2024-06-30 04:31:26,693] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
[2024-06-30 04:31:26,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_05_model_states.pt.
[2024-06-30 04:31:26,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
[2024-06-30 04:31:26,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_01_model_states.pt.
[2024-06-30 04:31:26,782] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
[2024-06-30 04:31:26,937] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/layer_09-model_states.pt.
[2024-06-30 04:31:26,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_07_model_states.pt...
[2024-06-30 04:31:27,009] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step790/mp_rank_07_model_states.pt.
[2024-06-30 04:31:27,010] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step790 is ready now!
Checkpoint saved using --- 299.6525242328644 seconds ---
 15%|█▌        | 800/5198 [4:41:09<254:55:38, 208.67s/it] 15%|█▌        | 800/5198 [4:41:32<255:03:21, 208.78s/it] 15%|█▌        | 800/5198 [4:40:38<254:56:36, 208.68s/it] 15%|█▌        | 800/5198 [4:40:15<254:57:33, 208.70s/it] 15%|█▌        | 800/5198 [4:41:33<256:09:31, 209.68s/it] 15%|█▌        | 800/5198 [4:41:45<255:06:26, 208.82s/it] 15%|█▌        | 800/5198 [4:41:11<254:55:22, 208.67s/it] 15%|█▌        | 800/5198 [4:41:51<254:59:56, 208.73s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_750
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.82s/it][A100%|██████████| 1/1 [01:24<00:00, 84.82s/it]
 15%|█▌        | 801/5198 [4:42:58<210:25:04, 172.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:32:53,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=791, skipped=0, lr=[1.939079846018025e-05], mom=[(0.9, 0.999)]
steps: 791 loss: 0.5941 iter time (s): 86.810 samples/sec: 1.474

100%|██████████| 1/1 [01:27<00:00, 87.23s/it][A100%|██████████| 1/1 [01:27<00:00, 87.23s/it]
 15%|█▌        | 801/5198 [4:43:13<210:34:00, 172.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.32s/it][A100%|██████████| 1/1 [01:27<00:00, 87.32s/it]
 15%|█▌        | 801/5198 [4:42:59<210:33:45, 172.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.57s/it][A100%|██████████| 1/1 [01:27<00:00, 87.57s/it]
 15%|█▌        | 801/5198 [4:41:43<210:35:08, 172.42s/it]
100%|██████████| 1/1 [01:27<00:00, 87.57s/it][A100%|██████████| 1/1 [01:27<00:00, 87.57s/it]
 15%|█▌        | 801/5198 [4:43:19<210:36:59, 172.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.66s/it][A100%|██████████| 1/1 [01:27<00:00, 87.66s/it]
 15%|█▌        | 801/5198 [4:42:06<210:36:38, 172.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.71s/it][A100%|██████████| 1/1 [01:27<00:00, 87.71s/it]
 15%|█▌        | 801/5198 [4:42:37<210:36:56, 172.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.74s/it][A100%|██████████| 1/1 [01:27<00:00, 87.74s/it]
 15%|█▌        | 801/5198 [4:42:39<210:37:21, 172.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_751
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.54s/it][A100%|██████████| 1/1 [01:54<00:00, 114.54s/it]
 15%|█▌        | 802/5198 [4:44:53<189:16:13, 155.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:34:49,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=792, skipped=0, lr=[1.9388850294820995e-05], mom=[(0.9, 0.999)]
steps: 792 loss: 0.5572 iter time (s): 114.495 samples/sec: 1.118

100%|██████████| 1/1 [01:55<00:00, 115.47s/it][A100%|██████████| 1/1 [01:55<00:00, 115.47s/it]
 15%|█▌        | 802/5198 [4:45:08<189:40:00, 155.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.46s/it][A100%|██████████| 1/1 [01:55<00:00, 115.46s/it]
 15%|█▌        | 802/5198 [4:44:55<189:39:37, 155.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.35s/it][A100%|██████████| 1/1 [01:55<00:00, 115.35s/it]
 15%|█▌        | 802/5198 [4:45:14<189:39:29, 155.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.48s/it][A100%|██████████| 1/1 [01:55<00:00, 115.48s/it]
 15%|█▌        | 802/5198 [4:43:39<189:41:09, 155.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.42s/it][A100%|██████████| 1/1 [01:55<00:00, 115.42s/it]
 15%|█▌        | 802/5198 [4:44:33<189:40:52, 155.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.47s/it][A100%|██████████| 1/1 [01:55<00:00, 115.47s/it]
 15%|█▌        | 802/5198 [4:44:02<189:41:58, 155.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.41s/it][A100%|██████████| 1/1 [01:55<00:00, 115.41s/it]
 15%|█▌        | 802/5198 [4:44:35<189:41:06, 155.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_752
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.92s/it][A100%|██████████| 1/1 [01:36<00:00, 96.92s/it]
 15%|█▌        | 803/5198 [4:46:30<167:59:36, 137.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:36:25,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=793, skipped=0, lr=[1.9386899117606326e-05], mom=[(0.9, 0.999)]
steps: 793 loss: 0.5517 iter time (s): 95.543 samples/sec: 1.340

100%|██████████| 1/1 [01:36<00:00, 96.30s/it][A100%|██████████| 1/1 [01:36<00:00, 96.30s/it]
 15%|█▌        | 803/5198 [4:46:45<168:00:38, 137.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.45s/it][A100%|██████████| 1/1 [01:36<00:00, 96.45s/it]
 15%|█▌        | 803/5198 [4:46:31<168:03:33, 137.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.38s/it][A100%|██████████| 1/1 [01:36<00:00, 96.38s/it]
 15%|█▌        | 803/5198 [4:46:51<168:02:07, 137.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.40s/it][A100%|██████████| 1/1 [01:36<00:00, 96.40s/it]
 15%|█▌        | 803/5198 [4:45:15<168:03:27, 137.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.41s/it][A100%|██████████| 1/1 [01:36<00:00, 96.41s/it]
 15%|█▌        | 803/5198 [4:45:38<168:04:18, 137.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.48s/it][A100%|██████████| 1/1 [01:36<00:00, 96.48s/it]
 15%|█▌        | 803/5198 [4:46:09<168:05:04, 137.68s/it]
100%|██████████| 1/1 [01:36<00:00, 96.46s/it][A100%|██████████| 1/1 [01:36<00:00, 96.46s/it]
 15%|█▌        | 803/5198 [4:46:11<168:04:46, 137.68s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_753

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.17s/it][A100%|██████████| 1/1 [01:19<00:00, 79.17s/it]
 15%|█▌        | 804/5198 [4:47:49<146:37:33, 120.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:37:44,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=794, skipped=0, lr=[1.9384944929162236e-05], mom=[(0.9, 0.999)]
steps: 794 loss: 0.5482 iter time (s): 78.013 samples/sec: 1.641

100%|██████████| 1/1 [01:19<00:00, 79.01s/it][A100%|██████████| 1/1 [01:19<00:00, 79.01s/it]
 15%|█▌        | 804/5198 [4:48:04<146:30:56, 120.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.86s/it][A100%|██████████| 1/1 [01:18<00:00, 78.86s/it]
 15%|█▌        | 804/5198 [4:47:50<146:29:42, 120.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.96s/it][A100%|██████████| 1/1 [01:18<00:00, 78.97s/it]
 15%|█▌        | 804/5198 [4:48:10<146:30:54, 120.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.91s/it][A100%|██████████| 1/1 [01:18<00:00, 78.91s/it]
 15%|█▌        | 804/5198 [4:46:34<146:30:35, 120.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.85s/it][A100%|██████████| 1/1 [01:18<00:00, 78.85s/it]
 15%|█▌        | 804/5198 [4:46:57<146:30:00, 120.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.87s/it][A100%|██████████| 1/1 [01:18<00:00, 78.87s/it]
 15%|█▌        | 804/5198 [4:47:28<146:30:52, 120.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.88s/it][A100%|██████████| 1/1 [01:18<00:00, 78.88s/it]
 15%|█▌        | 804/5198 [4:47:30<146:30:54, 120.04s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_754
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.19s/it][A100%|██████████| 1/1 [01:39<00:00, 99.19s/it]
 15%|█▌        | 805/5198 [4:49:29<138:59:58, 113.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:39:24,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=795, skipped=0, lr=[1.938298773011568e-05], mom=[(0.9, 0.999)]
steps: 795 loss: 0.5646 iter time (s): 99.082 samples/sec: 1.292

100%|██████████| 1/1 [01:39<00:00, 99.99s/it][A100%|██████████| 1/1 [01:39<00:00, 99.99s/it]
 15%|█▌        | 805/5198 [4:49:44<139:08:38, 114.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.03s/it][A100%|██████████| 1/1 [01:40<00:00, 100.03s/it]
 15%|█▌        | 805/5198 [4:49:30<139:08:54, 114.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.94s/it][A100%|██████████| 1/1 [01:39<00:00, 99.94s/it]
 15%|█▌        | 805/5198 [4:49:50<139:07:39, 114.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.94s/it][A100%|██████████| 1/1 [01:39<00:00, 99.94s/it]
 15%|█▌        | 805/5198 [4:48:14<139:07:25, 114.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.02s/it][A100%|██████████| 1/1 [01:40<00:00, 100.02s/it]
 15%|█▌        | 805/5198 [4:48:37<139:08:49, 114.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.97s/it][A100%|██████████| 1/1 [01:39<00:00, 99.97s/it]
 15%|█▌        | 805/5198 [4:49:08<139:08:16, 114.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.97s/it][A100%|██████████| 1/1 [01:39<00:00, 99.97s/it]
 15%|█▌        | 805/5198 [4:49:10<139:08:13, 114.02s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_755
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.08s/it][A100%|██████████| 1/1 [01:22<00:00, 82.08s/it]
 16%|█▌        | 806/5198 [4:50:51<127:21:41, 104.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:40:46,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=796, skipped=0, lr=[1.938102752109457e-05], mom=[(0.9, 0.999)]
steps: 796 loss: 0.6159 iter time (s): 80.778 samples/sec: 1.585

100%|██████████| 1/1 [01:21<00:00, 81.68s/it][A100%|██████████| 1/1 [01:21<00:00, 81.68s/it]
 16%|█▌        | 806/5198 [4:51:05<127:16:40, 104.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.63s/it][A100%|██████████| 1/1 [01:21<00:00, 81.63s/it]
 16%|█▌        | 806/5198 [4:50:52<127:15:53, 104.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.68s/it][A100%|██████████| 1/1 [01:21<00:00, 81.69s/it]
 16%|█▌        | 806/5198 [4:51:11<127:15:59, 104.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.68s/it][A100%|██████████| 1/1 [01:21<00:00, 81.68s/it]
 16%|█▌        | 806/5198 [4:49:35<127:15:37, 104.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.63s/it][A100%|██████████| 1/1 [01:21<00:00, 81.63s/it]
 16%|█▌        | 806/5198 [4:49:59<127:15:43, 104.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.67s/it][A100%|██████████| 1/1 [01:21<00:00, 81.67s/it]
 16%|█▌        | 806/5198 [4:50:30<127:16:08, 104.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.71s/it][A100%|██████████| 1/1 [01:21<00:00, 81.71s/it]
 16%|█▌        | 806/5198 [4:50:32<127:16:51, 104.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_756
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.18s/it][A100%|██████████| 1/1 [01:30<00:00, 90.18s/it]
 16%|█▌        | 807/5198 [4:52:21<122:11:14, 100.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:42:17,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=797, skipped=0, lr=[1.9379064302727794e-05], mom=[(0.9, 0.999)]
steps: 797 loss: 0.5157 iter time (s): 89.652 samples/sec: 1.428

100%|██████████| 1/1 [01:30<00:00, 90.55s/it][A100%|██████████| 1/1 [01:30<00:00, 90.55s/it]
 16%|█▌        | 807/5198 [4:52:36<122:12:49, 100.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.65s/it][A100%|██████████| 1/1 [01:30<00:00, 90.65s/it]
 16%|█▌        | 807/5198 [4:52:22<122:14:21, 100.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.60s/it][A100%|██████████| 1/1 [01:30<00:00, 90.60s/it]
 16%|█▌        | 807/5198 [4:52:42<122:13:13, 100.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.65s/it][A100%|██████████| 1/1 [01:30<00:00, 90.65s/it]
 16%|█▌        | 807/5198 [4:51:06<122:14:01, 100.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.63s/it][A100%|██████████| 1/1 [01:30<00:00, 90.63s/it]
 16%|█▌        | 807/5198 [4:51:29<122:13:46, 100.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.60s/it][A100%|██████████| 1/1 [01:30<00:00, 90.60s/it]
 16%|█▌        | 807/5198 [4:52:00<122:13:24, 100.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.57s/it][A100%|██████████| 1/1 [01:30<00:00, 90.57s/it]
 16%|█▌        | 807/5198 [4:52:02<122:13:18, 100.20s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_757
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.49s/it][A100%|██████████| 1/1 [01:25<00:00, 85.49s/it]
 16%|█▌        | 808/5198 [4:53:47<116:49:57, 95.81s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:43:42,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=798, skipped=0, lr=[1.9377098075645195e-05], mom=[(0.9, 0.999)]
steps: 798 loss: 0.5469 iter time (s): 84.493 samples/sec: 1.515

100%|██████████| 1/1 [01:25<00:00, 85.41s/it][A100%|██████████| 1/1 [01:25<00:00, 85.42s/it]
 16%|█▌        | 808/5198 [4:54:01<116:46:52, 95.77s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.30s/it][A100%|██████████| 1/1 [01:25<00:00, 85.30s/it]
 16%|█▌        | 808/5198 [4:53:48<116:45:18, 95.74s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.40s/it][A100%|██████████| 1/1 [01:25<00:00, 85.40s/it]
 16%|█▌        | 808/5198 [4:54:07<116:46:45, 95.76s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.32s/it][A100%|██████████| 1/1 [01:25<00:00, 85.32s/it]
 16%|█▌        | 808/5198 [4:52:31<116:45:36, 95.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.33s/it][A100%|██████████| 1/1 [01:25<00:00, 85.33s/it]
 16%|█▌        | 808/5198 [4:52:55<116:45:45, 95.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.33s/it][A100%|██████████| 1/1 [01:25<00:00, 85.34s/it]
 16%|█▌        | 808/5198 [4:53:26<116:45:31, 95.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.35s/it][A100%|██████████| 1/1 [01:25<00:00, 85.35s/it]
 16%|█▌        | 808/5198 [4:53:28<116:45:46, 95.75s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_758
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.78s/it][A100%|██████████| 1/1 [01:42<00:00, 102.78s/it]
 16%|█▌        | 809/5198 [4:55:30<119:26:41, 97.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:45:26,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=799, skipped=0, lr=[1.9375128840477593e-05], mom=[(0.9, 0.999)]
steps: 799 loss: 0.5403 iter time (s): 102.684 samples/sec: 1.247

100%|██████████| 1/1 [01:43<00:00, 103.54s/it][A100%|██████████| 1/1 [01:43<00:00, 103.54s/it]
 16%|█▌        | 809/5198 [4:55:45<119:36:05, 98.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.61s/it][A100%|██████████| 1/1 [01:43<00:00, 103.61s/it]
 16%|█▌        | 809/5198 [4:55:31<119:36:31, 98.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.55s/it][A100%|██████████| 1/1 [01:43<00:00, 103.55s/it]
 16%|█▌        | 809/5198 [4:55:51<119:36:16, 98.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.61s/it][A100%|██████████| 1/1 [01:43<00:00, 103.61s/it]
 16%|█▌        | 809/5198 [4:54:15<119:36:43, 98.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.63s/it][A100%|██████████| 1/1 [01:43<00:00, 103.63s/it]
 16%|█▌        | 809/5198 [4:55:09<119:37:07, 98.12s/it]
100%|██████████| 1/1 [01:43<00:00, 103.66s/it][A100%|██████████| 1/1 [01:43<00:00, 103.66s/it]
 16%|█▌        | 809/5198 [4:54:38<119:37:50, 98.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.63s/it][A100%|██████████| 1/1 [01:43<00:00, 103.63s/it]
 16%|█▌        | 809/5198 [4:55:11<119:37:20, 98.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_759
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.71s/it][A100%|██████████| 1/1 [01:23<00:00, 83.71s/it]
 16%|█▌        | 810/5198 [4:56:54<114:14:33, 93.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:46:49,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[1.9373156597856765e-05], mom=[(0.9, 0.999)]
steps: 800 loss: 0.5874 iter time (s): 82.332 samples/sec: 1.555

100%|██████████| 1/1 [01:23<00:00, 83.29s/it][A100%|██████████| 1/1 [01:23<00:00, 83.29s/it]
 16%|█▌        | 810/5198 [4:57:08<114:09:46, 93.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.24s/it][A100%|██████████| 1/1 [01:23<00:00, 83.24s/it]
 16%|█▌        | 810/5198 [4:56:55<114:08:57, 93.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.31s/it][A100%|██████████| 1/1 [01:23<00:00, 83.31s/it]
 16%|█▌        | 810/5198 [4:57:14<114:10:12, 93.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.25s/it][A100%|██████████| 1/1 [01:23<00:00, 83.25s/it]
 16%|█▌        | 810/5198 [4:55:38<114:09:17, 93.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.24s/it][A100%|██████████| 1/1 [01:23<00:00, 83.24s/it]
 16%|█▌        | 810/5198 [4:56:02<114:09:51, 93.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.26s/it][A100%|██████████| 1/1 [01:23<00:00, 83.26s/it]
 16%|█▌        | 810/5198 [4:56:33<114:09:48, 93.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.27s/it][A100%|██████████| 1/1 [01:23<00:00, 83.27s/it]
 16%|█▌        | 810/5198 [4:56:35<114:10:00, 93.66s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_760
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.28s/it][A100%|██████████| 1/1 [01:27<00:00, 87.28s/it]
 16%|█▌        | 811/5198 [4:58:21<111:55:23, 91.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:48:16,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=801, skipped=0, lr=[1.9371181348415456e-05], mom=[(0.9, 0.999)]
steps: 801 loss: 0.5983 iter time (s): 86.590 samples/sec: 1.478

100%|██████████| 1/1 [01:27<00:00, 87.45s/it][A100%|██████████| 1/1 [01:27<00:00, 87.45s/it]
 16%|█▌        | 811/5198 [4:58:36<111:52:12, 91.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.52s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 16%|█▌        | 811/5198 [4:58:22<111:53:11, 91.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.49s/it][A100%|██████████| 1/1 [01:27<00:00, 87.49s/it]
 16%|█▌        | 811/5198 [4:58:42<111:53:18, 91.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.52s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 16%|█▌        | 811/5198 [4:57:06<111:53:25, 91.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.48s/it][A100%|██████████| 1/1 [01:27<00:00, 87.48s/it]
 16%|█▌        | 811/5198 [4:57:29<111:52:54, 91.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.52s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 16%|█▌        | 811/5198 [4:58:00<111:53:36, 91.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.50s/it][A100%|██████████| 1/1 [01:27<00:00, 87.50s/it]
 16%|█▌        | 811/5198 [4:58:02<111:53:20, 91.82s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_761
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.02s/it][A100%|██████████| 1/1 [02:13<00:00, 133.02s/it]
 16%|█▌        | 812/5198 [5:00:34<127:00:08, 104.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:50:31,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=802, skipped=0, lr=[1.9369203092787376e-05], mom=[(0.9, 0.999)]
steps: 802 loss: 0.5696 iter time (s): 133.586 samples/sec: 0.958

100%|██████████| 1/1 [02:14<00:00, 134.61s/it][A100%|██████████| 1/1 [02:14<00:00, 134.61s/it]
 16%|█▌        | 812/5198 [5:00:50<127:29:49, 104.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.73s/it][A100%|██████████| 1/1 [02:14<00:00, 134.73s/it]
 16%|█▌        | 812/5198 [5:00:37<127:32:53, 104.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.63s/it][A100%|██████████| 1/1 [02:14<00:00, 134.63s/it]
 16%|█▌        | 812/5198 [5:00:56<127:30:48, 104.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.68s/it][A100%|██████████| 1/1 [02:14<00:00, 134.68s/it]
 16%|█▌        | 812/5198 [4:59:21<127:31:55, 104.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.64s/it][A100%|██████████| 1/1 [02:14<00:00, 134.64s/it]
 16%|█▌        | 812/5198 [5:00:15<127:31:18, 104.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.98s/it][A100%|██████████| 1/1 [02:14<00:00, 134.98s/it]
 16%|█▌        | 812/5198 [4:59:44<127:38:19, 104.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:15<00:00, 135.30s/it][A100%|██████████| 1/1 [02:15<00:00, 135.30s/it]
 16%|█▌        | 812/5198 [5:00:18<127:45:36, 104.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_762
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.72s/it][A100%|██████████| 1/1 [02:02<00:00, 122.72s/it]
 16%|█▌        | 813/5198 [5:02:37<133:46:34, 109.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:52:33,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=803, skipped=0, lr=[1.9367221831607197e-05], mom=[(0.9, 0.999)]
steps: 803 loss: 0.5455 iter time (s): 120.756 samples/sec: 1.060

100%|██████████| 1/1 [02:02<00:00, 122.43s/it][A100%|██████████| 1/1 [02:02<00:00, 122.43s/it]
 16%|█▌        | 813/5198 [5:02:53<133:58:08, 109.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.34s/it][A100%|██████████| 1/1 [02:02<00:00, 122.34s/it]
 16%|█▌        | 813/5198 [5:02:39<133:58:15, 109.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.46s/it][A100%|██████████| 1/1 [02:02<00:00, 122.46s/it]
 16%|█▌        | 813/5198 [5:02:59<133:59:29, 110.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.39s/it][A100%|██████████| 1/1 [02:02<00:00, 122.39s/it]
 16%|█▌        | 813/5198 [5:01:23<133:58:47, 109.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.08s/it][A100%|██████████| 1/1 [02:02<00:00, 122.08s/it]
 16%|█▌        | 813/5198 [5:01:46<133:56:30, 109.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.40s/it][A100%|██████████| 1/1 [02:02<00:00, 122.40s/it]
 16%|█▌        | 813/5198 [5:02:17<133:58:43, 109.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.76s/it][A100%|██████████| 1/1 [02:01<00:00, 121.76s/it]
 16%|█▌        | 813/5198 [5:02:19<133:54:26, 109.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_763
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.70s/it][A100%|██████████| 1/1 [01:20<00:00, 80.70s/it]
 16%|█▌        | 814/5198 [5:03:58<123:09:37, 101.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:53:53,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=804, skipped=0, lr=[1.936523756551056e-05], mom=[(0.9, 0.999)]
steps: 804 loss: 0.5925 iter time (s): 78.578 samples/sec: 1.629

100%|██████████| 1/1 [01:19<00:00, 79.52s/it][A100%|██████████| 1/1 [01:19<00:00, 79.52s/it]
 16%|█▌        | 814/5198 [5:04:12<122:48:35, 100.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.52s/it][A100%|██████████| 1/1 [01:19<00:00, 79.52s/it]
 16%|█▌        | 814/5198 [5:03:59<122:48:44, 100.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.45s/it][A100%|██████████| 1/1 [01:19<00:00, 79.45s/it]
 16%|█▌        | 814/5198 [5:04:18<122:48:06, 100.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.56s/it][A100%|██████████| 1/1 [01:19<00:00, 79.56s/it]
 16%|█▌        | 814/5198 [5:02:43<122:50:04, 100.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.55s/it][A100%|██████████| 1/1 [01:19<00:00, 79.55s/it]
 16%|█▌        | 814/5198 [5:03:06<122:48:11, 100.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.54s/it][A100%|██████████| 1/1 [01:19<00:00, 79.54s/it]
 16%|█▌        | 814/5198 [5:03:37<122:49:31, 100.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.52s/it][A100%|██████████| 1/1 [01:19<00:00, 79.52s/it]
 16%|█▌        | 814/5198 [5:03:39<122:46:08, 100.81s/it]Shard 814 in [76, 158, 182, 242, 293, 363, 418, 421, 664, 752, 814, 842, 991, 1266, 1366, 1425, 1464, 1574, 1728, 2166, 2441, 2563, 2739, 2854, 2894, 3089, 3181, 3395, 3576, 3831, 4300, 4589, 4947, 4950]: file /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_764 skipped to avoid exceeding cuda memory
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_50
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.90s/it][A100%|██████████| 1/1 [01:53<00:00, 113.90s/it]
 16%|█▌        | 816/5198 [5:05:52<98:18:34, 80.77s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:55:48,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=805, skipped=0, lr=[1.9363250295134063e-05], mom=[(0.9, 0.999)]
steps: 805 loss: 0.8115 iter time (s): 114.189 samples/sec: 1.121

100%|██████████| 1/1 [01:55<00:00, 115.16s/it][A100%|██████████| 1/1 [01:55<00:00, 115.16s/it]
 16%|█▌        | 816/5198 [5:06:07<98:26:56, 80.88s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.27s/it][A100%|██████████| 1/1 [01:55<00:00, 115.27s/it]
 16%|█▌        | 816/5198 [5:05:54<98:28:57, 80.91s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.27s/it][A100%|██████████| 1/1 [01:55<00:00, 115.27s/it]
 16%|█▌        | 816/5198 [5:06:14<98:28:29, 80.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.21s/it][A100%|██████████| 1/1 [01:55<00:00, 115.21s/it]
 16%|█▌        | 816/5198 [5:04:38<98:28:28, 80.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.28s/it][A100%|██████████| 1/1 [01:55<00:00, 115.28s/it]
 16%|█▌        | 816/5198 [5:05:01<98:28:38, 80.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.27s/it][A100%|██████████| 1/1 [01:55<00:00, 115.27s/it]
 16%|█▌        | 816/5198 [5:05:32<98:29:15, 80.91s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.27s/it][A100%|██████████| 1/1 [01:55<00:00, 115.27s/it]
 16%|█▌        | 816/5198 [5:05:34<98:27:25, 80.89s/it]  Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_765
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.58s/it][A100%|██████████| 1/1 [01:45<00:00, 105.58s/it]
 16%|█▌        | 817/5198 [5:07:38<105:49:46, 86.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:57:33,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=806, skipped=0, lr=[1.9361260021115272e-05], mom=[(0.9, 0.999)]
steps: 806 loss: 0.5772 iter time (s): 104.135 samples/sec: 1.229

100%|██████████| 1/1 [01:45<00:00, 105.17s/it][A100%|██████████| 1/1 [01:45<00:00, 105.17s/it]
 16%|█▌        | 817/5198 [5:07:52<105:45:33, 86.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.06s/it][A100%|██████████| 1/1 [01:45<00:00, 105.06s/it]
 16%|█▌        | 817/5198 [5:07:39<105:44:58, 86.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.09s/it][A100%|██████████| 1/1 [01:45<00:00, 105.09s/it]
 16%|█▌        | 817/5198 [5:07:59<105:45:10, 86.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.05s/it][A100%|██████████| 1/1 [01:45<00:00, 105.05s/it]
 16%|█▌        | 817/5198 [5:06:23<105:44:22, 86.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.00s/it][A100%|██████████| 1/1 [01:45<00:00, 105.00s/it]
 16%|█▌        | 817/5198 [5:06:46<105:43:45, 86.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.01s/it][A100%|██████████| 1/1 [01:45<00:00, 105.01s/it]
 16%|█▌        | 817/5198 [5:07:17<105:44:22, 86.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.02s/it][A100%|██████████| 1/1 [01:45<00:00, 105.02s/it]
 16%|█▌        | 817/5198 [5:07:19<105:43:06, 86.87s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_766
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.17s/it][A100%|██████████| 1/1 [01:54<00:00, 114.17s/it]
 16%|█▌        | 818/5198 [5:09:32<114:31:15, 94.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 04:59:28,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=807, skipped=0, lr=[1.9359266744092718e-05], mom=[(0.9, 0.999)]
steps: 807 loss: 0.5909 iter time (s): 113.988 samples/sec: 1.123

100%|██████████| 1/1 [01:54<00:00, 114.90s/it][A100%|██████████| 1/1 [01:54<00:00, 114.90s/it]
 16%|█▌        | 818/5198 [5:09:47<114:38:43, 94.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.83s/it][A100%|██████████| 1/1 [01:54<00:00, 114.83s/it]
 16%|█▌        | 818/5198 [5:09:34<114:36:59, 94.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.86s/it][A100%|██████████| 1/1 [01:54<00:00, 114.86s/it]
 16%|█▌        | 818/5198 [5:09:54<114:37:42, 94.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.90s/it][A100%|██████████| 1/1 [01:54<00:00, 114.90s/it]
 16%|█▌        | 818/5198 [5:08:18<114:37:58, 94.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.89s/it][A100%|██████████| 1/1 [01:54<00:00, 114.89s/it]
 16%|█▌        | 818/5198 [5:08:41<114:37:14, 94.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.92s/it][A100%|██████████| 1/1 [01:54<00:00, 114.92s/it]
 16%|█▌        | 818/5198 [5:09:12<114:38:18, 94.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.92s/it][A100%|██████████| 1/1 [01:54<00:00, 114.92s/it]
 16%|█▌        | 818/5198 [5:09:14<114:37:19, 94.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_767
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.97s/it][A100%|██████████| 1/1 [01:47<00:00, 107.97s/it]
 16%|█▌        | 819/5198 [5:11:20<119:07:56, 97.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:01:16,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=808, skipped=0, lr=[1.9357270464705897e-05], mom=[(0.9, 0.999)]
steps: 808 loss: 0.5099 iter time (s): 106.887 samples/sec: 1.198

100%|██████████| 1/1 [01:47<00:00, 107.73s/it][A100%|██████████| 1/1 [01:47<00:00, 107.74s/it]
 16%|█▌        | 819/5198 [5:11:35<119:05:30, 97.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.92s/it][A100%|██████████| 1/1 [01:47<00:00, 107.92s/it]
 16%|█▌        | 819/5198 [5:11:22<119:07:51, 97.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.83s/it][A100%|██████████| 1/1 [01:47<00:00, 107.83s/it]
 16%|█▌        | 819/5198 [5:11:41<119:06:33, 97.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.82s/it][A100%|██████████| 1/1 [01:47<00:00, 107.82s/it]
 16%|█▌        | 819/5198 [5:10:06<119:06:32, 97.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.83s/it][A100%|██████████| 1/1 [01:47<00:00, 107.83s/it]
 16%|█▌        | 819/5198 [5:10:29<119:06:11, 97.92s/it]
100%|██████████| 1/1 [01:47<00:00, 107.77s/it][A100%|██████████| 1/1 [01:47<00:00, 107.77s/it]
 16%|█▌        | 819/5198 [5:11:00<119:05:54, 97.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.80s/it][A100%|██████████| 1/1 [01:47<00:00, 107.80s/it]
 16%|█▌        | 819/5198 [5:11:02<119:05:42, 97.91s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_768
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.20s/it][A100%|██████████| 1/1 [01:29<00:00, 89.20s/it]
[2024-06-30 05:02:45,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=809, skipped=0, lr=[1.935527118359526e-05], mom=[(0.9, 0.999)]
steps: 809 loss: 0.5411 iter time (s): 87.947 samples/sec: 1.455

100%|██████████| 1/1 [01:28<00:00, 88.95s/it][A100%|██████████| 1/1 [01:28<00:00, 88.95s/it]

100%|██████████| 1/1 [01:28<00:00, 88.80s/it][A100%|██████████| 1/1 [01:28<00:00, 88.80s/it]

100%|██████████| 1/1 [01:28<00:00, 88.89s/it][A100%|██████████| 1/1 [01:28<00:00, 88.89s/it]

100%|██████████| 1/1 [01:28<00:00, 88.94s/it][A100%|██████████| 1/1 [01:28<00:00, 88.94s/it]

100%|██████████| 1/1 [01:28<00:00, 88.87s/it][A100%|██████████| 1/1 [01:28<00:00, 88.87s/it]

100%|██████████| 1/1 [01:28<00:00, 88.90s/it][A100%|██████████| 1/1 [01:28<00:00, 88.90s/it]

100%|██████████| 1/1 [01:28<00:00, 88.90s/it][A100%|██████████| 1/1 [01:28<00:00, 88.90s/it]
Checkpointing at shard 819
[2024-06-30 05:02:46,317] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step809 is about to be saved!
[2024-06-30 05:02:47,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_00-model_states.pt...
[2024-06-30 05:02:51,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_02-model_states.pt...
[2024-06-30 05:02:52,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_00-model_states.pt.
[2024-06-30 05:02:58,108] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_08-model_states.pt...
[2024-06-30 05:02:59,825] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_07-model_states.pt...
[2024-06-30 05:02:59,976] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_03-model_states.pt...
[2024-06-30 05:03:00,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_04-model_states.pt...
[2024-06-30 05:03:00,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_06-model_states.pt...
[2024-06-30 05:03:01,468] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_05-model_states.pt...
[2024-06-30 05:03:01,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_01-model_states.pt...
[2024-06-30 05:06:37,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_02-model_states.pt.
[2024-06-30 05:06:37,365] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_01_model_states.pt
[2024-06-30 05:06:37,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_01_model_states.pt...
[2024-06-30 05:06:37,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_01_model_states.pt.
[2024-06-30 05:06:37,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
[2024-06-30 05:06:43,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_05-model_states.pt.
[2024-06-30 05:06:44,477] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_04_model_states.pt...
[2024-06-30 05:06:44,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_04_model_states.pt.
[2024-06-30 05:06:44,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
[2024-06-30 05:06:45,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_01-model_states.pt.
[2024-06-30 05:06:46,018] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_00_model_states.pt
[2024-06-30 05:06:46,019] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_00_model_states.pt...
[2024-06-30 05:06:46,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_00_model_states.pt.
[2024-06-30 05:06:46,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
[2024-06-30 05:06:49,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_07-model_states.pt.
[2024-06-30 05:06:49,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_03-model_states.pt.
[2024-06-30 05:06:50,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_04-model_states.pt.
[2024-06-30 05:06:50,314] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_06_model_states.pt...
[2024-06-30 05:06:50,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_06_model_states.pt.
[2024-06-30 05:06:50,397] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
[2024-06-30 05:06:50,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_02_model_states.pt...
[2024-06-30 05:06:50,649] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_02_model_states.pt.
[2024-06-30 05:06:50,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
[2024-06-30 05:06:50,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_06-model_states.pt.
[2024-06-30 05:06:50,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_08-model_states.pt.
[2024-06-30 05:06:50,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_03_model_states.pt...
[2024-06-30 05:06:50,957] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_03_model_states.pt.
[2024-06-30 05:06:50,957] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
[2024-06-30 05:06:51,191] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_09-model_states.pt...
[2024-06-30 05:06:51,347] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_05_model_states.pt...
[2024-06-30 05:06:51,399] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_05_model_states.pt.
[2024-06-30 05:06:51,400] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
[2024-06-30 05:06:51,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/layer_09-model_states.pt.
[2024-06-30 05:06:51,825] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_07_model_states.pt...
[2024-06-30 05:06:51,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step809/mp_rank_07_model_states.pt.
[2024-06-30 05:06:51,876] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step809 is ready now!
 16%|█▌        | 820/5198 [5:17:10<199:46:09, 164.27s/it]Checkpoint saved using --- 245.5603232383728 seconds ---
 16%|█▌        | 820/5198 [5:16:57<199:42:12, 164.21s/it] 16%|█▌        | 820/5198 [5:15:40<199:37:54, 164.16s/it] 16%|█▌        | 820/5198 [5:16:03<199:35:43, 164.13s/it] 16%|█▌        | 820/5198 [5:16:58<200:44:38, 165.07s/it] 16%|█▌        | 820/5198 [5:16:34<199:35:23, 164.12s/it] 16%|█▌        | 820/5198 [5:16:36<199:34:26, 164.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_769
 16%|█▌        | 820/5198 [5:17:16<199:39:47, 164.18s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.04s/it][A100%|██████████| 1/1 [01:23<00:00, 83.04s/it]
 16%|█▌        | 821/5198 [5:18:21<172:16:55, 141.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:08:16,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[1.9353268901402226e-05], mom=[(0.9, 0.999)]
steps: 810 loss: 0.5895 iter time (s): 84.928 samples/sec: 1.507

100%|██████████| 1/1 [01:25<00:00, 85.39s/it][A100%|██████████| 1/1 [01:25<00:00, 85.39s/it]
 16%|█▌        | 821/5198 [5:18:36<172:24:13, 141.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.47s/it][A100%|██████████| 1/1 [01:25<00:00, 85.47s/it]
 16%|█▌        | 821/5198 [5:18:22<172:22:57, 141.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.63s/it][A100%|██████████| 1/1 [01:25<00:00, 85.63s/it]
 16%|█▌        | 821/5198 [5:18:42<172:24:38, 141.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.70s/it][A100%|██████████| 1/1 [01:25<00:00, 85.70s/it]
 16%|█▌        | 821/5198 [5:17:06<172:24:37, 141.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.77s/it][A100%|██████████| 1/1 [01:25<00:00, 85.77s/it]
 16%|█▌        | 821/5198 [5:17:29<172:24:39, 141.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.79s/it][A100%|██████████| 1/1 [01:25<00:00, 85.79s/it]
 16%|█▌        | 821/5198 [5:18:00<172:24:48, 141.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.84s/it][A100%|██████████| 1/1 [01:25<00:00, 85.84s/it]
 16%|█▌        | 821/5198 [5:18:02<172:25:10, 141.81s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_770
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.06s/it][A100%|██████████| 1/1 [01:41<00:00, 101.06s/it]
 16%|█▌        | 822/5198 [5:20:03<157:58:38, 129.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:09:58,703] [INFO] [logging.py:96:log_dist] [Rank 0] step=811, skipped=0, lr=[1.9351263618769177e-05], mom=[(0.9, 0.999)]
steps: 811 loss: 0.5852 iter time (s): 100.760 samples/sec: 1.270

100%|██████████| 1/1 [01:41<00:00, 101.63s/it][A100%|██████████| 1/1 [01:41<00:00, 101.63s/it]
 16%|█▌        | 822/5198 [5:20:17<158:13:07, 130.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.68s/it][A100%|██████████| 1/1 [01:41<00:00, 101.68s/it]
 16%|█▌        | 822/5198 [5:20:04<158:13:09, 130.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.60s/it][A100%|██████████| 1/1 [01:41<00:00, 101.61s/it]
 16%|█▌        | 822/5198 [5:20:24<158:12:52, 130.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.67s/it][A100%|██████████| 1/1 [01:41<00:00, 101.67s/it]
 16%|█▌        | 822/5198 [5:18:48<158:14:15, 130.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.68s/it][A100%|██████████| 1/1 [01:41<00:00, 101.68s/it]
 16%|█▌        | 822/5198 [5:19:11<158:14:28, 130.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.72s/it][A100%|██████████| 1/1 [01:41<00:00, 101.72s/it]
 16%|█▌        | 822/5198 [5:19:42<158:15:24, 130.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.68s/it][A100%|██████████| 1/1 [01:41<00:00, 101.68s/it]
 16%|█▌        | 822/5198 [5:19:44<158:14:51, 130.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_771
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.28s/it][A100%|██████████| 1/1 [01:27<00:00, 87.28s/it]
 16%|█▌        | 823/5198 [5:21:30<142:49:40, 117.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:11:25,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=812, skipped=0, lr=[1.9349255336339455e-05], mom=[(0.9, 0.999)]
steps: 812 loss: 0.5971 iter time (s): 86.133 samples/sec: 1.486

100%|██████████| 1/1 [01:27<00:00, 87.03s/it][A100%|██████████| 1/1 [01:27<00:00, 87.03s/it]
 16%|█▌        | 823/5198 [5:21:44<142:50:33, 117.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.93s/it][A100%|██████████| 1/1 [01:26<00:00, 86.93s/it]
 16%|█▌        | 823/5198 [5:21:31<142:48:27, 117.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.08s/it][A100%|██████████| 1/1 [01:27<00:00, 87.08s/it]
 16%|█▌        | 823/5198 [5:21:51<142:51:21, 117.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.99s/it][A100%|██████████| 1/1 [01:26<00:00, 86.99s/it]
 16%|█▌        | 823/5198 [5:20:15<142:50:21, 117.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.98s/it][A100%|██████████| 1/1 [01:26<00:00, 86.98s/it]
 16%|█▌        | 823/5198 [5:20:38<142:50:19, 117.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.00s/it][A100%|██████████| 1/1 [01:27<00:00, 87.00s/it]
 16%|█▌        | 823/5198 [5:21:09<142:51:24, 117.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.00s/it][A100%|██████████| 1/1 [01:27<00:00, 87.00s/it]
 16%|█▌        | 823/5198 [5:21:11<142:50:59, 117.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_772
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.26s/it][A100%|██████████| 1/1 [01:37<00:00, 97.26s/it]
 16%|█▌        | 824/5198 [5:23:08<135:35:12, 111.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:13:03,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=813, skipped=0, lr=[1.9347244054757374e-05], mom=[(0.9, 0.999)]
steps: 813 loss: 0.5538 iter time (s): 96.867 samples/sec: 1.321

100%|██████████| 1/1 [01:37<00:00, 97.74s/it][A100%|██████████| 1/1 [01:37<00:00, 97.74s/it]
 16%|█▌        | 824/5198 [5:23:22<135:43:21, 111.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.86s/it][A100%|██████████| 1/1 [01:37<00:00, 97.86s/it]
 16%|█▌        | 824/5198 [5:23:09<135:44:16, 111.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.73s/it][A100%|██████████| 1/1 [01:37<00:00, 97.73s/it]
 16%|█▌        | 824/5198 [5:23:28<135:43:30, 111.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.79s/it][A100%|██████████| 1/1 [01:37<00:00, 97.79s/it]
 16%|█▌        | 824/5198 [5:21:52<135:43:59, 111.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.77s/it][A100%|██████████| 1/1 [01:37<00:00, 97.77s/it]
 16%|█▌        | 824/5198 [5:22:16<135:43:37, 111.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.75s/it][A100%|██████████| 1/1 [01:37<00:00, 97.75s/it]
 16%|█▌        | 824/5198 [5:22:47<135:43:52, 111.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.76s/it][A100%|██████████| 1/1 [01:37<00:00, 97.76s/it]
 16%|█▌        | 824/5198 [5:22:49<135:43:53, 111.71s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_773
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.66s/it][A100%|██████████| 1/1 [01:52<00:00, 112.66s/it]
 16%|█▌        | 825/5198 [5:25:00<135:59:29, 111.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:14:56,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=814, skipped=0, lr=[1.93452297746682e-05], mom=[(0.9, 0.999)]
steps: 814 loss: 0.5524 iter time (s): 112.316 samples/sec: 1.140

100%|██████████| 1/1 [01:53<00:00, 113.11s/it][A100%|██████████| 1/1 [01:53<00:00, 113.11s/it]
 16%|█▌        | 825/5198 [5:25:02<136:12:41, 112.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.30s/it][A100%|██████████| 1/1 [01:53<00:00, 113.30s/it]
 16%|█▌        | 825/5198 [5:25:16<136:16:10, 112.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.26s/it][A100%|██████████| 1/1 [01:53<00:00, 113.26s/it]
 16%|█▌        | 825/5198 [5:25:22<136:15:20, 112.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.23s/it][A100%|██████████| 1/1 [01:53<00:00, 113.23s/it]
 16%|█▌        | 825/5198 [5:23:46<136:14:56, 112.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.22s/it][A100%|██████████| 1/1 [01:53<00:00, 113.22s/it]
 16%|█▌        | 825/5198 [5:24:09<136:14:33, 112.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.20s/it][A100%|██████████| 1/1 [01:53<00:00, 113.20s/it]
 16%|█▌        | 825/5198 [5:24:40<136:14:24, 112.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.21s/it][A100%|██████████| 1/1 [01:53<00:00, 113.21s/it]
 16%|█▌        | 825/5198 [5:24:42<136:14:43, 112.16s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_774
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.16s/it][A100%|██████████| 1/1 [01:36<00:00, 96.16s/it]
 16%|█▌        | 826/5198 [5:26:37<130:19:35, 107.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:16:32,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=815, skipped=0, lr=[1.9343212496718163e-05], mom=[(0.9, 0.999)]
steps: 815 loss: 0.5603 iter time (s): 94.962 samples/sec: 1.348

100%|██████████| 1/1 [01:35<00:00, 95.80s/it][A100%|██████████| 1/1 [01:35<00:00, 95.80s/it]
 16%|█▌        | 826/5198 [5:26:51<130:19:22, 107.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.07s/it][A100%|██████████| 1/1 [01:36<00:00, 96.07s/it]
 16%|█▌        | 826/5198 [5:26:38<130:22:53, 107.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.03s/it][A100%|██████████| 1/1 [01:36<00:00, 96.03s/it]
 16%|█▌        | 826/5198 [5:26:58<130:23:51, 107.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.04s/it][A100%|██████████| 1/1 [01:36<00:00, 96.04s/it]
 16%|█▌        | 826/5198 [5:25:22<130:23:53, 107.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.02s/it][A100%|██████████| 1/1 [01:36<00:00, 96.02s/it]
 16%|█▌        | 826/5198 [5:25:45<130:23:05, 107.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.13s/it][A100%|██████████| 1/1 [01:36<00:00, 96.13s/it]
 16%|█▌        | 826/5198 [5:26:16<130:25:17, 107.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.14s/it][A100%|██████████| 1/1 [01:36<00:00, 96.14s/it]
 16%|█▌        | 826/5198 [5:26:18<130:25:38, 107.40s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_775
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.45s/it][A100%|██████████| 1/1 [01:29<00:00, 89.45s/it]
 16%|█▌        | 827/5198 [5:28:06<123:52:31, 102.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:18:01,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=816, skipped=0, lr=[1.9341192221554456e-05], mom=[(0.9, 0.999)]
steps: 816 loss: 0.5784 iter time (s): 88.178 samples/sec: 1.452

100%|██████████| 1/1 [01:29<00:00, 89.25s/it][A100%|██████████| 1/1 [01:29<00:00, 89.25s/it]
 16%|█▌        | 827/5198 [5:28:21<123:45:21, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.24s/it][A100%|██████████| 1/1 [01:29<00:00, 89.24s/it]
 16%|█▌        | 827/5198 [5:28:07<123:47:34, 101.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.19s/it][A100%|██████████| 1/1 [01:29<00:00, 89.19s/it]
 16%|█▌        | 827/5198 [5:28:27<123:47:11, 101.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.23s/it][A100%|██████████| 1/1 [01:29<00:00, 89.23s/it]
 16%|█▌        | 827/5198 [5:26:51<123:48:07, 101.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.26s/it][A100%|██████████| 1/1 [01:29<00:00, 89.26s/it]
 16%|█▌        | 827/5198 [5:27:14<123:48:15, 101.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.20s/it][A100%|██████████| 1/1 [01:29<00:00, 89.20s/it]
 16%|█▌        | 827/5198 [5:27:45<123:48:29, 101.97s/it]
100%|██████████| 1/1 [01:29<00:00, 89.17s/it][A100%|██████████| 1/1 [01:29<00:00, 89.17s/it]

 16%|█▌        | 827/5198 [5:27:47<123:47:56, 101.96s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_776
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.26s/it][A100%|██████████| 1/1 [01:29<00:00, 89.26s/it]
 16%|█▌        | 828/5198 [5:29:36<119:16:31, 98.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:19:31,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=817, skipped=0, lr=[1.9339168949825232e-05], mom=[(0.9, 0.999)]
steps: 817 loss: 0.5454 iter time (s): 88.474 samples/sec: 1.447

100%|██████████| 1/1 [01:29<00:00, 89.63s/it][A100%|██████████| 1/1 [01:29<00:00, 89.63s/it]
 16%|█▌        | 828/5198 [5:29:50<119:16:18, 98.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.54s/it][A100%|██████████| 1/1 [01:29<00:00, 89.54s/it]
 16%|█▌        | 828/5198 [5:29:37<119:15:50, 98.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.57s/it][A100%|██████████| 1/1 [01:29<00:00, 89.57s/it]
 16%|█▌        | 828/5198 [5:29:56<119:16:14, 98.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.76s/it][A100%|██████████| 1/1 [01:29<00:00, 89.76s/it]
 16%|█▌        | 828/5198 [5:28:21<119:21:03, 98.32s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.67s/it][A100%|██████████| 1/1 [01:29<00:00, 89.67s/it]
 16%|█▌        | 828/5198 [5:29:15<119:19:25, 98.30s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.84s/it][A100%|██████████| 1/1 [01:29<00:00, 89.84s/it]
 16%|█▌        | 828/5198 [5:28:44<119:22:56, 98.35s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.11s/it][A100%|██████████| 1/1 [01:30<00:00, 90.11s/it]
 16%|█▌        | 828/5198 [5:29:17<119:28:24, 98.42s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_777
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.48s/it][A100%|██████████| 1/1 [01:42<00:00, 102.48s/it]
 16%|█▌        | 829/5198 [5:31:18<120:50:33, 99.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:21:14,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=818, skipped=0, lr=[1.9337142682179614e-05], mom=[(0.9, 0.999)]
steps: 818 loss: 0.6079 iter time (s): 101.302 samples/sec: 1.264

100%|██████████| 1/1 [01:42<00:00, 102.84s/it][A100%|██████████| 1/1 [01:42<00:00, 102.84s/it]
 16%|█▌        | 829/5198 [5:31:33<120:54:42, 99.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.87s/it][A100%|██████████| 1/1 [01:42<00:00, 102.87s/it]
 16%|█▌        | 829/5198 [5:31:20<120:55:04, 99.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.86s/it][A100%|██████████| 1/1 [01:42<00:00, 102.86s/it]
 16%|█▌        | 829/5198 [5:31:39<120:55:10, 99.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.59s/it][A100%|██████████| 1/1 [01:42<00:00, 102.59s/it]
 16%|█▌        | 829/5198 [5:30:03<120:52:32, 99.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.56s/it][A100%|██████████| 1/1 [01:42<00:00, 102.56s/it]
 16%|█▌        | 829/5198 [5:30:27<120:53:14, 99.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.68s/it][A100%|██████████| 1/1 [01:42<00:00, 102.68s/it]
 16%|█▌        | 829/5198 [5:30:58<120:53:22, 99.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.26s/it][A100%|██████████| 1/1 [01:42<00:00, 102.26s/it]
 16%|█▌        | 829/5198 [5:31:00<120:50:37, 99.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_778
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.28s/it][A100%|██████████| 1/1 [01:46<00:00, 106.28s/it]
 16%|█▌        | 830/5198 [5:33:05<123:18:09, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:23:00,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=819, skipped=0, lr=[1.933511341926767e-05], mom=[(0.9, 0.999)]
steps: 819 loss: 0.5913 iter time (s): 105.615 samples/sec: 1.212

100%|██████████| 1/1 [01:46<00:00, 106.52s/it][A100%|██████████| 1/1 [01:46<00:00, 106.52s/it]
 16%|█▌        | 830/5198 [5:33:20<123:23:23, 101.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.63s/it][A100%|██████████| 1/1 [01:46<00:00, 106.63s/it]
 16%|█▌        | 830/5198 [5:33:06<123:25:59, 101.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.58s/it][A100%|██████████| 1/1 [01:46<00:00, 106.58s/it]
 16%|█▌        | 830/5198 [5:33:26<123:25:03, 101.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.66s/it][A100%|██████████| 1/1 [01:46<00:00, 106.66s/it]
 16%|█▌        | 830/5198 [5:31:50<123:24:56, 101.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.56s/it][A100%|██████████| 1/1 [01:46<00:00, 106.56s/it]
 16%|█▌        | 830/5198 [5:32:13<123:23:19, 101.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.56s/it][A100%|██████████| 1/1 [01:46<00:00, 106.56s/it]
 16%|█▌        | 830/5198 [5:32:44<123:23:20, 101.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.57s/it][A100%|██████████| 1/1 [01:46<00:00, 106.57s/it]
 16%|█▌        | 830/5198 [5:32:46<123:21:38, 101.67s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_779
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.87s/it][A100%|██████████| 1/1 [01:20<00:00, 80.87s/it]
 16%|█▌        | 831/5198 [5:34:26<115:46:44, 95.44s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:24:21,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[1.9333081161740437e-05], mom=[(0.9, 0.999)]
steps: 820 loss: 0.5754 iter time (s): 79.315 samples/sec: 1.614

100%|██████████| 1/1 [01:20<00:00, 80.37s/it][A100%|██████████| 1/1 [01:20<00:00, 80.37s/it]
 16%|█▌        | 831/5198 [5:34:40<115:36:50, 95.31s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.13s/it][A100%|██████████| 1/1 [01:20<00:00, 80.13s/it]
 16%|█▌        | 831/5198 [5:34:27<115:33:34, 95.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.16s/it][A100%|██████████| 1/1 [01:20<00:00, 80.16s/it]
 16%|█▌        | 831/5198 [5:34:46<115:33:31, 95.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.16s/it][A100%|██████████| 1/1 [01:20<00:00, 80.16s/it]
 16%|█▌        | 831/5198 [5:33:10<115:33:19, 95.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.15s/it][A100%|██████████| 1/1 [01:20<00:00, 80.16s/it]
 16%|█▌        | 831/5198 [5:33:33<115:32:10, 95.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.19s/it][A100%|██████████| 1/1 [01:20<00:00, 80.19s/it]
 16%|█▌        | 831/5198 [5:34:05<115:32:59, 95.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.19s/it][A100%|██████████| 1/1 [01:20<00:00, 80.19s/it]
 16%|█▌        | 831/5198 [5:34:07<115:31:42, 95.24s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_51
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.05s/it][A100%|██████████| 1/1 [02:02<00:00, 122.05s/it]
 16%|█▌        | 832/5198 [5:36:28<125:27:37, 103.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:26:24,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=821, skipped=0, lr=[1.9331045910249925e-05], mom=[(0.9, 0.999)]
steps: 821 loss: 0.8203 iter time (s): 122.425 samples/sec: 1.046

100%|██████████| 1/1 [02:03<00:00, 123.23s/it][A100%|██████████| 1/1 [02:03<00:00, 123.23s/it]
 16%|█▌        | 832/5198 [5:36:43<125:44:29, 103.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.32s/it][A100%|██████████| 1/1 [02:03<00:00, 123.32s/it]
 16%|█▌        | 832/5198 [5:36:30<125:44:03, 103.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.22s/it][A100%|██████████| 1/1 [02:03<00:00, 123.22s/it]
 16%|█▌        | 832/5198 [5:36:49<125:41:43, 103.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.25s/it][A100%|██████████| 1/1 [02:03<00:00, 123.25s/it]
 16%|█▌        | 832/5198 [5:35:13<125:42:17, 103.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.36s/it][A100%|██████████| 1/1 [02:03<00:00, 123.36s/it]
 16%|█▌        | 832/5198 [5:35:37<125:43:58, 103.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.31s/it][A100%|██████████| 1/1 [02:03<00:00, 123.31s/it]
 16%|█▌        | 832/5198 [5:36:08<125:43:26, 103.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.30s/it][A100%|██████████| 1/1 [02:03<00:00, 123.30s/it]
 16%|█▌        | 832/5198 [5:36:10<125:42:13, 103.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_780
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.29s/it][A100%|██████████| 1/1 [01:29<00:00, 89.29s/it]
 16%|█▌        | 833/5198 [5:37:57<120:20:11, 99.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:27:52,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=822, skipped=0, lr=[1.9329007665449083e-05], mom=[(0.9, 0.999)]
steps: 822 loss: 0.5523 iter time (s): 87.593 samples/sec: 1.461

100%|██████████| 1/1 [01:28<00:00, 88.45s/it][A100%|██████████| 1/1 [01:28<00:00, 88.45s/it]
 16%|█▌        | 833/5198 [5:38:12<120:10:41, 99.12s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.53s/it][A100%|██████████| 1/1 [01:28<00:00, 88.53s/it]
 16%|█▌        | 833/5198 [5:37:58<120:12:20, 99.14s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.50s/it][A100%|██████████| 1/1 [01:28<00:00, 88.50s/it]
 16%|█▌        | 833/5198 [5:38:18<120:10:01, 99.11s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.51s/it][A100%|██████████| 1/1 [01:28<00:00, 88.51s/it]
 16%|█▌        | 833/5198 [5:36:42<120:10:32, 99.11s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.51s/it][A100%|██████████| 1/1 [01:28<00:00, 88.51s/it]
 16%|█▌        | 833/5198 [5:37:36<120:11:24, 99.13s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.54s/it][A100%|██████████| 1/1 [01:28<00:00, 88.54s/it]
 16%|█▌        | 833/5198 [5:37:05<120:12:21, 99.14s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.52s/it][A100%|██████████| 1/1 [01:28<00:00, 88.52s/it]
 16%|█▌        | 833/5198 [5:37:38<120:10:42, 99.12s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_781
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.43s/it][A100%|██████████| 1/1 [01:27<00:00, 87.43s/it]
 16%|█▌        | 834/5198 [5:39:25<116:03:24, 95.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:29:20,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=823, skipped=0, lr=[1.9326966427991833e-05], mom=[(0.9, 0.999)]
steps: 823 loss: 0.5512 iter time (s): 86.633 samples/sec: 1.478

100%|██████████| 1/1 [01:27<00:00, 87.54s/it][A100%|██████████| 1/1 [01:27<00:00, 87.54s/it]
 16%|█▌        | 834/5198 [5:39:39<115:56:43, 95.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.43s/it][A100%|██████████| 1/1 [01:27<00:00, 87.43s/it]
 16%|█▌        | 834/5198 [5:39:26<115:55:31, 95.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.57s/it][A100%|██████████| 1/1 [01:27<00:00, 87.57s/it]
 16%|█▌        | 834/5198 [5:39:45<115:56:54, 95.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.51s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 16%|█▌        | 834/5198 [5:38:10<115:56:12, 95.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.50s/it][A100%|██████████| 1/1 [01:27<00:00, 87.50s/it]
 16%|█▌        | 834/5198 [5:38:33<115:56:57, 95.65s/it]
100%|██████████| 1/1 [01:27<00:00, 87.51s/it][A100%|██████████| 1/1 [01:27<00:00, 87.51s/it]
 16%|█▌        | 834/5198 [5:39:04<115:56:35, 95.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.52s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 16%|█▌        | 834/5198 [5:39:06<115:56:14, 95.64s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_782
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.72s/it][A100%|██████████| 1/1 [01:30<00:00, 90.72s/it]
 16%|█▌        | 835/5198 [5:40:56<114:15:08, 94.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:30:51,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=824, skipped=0, lr=[1.932492219853306e-05], mom=[(0.9, 0.999)]
steps: 824 loss: 0.5775 iter time (s): 90.031 samples/sec: 1.422

100%|██████████| 1/1 [01:30<00:00, 90.95s/it][A100%|██████████| 1/1 [01:30<00:00, 90.95s/it]
 16%|█▌        | 835/5198 [5:41:10<114:13:03, 94.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.96s/it][A100%|██████████| 1/1 [01:30<00:00, 90.96s/it]
 16%|█▌        | 835/5198 [5:40:57<114:12:09, 94.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.98s/it][A100%|██████████| 1/1 [01:30<00:00, 90.98s/it]
 16%|█▌        | 835/5198 [5:41:16<114:13:42, 94.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.00s/it][A100%|██████████| 1/1 [01:31<00:00, 91.00s/it]
 16%|█▌        | 835/5198 [5:39:41<114:13:35, 94.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
 16%|█▌        | 835/5198 [5:40:04<114:12:54, 94.24s/it]
100%|██████████| 1/1 [01:30<00:00, 90.95s/it][A100%|██████████| 1/1 [01:30<00:00, 90.95s/it]
 16%|█▌        | 835/5198 [5:40:35<114:12:40, 94.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.96s/it][A100%|██████████| 1/1 [01:30<00:00, 90.96s/it]
 16%|█▌        | 835/5198 [5:40:37<114:12:46, 94.24s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_783
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.66s/it][A100%|██████████| 1/1 [01:24<00:00, 84.66s/it]
 16%|█▌        | 836/5198 [5:42:21<110:46:34, 91.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:32:16,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=825, skipped=0, lr=[1.9322874977728593e-05], mom=[(0.9, 0.999)]
steps: 825 loss: 0.5888 iter time (s): 83.636 samples/sec: 1.530

100%|██████████| 1/1 [01:24<00:00, 84.53s/it][A100%|██████████| 1/1 [01:24<00:00, 84.53s/it]
 16%|█▌        | 836/5198 [5:42:35<110:39:51, 91.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.57s/it][A100%|██████████| 1/1 [01:24<00:00, 84.57s/it]
 16%|█▌        | 836/5198 [5:42:21<110:40:06, 91.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.52s/it][A100%|██████████| 1/1 [01:24<00:00, 84.52s/it]
 16%|█▌        | 836/5198 [5:42:41<110:40:00, 91.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.45s/it][A100%|██████████| 1/1 [01:24<00:00, 84.45s/it]
 16%|█▌        | 836/5198 [5:41:05<110:38:32, 91.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.50s/it][A100%|██████████| 1/1 [01:24<00:00, 84.50s/it]
 16%|█▌        | 836/5198 [5:41:28<110:39:07, 91.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.54s/it][A100%|██████████| 1/1 [01:24<00:00, 84.54s/it]
 16%|█▌        | 836/5198 [5:41:59<110:39:47, 91.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.51s/it][A100%|██████████| 1/1 [01:24<00:00, 84.51s/it]
 16%|█▌        | 836/5198 [5:42:01<110:39:17, 91.32s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_784
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.97s/it][A100%|██████████| 1/1 [01:37<00:00, 97.97s/it]
 16%|█▌        | 837/5198 [5:43:59<113:09:59, 93.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:33:54,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=826, skipped=0, lr=[1.932082476623524e-05], mom=[(0.9, 0.999)]
steps: 826 loss: 0.5645 iter time (s): 97.599 samples/sec: 1.311

100%|██████████| 1/1 [01:38<00:00, 98.39s/it][A100%|██████████| 1/1 [01:38<00:00, 98.39s/it]
 16%|█▌        | 837/5198 [5:44:00<113:12:38, 93.46s/it]
100%|██████████| 1/1 [01:38<00:00, 98.52s/it][A100%|██████████| 1/1 [01:38<00:00, 98.53s/it]
 16%|█▌        | 837/5198 [5:44:13<113:15:19, 93.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.48s/it][A100%|██████████| 1/1 [01:38<00:00, 98.48s/it]
 16%|█▌        | 837/5198 [5:44:19<113:14:30, 93.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.60s/it][A100%|██████████| 1/1 [01:38<00:00, 98.60s/it]
 16%|█▌        | 837/5198 [5:42:44<113:16:02, 93.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.49s/it][A100%|██████████| 1/1 [01:38<00:00, 98.49s/it]
 16%|█▌        | 837/5198 [5:43:38<113:14:26, 93.48s/it]
100%|██████████| 1/1 [01:38<00:00, 98.54s/it][A100%|██████████| 1/1 [01:38<00:00, 98.54s/it]
 16%|█▌        | 837/5198 [5:43:07<113:15:02, 93.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.50s/it][A100%|██████████| 1/1 [01:38<00:00, 98.50s/it]
 16%|█▌        | 837/5198 [5:43:40<113:14:19, 93.48s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_785
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.60s/it][A100%|██████████| 1/1 [01:45<00:00, 105.60s/it]
 16%|█▌        | 838/5198 [5:45:44<117:37:11, 97.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:35:40,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=827, skipped=0, lr=[1.9318771564710753e-05], mom=[(0.9, 0.999)]
steps: 827 loss: 0.5749 iter time (s): 105.059 samples/sec: 1.218

100%|██████████| 1/1 [01:45<00:00, 105.99s/it][A100%|██████████| 1/1 [01:45<00:00, 105.99s/it]
 16%|█▌        | 838/5198 [5:45:59<117:46:33, 97.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.22s/it][A100%|██████████| 1/1 [01:46<00:00, 106.22s/it]
 16%|█▌        | 838/5198 [5:45:46<117:49:38, 97.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.03s/it][A100%|██████████| 1/1 [01:46<00:00, 106.03s/it]
 16%|█▌        | 838/5198 [5:46:05<117:46:46, 97.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.99s/it][A100%|██████████| 1/1 [01:45<00:00, 106.00s/it]
 16%|█▌        | 838/5198 [5:44:30<117:46:58, 97.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.01s/it][A100%|██████████| 1/1 [01:46<00:00, 106.01s/it]
 16%|█▌        | 838/5198 [5:44:53<117:46:36, 97.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.07s/it][A100%|██████████| 1/1 [01:46<00:00, 106.07s/it]
 16%|█▌        | 838/5198 [5:45:24<117:47:30, 97.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.06s/it][A100%|██████████| 1/1 [01:46<00:00, 106.06s/it]
 16%|█▌        | 838/5198 [5:45:26<117:47:07, 97.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_786
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.50s/it][A100%|██████████| 1/1 [01:21<00:00, 81.50s/it]
 16%|█▌        | 839/5198 [5:47:06<111:58:11, 92.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:37:01,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=828, skipped=0, lr=[1.931671537381386e-05], mom=[(0.9, 0.999)]
steps: 828 loss: 0.5803 iter time (s): 79.929 samples/sec: 1.601

100%|██████████| 1/1 [01:20<00:00, 80.83s/it][A100%|██████████| 1/1 [01:20<00:00, 80.83s/it]
 16%|█▌        | 839/5198 [5:47:20<111:47:17, 92.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.78s/it][A100%|██████████| 1/1 [01:20<00:00, 80.78s/it]
 16%|█▌        | 839/5198 [5:47:07<111:48:36, 92.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.82s/it][A100%|██████████| 1/1 [01:20<00:00, 80.82s/it]
 16%|█▌        | 839/5198 [5:47:26<111:47:28, 92.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.80s/it][A100%|██████████| 1/1 [01:20<00:00, 80.80s/it]
 16%|█▌        | 839/5198 [5:45:50<111:46:54, 92.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.80s/it][A100%|██████████| 1/1 [01:20<00:00, 80.80s/it]
 16%|█▌        | 839/5198 [5:46:14<111:46:45, 92.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.83s/it][A100%|██████████| 1/1 [01:20<00:00, 80.83s/it]
 16%|█▌        | 839/5198 [5:46:45<111:48:06, 92.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [01:20<00:00, 80.83s/it][A100%|██████████| 1/1 [01:20<00:00, 80.83s/it]
 16%|█▌        | 839/5198 [5:46:47<111:47:41, 92.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_787
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.12s/it][A100%|██████████| 1/1 [01:38<00:00, 98.12s/it]
[2024-06-30 05:38:40,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=829, skipped=0, lr=[1.9314656194204227e-05], mom=[(0.9, 0.999)]
steps: 829 loss: 0.5560 iter time (s): 97.871 samples/sec: 1.308

100%|██████████| 1/1 [01:38<00:00, 98.78s/it][A100%|██████████| 1/1 [01:38<00:00, 98.78s/it]

100%|██████████| 1/1 [01:38<00:00, 98.76s/it][A100%|██████████| 1/1 [01:38<00:00, 98.76s/it]

100%|██████████| 1/1 [01:38<00:00, 98.82s/it][A100%|██████████| 1/1 [01:38<00:00, 98.82s/it]

100%|██████████| 1/1 [01:38<00:00, 98.75s/it][A100%|██████████| 1/1 [01:38<00:00, 98.75s/it]

100%|██████████| 1/1 [01:38<00:00, 98.75s/it][A100%|██████████| 1/1 [01:38<00:00, 98.75s/it]

100%|██████████| 1/1 [01:38<00:00, 98.71s/it][A100%|██████████| 1/1 [01:38<00:00, 98.71s/it]

100%|██████████| 1/1 [01:38<00:00, 98.73s/it][A100%|██████████| 1/1 [01:38<00:00, 98.73s/it]
Checkpointing at shard 839
[2024-06-30 05:38:41,076] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step829 is about to be saved!
[2024-06-30 05:38:42,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_00-model_states.pt...
[2024-06-30 05:38:46,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_02-model_states.pt...
[2024-06-30 05:38:48,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_00-model_states.pt.
[2024-06-30 05:38:55,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_03-model_states.pt...
[2024-06-30 05:38:55,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_04-model_states.pt...
[2024-06-30 05:38:56,670] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_08-model_states.pt...
[2024-06-30 05:38:56,812] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_06-model_states.pt...
[2024-06-30 05:38:57,061] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_05-model_states.pt...
[2024-06-30 05:38:57,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_07-model_states.pt...
[2024-06-30 05:38:58,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_01-model_states.pt...
[2024-06-30 05:42:20,678] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_02-model_states.pt.
[2024-06-30 05:42:20,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_01-model_states.pt.
[2024-06-30 05:42:20,890] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_01_model_states.pt
[2024-06-30 05:42:20,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_01_model_states.pt...
[2024-06-30 05:42:21,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_01_model_states.pt.
[2024-06-30 05:42:21,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
[2024-06-30 05:42:22,788] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_00_model_states.pt
[2024-06-30 05:42:22,789] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_00_model_states.pt...
[2024-06-30 05:42:23,223] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_00_model_states.pt.
[2024-06-30 05:42:23,223] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
[2024-06-30 05:42:23,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_07-model_states.pt.
[2024-06-30 05:42:24,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_03-model_states.pt.
[2024-06-30 05:42:24,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_04-model_states.pt.
[2024-06-30 05:42:25,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_02_model_states.pt...
[2024-06-30 05:42:25,326] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_02_model_states.pt.
[2024-06-30 05:42:25,326] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
[2024-06-30 05:42:25,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_08-model_states.pt.
[2024-06-30 05:42:25,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_03_model_states.pt...
[2024-06-30 05:42:25,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_03_model_states.pt.
[2024-06-30 05:42:25,633] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
[2024-06-30 05:42:25,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_06-model_states.pt.
[2024-06-30 05:42:25,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_05-model_states.pt.
[2024-06-30 05:42:25,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_09-model_states.pt...
[2024-06-30 05:42:25,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_06_model_states.pt...
[2024-06-30 05:42:26,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_06_model_states.pt.
[2024-06-30 05:42:26,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
[2024-06-30 05:42:26,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_04_model_states.pt...
[2024-06-30 05:42:26,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_05_model_states.pt...
[2024-06-30 05:42:26,497] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_04_model_states.pt.
[2024-06-30 05:42:26,497] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
[2024-06-30 05:42:26,524] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_05_model_states.pt.
[2024-06-30 05:42:26,524] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
[2024-06-30 05:42:26,544] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/layer_09-model_states.pt.
[2024-06-30 05:42:26,561] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_07_model_states.pt...
[2024-06-30 05:42:26,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step829/mp_rank_07_model_states.pt.
[2024-06-30 05:42:26,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step829 is ready now!
 16%|█▌        | 840/5198 [5:52:31<196:08:16, 162.02s/it]Checkpoint saved using --- 225.53614139556885 seconds ---
 16%|█▌        | 840/5198 [5:52:51<196:05:42, 161.99s/it] 16%|█▌        | 840/5198 [5:52:45<196:11:13, 162.06s/it] 16%|█▌        | 840/5198 [5:52:33<197:05:44, 162.81s/it] 16%|█▌        | 840/5198 [5:51:38<196:01:19, 161.93s/it] 16%|█▌        | 840/5198 [5:52:09<196:00:19, 161.91s/it] 16%|█▌        | 840/5198 [5:51:15<196:03:19, 161.96s/it] 16%|█▌        | 840/5198 [5:52:11<195:59:51, 161.91s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_788
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.26s/it][A100%|██████████| 1/1 [01:41<00:00, 101.26s/it]
 16%|█▌        | 841/5198 [5:54:14<174:45:01, 144.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:44:10,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[1.9312594026542498e-05], mom=[(0.9, 0.999)]
steps: 830 loss: 0.5752 iter time (s): 103.678 samples/sec: 1.235

100%|██████████| 1/1 [01:44<00:00, 104.10s/it][A100%|██████████| 1/1 [01:44<00:00, 104.10s/it]
 16%|█▌        | 841/5198 [5:54:29<175:08:59, 144.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.27s/it][A100%|██████████| 1/1 [01:44<00:00, 104.27s/it]
 16%|█▌        | 841/5198 [5:54:16<175:10:28, 144.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.39s/it][A100%|██████████| 1/1 [01:44<00:00, 104.39s/it]
 16%|█▌        | 841/5198 [5:54:35<175:11:26, 144.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.45s/it][A100%|██████████| 1/1 [01:44<00:00, 104.45s/it]
 16%|█▌        | 841/5198 [5:52:59<175:11:04, 144.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.56s/it][A100%|██████████| 1/1 [01:44<00:00, 104.56s/it]
 16%|█▌        | 841/5198 [5:53:23<175:12:02, 144.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.60s/it][A100%|██████████| 1/1 [01:44<00:00, 104.60s/it]
 16%|█▌        | 841/5198 [5:53:54<175:12:14, 144.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.63s/it][A100%|██████████| 1/1 [01:44<00:00, 104.63s/it]
 16%|█▌        | 841/5198 [5:53:56<175:12:21, 144.77s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_789
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.65s/it][A100%|██████████| 1/1 [01:24<00:00, 84.65s/it]
 16%|█▌        | 842/5198 [5:55:39<153:04:21, 126.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:45:34,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=831, skipped=0, lr=[1.9310528871490265e-05], mom=[(0.9, 0.999)]
steps: 831 loss: 0.5750 iter time (s): 83.273 samples/sec: 1.537

100%|██████████| 1/1 [01:24<00:00, 84.27s/it][A100%|██████████| 1/1 [01:24<00:00, 84.27s/it]
 16%|█▌        | 842/5198 [5:55:53<153:10:07, 126.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.18s/it][A100%|██████████| 1/1 [01:24<00:00, 84.18s/it]
 16%|█▌        | 842/5198 [5:55:40<153:09:26, 126.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.22s/it][A100%|██████████| 1/1 [01:24<00:00, 84.22s/it]
 16%|█▌        | 842/5198 [5:56:00<153:10:56, 126.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.25s/it][A100%|██████████| 1/1 [01:24<00:00, 84.25s/it]
 16%|█▌        | 842/5198 [5:54:24<153:11:09, 126.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.19s/it][A100%|██████████| 1/1 [01:24<00:00, 84.19s/it]
 16%|█▌        | 842/5198 [5:54:47<153:10:43, 126.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.19s/it][A100%|██████████| 1/1 [01:24<00:00, 84.19s/it]
 16%|█▌        | 842/5198 [5:55:18<153:10:47, 126.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.20s/it][A100%|██████████| 1/1 [01:24<00:00, 84.20s/it]
 16%|█▌        | 842/5198 [5:55:20<153:10:57, 126.60s/it]Shard 842 in [76, 158, 182, 242, 293, 363, 418, 421, 664, 752, 814, 842, 991, 1266, 1366, 1425, 1464, 1574, 1728, 2166, 2441, 2563, 2739, 2854, 2894, 3089, 3181, 3395, 3576, 3831, 4300, 4589, 4947, 4950]: file /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_790 skipped to avoid exceeding cuda memory
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_791
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.70s/it][A100%|██████████| 1/1 [01:56<00:00, 116.70s/it]
 16%|█▌        | 844/5198 [5:57:36<115:00:39, 95.09s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:47:32,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=832, skipped=0, lr=[1.9308460729710075e-05], mom=[(0.9, 0.999)]
steps: 832 loss: 0.5637 iter time (s): 116.867 samples/sec: 1.095

100%|██████████| 1/1 [01:57<00:00, 117.75s/it][A100%|██████████| 1/1 [01:57<00:00, 117.75s/it]
 16%|█▌        | 844/5198 [5:57:51<115:18:21, 95.34s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.79s/it][A100%|██████████| 1/1 [01:57<00:00, 117.79s/it]
 16%|█▌        | 844/5198 [5:57:38<115:18:42, 95.34s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.79s/it][A100%|██████████| 1/1 [01:57<00:00, 117.79s/it]
 16%|█▌        | 844/5198 [5:57:57<115:19:27, 95.35s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.80s/it][A100%|██████████| 1/1 [01:57<00:00, 117.80s/it]
 16%|█▌        | 844/5198 [5:56:22<115:19:42, 95.36s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.77s/it][A100%|██████████| 1/1 [01:57<00:00, 117.77s/it]
 16%|█▌        | 844/5198 [5:57:16<115:18:57, 95.35s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.82s/it][A100%|██████████| 1/1 [01:57<00:00, 117.82s/it]
 16%|█▌        | 844/5198 [5:56:45<115:19:47, 95.36s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.79s/it][A100%|██████████| 1/1 [01:57<00:00, 117.79s/it]
 16%|█▌        | 844/5198 [5:57:18<115:19:23, 95.35s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_792
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.30s/it][A100%|██████████| 1/1 [01:24<00:00, 84.30s/it]
 16%|█▋        | 845/5198 [5:59:00<111:47:01, 92.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:48:55,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=833, skipped=0, lr=[1.9306389601865453e-05], mom=[(0.9, 0.999)]
steps: 833 loss: 0.5442 iter time (s): 82.528 samples/sec: 1.551

100%|██████████| 1/1 [01:23<00:00, 83.44s/it][A100%|██████████| 1/1 [01:23<00:00, 83.44s/it]
 16%|█▋        | 845/5198 [5:59:15<111:42:51, 92.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.39s/it][A100%|██████████| 1/1 [01:23<00:00, 83.39s/it]
 16%|█▋        | 845/5198 [5:59:01<111:42:25, 92.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.36s/it][A100%|██████████| 1/1 [01:23<00:00, 83.36s/it]
 16%|█▋        | 845/5198 [5:59:21<111:42:14, 92.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.35s/it][A100%|██████████| 1/1 [01:23<00:00, 83.35s/it]
 16%|█▋        | 845/5198 [5:57:45<111:42:23, 92.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.37s/it][A100%|██████████| 1/1 [01:23<00:00, 83.37s/it]
 16%|█▋        | 845/5198 [5:58:08<111:42:39, 92.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.44s/it][A100%|██████████| 1/1 [01:23<00:00, 83.44s/it]
 16%|█▋        | 845/5198 [5:58:39<111:43:19, 92.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.43s/it][A100%|██████████| 1/1 [01:23<00:00, 83.43s/it]
 16%|█▋        | 845/5198 [5:58:41<111:43:25, 92.40s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_793
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.30s/it][A100%|██████████| 1/1 [01:25<00:00, 85.30s/it]
 16%|█▋        | 846/5198 [6:00:26<109:32:21, 90.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:50:21,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=834, skipped=0, lr=[1.9304315488620854e-05], mom=[(0.9, 0.999)]
steps: 834 loss: 0.6096 iter time (s): 84.624 samples/sec: 1.513

100%|██████████| 1/1 [01:25<00:00, 85.47s/it][A100%|██████████| 1/1 [01:25<00:00, 85.47s/it]
 16%|█▋        | 846/5198 [6:00:40<109:30:09, 90.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.50s/it][A100%|██████████| 1/1 [01:25<00:00, 85.50s/it]
 16%|█▋        | 846/5198 [6:00:27<109:30:33, 90.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.49s/it][A100%|██████████| 1/1 [01:25<00:00, 85.49s/it]
 16%|█▋        | 846/5198 [6:00:46<109:30:07, 90.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.58s/it][A100%|██████████| 1/1 [01:25<00:00, 85.58s/it]
 16%|█▋        | 846/5198 [5:59:10<109:31:51, 90.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.57s/it][A100%|██████████| 1/1 [01:25<00:00, 85.57s/it]
 16%|█▋        | 846/5198 [5:59:34<109:32:00, 90.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.54s/it][A100%|██████████| 1/1 [01:25<00:00, 85.54s/it]
 16%|█▋        | 846/5198 [6:00:05<109:31:49, 90.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.53s/it][A100%|██████████| 1/1 [01:25<00:00, 85.54s/it]
 16%|█▋        | 846/5198 [6:00:07<109:31:50, 90.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_794
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.03s/it][A100%|██████████| 1/1 [01:40<00:00, 100.03s/it]
 16%|█▋        | 847/5198 [6:02:06<112:38:58, 93.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:52:02,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=835, skipped=0, lr=[1.9302238390641716e-05], mom=[(0.9, 0.999)]
steps: 835 loss: 0.5480 iter time (s): 99.632 samples/sec: 1.285

100%|██████████| 1/1 [01:40<00:00, 100.58s/it][A100%|██████████| 1/1 [01:40<00:00, 100.59s/it]
 16%|█▋        | 847/5198 [6:02:21<112:46:07, 93.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.64s/it][A100%|██████████| 1/1 [01:40<00:00, 100.64s/it]
 16%|█▋        | 847/5198 [6:02:07<112:47:29, 93.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.63s/it][A100%|██████████| 1/1 [01:40<00:00, 100.63s/it]
 16%|█▋        | 847/5198 [6:02:27<112:46:55, 93.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.61s/it][A100%|██████████| 1/1 [01:40<00:00, 100.61s/it]
 16%|█▋        | 847/5198 [6:00:51<112:47:47, 93.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.56s/it][A100%|██████████| 1/1 [01:40<00:00, 100.56s/it]
 16%|█▋        | 847/5198 [6:01:14<112:47:02, 93.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.57s/it][A100%|██████████| 1/1 [01:40<00:00, 100.57s/it]
 16%|█▋        | 847/5198 [6:01:45<112:47:07, 93.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.56s/it][A100%|██████████| 1/1 [01:40<00:00, 100.56s/it]
 16%|█▋        | 847/5198 [6:01:47<112:46:54, 93.32s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_52
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.07s/it][A100%|██████████| 1/1 [02:02<00:00, 122.08s/it]
 16%|█▋        | 848/5198 [6:04:08<122:26:02, 101.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:54:04,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=836, skipped=0, lr=[1.930015830859441e-05], mom=[(0.9, 0.999)]
steps: 836 loss: 0.8110 iter time (s): 121.879 samples/sec: 1.050

100%|██████████| 1/1 [02:02<00:00, 122.89s/it][A100%|██████████| 1/1 [02:02<00:00, 122.89s/it]
 16%|█▋        | 848/5198 [6:04:24<122:45:08, 101.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.90s/it][A100%|██████████| 1/1 [02:02<00:00, 122.90s/it]
 16%|█▋        | 848/5198 [6:04:10<122:46:11, 101.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.91s/it][A100%|██████████| 1/1 [02:02<00:00, 122.91s/it]
 16%|█▋        | 848/5198 [6:02:54<122:46:32, 101.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.08s/it][A100%|██████████| 1/1 [02:03<00:00, 123.08s/it]
 16%|█▋        | 848/5198 [6:04:30<122:49:24, 101.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.95s/it][A100%|██████████| 1/1 [02:02<00:00, 122.95s/it]
 16%|█▋        | 848/5198 [6:03:50<122:46:45, 101.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_795

100%|██████████| 1/1 [02:03<00:00, 123.00s/it][A100%|██████████| 1/1 [02:03<00:00, 123.00s/it]
 16%|█▋        | 848/5198 [6:03:17<122:48:00, 101.63s/it]Training on 128 of 128 sentences.

100%|██████████| 1/1 [02:02<00:00, 122.98s/it][A100%|██████████| 1/1 [02:02<00:00, 122.98s/it]
 16%|█▋        | 848/5198 [6:03:48<122:47:26, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.24s/it][A100%|██████████| 1/1 [01:56<00:00, 116.24s/it]
 16%|█▋        | 849/5198 [6:06:05<127:35:37, 105.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:56:01,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=837, skipped=0, lr=[1.9298075243146295e-05], mom=[(0.9, 0.999)]
steps: 837 loss: 0.6030 iter time (s): 115.177 samples/sec: 1.111

100%|██████████| 1/1 [01:56<00:00, 116.14s/it][A100%|██████████| 1/1 [01:56<00:00, 116.14s/it]
 16%|█▋        | 849/5198 [6:06:20<127:44:51, 105.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.04s/it][A100%|██████████| 1/1 [01:56<00:00, 116.04s/it]
 16%|█▋        | 849/5198 [6:06:06<127:43:35, 105.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.99s/it][A100%|██████████| 1/1 [01:55<00:00, 115.99s/it]
 16%|█▋        | 849/5198 [6:06:26<127:44:52, 105.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.15s/it][A100%|██████████| 1/1 [01:56<00:00, 116.15s/it]
 16%|█▋        | 849/5198 [6:04:50<127:46:00, 105.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.10s/it][A100%|██████████| 1/1 [01:56<00:00, 116.10s/it]
 16%|█▋        | 849/5198 [6:05:13<127:46:05, 105.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.12s/it][A100%|██████████| 1/1 [01:56<00:00, 116.12s/it]
 16%|█▋        | 849/5198 [6:05:45<127:46:04, 105.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.14s/it][A100%|██████████| 1/1 [01:56<00:00, 116.14s/it]
 16%|█▋        | 849/5198 [6:05:46<127:46:05, 105.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_796
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.32s/it][A100%|██████████| 1/1 [01:32<00:00, 92.32s/it]
 16%|█▋        | 850/5198 [6:07:37<122:59:29, 101.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:57:32,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=838, skipped=0, lr=[1.9295989194965662e-05], mom=[(0.9, 0.999)]
steps: 838 loss: 0.5554 iter time (s): 90.864 samples/sec: 1.409

100%|██████████| 1/1 [01:31<00:00, 91.80s/it][A100%|██████████| 1/1 [01:31<00:00, 91.80s/it]
 16%|█▋        | 850/5198 [6:07:52<122:50:32, 101.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.86s/it][A100%|██████████| 1/1 [01:31<00:00, 91.86s/it]
 16%|█▋        | 850/5198 [6:07:38<122:50:45, 101.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.79s/it][A100%|██████████| 1/1 [01:31<00:00, 91.79s/it]
 16%|█▋        | 850/5198 [6:07:58<122:50:08, 101.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.71s/it][A100%|██████████| 1/1 [01:31<00:00, 91.71s/it]
 16%|█▋        | 850/5198 [6:06:22<122:49:20, 101.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.73s/it][A100%|██████████| 1/1 [01:31<00:00, 91.73s/it]
 16%|█▋        | 850/5198 [6:06:45<122:49:46, 101.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.74s/it][A100%|██████████| 1/1 [01:31<00:00, 91.75s/it]
 16%|█▋        | 850/5198 [6:07:16<122:50:07, 101.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.77s/it][A100%|██████████| 1/1 [01:31<00:00, 91.77s/it]
 16%|█▋        | 850/5198 [6:07:18<122:50:36, 101.71s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_797
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.94s/it][A100%|██████████| 1/1 [01:21<00:00, 81.94s/it]
 16%|█▋        | 851/5198 [6:08:59<115:58:23, 96.04s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 05:58:54,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=839, skipped=0, lr=[1.929390016472176e-05], mom=[(0.9, 0.999)]
steps: 839 loss: 0.6019 iter time (s): 80.866 samples/sec: 1.583

100%|██████████| 1/1 [01:21<00:00, 81.76s/it][A100%|██████████| 1/1 [01:21<00:00, 81.76s/it]
 16%|█▋        | 851/5198 [6:09:13<115:46:05, 95.87s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.78s/it][A100%|██████████| 1/1 [01:21<00:00, 81.78s/it]
 16%|█▋        | 851/5198 [6:09:00<115:46:28, 95.88s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.71s/it][A100%|██████████| 1/1 [01:21<00:00, 81.71s/it]
 16%|█▋        | 851/5198 [6:09:19<115:44:26, 95.85s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.75s/it][A100%|██████████| 1/1 [01:21<00:00, 81.75s/it]
 16%|█▋        | 851/5198 [6:07:44<115:44:45, 95.86s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.72s/it][A100%|██████████| 1/1 [01:21<00:00, 81.72s/it]
 16%|█▋        | 851/5198 [6:08:07<115:44:29, 95.85s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.71s/it][A100%|██████████| 1/1 [01:21<00:00, 81.71s/it]
 16%|█▋        | 851/5198 [6:08:38<115:44:31, 95.85s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.70s/it][A100%|██████████| 1/1 [01:21<00:00, 81.70s/it]
 16%|█▋        | 851/5198 [6:08:40<115:44:40, 95.85s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_798
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.15s/it][A100%|██████████| 1/1 [01:32<00:00, 92.15s/it]
 16%|█▋        | 852/5198 [6:10:32<114:38:39, 94.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:00:27,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[1.9291808153084812e-05], mom=[(0.9, 0.999)]
steps: 840 loss: 0.6453 iter time (s): 91.860 samples/sec: 1.393

100%|██████████| 1/1 [01:32<00:00, 92.77s/it][A100%|██████████| 1/1 [01:32<00:00, 92.77s/it]
 16%|█▋        | 852/5198 [6:10:46<114:38:15, 94.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.68s/it][A100%|██████████| 1/1 [01:32<00:00, 92.68s/it]
 16%|█▋        | 852/5198 [6:10:33<114:36:44, 94.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.70s/it][A100%|██████████| 1/1 [01:32<00:00, 92.70s/it]
 16%|█▋        | 852/5198 [6:10:52<114:35:41, 94.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.82s/it][A100%|██████████| 1/1 [01:32<00:00, 92.82s/it]
 16%|█▋        | 852/5198 [6:09:16<114:38:25, 94.96s/it]
100%|██████████| 1/1 [01:32<00:00, 92.74s/it][A100%|██████████| 1/1 [01:32<00:00, 92.74s/it]
 16%|█▋        | 852/5198 [6:09:40<114:36:38, 94.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.74s/it][A100%|██████████| 1/1 [01:32<00:00, 92.74s/it]
 16%|█▋        | 852/5198 [6:10:11<114:36:38, 94.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.74s/it][A100%|██████████| 1/1 [01:32<00:00, 92.74s/it]
 16%|█▋        | 852/5198 [6:10:13<114:36:43, 94.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_799
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.91s/it][A100%|██████████| 1/1 [01:32<00:00, 92.91s/it]
 16%|█▋        | 853/5198 [6:12:05<113:56:42, 94.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:02:00,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=841, skipped=0, lr=[1.9289713160725984e-05], mom=[(0.9, 0.999)]
steps: 841 loss: 0.5571 iter time (s): 92.188 samples/sec: 1.388

100%|██████████| 1/1 [01:33<00:00, 93.07s/it][A100%|██████████| 1/1 [01:33<00:00, 93.07s/it]
 16%|█▋        | 853/5198 [6:12:19<113:56:25, 94.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.14s/it][A100%|██████████| 1/1 [01:33<00:00, 93.14s/it]
 16%|█▋        | 853/5198 [6:12:06<113:56:46, 94.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.20s/it][A100%|██████████| 1/1 [01:33<00:00, 93.20s/it]
 16%|█▋        | 853/5198 [6:12:25<113:57:24, 94.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.13s/it][A100%|██████████| 1/1 [01:33<00:00, 93.13s/it]
 16%|█▋        | 853/5198 [6:10:50<113:57:45, 94.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.16s/it][A100%|██████████| 1/1 [01:33<00:00, 93.16s/it]
 16%|█▋        | 853/5198 [6:11:13<113:57:09, 94.41s/it]
100%|██████████| 1/1 [01:33<00:00, 93.14s/it][A100%|██████████| 1/1 [01:33<00:00, 93.14s/it]
 16%|█▋        | 853/5198 [6:11:44<113:56:37, 94.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.15s/it][A100%|██████████| 1/1 [01:33<00:00, 93.15s/it]
 16%|█▋        | 853/5198 [6:11:46<113:56:58, 94.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_800
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.77s/it][A100%|██████████| 1/1 [01:34<00:00, 94.77s/it]
 16%|█▋        | 854/5198 [6:13:40<114:05:20, 94.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:03:35,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=842, skipped=0, lr=[1.92876151883174e-05], mom=[(0.9, 0.999)]
steps: 842 loss: 0.5921 iter time (s): 93.915 samples/sec: 1.363

100%|██████████| 1/1 [01:34<00:00, 94.88s/it][A100%|██████████| 1/1 [01:34<00:00, 94.88s/it]
 16%|█▋        | 854/5198 [6:13:54<114:05:24, 94.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.87s/it][A100%|██████████| 1/1 [01:34<00:00, 94.87s/it]
 16%|█▋        | 854/5198 [6:13:41<114:05:22, 94.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.89s/it][A100%|██████████| 1/1 [01:34<00:00, 94.89s/it]
 16%|█▋        | 854/5198 [6:14:00<114:06:16, 94.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.82s/it][A100%|██████████| 1/1 [01:34<00:00, 94.82s/it]
 16%|█▋        | 854/5198 [6:12:24<114:04:57, 94.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.85s/it][A100%|██████████| 1/1 [01:34<00:00, 94.85s/it]
 16%|█▋        | 854/5198 [6:12:48<114:05:04, 94.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.89s/it][A100%|██████████| 1/1 [01:34<00:00, 94.89s/it]
 16%|█▋        | 854/5198 [6:13:19<114:05:37, 94.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.86s/it][A100%|██████████| 1/1 [01:34<00:00, 94.86s/it]
 16%|█▋        | 854/5198 [6:13:21<114:05:08, 94.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_801
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.75s/it][A100%|██████████| 1/1 [01:31<00:00, 91.75s/it]
 16%|█▋        | 855/5198 [6:15:11<113:06:50, 93.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:05:07,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=843, skipped=0, lr=[1.9285514236532143e-05], mom=[(0.9, 0.999)]
steps: 843 loss: 0.5877 iter time (s): 90.899 samples/sec: 1.408

100%|██████████| 1/1 [01:31<00:00, 91.86s/it][A100%|██████████| 1/1 [01:31<00:00, 91.86s/it]
 16%|█▋        | 855/5198 [6:15:26<113:06:03, 93.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.89s/it][A100%|██████████| 1/1 [01:31<00:00, 91.89s/it]
 16%|█▋        | 855/5198 [6:15:13<113:06:29, 93.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.87s/it][A100%|██████████| 1/1 [01:31<00:00, 91.87s/it]
 16%|█▋        | 855/5198 [6:15:32<113:06:47, 93.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.88s/it][A100%|██████████| 1/1 [01:31<00:00, 91.88s/it]
 16%|█▋        | 855/5198 [6:13:56<113:06:03, 93.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.87s/it][A100%|██████████| 1/1 [01:31<00:00, 91.87s/it]
 16%|█▋        | 855/5198 [6:14:20<113:05:57, 93.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.88s/it][A100%|██████████| 1/1 [01:31<00:00, 91.88s/it]
 16%|█▋        | 855/5198 [6:14:51<113:06:32, 93.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.89s/it][A100%|██████████| 1/1 [01:31<00:00, 91.89s/it]
 16%|█▋        | 855/5198 [6:14:53<113:06:26, 93.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_802
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.33s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 16%|█▋        | 856/5198 [6:16:43<112:16:12, 93.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:06:38,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=844, skipped=0, lr=[1.9283410306044252e-05], mom=[(0.9, 0.999)]
steps: 844 loss: 0.5544 iter time (s): 90.493 samples/sec: 1.414

100%|██████████| 1/1 [01:31<00:00, 91.37s/it][A100%|██████████| 1/1 [01:31<00:00, 91.37s/it]
 16%|█▋        | 856/5198 [6:16:57<112:13:07, 93.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.37s/it][A100%|██████████| 1/1 [01:31<00:00, 91.37s/it]
 16%|█▋        | 856/5198 [6:16:44<112:13:23, 93.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.37s/it][A100%|██████████| 1/1 [01:31<00:00, 91.37s/it]
 16%|█▋        | 856/5198 [6:17:04<112:13:44, 93.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.41s/it][A100%|██████████| 1/1 [01:31<00:00, 91.42s/it]
 16%|█▋        | 856/5198 [6:15:28<112:14:08, 93.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.43s/it][A100%|██████████| 1/1 [01:31<00:00, 91.43s/it]
 16%|█▋        | 856/5198 [6:15:51<112:14:27, 93.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.42s/it][A100%|██████████| 1/1 [01:31<00:00, 91.42s/it]
 16%|█▋        | 856/5198 [6:16:22<112:14:40, 93.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.44s/it][A100%|██████████| 1/1 [01:31<00:00, 91.44s/it]
 16%|█▋        | 856/5198 [6:16:24<112:14:52, 93.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_803
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.73s/it][A100%|██████████| 1/1 [01:26<00:00, 86.73s/it]
 16%|█▋        | 857/5198 [6:18:10<110:00:59, 91.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:08:05,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=845, skipped=0, lr=[1.9281303397528716e-05], mom=[(0.9, 0.999)]
steps: 845 loss: 0.5866 iter time (s): 85.826 samples/sec: 1.491

100%|██████████| 1/1 [01:26<00:00, 86.87s/it][A100%|██████████| 1/1 [01:26<00:00, 86.87s/it]
 16%|█▋        | 857/5198 [6:18:24<109:58:15, 91.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.81s/it][A100%|██████████| 1/1 [01:26<00:00, 86.81s/it]
 16%|█▋        | 857/5198 [6:18:11<109:57:04, 91.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.80s/it][A100%|██████████| 1/1 [01:26<00:00, 86.80s/it]
 16%|█▋        | 857/5198 [6:18:30<109:57:11, 91.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.78s/it][A100%|██████████| 1/1 [01:26<00:00, 86.78s/it]
 16%|█▋        | 857/5198 [6:16:55<109:56:52, 91.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.81s/it][A100%|██████████| 1/1 [01:26<00:00, 86.81s/it]
 16%|█▋        | 857/5198 [6:17:18<109:57:49, 91.19s/it]
100%|██████████| 1/1 [01:26<00:00, 86.77s/it][A100%|██████████| 1/1 [01:26<00:00, 86.77s/it]
 16%|█▋        | 857/5198 [6:17:49<109:57:10, 91.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.78s/it][A100%|██████████| 1/1 [01:26<00:00, 86.78s/it]
 16%|█▋        | 857/5198 [6:17:51<109:57:19, 91.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_804
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.45s/it][A100%|██████████| 1/1 [01:28<00:00, 88.45s/it]
 17%|█▋        | 858/5198 [6:19:38<109:02:44, 90.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:09:33,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=846, skipped=0, lr=[1.9279193511661488e-05], mom=[(0.9, 0.999)]
steps: 846 loss: 0.6053 iter time (s): 87.365 samples/sec: 1.465

100%|██████████| 1/1 [01:28<00:00, 88.27s/it][A100%|██████████| 1/1 [01:28<00:00, 88.27s/it]
 17%|█▋        | 858/5198 [6:19:53<108:53:34, 90.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.25s/it][A100%|██████████| 1/1 [01:28<00:00, 88.25s/it]
 17%|█▋        | 858/5198 [6:19:39<108:52:17, 90.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.33s/it][A100%|██████████| 1/1 [01:28<00:00, 88.33s/it]
 17%|█▋        | 858/5198 [6:19:59<108:54:09, 90.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.39s/it][A100%|██████████| 1/1 [01:28<00:00, 88.39s/it]
 17%|█▋        | 858/5198 [6:18:23<108:55:00, 90.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.34s/it][A100%|██████████| 1/1 [01:28<00:00, 88.34s/it]
 17%|█▋        | 858/5198 [6:18:46<108:54:43, 90.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.37s/it][A100%|██████████| 1/1 [01:28<00:00, 88.37s/it]
 17%|█▋        | 858/5198 [6:19:17<108:54:57, 90.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.36s/it][A100%|██████████| 1/1 [01:28<00:00, 88.36s/it]
 17%|█▋        | 858/5198 [6:19:19<108:54:45, 90.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_805
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.19s/it][A100%|██████████| 1/1 [02:18<00:00, 138.19s/it]
 17%|█▋        | 859/5198 [6:21:57<126:20:17, 104.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:11:53,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=847, skipped=0, lr=[1.9277080649119468e-05], mom=[(0.9, 0.999)]
steps: 847 loss: 0.5818 iter time (s): 138.485 samples/sec: 0.924

100%|██████████| 1/1 [02:19<00:00, 139.52s/it][A100%|██████████| 1/1 [02:19<00:00, 139.52s/it]
 17%|█▋        | 859/5198 [6:22:12<126:38:01, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.57s/it][A100%|██████████| 1/1 [02:19<00:00, 139.57s/it]
 17%|█▋        | 859/5198 [6:21:59<126:38:05, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.51s/it][A100%|██████████| 1/1 [02:19<00:00, 139.51s/it]
 17%|█▋        | 859/5198 [6:22:18<126:38:22, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.40s/it][A100%|██████████| 1/1 [02:19<00:00, 139.40s/it]
 17%|█▋        | 859/5198 [6:20:42<126:36:26, 105.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.43s/it][A100%|██████████| 1/1 [02:19<00:00, 139.43s/it]
 17%|█▋        | 859/5198 [6:21:37<126:37:03, 105.05s/it]
100%|██████████| 1/1 [02:19<00:00, 139.49s/it][A100%|██████████| 1/1 [02:19<00:00, 139.49s/it]
 17%|█▋        | 859/5198 [6:21:06<126:38:08, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.51s/it][A100%|██████████| 1/1 [02:19<00:00, 139.51s/it]
 17%|█▋        | 859/5198 [6:21:39<126:38:33, 105.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_806
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.02s/it][A100%|██████████| 1/1 [01:37<00:00, 97.02s/it]
[2024-06-30 06:13:30,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=848, skipped=0, lr=[1.927496481058052e-05], mom=[(0.9, 0.999)]
steps: 848 loss: 0.6146 iter time (s): 95.752 samples/sec: 1.337

100%|██████████| 1/1 [01:36<00:00, 96.71s/it][A100%|██████████| 1/1 [01:36<00:00, 96.71s/it]

100%|██████████| 1/1 [01:36<00:00, 96.65s/it][A100%|██████████| 1/1 [01:36<00:00, 96.65s/it]

100%|██████████| 1/1 [01:36<00:00, 96.65s/it][A100%|██████████| 1/1 [01:36<00:00, 96.65s/it]

100%|██████████| 1/1 [01:36<00:00, 96.74s/it][A100%|██████████| 1/1 [01:36<00:00, 96.74s/it]

100%|██████████| 1/1 [01:36<00:00, 96.65s/it][A100%|██████████| 1/1 [01:36<00:00, 96.65s/it]

100%|██████████| 1/1 [01:36<00:00, 96.69s/it][A100%|██████████| 1/1 [01:36<00:00, 96.69s/it]

100%|██████████| 1/1 [01:36<00:00, 96.63s/it][A100%|██████████| 1/1 [01:36<00:00, 96.63s/it]
Checkpointing at shard 859
[2024-06-30 06:13:30,945] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step848 is about to be saved!
[2024-06-30 06:13:32,568] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_00-model_states.pt...
[2024-06-30 06:13:36,180] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_02-model_states.pt...
[2024-06-30 06:13:37,452] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_00-model_states.pt.
[2024-06-30 06:13:44,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_03-model_states.pt...
[2024-06-30 06:13:44,186] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_04-model_states.pt...
[2024-06-30 06:13:45,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_06-model_states.pt...
[2024-06-30 06:13:46,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_08-model_states.pt...
[2024-06-30 06:13:46,384] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_05-model_states.pt...
[2024-06-30 06:13:46,460] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_01-model_states.pt...
[2024-06-30 06:13:47,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_07-model_states.pt...
[2024-06-30 06:16:52,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_07-model_states.pt.
[2024-06-30 06:16:53,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_06_model_states.pt...
[2024-06-30 06:16:53,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_06_model_states.pt.
[2024-06-30 06:16:53,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
[2024-06-30 06:16:54,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_02-model_states.pt.
[2024-06-30 06:16:55,111] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_01_model_states.pt
[2024-06-30 06:16:55,111] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_01_model_states.pt...
[2024-06-30 06:16:55,316] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_01_model_states.pt.
[2024-06-30 06:16:55,316] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
[2024-06-30 06:17:04,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_06-model_states.pt.
[2024-06-30 06:17:04,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_05_model_states.pt...
[2024-06-30 06:17:04,892] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_05_model_states.pt.
[2024-06-30 06:17:04,892] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
[2024-06-30 06:17:04,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_05-model_states.pt.
[2024-06-30 06:17:05,097] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_01-model_states.pt.
[2024-06-30 06:17:05,865] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_04_model_states.pt...
[2024-06-30 06:17:05,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_04_model_states.pt.
[2024-06-30 06:17:05,990] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
[2024-06-30 06:17:06,117] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_00_model_states.pt
[2024-06-30 06:17:06,117] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_00_model_states.pt...
[2024-06-30 06:17:06,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_08-model_states.pt.
[2024-06-30 06:17:06,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_00_model_states.pt.
[2024-06-30 06:17:06,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
[2024-06-30 06:17:06,770] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_09-model_states.pt...
[2024-06-30 06:17:07,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_09-model_states.pt.
[2024-06-30 06:17:07,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_07_model_states.pt...
[2024-06-30 06:17:08,011] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_07_model_states.pt.
[2024-06-30 06:17:08,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
[2024-06-30 06:17:08,280] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_03-model_states.pt.
[2024-06-30 06:17:08,313] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/layer_04-model_states.pt.
[2024-06-30 06:17:08,916] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_03_model_states.pt...
[2024-06-30 06:17:08,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_03_model_states.pt.
[2024-06-30 06:17:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
[2024-06-30 06:17:08,999] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_02_model_states.pt...
[2024-06-30 06:17:09,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step848/mp_rank_02_model_states.pt.
[2024-06-30 06:17:09,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step848 is ready now!
 17%|█▋        | 860/5198 [6:25:57<202:23:37, 167.96s/it]Checkpoint saved using --- 218.10575795173645 seconds ---
 17%|█▋        | 860/5198 [6:27:15<203:28:23, 168.86s/it] 17%|█▋        | 860/5198 [6:27:33<202:25:33, 167.99s/it] 17%|█▋        | 860/5198 [6:27:14<202:29:36, 168.04s/it] 17%|█▋        | 860/5198 [6:26:52<202:21:06, 167.93s/it] 17%|█▋        | 860/5198 [6:27:27<202:31:43, 168.07s/it] 17%|█▋        | 860/5198 [6:26:53<202:20:08, 167.91s/it] 17%|█▋        | 860/5198 [6:26:20<202:21:29, 167.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_807
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.36s/it][A100%|██████████| 1/1 [01:33<00:00, 93.36s/it]
 17%|█▋        | 861/5198 [6:28:49<176:14:27, 146.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:18:44,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=849, skipped=0, lr=[1.927284599672345e-05], mom=[(0.9, 0.999)]
steps: 849 loss: 0.5667 iter time (s): 95.616 samples/sec: 1.339

100%|██████████| 1/1 [01:36<00:00, 96.05s/it][A100%|██████████| 1/1 [01:36<00:00, 96.05s/it]
 17%|█▋        | 861/5198 [6:29:04<176:32:59, 146.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.25s/it][A100%|██████████| 1/1 [01:36<00:00, 96.25s/it]
 17%|█▋        | 861/5198 [6:28:50<176:35:49, 146.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.38s/it][A100%|██████████| 1/1 [01:36<00:00, 96.38s/it]
 17%|█▋        | 861/5198 [6:29:10<176:35:47, 146.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.47s/it][A100%|██████████| 1/1 [01:36<00:00, 96.47s/it]
 17%|█▋        | 861/5198 [6:27:34<176:36:20, 146.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.54s/it][A100%|██████████| 1/1 [01:36<00:00, 96.54s/it]
 17%|█▋        | 861/5198 [6:28:28<176:36:11, 146.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.55s/it][A100%|██████████| 1/1 [01:36<00:00, 96.55s/it]
 17%|█▋        | 861/5198 [6:27:57<176:36:44, 146.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.58s/it][A100%|██████████| 1/1 [01:36<00:00, 96.58s/it]
 17%|█▋        | 861/5198 [6:28:30<176:36:20, 146.59s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_808
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.04s/it][A100%|██████████| 1/1 [01:29<00:00, 89.04s/it]
 17%|█▋        | 862/5198 [6:30:18<155:34:11, 129.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:20:13,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[1.9270724208228035e-05], mom=[(0.9, 0.999)]
steps: 850 loss: 0.5843 iter time (s): 87.905 samples/sec: 1.456

100%|██████████| 1/1 [01:28<00:00, 88.81s/it][A100%|██████████| 1/1 [01:28<00:00, 88.82s/it]
 17%|█▋        | 862/5198 [6:30:32<155:40:04, 129.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.74s/it][A100%|██████████| 1/1 [01:28<00:00, 88.74s/it]
 17%|█▋        | 862/5198 [6:30:19<155:40:05, 129.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.78s/it][A100%|██████████| 1/1 [01:28<00:00, 88.78s/it]
 17%|█▋        | 862/5198 [6:30:39<155:40:58, 129.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.85s/it][A100%|██████████| 1/1 [01:28<00:00, 88.85s/it]
 17%|█▋        | 862/5198 [6:29:03<155:42:43, 129.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.79s/it][A100%|██████████| 1/1 [01:28<00:00, 88.79s/it]
 17%|█▋        | 862/5198 [6:29:26<155:41:43, 129.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.82s/it][A100%|██████████| 1/1 [01:28<00:00, 88.83s/it]
 17%|█▋        | 862/5198 [6:29:57<155:42:08, 129.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.84s/it][A100%|██████████| 1/1 [01:28<00:00, 88.84s/it]
 17%|█▋        | 862/5198 [6:29:59<155:42:27, 129.28s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_809
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.82s/it][A100%|██████████| 1/1 [01:23<00:00, 83.82s/it]
 17%|█▋        | 863/5198 [6:31:42<139:12:29, 115.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:21:37,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=851, skipped=0, lr=[1.9268599445774986e-05], mom=[(0.9, 0.999)]
steps: 851 loss: 0.5579 iter time (s): 82.892 samples/sec: 1.544

100%|██████████| 1/1 [01:23<00:00, 83.77s/it][A100%|██████████| 1/1 [01:23<00:00, 83.77s/it]
 17%|█▋        | 863/5198 [6:31:56<139:12:50, 115.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.88s/it][A100%|██████████| 1/1 [01:23<00:00, 83.88s/it]
 17%|█▋        | 863/5198 [6:31:43<139:15:10, 115.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.87s/it][A100%|██████████| 1/1 [01:23<00:00, 83.87s/it]
 17%|█▋        | 863/5198 [6:32:03<139:15:35, 115.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.78s/it][A100%|██████████| 1/1 [01:23<00:00, 83.78s/it]
 17%|█▋        | 863/5198 [6:30:27<139:14:43, 115.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.80s/it][A100%|██████████| 1/1 [01:23<00:00, 83.80s/it]
 17%|█▋        | 863/5198 [6:30:50<139:14:43, 115.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.84s/it][A100%|██████████| 1/1 [01:23<00:00, 83.84s/it]
 17%|█▋        | 863/5198 [6:31:21<139:15:38, 115.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.81s/it][A100%|██████████| 1/1 [01:23<00:00, 83.81s/it]
 17%|█▋        | 863/5198 [6:31:23<139:15:15, 115.64s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_53
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.49s/it][A100%|██████████| 1/1 [02:01<00:00, 121.50s/it]
 17%|█▋        | 864/5198 [6:33:44<141:20:32, 117.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:23:40,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=852, skipped=0, lr=[1.926647171004599e-05], mom=[(0.9, 0.999)]
steps: 852 loss: 0.7887 iter time (s): 121.795 samples/sec: 1.051

100%|██████████| 1/1 [02:02<00:00, 122.72s/it][A100%|██████████| 1/1 [02:02<00:00, 122.72s/it]
 17%|█▋        | 864/5198 [6:33:59<141:45:03, 117.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.67s/it][A100%|██████████| 1/1 [02:02<00:00, 122.67s/it]
 17%|█▋        | 864/5198 [6:33:46<141:45:38, 117.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.69s/it][A100%|██████████| 1/1 [02:02<00:00, 122.69s/it]
 17%|█▋        | 864/5198 [6:34:05<141:46:18, 117.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.72s/it][A100%|██████████| 1/1 [02:02<00:00, 122.72s/it]
 17%|█▋        | 864/5198 [6:32:29<141:46:24, 117.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.72s/it][A100%|██████████| 1/1 [02:02<00:00, 122.72s/it]
 17%|█▋        | 864/5198 [6:32:53<141:46:24, 117.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.69s/it][A100%|██████████| 1/1 [02:02<00:00, 122.69s/it]
 17%|█▋        | 864/5198 [6:33:24<141:46:19, 117.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.68s/it][A100%|██████████| 1/1 [02:02<00:00, 122.68s/it]
 17%|█▋        | 864/5198 [6:33:26<141:45:56, 117.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_810
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.79s/it][A100%|██████████| 1/1 [01:41<00:00, 101.79s/it]
 17%|█▋        | 865/5198 [6:35:26<135:43:59, 112.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:25:21,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=853, skipped=0, lr=[1.9264341001723675e-05], mom=[(0.9, 0.999)]
steps: 853 loss: 0.5758 iter time (s): 100.420 samples/sec: 1.275

100%|██████████| 1/1 [01:41<00:00, 101.41s/it][A100%|██████████| 1/1 [01:41<00:00, 101.41s/it]
 17%|█▋        | 865/5198 [6:35:40<135:49:29, 112.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.38s/it][A100%|██████████| 1/1 [01:41<00:00, 101.38s/it]
 17%|█▋        | 865/5198 [6:35:27<135:49:14, 112.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.31s/it][A100%|██████████| 1/1 [01:41<00:00, 101.31s/it]
 17%|█▋        | 865/5198 [6:35:47<135:48:13, 112.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.33s/it][A100%|██████████| 1/1 [01:41<00:00, 101.33s/it]
 17%|█▋        | 865/5198 [6:34:11<135:48:43, 112.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.33s/it][A100%|██████████| 1/1 [01:41<00:00, 101.33s/it]
 17%|█▋        | 865/5198 [6:34:34<135:48:44, 112.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.37s/it][A100%|██████████| 1/1 [01:41<00:00, 101.37s/it]
 17%|█▋        | 865/5198 [6:35:05<135:49:25, 112.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.37s/it][A100%|██████████| 1/1 [01:41<00:00, 101.37s/it]
 17%|█▋        | 865/5198 [6:35:07<135:49:14, 112.84s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_811
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.29s/it][A100%|██████████| 1/1 [01:46<00:00, 106.29s/it]
 17%|█▋        | 866/5198 [6:37:12<133:25:48, 110.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:27:08,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=854, skipped=0, lr=[1.9262207321491622e-05], mom=[(0.9, 0.999)]
steps: 854 loss: 0.5798 iter time (s): 105.721 samples/sec: 1.211

100%|██████████| 1/1 [01:46<00:00, 106.59s/it][A100%|██████████| 1/1 [01:46<00:00, 106.59s/it]
 17%|█▋        | 866/5198 [6:37:27<133:32:18, 110.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.61s/it][A100%|██████████| 1/1 [01:46<00:00, 106.61s/it]
 17%|█▋        | 866/5198 [6:37:14<133:32:37, 110.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.66s/it][A100%|██████████| 1/1 [01:46<00:00, 106.66s/it]
 17%|█▋        | 866/5198 [6:37:33<133:32:47, 110.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.59s/it][A100%|██████████| 1/1 [01:46<00:00, 106.59s/it]
 17%|█▋        | 866/5198 [6:35:57<133:31:45, 110.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.64s/it][A100%|██████████| 1/1 [01:46<00:00, 106.64s/it]
 17%|█▋        | 866/5198 [6:36:21<133:32:53, 110.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.63s/it][A100%|██████████| 1/1 [01:46<00:00, 106.63s/it]
 17%|█▋        | 866/5198 [6:36:52<133:33:09, 110.99s/it]
100%|██████████| 1/1 [01:46<00:00, 106.62s/it][A100%|██████████| 1/1 [01:46<00:00, 106.62s/it]
 17%|█▋        | 866/5198 [6:36:54<133:32:39, 110.98s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_812

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.55s/it][A100%|██████████| 1/1 [01:38<00:00, 98.55s/it]
 17%|█▋        | 867/5198 [6:38:51<129:00:09, 107.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:28:46,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=855, skipped=0, lr=[1.926007067003437e-05], mom=[(0.9, 0.999)]
steps: 855 loss: 0.5735 iter time (s): 97.588 samples/sec: 1.312

100%|██████████| 1/1 [01:38<00:00, 98.51s/it][A100%|██████████| 1/1 [01:38<00:00, 98.51s/it]
 17%|█▋        | 867/5198 [6:39:06<129:00:48, 107.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.46s/it][A100%|██████████| 1/1 [01:38<00:00, 98.46s/it]
 17%|█▋        | 867/5198 [6:38:52<129:00:12, 107.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.44s/it][A100%|██████████| 1/1 [01:38<00:00, 98.44s/it]
 17%|█▋        | 867/5198 [6:39:12<128:59:36, 107.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.42s/it][A100%|██████████| 1/1 [01:38<00:00, 98.42s/it]
 17%|█▋        | 867/5198 [6:37:36<128:58:22, 107.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.44s/it][A100%|██████████| 1/1 [01:38<00:00, 98.44s/it]
 17%|█▋        | 867/5198 [6:37:59<128:59:37, 107.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.47s/it][A100%|██████████| 1/1 [01:38<00:00, 98.47s/it]
 17%|█▋        | 867/5198 [6:38:30<129:00:30, 107.23s/it]
100%|██████████| 1/1 [01:38<00:00, 98.48s/it][A100%|██████████| 1/1 [01:38<00:00, 98.48s/it]
 17%|█▋        | 867/5198 [6:38:32<129:00:15, 107.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_813

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 17%|█▋        | 868/5198 [6:40:19<122:13:21, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:30:14,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=856, skipped=0, lr=[1.925793104803741e-05], mom=[(0.9, 0.999)]
steps: 856 loss: 0.6012 iter time (s): 87.284 samples/sec: 1.466

100%|██████████| 1/1 [01:28<00:00, 88.17s/it][A100%|██████████| 1/1 [01:28<00:00, 88.17s/it]
 17%|█▋        | 868/5198 [6:40:34<122:06:26, 101.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.24s/it][A100%|██████████| 1/1 [01:28<00:00, 88.24s/it]
 17%|█▋        | 868/5198 [6:40:20<122:07:28, 101.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.24s/it][A100%|██████████| 1/1 [01:28<00:00, 88.25s/it]
 17%|█▋        | 868/5198 [6:40:40<122:07:17, 101.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.32s/it][A100%|██████████| 1/1 [01:28<00:00, 88.32s/it]
 17%|█▋        | 868/5198 [6:39:04<122:07:56, 101.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.29s/it][A100%|██████████| 1/1 [01:28<00:00, 88.29s/it]
 17%|█▋        | 868/5198 [6:39:27<122:08:05, 101.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.23s/it][A100%|██████████| 1/1 [01:28<00:00, 88.23s/it]
 17%|█▋        | 868/5198 [6:39:58<122:07:31, 101.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.24s/it][A100%|██████████| 1/1 [01:28<00:00, 88.24s/it]
 17%|█▋        | 868/5198 [6:40:00<122:07:35, 101.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_814
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.07s/it][A100%|██████████| 1/1 [01:53<00:00, 113.07s/it]
 17%|█▋        | 869/5198 [6:42:13<126:23:48, 105.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:32:08,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=857, skipped=0, lr=[1.9255788456187185e-05], mom=[(0.9, 0.999)]
steps: 857 loss: 0.5713 iter time (s): 113.024 samples/sec: 1.133

100%|██████████| 1/1 [01:54<00:00, 114.05s/it][A100%|██████████| 1/1 [01:54<00:00, 114.05s/it]
 17%|█▋        | 869/5198 [6:42:28<126:36:09, 105.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.94s/it][A100%|██████████| 1/1 [01:53<00:00, 113.94s/it]
 17%|█▋        | 869/5198 [6:42:14<126:34:29, 105.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.02s/it][A100%|██████████| 1/1 [01:54<00:00, 114.02s/it]
 17%|█▋        | 869/5198 [6:42:34<126:35:57, 105.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.01s/it][A100%|██████████| 1/1 [01:54<00:00, 114.01s/it]
 17%|█▋        | 869/5198 [6:40:58<126:36:13, 105.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.95s/it][A100%|██████████| 1/1 [01:53<00:00, 113.95s/it]
 17%|█▋        | 869/5198 [6:41:21<126:35:01, 105.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.97s/it][A100%|██████████| 1/1 [01:53<00:00, 113.97s/it]
 17%|█▋        | 869/5198 [6:41:52<126:35:12, 105.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.97s/it][A100%|██████████| 1/1 [01:53<00:00, 113.97s/it]
 17%|█▋        | 869/5198 [6:41:54<126:35:05, 105.27s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_815
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.73s/it][A100%|██████████| 1/1 [01:35<00:00, 95.73s/it]
 17%|█▋        | 870/5198 [6:43:49<123:02:58, 102.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:33:44,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=858, skipped=0, lr=[1.9253642895171096e-05], mom=[(0.9, 0.999)]
steps: 858 loss: 0.6165 iter time (s): 94.481 samples/sec: 1.355

100%|██████████| 1/1 [01:35<00:00, 95.42s/it][A100%|██████████| 1/1 [01:35<00:00, 95.42s/it]
 17%|█▋        | 870/5198 [6:44:03<123:01:11, 102.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.39s/it][A100%|██████████| 1/1 [01:35<00:00, 95.39s/it]
 17%|█▋        | 870/5198 [6:43:50<122:59:27, 102.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.38s/it][A100%|██████████| 1/1 [01:35<00:00, 95.38s/it]
 17%|█▋        | 870/5198 [6:44:09<123:00:10, 102.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.41s/it][A100%|██████████| 1/1 [01:35<00:00, 95.42s/it]
 17%|█▋        | 870/5198 [6:42:33<123:01:08, 102.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.40s/it][A100%|██████████| 1/1 [01:35<00:00, 95.40s/it]
 17%|█▋        | 870/5198 [6:42:57<123:00:01, 102.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.42s/it][A100%|██████████| 1/1 [01:35<00:00, 95.42s/it]
 17%|█▋        | 870/5198 [6:43:28<123:00:22, 102.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.43s/it][A100%|██████████| 1/1 [01:35<00:00, 95.43s/it]
 17%|█▋        | 870/5198 [6:43:30<123:00:32, 102.32s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_816
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.51s/it][A100%|██████████| 1/1 [01:55<00:00, 115.51s/it]
 17%|█▋        | 871/5198 [6:45:44<127:48:44, 106.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:35:40,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=859, skipped=0, lr=[1.9251494365677487e-05], mom=[(0.9, 0.999)]
steps: 859 loss: 0.5809 iter time (s): 115.215 samples/sec: 1.111

100%|██████████| 1/1 [01:56<00:00, 116.29s/it][A100%|██████████| 1/1 [01:56<00:00, 116.29s/it]
 17%|█▋        | 871/5198 [6:45:59<128:01:43, 106.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.41s/it][A100%|██████████| 1/1 [01:56<00:00, 116.41s/it]
 17%|█▋        | 871/5198 [6:45:46<128:03:16, 106.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.32s/it][A100%|██████████| 1/1 [01:56<00:00, 116.32s/it]
 17%|█▋        | 871/5198 [6:46:06<128:01:42, 106.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.32s/it][A100%|██████████| 1/1 [01:56<00:00, 116.32s/it]
 17%|█▋        | 871/5198 [6:44:30<128:02:23, 106.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.35s/it][A100%|██████████| 1/1 [01:56<00:00, 116.35s/it]
 17%|█▋        | 871/5198 [6:44:53<128:02:19, 106.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.32s/it][A100%|██████████| 1/1 [01:56<00:00, 116.32s/it]
 17%|█▋        | 871/5198 [6:45:24<128:01:55, 106.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.29s/it][A100%|██████████| 1/1 [01:56<00:00, 116.29s/it]
 17%|█▋        | 871/5198 [6:45:26<128:01:58, 106.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_817
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.81s/it][A100%|██████████| 1/1 [01:46<00:00, 106.81s/it]
 17%|█▋        | 872/5198 [6:47:31<127:59:52, 106.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:37:27,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.924934286839566e-05], mom=[(0.9, 0.999)]
steps: 860 loss: 0.6178 iter time (s): 105.604 samples/sec: 1.212

100%|██████████| 1/1 [01:46<00:00, 106.43s/it][A100%|██████████| 1/1 [01:46<00:00, 106.43s/it]
 17%|█▋        | 872/5198 [6:47:46<127:58:18, 106.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.38s/it][A100%|██████████| 1/1 [01:46<00:00, 106.38s/it]
 17%|█▋        | 872/5198 [6:47:33<127:58:08, 106.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.51s/it][A100%|██████████| 1/1 [01:46<00:00, 106.51s/it]
 17%|█▋        | 872/5198 [6:47:52<127:59:57, 106.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.51s/it][A100%|██████████| 1/1 [01:46<00:00, 106.52s/it]
 17%|█▋        | 872/5198 [6:46:16<128:00:37, 106.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.54s/it][A100%|██████████| 1/1 [01:46<00:00, 106.54s/it]
 17%|█▋        | 872/5198 [6:46:40<128:00:57, 106.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.54s/it][A100%|██████████| 1/1 [01:46<00:00, 106.55s/it]
 17%|█▋        | 872/5198 [6:47:11<128:00:50, 106.53s/it]
100%|██████████| 1/1 [01:46<00:00, 106.53s/it][A100%|██████████| 1/1 [01:46<00:00, 106.53s/it]
 17%|█▋        | 872/5198 [6:47:13<128:00:33, 106.53s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_818

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.32s/it][A100%|██████████| 1/1 [01:31<00:00, 91.32s/it]
 17%|█▋        | 873/5198 [6:49:03<122:33:11, 102.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:38:58,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=861, skipped=0, lr=[1.9247188404015873e-05], mom=[(0.9, 0.999)]
steps: 861 loss: 0.5905 iter time (s): 90.097 samples/sec: 1.421

100%|██████████| 1/1 [01:31<00:00, 91.13s/it][A100%|██████████| 1/1 [01:31<00:00, 91.13s/it]
 17%|█▋        | 873/5198 [6:49:17<122:24:30, 101.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.33s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 17%|█▋        | 873/5198 [6:49:04<122:28:42, 101.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.22s/it][A100%|██████████| 1/1 [01:31<00:00, 91.22s/it]
 17%|█▋        | 873/5198 [6:49:23<122:27:33, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.24s/it][A100%|██████████| 1/1 [01:31<00:00, 91.24s/it]
 17%|█▋        | 873/5198 [6:47:48<122:28:27, 101.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.18s/it][A100%|██████████| 1/1 [01:31<00:00, 91.18s/it]
 17%|█▋        | 873/5198 [6:48:11<122:27:25, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.26s/it][A100%|██████████| 1/1 [01:31<00:00, 91.26s/it]
 17%|█▋        | 873/5198 [6:48:42<122:28:58, 101.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.26s/it][A100%|██████████| 1/1 [01:31<00:00, 91.27s/it]
 17%|█▋        | 873/5198 [6:48:44<122:28:53, 101.95s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_819
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.58s/it][A100%|██████████| 1/1 [01:43<00:00, 103.58s/it]
 17%|█▋        | 874/5198 [6:50:46<123:08:00, 102.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:40:42,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=862, skipped=0, lr=[1.9245030973229325e-05], mom=[(0.9, 0.999)]
steps: 862 loss: 0.5663 iter time (s): 102.934 samples/sec: 1.244

100%|██████████| 1/1 [01:44<00:00, 104.04s/it][A100%|██████████| 1/1 [01:44<00:00, 104.04s/it]
 17%|█▋        | 874/5198 [6:51:01<123:09:36, 102.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.86s/it][A100%|██████████| 1/1 [01:43<00:00, 103.86s/it]
 17%|█▋        | 874/5198 [6:50:48<123:08:30, 102.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.90s/it][A100%|██████████| 1/1 [01:43<00:00, 103.90s/it]
 17%|█▋        | 874/5198 [6:51:07<123:08:31, 102.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.84s/it][A100%|██████████| 1/1 [01:43<00:00, 103.84s/it]
 17%|█▋        | 874/5198 [6:49:31<123:07:58, 102.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.90s/it][A100%|██████████| 1/1 [01:43<00:00, 103.90s/it]
 17%|█▋        | 874/5198 [6:49:55<123:08:26, 102.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.81s/it][A100%|██████████| 1/1 [01:43<00:00, 103.81s/it]
 17%|█▋        | 874/5198 [6:50:26<123:07:40, 102.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.85s/it][A100%|██████████| 1/1 [01:43<00:00, 103.85s/it]
 17%|█▋        | 874/5198 [6:50:28<123:08:24, 102.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_820
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.83s/it][A100%|██████████| 1/1 [01:31<00:00, 91.83s/it]
 17%|█▋        | 875/5198 [6:52:18<119:17:47, 99.34s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:42:14,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=863, skipped=0, lr=[1.924287057672818e-05], mom=[(0.9, 0.999)]
steps: 863 loss: 0.6328 iter time (s): 90.654 samples/sec: 1.412

100%|██████████| 1/1 [01:31<00:00, 91.56s/it][A100%|██████████| 1/1 [01:31<00:00, 91.56s/it]
 17%|█▋        | 875/5198 [6:52:33<119:10:43, 99.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.55s/it][A100%|██████████| 1/1 [01:31<00:00, 91.55s/it]
 17%|█▋        | 875/5198 [6:52:19<119:09:44, 99.23s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.53s/it][A100%|██████████| 1/1 [01:31<00:00, 91.53s/it]
 17%|█▋        | 875/5198 [6:52:39<119:09:23, 99.23s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.54s/it][A100%|██████████| 1/1 [01:31<00:00, 91.54s/it]
 17%|█▋        | 875/5198 [6:51:03<119:09:05, 99.22s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.57s/it][A100%|██████████| 1/1 [01:31<00:00, 91.58s/it]
 17%|█▋        | 875/5198 [6:51:26<119:10:17, 99.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.59s/it][A100%|██████████| 1/1 [01:31<00:00, 91.59s/it]
 17%|█▋        | 875/5198 [6:51:57<119:10:00, 99.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.55s/it][A100%|██████████| 1/1 [01:31<00:00, 91.55s/it]
 17%|█▋        | 875/5198 [6:51:59<119:09:43, 99.23s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_821
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.83s/it][A100%|██████████| 1/1 [01:37<00:00, 97.83s/it]
 17%|█▋        | 876/5198 [6:53:56<118:46:20, 98.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:43:52,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=864, skipped=0, lr=[1.9240707215205542e-05], mom=[(0.9, 0.999)]
steps: 864 loss: 0.5487 iter time (s): 97.254 samples/sec: 1.316

100%|██████████| 1/1 [01:38<00:00, 98.15s/it][A100%|██████████| 1/1 [01:38<00:00, 98.15s/it]
 17%|█▋        | 876/5198 [6:54:11<118:45:42, 98.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.27s/it][A100%|██████████| 1/1 [01:38<00:00, 98.27s/it]
 17%|█▋        | 876/5198 [6:53:58<118:47:23, 98.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.22s/it][A100%|██████████| 1/1 [01:38<00:00, 98.22s/it]
 17%|█▋        | 876/5198 [6:54:17<118:46:01, 98.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.19s/it][A100%|██████████| 1/1 [01:38<00:00, 98.19s/it]
 17%|█▋        | 876/5198 [6:52:41<118:45:21, 98.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.20s/it][A100%|██████████| 1/1 [01:38<00:00, 98.21s/it]
 17%|█▋        | 876/5198 [6:53:05<118:46:27, 98.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.19s/it][A100%|██████████| 1/1 [01:38<00:00, 98.19s/it]
 17%|█▋        | 876/5198 [6:53:36<118:45:57, 98.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.20s/it][A100%|██████████| 1/1 [01:38<00:00, 98.20s/it]
 17%|█▋        | 876/5198 [6:53:38<118:45:55, 98.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_822
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.92s/it][A100%|██████████| 1/1 [02:03<00:00, 123.92s/it]
 17%|█▋        | 877/5198 [6:56:00<127:47:42, 106.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:45:56,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=865, skipped=0, lr=[1.9238540889355472e-05], mom=[(0.9, 0.999)]
steps: 865 loss: 0.5508 iter time (s): 123.824 samples/sec: 1.034

100%|██████████| 1/1 [02:04<00:00, 124.76s/it][A100%|██████████| 1/1 [02:04<00:00, 124.76s/it]
 17%|█▋        | 877/5198 [6:56:16<128:02:30, 106.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.67s/it][A100%|██████████| 1/1 [02:04<00:00, 124.67s/it]
 17%|█▋        | 877/5198 [6:56:02<128:01:46, 106.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.75s/it][A100%|██████████| 1/1 [02:04<00:00, 124.75s/it]
 17%|█▋        | 877/5198 [6:56:22<128:02:26, 106.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.84s/it][A100%|██████████| 1/1 [02:04<00:00, 124.84s/it]
 17%|█▋        | 877/5198 [6:54:46<128:04:01, 106.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.73s/it][A100%|██████████| 1/1 [02:04<00:00, 124.73s/it]
 17%|█▋        | 877/5198 [6:55:09<128:02:25, 106.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.76s/it][A100%|██████████| 1/1 [02:04<00:00, 124.76s/it]
 17%|█▋        | 877/5198 [6:55:40<128:02:33, 106.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.76s/it][A100%|██████████| 1/1 [02:04<00:00, 124.76s/it]
 17%|█▋        | 877/5198 [6:55:42<128:02:38, 106.68s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_823
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.82s/it][A100%|██████████| 1/1 [02:02<00:00, 122.83s/it]
 17%|█▋        | 878/5198 [6:58:03<133:42:21, 111.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:47:59,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=866, skipped=0, lr=[1.9236371599872987e-05], mom=[(0.9, 0.999)]
steps: 866 loss: 0.6014 iter time (s): 121.994 samples/sec: 1.049

100%|██████████| 1/1 [02:03<00:00, 123.14s/it][A100%|██████████| 1/1 [02:03<00:00, 123.14s/it]
 17%|█▋        | 878/5198 [6:58:19<133:56:26, 111.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.11s/it][A100%|██████████| 1/1 [02:03<00:00, 123.12s/it]
 17%|█▋        | 878/5198 [6:58:05<133:55:26, 111.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.94s/it][A100%|██████████| 1/1 [02:02<00:00, 122.94s/it]
 17%|█▋        | 878/5198 [6:56:49<133:53:20, 111.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.15s/it][A100%|██████████| 1/1 [02:03<00:00, 123.15s/it]
 17%|█▋        | 878/5198 [6:58:25<133:56:37, 111.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.09s/it][A100%|██████████| 1/1 [02:03<00:00, 123.09s/it]
 17%|█▋        | 878/5198 [6:57:12<133:55:24, 111.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.08s/it][A100%|██████████| 1/1 [02:03<00:00, 123.08s/it]
 17%|█▋        | 878/5198 [6:57:43<133:55:17, 111.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [02:03<00:00, 123.07s/it][A100%|██████████| 1/1 [02:03<00:00, 123.07s/it]
 17%|█▋        | 878/5198 [6:57:45<133:55:03, 111.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_824
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.47s/it][A100%|██████████| 1/1 [01:22<00:00, 82.47s/it]
 17%|█▋        | 879/5198 [6:59:26<123:18:16, 102.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:49:21,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=867, skipped=0, lr=[1.923419934745404e-05], mom=[(0.9, 0.999)]
steps: 867 loss: 0.5753 iter time (s): 80.359 samples/sec: 1.593

100%|██████████| 1/1 [01:21<00:00, 81.24s/it][A100%|██████████| 1/1 [01:21<00:00, 81.24s/it]
 17%|█▋        | 879/5198 [6:59:40<122:58:56, 102.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.28s/it][A100%|██████████| 1/1 [01:21<00:00, 81.28s/it]
 17%|█▋        | 879/5198 [6:59:27<122:59:05, 102.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.27s/it][A100%|██████████| 1/1 [01:21<00:00, 81.27s/it]
 17%|█▋        | 879/5198 [6:59:46<122:59:28, 102.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.23s/it][A100%|██████████| 1/1 [01:21<00:00, 81.23s/it]
 17%|█▋        | 879/5198 [6:58:34<122:58:01, 102.50s/it]
100%|██████████| 1/1 [01:21<00:00, 81.41s/it][A100%|██████████| 1/1 [01:21<00:00, 81.41s/it]
 17%|█▋        | 879/5198 [6:58:10<123:00:16, 102.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [01:21<00:00, 81.30s/it][A100%|██████████| 1/1 [01:21<00:00, 81.30s/it]
100%|██████████| 1/1 [01:21<00:00, 81.30s/it][A 17%|█▋        | 879/5198 [6:59:05<122:59:19, 102.51s/it]100%|██████████| 1/1 [01:21<00:00, 81.30s/it]
 17%|█▋        | 879/5198 [6:59:07<122:58:59, 102.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_54
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:08<00:00, 128.69s/it][A100%|██████████| 1/1 [02:08<00:00, 128.69s/it]
[2024-06-30 06:51:31,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=868, skipped=0, lr=[1.923202413279555e-05], mom=[(0.9, 0.999)]
steps: 868 loss: 0.8372 iter time (s): 129.163 samples/sec: 0.991

100%|██████████| 1/1 [02:10<00:00, 130.25s/it][A100%|██████████| 1/1 [02:10<00:00, 130.25s/it]

100%|██████████| 1/1 [02:10<00:00, 130.24s/it][A100%|██████████| 1/1 [02:10<00:00, 130.24s/it]

100%|██████████| 1/1 [02:10<00:00, 130.18s/it][A100%|██████████| 1/1 [02:10<00:00, 130.18s/it]

100%|██████████| 1/1 [02:10<00:00, 130.30s/it][A100%|██████████| 1/1 [02:10<00:00, 130.30s/it]

100%|██████████| 1/1 [02:10<00:00, 130.37s/it][A100%|██████████| 1/1 [02:10<00:00, 130.37s/it]

100%|██████████| 1/1 [02:10<00:00, 130.48s/it][A100%|██████████| 1/1 [02:10<00:00, 130.48s/it]

100%|██████████| 1/1 [02:10<00:00, 130.64s/it][A100%|██████████| 1/1 [02:10<00:00, 130.64s/it]
Checkpointing at shard 879
[2024-06-30 06:51:32,907] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step868 is about to be saved!
[2024-06-30 06:51:34,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_00-model_states.pt...
[2024-06-30 06:51:38,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_02-model_states.pt...
[2024-06-30 06:51:39,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_00-model_states.pt.
[2024-06-30 06:51:46,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_08-model_states.pt...
[2024-06-30 06:51:46,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_03-model_states.pt...
[2024-06-30 06:51:46,734] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_04-model_states.pt...
[2024-06-30 06:51:47,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_06-model_states.pt...
[2024-06-30 06:51:47,756] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_07-model_states.pt...
[2024-06-30 06:51:48,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_01-model_states.pt...
[2024-06-30 06:51:48,543] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_05-model_states.pt...
[2024-06-30 06:56:02,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_06-model_states.pt.
[2024-06-30 06:56:03,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_05_model_states.pt...
[2024-06-30 06:56:04,850] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_05_model_states.pt.
[2024-06-30 06:56:04,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
[2024-06-30 06:56:06,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_07-model_states.pt.
[2024-06-30 06:56:07,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_01-model_states.pt.
[2024-06-30 06:56:07,357] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_02-model_states.pt.
[2024-06-30 06:56:07,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_05-model_states.pt.
[2024-06-30 06:56:07,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_06_model_states.pt...
[2024-06-30 06:56:07,570] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_06_model_states.pt.
[2024-06-30 06:56:07,571] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
[2024-06-30 06:56:07,573] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_01_model_states.pt
[2024-06-30 06:56:07,573] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_01_model_states.pt...
[2024-06-30 06:56:07,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_01_model_states.pt.
[2024-06-30 06:56:07,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
[2024-06-30 06:56:07,820] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_08-model_states.pt.
[2024-06-30 06:56:08,108] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_00_model_states.pt
[2024-06-30 06:56:08,108] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_00_model_states.pt...
[2024-06-30 06:56:08,451] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_03-model_states.pt.
[2024-06-30 06:56:08,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_04_model_states.pt...
[2024-06-30 06:56:08,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_09-model_states.pt...
[2024-06-30 06:56:08,959] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_04_model_states.pt.
[2024-06-30 06:56:08,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
[2024-06-30 06:56:09,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_02_model_states.pt...
[2024-06-30 06:56:10,961] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_02_model_states.pt.
[2024-06-30 06:56:10,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
[2024-06-30 06:56:13,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_00_model_states.pt.
[2024-06-30 06:56:13,270] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
[2024-06-30 06:56:15,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_04-model_states.pt.
[2024-06-30 06:56:15,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/layer_09-model_states.pt.
[2024-06-30 06:56:15,375] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_07_model_states.pt...
[2024-06-30 06:56:15,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_07_model_states.pt.
[2024-06-30 06:56:15,447] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
[2024-06-30 06:56:15,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_03_model_states.pt...
[2024-06-30 06:56:16,032] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step868/mp_rank_03_model_states.pt.
[2024-06-30 06:56:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step868 is ready now!
Checkpoint saved using --- 283.12633299827576 seconds ---
 17%|█▋        | 880/5198 [7:06:22<236:10:04, 196.90s/it] 17%|█▋        | 880/5198 [7:06:34<235:09:05, 196.05s/it] 17%|█▋        | 880/5198 [7:06:00<234:57:29, 195.89s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_825
 17%|█▋        | 880/5198 [7:06:40<235:02:14, 195.96s/it] 17%|█▋        | 880/5198 [7:05:59<234:57:43, 195.89s/it] 17%|█▋        | 880/5198 [7:05:27<234:58:59, 195.91s/it] 17%|█▋        | 880/5198 [7:06:21<235:05:40, 196.00s/it] 17%|█▋        | 880/5198 [7:05:04<235:00:25, 195.93s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.99s/it][A100%|██████████| 1/1 [01:34<00:00, 94.99s/it]
 17%|█▋        | 881/5198 [7:07:57<199:30:02, 166.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:57:53,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=869, skipped=0, lr=[1.9229845956595375e-05], mom=[(0.9, 0.999)]
steps: 869 loss: 0.5483 iter time (s): 97.220 samples/sec: 1.317

100%|██████████| 1/1 [01:37<00:00, 97.63s/it][A100%|██████████| 1/1 [01:37<00:00, 97.63s/it]
 17%|█▋        | 881/5198 [7:08:12<199:44:28, 166.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.81s/it][A100%|██████████| 1/1 [01:37<00:00, 97.81s/it]
 17%|█▋        | 881/5198 [7:07:59<199:45:55, 166.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.98s/it][A100%|██████████| 1/1 [01:37<00:00, 97.98s/it]
 17%|█▋        | 881/5198 [7:08:18<199:47:14, 166.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 98.00s/it][A100%|██████████| 1/1 [01:37<00:00, 98.00s/it]
 17%|█▋        | 881/5198 [7:06:42<199:46:19, 166.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.13s/it][A100%|██████████| 1/1 [01:38<00:00, 98.13s/it]
 17%|█▋        | 881/5198 [7:07:06<199:48:07, 166.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.17s/it][A100%|██████████| 1/1 [01:38<00:00, 98.17s/it]
 17%|█▋        | 881/5198 [7:07:37<199:48:05, 166.62s/it]
100%|██████████| 1/1 [01:38<00:00, 98.17s/it][A100%|██████████| 1/1 [01:38<00:00, 98.17s/it]
 17%|█▋        | 881/5198 [7:07:39<199:48:01, 166.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_826

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.24s/it][A100%|██████████| 1/1 [01:54<00:00, 114.24s/it]
 17%|█▋        | 882/5198 [7:09:52<180:45:27, 150.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 06:59:48,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.9227664819552333e-05], mom=[(0.9, 0.999)]
steps: 870 loss: 0.5518 iter time (s): 113.973 samples/sec: 1.123

100%|██████████| 1/1 [01:54<00:00, 114.81s/it][A100%|██████████| 1/1 [01:54<00:00, 114.81s/it]
 17%|█▋        | 882/5198 [7:10:07<181:05:01, 151.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.85s/it][A100%|██████████| 1/1 [01:54<00:00, 114.85s/it]
 17%|█▋        | 882/5198 [7:09:54<181:06:54, 151.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.84s/it][A100%|██████████| 1/1 [01:54<00:00, 114.84s/it]
 17%|█▋        | 882/5198 [7:10:13<181:07:35, 151.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.86s/it][A100%|██████████| 1/1 [01:54<00:00, 114.86s/it]

 17%|█▋        | 882/5198 [7:09:01<181:08:42, 151.09s/it]100%|██████████| 1/1 [01:54<00:00, 115.00s/it][A100%|██████████| 1/1 [01:54<00:00, 115.00s/it]
 17%|█▋        | 882/5198 [7:08:37<181:10:16, 151.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.90s/it][A100%|██████████| 1/1 [01:54<00:00, 114.90s/it]
 17%|█▋        | 882/5198 [7:09:32<181:09:24, 151.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.91s/it][A100%|██████████| 1/1 [01:54<00:00, 114.91s/it]
 17%|█▋        | 882/5198 [7:09:34<181:09:29, 151.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_827
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.79s/it][A100%|██████████| 1/1 [01:30<00:00, 90.79s/it]
 17%|█▋        | 883/5198 [7:11:23<159:11:32, 132.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:01:18,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=871, skipped=0, lr=[1.922548072236618e-05], mom=[(0.9, 0.999)]
steps: 871 loss: 0.5671 iter time (s): 89.195 samples/sec: 1.435

100%|██████████| 1/1 [01:30<00:00, 90.22s/it][A100%|██████████| 1/1 [01:30<00:00, 90.22s/it]
 17%|█▋        | 883/5198 [7:11:37<159:10:20, 132.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.09s/it][A100%|██████████| 1/1 [01:30<00:00, 90.09s/it]
 17%|█▋        | 883/5198 [7:11:24<159:08:59, 132.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.09s/it][A100%|██████████| 1/1 [01:30<00:00, 90.09s/it]
 17%|█▋        | 883/5198 [7:11:43<159:09:22, 132.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 90.00s/it][A100%|██████████| 1/1 [01:29<00:00, 90.00s/it]
 17%|█▋        | 883/5198 [7:10:07<159:09:17, 132.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.13s/it][A100%|██████████| 1/1 [01:30<00:00, 90.13s/it]
 17%|█▋        | 883/5198 [7:10:31<159:10:58, 132.81s/it]
100%|██████████| 1/1 [01:30<00:00, 90.06s/it][A100%|██████████| 1/1 [01:30<00:00, 90.06s/it]
 17%|█▋        | 883/5198 [7:11:02<159:10:01, 132.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.08s/it][A100%|██████████| 1/1 [01:30<00:00, 90.08s/it]
 17%|█▋        | 883/5198 [7:11:04<159:10:35, 132.80s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_828
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.61s/it][A100%|██████████| 1/1 [01:42<00:00, 102.61s/it]
 17%|█▋        | 884/5198 [7:13:06<148:21:10, 123.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:03:01,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=872, skipped=0, lr=[1.9223293665737637e-05], mom=[(0.9, 0.999)]
steps: 872 loss: 0.5273 iter time (s): 102.301 samples/sec: 1.251

100%|██████████| 1/1 [01:43<00:00, 103.18s/it][A100%|██████████| 1/1 [01:43<00:00, 103.18s/it]
 17%|█▋        | 884/5198 [7:13:20<148:29:32, 123.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.34s/it][A100%|██████████| 1/1 [01:43<00:00, 103.34s/it]
 17%|█▋        | 884/5198 [7:13:07<148:31:59, 123.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.29s/it][A100%|██████████| 1/1 [01:43<00:00, 103.29s/it]
 17%|█▋        | 884/5198 [7:13:27<148:31:08, 123.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.30s/it][A100%|██████████| 1/1 [01:43<00:00, 103.30s/it]
 17%|█▋        | 884/5198 [7:11:51<148:31:23, 123.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.22s/it][A100%|██████████| 1/1 [01:43<00:00, 103.22s/it]
 17%|█▋        | 884/5198 [7:12:14<148:30:43, 123.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.28s/it][A100%|██████████| 1/1 [01:43<00:00, 103.28s/it]
 17%|█▋        | 884/5198 [7:12:45<148:31:20, 123.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.29s/it][A100%|██████████| 1/1 [01:43<00:00, 103.29s/it]
 17%|█▋        | 884/5198 [7:12:47<148:31:55, 123.95s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_829
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.98s/it][A100%|██████████| 1/1 [01:19<00:00, 79.98s/it]
 17%|█▋        | 885/5198 [7:14:26<132:36:36, 110.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:04:21,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=873, skipped=0, lr=[1.922110365036836e-05], mom=[(0.9, 0.999)]
steps: 873 loss: 0.5476 iter time (s): 78.455 samples/sec: 1.632

100%|██████████| 1/1 [01:19<00:00, 79.46s/it][A100%|██████████| 1/1 [01:19<00:00, 79.46s/it]
 17%|█▋        | 885/5198 [7:14:40<132:29:03, 110.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.28s/it][A100%|██████████| 1/1 [01:19<00:00, 79.28s/it]
 17%|█▋        | 885/5198 [7:14:26<132:26:57, 110.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.38s/it][A100%|██████████| 1/1 [01:19<00:00, 79.38s/it]
 17%|█▋        | 885/5198 [7:14:46<132:28:25, 110.57s/it]
100%|██████████| 1/1 [01:19<00:00, 79.30s/it][A100%|██████████| 1/1 [01:19<00:00, 79.30s/it]
 17%|█▋        | 885/5198 [7:13:10<132:26:48, 110.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.39s/it][A100%|██████████| 1/1 [01:19<00:00, 79.39s/it]
 17%|█▋        | 885/5198 [7:13:33<132:28:15, 110.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.47s/it][A100%|██████████| 1/1 [01:19<00:00, 79.47s/it]
 17%|█▋        | 885/5198 [7:14:05<132:30:29, 110.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.46s/it][A100%|██████████| 1/1 [01:19<00:00, 79.46s/it]
 17%|█▋        | 885/5198 [7:14:07<132:30:38, 110.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_830
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.54s/it][A100%|██████████| 1/1 [01:45<00:00, 105.54s/it]
 17%|█▋        | 886/5198 [7:16:11<130:46:19, 109.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:06:07,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=874, skipped=0, lr=[1.9218910676960963e-05], mom=[(0.9, 0.999)]
steps: 874 loss: 0.5687 iter time (s): 105.341 samples/sec: 1.215

100%|██████████| 1/1 [01:46<00:00, 106.38s/it][A100%|██████████| 1/1 [01:46<00:00, 106.38s/it]
 17%|█▋        | 886/5198 [7:16:26<130:56:51, 109.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.43s/it][A100%|██████████| 1/1 [01:46<00:00, 106.43s/it]
 17%|█▋        | 886/5198 [7:16:13<130:56:21, 109.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.40s/it][A100%|██████████| 1/1 [01:46<00:00, 106.40s/it]
 17%|█▋        | 886/5198 [7:16:32<130:56:52, 109.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.44s/it][A100%|██████████| 1/1 [01:46<00:00, 106.44s/it]
 17%|█▋        | 886/5198 [7:14:56<130:56:26, 109.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.39s/it][A100%|██████████| 1/1 [01:46<00:00, 106.39s/it]
 17%|█▋        | 886/5198 [7:15:20<130:57:05, 109.33s/it]
100%|██████████| 1/1 [01:46<00:00, 106.28s/it][A100%|██████████| 1/1 [01:46<00:00, 106.28s/it]
 17%|█▋        | 886/5198 [7:15:51<130:55:31, 109.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.27s/it][A100%|██████████| 1/1 [01:46<00:00, 106.27s/it]
 17%|█▋        | 886/5198 [7:15:53<130:55:27, 109.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_831
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.92s/it][A100%|██████████| 1/1 [01:36<00:00, 96.92s/it]
 17%|█▋        | 887/5198 [7:17:48<126:22:49, 105.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:07:44,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=875, skipped=0, lr=[1.9216714746219005e-05], mom=[(0.9, 0.999)]
steps: 875 loss: 0.5446 iter time (s): 95.860 samples/sec: 1.335

100%|██████████| 1/1 [01:36<00:00, 96.78s/it][A100%|██████████| 1/1 [01:36<00:00, 96.78s/it]
 17%|█▋        | 887/5198 [7:18:03<126:24:57, 105.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.73s/it][A100%|██████████| 1/1 [01:36<00:00, 96.73s/it]
 17%|█▋        | 887/5198 [7:17:50<126:23:21, 105.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.74s/it][A100%|██████████| 1/1 [01:36<00:00, 96.74s/it]
 17%|█▋        | 887/5198 [7:18:09<126:23:51, 105.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.82s/it][A100%|██████████| 1/1 [01:36<00:00, 96.82s/it]
 17%|█▋        | 887/5198 [7:16:33<126:25:18, 105.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.76s/it][A100%|██████████| 1/1 [01:36<00:00, 96.76s/it]
 17%|█▋        | 887/5198 [7:16:57<126:24:37, 105.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.78s/it][A100%|██████████| 1/1 [01:36<00:00, 96.78s/it]
 17%|█▋        | 887/5198 [7:17:28<126:23:50, 105.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.78s/it][A100%|██████████| 1/1 [01:36<00:00, 96.78s/it]
 17%|█▋        | 887/5198 [7:17:30<126:23:50, 105.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_832
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.24s/it][A100%|██████████| 1/1 [01:25<00:00, 85.24s/it]
 17%|█▋        | 888/5198 [7:19:14<119:06:11, 99.48s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:09:09,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=876, skipped=0, lr=[1.9214515858847e-05], mom=[(0.9, 0.999)]
steps: 876 loss: 0.5886 iter time (s): 84.056 samples/sec: 1.523

100%|██████████| 1/1 [01:25<00:00, 85.00s/it][A100%|██████████| 1/1 [01:25<00:00, 85.00s/it]
 17%|█▋        | 888/5198 [7:19:28<119:00:27, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.98s/it][A100%|██████████| 1/1 [01:24<00:00, 84.98s/it]
 17%|█▋        | 888/5198 [7:19:15<118:58:44, 99.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.97s/it][A100%|██████████| 1/1 [01:24<00:00, 84.97s/it]
 17%|█▋        | 888/5198 [7:19:34<118:58:45, 99.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.93s/it][A100%|██████████| 1/1 [01:24<00:00, 84.93s/it]
 17%|█▋        | 888/5198 [7:17:58<118:58:48, 99.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.97s/it][A100%|██████████| 1/1 [01:24<00:00, 84.97s/it]
 17%|█▋        | 888/5198 [7:18:22<118:59:17, 99.39s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.97s/it][A100%|██████████| 1/1 [01:24<00:00, 84.97s/it]
 17%|█▋        | 888/5198 [7:18:53<118:58:41, 99.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.97s/it][A100%|██████████| 1/1 [01:24<00:00, 84.97s/it]
 17%|█▋        | 888/5198 [7:18:55<118:58:43, 99.38s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_833
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.30s/it][A100%|██████████| 1/1 [01:41<00:00, 101.30s/it]
 17%|█▋        | 889/5198 [7:20:55<119:47:15, 100.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:10:51,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=877, skipped=0, lr=[1.9212314015550397e-05], mom=[(0.9, 0.999)]
steps: 877 loss: 0.6003 iter time (s): 101.010 samples/sec: 1.267

100%|██████████| 1/1 [01:41<00:00, 101.79s/it][A100%|██████████| 1/1 [01:41<00:00, 101.79s/it]
 17%|█▋        | 889/5198 [7:21:10<119:50:28, 100.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.86s/it][A100%|██████████| 1/1 [01:41<00:00, 101.86s/it]
 17%|█▋        | 889/5198 [7:20:56<119:50:44, 100.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.93s/it][A100%|██████████| 1/1 [01:41<00:00, 101.93s/it]
 17%|█▋        | 889/5198 [7:21:16<119:52:18, 100.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.92s/it][A100%|██████████| 1/1 [01:41<00:00, 101.92s/it]
 17%|█▋        | 889/5198 [7:19:40<119:52:04, 100.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.90s/it][A100%|██████████| 1/1 [01:41<00:00, 101.90s/it]
 17%|█▋        | 889/5198 [7:20:03<119:52:01, 100.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.93s/it][A100%|██████████| 1/1 [01:41<00:00, 101.93s/it]
 17%|█▋        | 889/5198 [7:20:35<119:52:15, 100.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.92s/it][A100%|██████████| 1/1 [01:41<00:00, 101.92s/it]
 17%|█▋        | 889/5198 [7:20:37<119:51:58, 100.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_834
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.60s/it][A100%|██████████| 1/1 [01:31<00:00, 91.60s/it]
 17%|█▋        | 890/5198 [7:22:27<116:47:05, 97.59s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:12:22,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=878, skipped=0, lr=[1.9210109217035607e-05], mom=[(0.9, 0.999)]
steps: 878 loss: 0.5605 iter time (s): 90.609 samples/sec: 1.413

100%|██████████| 1/1 [01:31<00:00, 91.66s/it][A100%|██████████| 1/1 [01:31<00:00, 91.66s/it]
 17%|█▋        | 890/5198 [7:22:41<116:46:37, 97.59s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.58s/it][A100%|██████████| 1/1 [01:31<00:00, 91.58s/it]
 17%|█▋        | 890/5198 [7:22:28<116:45:13, 97.57s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.46s/it][A100%|██████████| 1/1 [01:31<00:00, 91.46s/it]
 17%|█▋        | 890/5198 [7:22:48<116:43:36, 97.54s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.60s/it][A100%|██████████| 1/1 [01:31<00:00, 91.60s/it]
 17%|█▋        | 890/5198 [7:21:12<116:46:39, 97.59s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.53s/it][A100%|██████████| 1/1 [01:31<00:00, 91.53s/it]
 17%|█▋        | 890/5198 [7:22:06<116:45:12, 97.57s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.61s/it][A100%|██████████| 1/1 [01:31<00:00, 91.61s/it]
 17%|█▋        | 890/5198 [7:21:35<116:46:47, 97.59s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.59s/it][A100%|██████████| 1/1 [01:31<00:00, 91.59s/it]
 17%|█▋        | 890/5198 [7:22:08<116:46:11, 97.58s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_835
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.67s/it][A100%|██████████| 1/1 [02:01<00:00, 121.67s/it]
 17%|█▋        | 891/5198 [7:24:29<125:26:13, 104.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:14:25,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=879, skipped=0, lr=[1.920790146400998e-05], mom=[(0.9, 0.999)]
steps: 879 loss: 0.6134 iter time (s): 121.626 samples/sec: 1.052

100%|██████████| 1/1 [02:02<00:00, 122.47s/it][A100%|██████████| 1/1 [02:02<00:00, 122.47s/it]
 17%|█▋        | 891/5198 [7:24:44<125:41:06, 105.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.57s/it][A100%|██████████| 1/1 [02:02<00:00, 122.57s/it]
 17%|█▋        | 891/5198 [7:24:31<125:42:13, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.60s/it][A100%|██████████| 1/1 [02:02<00:00, 122.60s/it]
 17%|█▋        | 891/5198 [7:24:50<125:41:48, 105.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.51s/it][A100%|██████████| 1/1 [02:02<00:00, 122.51s/it]
 17%|█▋        | 891/5198 [7:23:14<125:41:59, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.46s/it][A100%|██████████| 1/1 [02:02<00:00, 122.46s/it]
 17%|█▋        | 891/5198 [7:23:38<125:40:59, 105.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.56s/it][A100%|██████████| 1/1 [02:02<00:00, 122.56s/it]
 17%|█▋        | 891/5198 [7:24:09<125:42:09, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.51s/it][A100%|██████████| 1/1 [02:02<00:00, 122.51s/it]
 17%|█▋        | 891/5198 [7:24:11<125:41:41, 105.06s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_836
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.22s/it][A100%|██████████| 1/1 [01:47<00:00, 107.22s/it]
 17%|█▋        | 892/5198 [7:26:16<126:21:24, 105.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:16:12,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.9205690757181824e-05], mom=[(0.9, 0.999)]
steps: 880 loss: 0.5783 iter time (s): 105.966 samples/sec: 1.208

100%|██████████| 1/1 [01:46<00:00, 106.86s/it][A100%|██████████| 1/1 [01:46<00:00, 106.86s/it]
 17%|█▋        | 892/5198 [7:26:31<126:18:32, 105.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.02s/it][A100%|██████████| 1/1 [01:47<00:00, 107.02s/it]
 17%|█▋        | 892/5198 [7:26:18<126:22:47, 105.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.94s/it][A100%|██████████| 1/1 [01:46<00:00, 106.94s/it]
 17%|█▋        | 892/5198 [7:26:37<126:20:40, 105.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.96s/it][A100%|██████████| 1/1 [01:46<00:00, 106.96s/it]
 17%|█▋        | 892/5198 [7:25:01<126:21:13, 105.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.93s/it][A100%|██████████| 1/1 [01:46<00:00, 106.94s/it]
 17%|█▋        | 892/5198 [7:25:25<126:19:57, 105.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.91s/it][A100%|██████████| 1/1 [01:46<00:00, 106.91s/it]
 17%|█▋        | 892/5198 [7:25:56<126:20:15, 105.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.93s/it][A100%|██████████| 1/1 [01:46<00:00, 106.93s/it]
 17%|█▋        | 892/5198 [7:25:58<126:20:14, 105.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_837
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.98s/it][A100%|██████████| 1/1 [01:22<00:00, 82.98s/it]
 17%|█▋        | 893/5198 [7:27:39<118:15:25, 98.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:17:34,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=881, skipped=0, lr=[1.920347709726039e-05], mom=[(0.9, 0.999)]
steps: 881 loss: 0.5815 iter time (s): 81.764 samples/sec: 1.565

100%|██████████| 1/1 [01:22<00:00, 82.72s/it][A100%|██████████| 1/1 [01:22<00:00, 82.72s/it]
 17%|█▋        | 893/5198 [7:27:54<118:04:26, 98.74s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.51s/it][A100%|██████████| 1/1 [01:22<00:00, 82.51s/it]
 17%|█▋        | 893/5198 [7:27:40<118:02:55, 98.72s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.63s/it][A100%|██████████| 1/1 [01:22<00:00, 82.63s/it]
 17%|█▋        | 893/5198 [7:28:00<118:03:55, 98.73s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.62s/it][A100%|██████████| 1/1 [01:22<00:00, 82.62s/it]
 17%|█▋        | 893/5198 [7:26:24<118:04:07, 98.73s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.60s/it][A100%|██████████| 1/1 [01:22<00:00, 82.60s/it]
 17%|█▋        | 893/5198 [7:27:18<118:03:03, 98.72s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.66s/it][A100%|██████████| 1/1 [01:22<00:00, 82.66s/it]
 17%|█▋        | 893/5198 [7:26:47<118:04:08, 98.73s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.61s/it][A100%|██████████| 1/1 [01:22<00:00, 82.62s/it]
 17%|█▋        | 893/5198 [7:27:20<118:03:22, 98.72s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_838
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.97s/it][A100%|██████████| 1/1 [01:40<00:00, 100.97s/it]
 17%|█▋        | 894/5198 [7:29:20<119:01:52, 99.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:19:16,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=882, skipped=0, lr=[1.9201260484955865e-05], mom=[(0.9, 0.999)]
steps: 882 loss: 0.6063 iter time (s): 100.796 samples/sec: 1.270

100%|██████████| 1/1 [01:41<00:00, 101.69s/it][A100%|██████████| 1/1 [01:41<00:00, 101.69s/it]
 17%|█▋        | 894/5198 [7:29:35<119:06:35, 99.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.67s/it][A100%|██████████| 1/1 [01:41<00:00, 101.67s/it]
 17%|█▋        | 894/5198 [7:29:22<119:04:59, 99.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.68s/it][A100%|██████████| 1/1 [01:41<00:00, 101.68s/it]
 17%|█▋        | 894/5198 [7:29:41<119:05:49, 99.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.65s/it][A100%|██████████| 1/1 [01:41<00:00, 101.65s/it]
 17%|█▋        | 894/5198 [7:28:06<119:05:22, 99.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.67s/it][A100%|██████████| 1/1 [01:41<00:00, 101.67s/it]
 17%|█▋        | 894/5198 [7:28:29<119:05:56, 99.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.73s/it][A100%|██████████| 1/1 [01:41<00:00, 101.73s/it]
 17%|█▋        | 894/5198 [7:29:00<119:06:21, 99.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.69s/it][A100%|██████████| 1/1 [01:41<00:00, 101.69s/it]
 17%|█▋        | 894/5198 [7:29:02<119:05:49, 99.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_839
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.57s/it][A100%|██████████| 1/1 [01:26<00:00, 86.57s/it]
 17%|█▋        | 895/5198 [7:30:47<114:23:39, 95.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:20:42,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=883, skipped=0, lr=[1.9199040920979404e-05], mom=[(0.9, 0.999)]
steps: 883 loss: 0.5737 iter time (s): 85.366 samples/sec: 1.499

100%|██████████| 1/1 [01:26<00:00, 86.24s/it][A100%|██████████| 1/1 [01:26<00:00, 86.24s/it]
 17%|█▋        | 895/5198 [7:31:02<114:17:00, 95.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.32s/it][A100%|██████████| 1/1 [01:26<00:00, 86.32s/it]
 17%|█▋        | 895/5198 [7:30:48<114:17:46, 95.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.25s/it][A100%|██████████| 1/1 [01:26<00:00, 86.25s/it]
 17%|█▋        | 895/5198 [7:31:08<114:16:47, 95.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.25s/it][A100%|██████████| 1/1 [01:26<00:00, 86.25s/it]
 17%|█▋        | 895/5198 [7:29:32<114:16:24, 95.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.28s/it][A100%|██████████| 1/1 [01:26<00:00, 86.28s/it]
 17%|█▋        | 895/5198 [7:29:55<114:17:26, 95.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.25s/it][A100%|██████████| 1/1 [01:26<00:00, 86.25s/it]
 17%|█▋        | 895/5198 [7:30:26<114:17:03, 95.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.26s/it][A100%|██████████| 1/1 [01:26<00:00, 86.26s/it]
 17%|█▋        | 895/5198 [7:30:28<114:17:02, 95.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_55
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.05s/it][A100%|██████████| 1/1 [02:03<00:00, 123.05s/it]
 17%|█▋        | 896/5198 [7:32:50<124:13:11, 103.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:22:46,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=884, skipped=0, lr=[1.919681840604309e-05], mom=[(0.9, 0.999)]
steps: 884 loss: 0.7859 iter time (s): 123.315 samples/sec: 1.038

100%|██████████| 1/1 [02:04<00:00, 124.26s/it][A100%|██████████| 1/1 [02:04<00:00, 124.26s/it]
 17%|█▋        | 896/5198 [7:33:06<124:31:48, 104.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.37s/it][A100%|██████████| 1/1 [02:04<00:00, 124.37s/it]
 17%|█▋        | 896/5198 [7:32:53<124:34:45, 104.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.28s/it][A100%|██████████| 1/1 [02:04<00:00, 124.28s/it]
 17%|█▋        | 896/5198 [7:33:12<124:32:11, 104.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.33s/it][A100%|██████████| 1/1 [02:04<00:00, 124.33s/it]
 17%|█▋        | 896/5198 [7:31:36<124:32:48, 104.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.25s/it][A100%|██████████| 1/1 [02:04<00:00, 124.25s/it]
 17%|█▋        | 896/5198 [7:31:59<124:31:59, 104.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.26s/it][A100%|██████████| 1/1 [02:04<00:00, 124.26s/it]
 17%|█▋        | 896/5198 [7:32:30<124:31:50, 104.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.25s/it][A100%|██████████| 1/1 [02:04<00:00, 124.25s/it]
 17%|█▋        | 896/5198 [7:32:32<124:31:37, 104.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_840
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.32s/it][A100%|██████████| 1/1 [01:38<00:00, 98.32s/it]
 17%|█▋        | 897/5198 [7:34:29<122:13:26, 102.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:24:24,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=885, skipped=0, lr=[1.9194592940859965e-05], mom=[(0.9, 0.999)]
steps: 885 loss: 0.5870 iter time (s): 96.803 samples/sec: 1.322

100%|██████████| 1/1 [01:37<00:00, 97.66s/it][A100%|██████████| 1/1 [01:37<00:00, 97.66s/it]
 17%|█▋        | 897/5198 [7:34:43<122:09:30, 102.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.62s/it][A100%|██████████| 1/1 [01:37<00:00, 97.62s/it]
 17%|█▋        | 897/5198 [7:34:30<122:10:40, 102.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.74s/it][A100%|██████████| 1/1 [01:37<00:00, 97.74s/it]
 17%|█▋        | 897/5198 [7:34:50<122:11:19, 102.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.67s/it][A100%|██████████| 1/1 [01:37<00:00, 97.67s/it]
 17%|█▋        | 897/5198 [7:33:14<122:10:23, 102.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.72s/it][A100%|██████████| 1/1 [01:37<00:00, 97.72s/it]
 17%|█▋        | 897/5198 [7:33:37<122:10:49, 102.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.73s/it][A100%|██████████| 1/1 [01:37<00:00, 97.73s/it]
 17%|█▋        | 897/5198 [7:34:08<122:10:50, 102.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.73s/it][A100%|██████████| 1/1 [01:37<00:00, 97.73s/it]
 17%|█▋        | 897/5198 [7:34:10<122:10:48, 102.27s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_841
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.63s/it][A100%|██████████| 1/1 [01:35<00:00, 95.63s/it]
 17%|█▋        | 898/5198 [7:36:05<119:51:39, 100.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:26:00,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=886, skipped=0, lr=[1.9192364526144013e-05], mom=[(0.9, 0.999)]
steps: 886 loss: 0.5881 iter time (s): 94.749 samples/sec: 1.351

100%|██████████| 1/1 [01:35<00:00, 95.79s/it][A100%|██████████| 1/1 [01:35<00:00, 95.79s/it]
 17%|█▋        | 898/5198 [7:36:19<119:49:11, 100.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.77s/it][A100%|██████████| 1/1 [01:35<00:00, 95.77s/it]
 17%|█▋        | 898/5198 [7:36:06<119:49:38, 100.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.72s/it][A100%|██████████| 1/1 [01:35<00:00, 95.72s/it]
 17%|█▋        | 898/5198 [7:36:25<119:49:01, 100.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.68s/it][A100%|██████████| 1/1 [01:35<00:00, 95.68s/it]
 17%|█▋        | 898/5198 [7:34:50<119:47:21, 100.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.69s/it][A100%|██████████| 1/1 [01:35<00:00, 95.69s/it]
 17%|█▋        | 898/5198 [7:35:44<119:47:50, 100.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.72s/it][A100%|██████████| 1/1 [01:35<00:00, 95.72s/it]
 17%|█▋        | 898/5198 [7:35:13<119:48:30, 100.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.72s/it][A100%|██████████| 1/1 [01:35<00:00, 95.72s/it]
 17%|█▋        | 898/5198 [7:35:46<119:48:30, 100.30s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_842
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.09s/it][A100%|██████████| 1/1 [01:42<00:00, 102.09s/it]
 17%|█▋        | 899/5198 [7:37:47<120:30:58, 100.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:27:42,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=887, skipped=0, lr=[1.9190133162610168e-05], mom=[(0.9, 0.999)]
steps: 887 loss: 0.6209 iter time (s): 101.498 samples/sec: 1.261

100%|██████████| 1/1 [01:42<00:00, 102.30s/it][A100%|██████████| 1/1 [01:42<00:00, 102.30s/it]
 17%|█▋        | 899/5198 [7:38:02<120:30:39, 100.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.35s/it][A100%|██████████| 1/1 [01:42<00:00, 102.35s/it]
 17%|█▋        | 899/5198 [7:37:48<120:31:42, 100.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.39s/it][A100%|██████████| 1/1 [01:42<00:00, 102.39s/it]
 17%|█▋        | 899/5198 [7:38:08<120:32:14, 100.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.45s/it][A100%|██████████| 1/1 [01:42<00:00, 102.45s/it]
 17%|█▋        | 899/5198 [7:36:32<120:32:21, 100.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.37s/it][A100%|██████████| 1/1 [01:42<00:00, 102.37s/it]
 17%|█▋        | 899/5198 [7:36:55<120:31:29, 100.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.43s/it][A100%|██████████| 1/1 [01:42<00:00, 102.43s/it]
 17%|█▋        | 899/5198 [7:37:26<120:32:10, 100.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.40s/it][A100%|██████████| 1/1 [01:42<00:00, 102.40s/it]
 17%|█▋        | 899/5198 [7:37:28<120:32:01, 100.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_843
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.09s/it][A100%|██████████| 1/1 [02:01<00:00, 121.09s/it]
[2024-06-30 07:29:44,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=888, skipped=0, lr=[1.91878988509743e-05], mom=[(0.9, 0.999)]
steps: 888 loss: 0.6034 iter time (s): 120.895 samples/sec: 1.059

100%|██████████| 1/1 [02:01<00:00, 121.98s/it][A100%|██████████| 1/1 [02:01<00:00, 121.98s/it]

100%|██████████| 1/1 [02:01<00:00, 121.80s/it][A100%|██████████| 1/1 [02:01<00:00, 121.80s/it]

100%|██████████| 1/1 [02:01<00:00, 121.80s/it][A100%|██████████| 1/1 [02:01<00:00, 121.80s/it]

100%|██████████| 1/1 [02:01<00:00, 121.89s/it][A100%|██████████| 1/1 [02:01<00:00, 121.89s/it]

100%|██████████| 1/1 [02:01<00:00, 121.88s/it][A100%|██████████| 1/1 [02:01<00:00, 121.88s/it]

100%|██████████| 1/1 [02:01<00:00, 121.87s/it][A100%|██████████| 1/1 [02:01<00:00, 121.87s/it]

100%|██████████| 1/1 [02:01<00:00, 121.88s/it][A100%|██████████| 1/1 [02:01<00:00, 121.88s/it]
Checkpointing at shard 899
[2024-06-30 07:29:45,744] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step888 is about to be saved!
[2024-06-30 07:29:47,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_00-model_states.pt...
[2024-06-30 07:29:52,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_00-model_states.pt.
[2024-06-30 07:29:52,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_02-model_states.pt...
[2024-06-30 07:29:59,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_08-model_states.pt...
[2024-06-30 07:29:59,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_03-model_states.pt...
[2024-06-30 07:29:59,443] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_07-model_states.pt...
[2024-06-30 07:29:59,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_06-model_states.pt...
[2024-06-30 07:29:59,730] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_04-model_states.pt...
[2024-06-30 07:30:01,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_05-model_states.pt...
[2024-06-30 07:30:01,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_01-model_states.pt...
[2024-06-30 07:32:03,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_02-model_states.pt.
[2024-06-30 07:32:04,257] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_01_model_states.pt
[2024-06-30 07:32:04,257] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_01_model_states.pt...
[2024-06-30 07:32:04,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_01_model_states.pt.
[2024-06-30 07:32:04,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
[2024-06-30 07:32:19,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_01-model_states.pt.
[2024-06-30 07:32:20,603] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_00_model_states.pt
[2024-06-30 07:32:20,603] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_00_model_states.pt...
[2024-06-30 07:32:20,963] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_07-model_states.pt.
[2024-06-30 07:32:21,037] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_00_model_states.pt.
[2024-06-30 07:32:21,037] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
[2024-06-30 07:32:21,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_06_model_states.pt...
[2024-06-30 07:32:22,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_06_model_states.pt.
[2024-06-30 07:32:22,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
[2024-06-30 07:32:23,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_08-model_states.pt.
[2024-06-30 07:32:24,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_05-model_states.pt.
[2024-06-30 07:32:25,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_09-model_states.pt...
[2024-06-30 07:32:25,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_04_model_states.pt...
[2024-06-30 07:32:25,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_04_model_states.pt.
[2024-06-30 07:32:25,963] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
[2024-06-30 07:32:29,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_09-model_states.pt.
[2024-06-30 07:32:29,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_07_model_states.pt...
[2024-06-30 07:32:29,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_07_model_states.pt.
[2024-06-30 07:32:29,112] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
[2024-06-30 07:32:29,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_03-model_states.pt.
[2024-06-30 07:32:29,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_04-model_states.pt.
[2024-06-30 07:32:29,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_02_model_states.pt...
[2024-06-30 07:32:30,051] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_02_model_states.pt.
[2024-06-30 07:32:30,051] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
[2024-06-30 07:32:30,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_03_model_states.pt...
[2024-06-30 07:32:30,399] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_03_model_states.pt.
[2024-06-30 07:32:30,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
[2024-06-30 07:32:37,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/layer_06-model_states.pt.
[2024-06-30 07:32:38,559] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_05_model_states.pt...
[2024-06-30 07:32:38,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step888/mp_rank_05_model_states.pt.
[2024-06-30 07:32:38,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step888 is ready now!
Checkpoint saved using --- 172.8787920475006 seconds ---
 17%|█▋        | 900/5198 [7:42:45<191:05:52, 160.06s/it] 17%|█▋        | 900/5198 [7:42:57<190:07:08, 159.24s/it] 17%|█▋        | 900/5198 [7:42:23<189:55:40, 159.08s/it] 17%|█▋        | 900/5198 [7:41:50<189:56:45, 159.10s/it] 17%|█▋        | 900/5198 [7:43:03<190:00:29, 159.15s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_844
 17%|█▋        | 900/5198 [7:42:43<190:02:58, 159.19s/it] 17%|█▋        | 900/5198 [7:42:21<189:56:17, 159.09s/it] 17%|█▋        | 900/5198 [7:41:27<189:58:30, 159.12s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.03s/it][A100%|██████████| 1/1 [01:17<00:00, 77.03s/it]
 17%|█▋        | 901/5198 [7:44:02<161:23:27, 135.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:33:57,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=889, skipped=0, lr=[1.9185661591953236e-05], mom=[(0.9, 0.999)]
steps: 889 loss: 0.5662 iter time (s): 78.814 samples/sec: 1.624

100%|██████████| 1/1 [01:19<00:00, 79.25s/it][A100%|██████████| 1/1 [01:19<00:00, 79.25s/it]
 17%|█▋        | 901/5198 [7:44:16<161:30:15, 135.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.36s/it][A100%|██████████| 1/1 [01:19<00:00, 79.36s/it]
 17%|█▋        | 901/5198 [7:44:03<161:29:40, 135.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.47s/it][A100%|██████████| 1/1 [01:19<00:00, 79.47s/it]
 17%|█▋        | 901/5198 [7:44:23<161:30:26, 135.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.61s/it][A100%|██████████| 1/1 [01:19<00:00, 79.61s/it]
 17%|█▋        | 901/5198 [7:42:47<161:32:01, 135.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.67s/it][A100%|██████████| 1/1 [01:19<00:00, 79.67s/it]
 17%|█▋        | 901/5198 [7:43:10<161:32:08, 135.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.71s/it][A100%|██████████| 1/1 [01:19<00:00, 79.71s/it]
 17%|█▋        | 901/5198 [7:43:41<161:32:39, 135.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.74s/it][A100%|██████████| 1/1 [01:19<00:00, 79.74s/it]
 17%|█▋        | 901/5198 [7:43:43<161:32:46, 135.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_845
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.25s/it][A100%|██████████| 1/1 [01:31<00:00, 91.25s/it]
 17%|█▋        | 902/5198 [7:45:34<145:40:28, 122.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:35:29,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.918342138626474e-05], mom=[(0.9, 0.999)]
steps: 890 loss: 0.5897 iter time (s): 90.787 samples/sec: 1.410

100%|██████████| 1/1 [01:31<00:00, 91.66s/it][A100%|██████████| 1/1 [01:31<00:00, 91.66s/it]
 17%|█▋        | 902/5198 [7:45:48<145:50:40, 122.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.78s/it][A100%|██████████| 1/1 [01:31<00:00, 91.78s/it]
 17%|█▋        | 902/5198 [7:45:35<145:52:45, 122.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.70s/it][A100%|██████████| 1/1 [01:31<00:00, 91.70s/it]
 17%|█▋        | 902/5198 [7:45:54<145:51:41, 122.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.76s/it][A100%|██████████| 1/1 [01:31<00:00, 91.76s/it]
 17%|█▋        | 902/5198 [7:44:18<145:54:00, 122.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.72s/it][A100%|██████████| 1/1 [01:31<00:00, 91.72s/it]
 17%|█▋        | 902/5198 [7:44:42<145:53:14, 122.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.71s/it][A100%|██████████| 1/1 [01:31<00:00, 91.71s/it]
 17%|█▋        | 902/5198 [7:45:13<145:53:20, 122.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.69s/it][A100%|██████████| 1/1 [01:31<00:00, 91.69s/it]
 17%|█▋        | 902/5198 [7:45:15<145:53:09, 122.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_846
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.27s/it][A100%|██████████| 1/1 [01:29<00:00, 89.27s/it]
 17%|█▋        | 903/5198 [7:47:03<133:58:09, 112.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:36:58,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=891, skipped=0, lr=[1.9181178234627532e-05], mom=[(0.9, 0.999)]
steps: 891 loss: 0.6180 iter time (s): 88.473 samples/sec: 1.447

100%|██████████| 1/1 [01:29<00:00, 89.38s/it][A100%|██████████| 1/1 [01:29<00:00, 89.38s/it]
 17%|█▋        | 903/5198 [7:47:17<134:03:44, 112.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.29s/it][A100%|██████████| 1/1 [01:29<00:00, 89.29s/it]
 17%|█▋        | 903/5198 [7:47:04<134:03:07, 112.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.35s/it][A100%|██████████| 1/1 [01:29<00:00, 89.35s/it]
 17%|█▋        | 903/5198 [7:47:24<134:03:50, 112.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.35s/it][A100%|██████████| 1/1 [01:29<00:00, 89.35s/it]
 17%|█▋        | 903/5198 [7:45:48<134:05:14, 112.39s/it]
100%|██████████| 1/1 [01:29<00:00, 89.32s/it][A100%|██████████| 1/1 [01:29<00:00, 89.32s/it]
 17%|█▋        | 903/5198 [7:46:11<134:04:14, 112.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.33s/it][A100%|██████████| 1/1 [01:29<00:00, 89.33s/it]
 17%|█▋        | 903/5198 [7:46:42<134:04:29, 112.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.37s/it][A100%|██████████| 1/1 [01:29<00:00, 89.37s/it]
 17%|█▋        | 903/5198 [7:46:44<134:05:03, 112.39s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_847
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.02s/it][A100%|██████████| 1/1 [01:29<00:00, 89.02s/it]
 17%|█▋        | 904/5198 [7:48:32<125:42:14, 105.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:38:28,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=892, skipped=0, lr=[1.9178932137761274e-05], mom=[(0.9, 0.999)]
steps: 892 loss: 0.5409 iter time (s): 88.431 samples/sec: 1.447

100%|██████████| 1/1 [01:29<00:00, 89.38s/it][A100%|██████████| 1/1 [01:29<00:00, 89.38s/it]
 17%|█▋        | 904/5198 [7:48:47<125:48:32, 105.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.35s/it][A100%|██████████| 1/1 [01:29<00:00, 89.35s/it]
 17%|█▋        | 904/5198 [7:48:33<125:47:24, 105.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.38s/it][A100%|██████████| 1/1 [01:29<00:00, 89.38s/it]
 17%|█▋        | 904/5198 [7:48:53<125:48:36, 105.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.26s/it][A100%|██████████| 1/1 [01:29<00:00, 89.26s/it]
 17%|█▋        | 904/5198 [7:47:17<125:47:01, 105.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.34s/it][A100%|██████████| 1/1 [01:29<00:00, 89.34s/it]
 17%|█▋        | 904/5198 [7:47:40<125:47:56, 105.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.33s/it][A100%|██████████| 1/1 [01:29<00:00, 89.33s/it]
 17%|█▋        | 904/5198 [7:48:11<125:47:54, 105.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.32s/it][A100%|██████████| 1/1 [01:29<00:00, 89.32s/it]
 17%|█▋        | 904/5198 [7:48:13<125:48:06, 105.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_848
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.60s/it][A100%|██████████| 1/1 [01:50<00:00, 110.60s/it]
 17%|█▋        | 905/5198 [7:50:23<127:36:27, 107.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:40:19,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=893, skipped=0, lr=[1.9176683096386558e-05], mom=[(0.9, 0.999)]
steps: 893 loss: 0.5582 iter time (s): 110.453 samples/sec: 1.159

100%|██████████| 1/1 [01:51<00:00, 111.30s/it][A100%|██████████| 1/1 [01:51<00:00, 111.30s/it]
 17%|█▋        | 905/5198 [7:50:38<127:52:00, 107.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.39s/it][A100%|██████████| 1/1 [01:51<00:00, 111.39s/it]
 17%|█▋        | 905/5198 [7:50:25<127:53:03, 107.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.41s/it][A100%|██████████| 1/1 [01:51<00:00, 111.41s/it]
 17%|█▋        | 905/5198 [7:50:44<127:54:25, 107.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.38s/it][A100%|██████████| 1/1 [01:51<00:00, 111.38s/it]
 17%|█▋        | 905/5198 [7:49:08<127:52:38, 107.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.38s/it][A100%|██████████| 1/1 [01:51<00:00, 111.38s/it]
 17%|█▋        | 905/5198 [7:50:03<127:53:16, 107.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.42s/it][A100%|██████████| 1/1 [01:51<00:00, 111.42s/it]
 17%|█▋        | 905/5198 [7:49:32<127:54:08, 107.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.40s/it][A100%|██████████| 1/1 [01:51<00:00, 111.40s/it]
 17%|█▋        | 905/5198 [7:50:05<127:53:46, 107.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_849
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.26s/it][A100%|██████████| 1/1 [01:38<00:00, 98.26s/it]
 17%|█▋        | 906/5198 [7:52:02<124:30:58, 104.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:41:57,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=894, skipped=0, lr=[1.9174431111224935e-05], mom=[(0.9, 0.999)]
steps: 894 loss: 0.5268 iter time (s): 97.077 samples/sec: 1.319

100%|██████████| 1/1 [01:37<00:00, 97.86s/it][A100%|██████████| 1/1 [01:37<00:00, 97.86s/it]
 17%|█▋        | 906/5198 [7:52:03<124:30:09, 104.43s/it]
100%|██████████| 1/1 [01:38<00:00, 98.05s/it][A100%|██████████| 1/1 [01:38<00:00, 98.05s/it]
 17%|█▋        | 906/5198 [7:52:16<124:33:28, 104.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.86s/it][A100%|██████████| 1/1 [01:37<00:00, 97.86s/it]
 17%|█▋        | 906/5198 [7:52:22<124:31:00, 104.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.05s/it][A100%|██████████| 1/1 [01:38<00:00, 98.05s/it]
 17%|█▋        | 906/5198 [7:50:47<124:33:51, 104.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.94s/it][A100%|██████████| 1/1 [01:37<00:00, 97.94s/it]
 17%|█▋        | 906/5198 [7:51:10<124:32:41, 104.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.01s/it][A100%|██████████| 1/1 [01:38<00:00, 98.01s/it]
 17%|█▋        | 906/5198 [7:51:41<124:33:26, 104.47s/it]
100%|██████████| 1/1 [01:37<00:00, 97.96s/it][A100%|██████████| 1/1 [01:37<00:00, 97.96s/it]
 17%|█▋        | 906/5198 [7:51:43<124:32:49, 104.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_850

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.35s/it][A100%|██████████| 1/1 [01:54<00:00, 114.35s/it]
 17%|█▋        | 907/5198 [7:53:56<128:06:02, 107.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:43:52,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=895, skipped=0, lr=[1.91721761829989e-05], mom=[(0.9, 0.999)]
steps: 895 loss: 0.5387 iter time (s): 114.198 samples/sec: 1.121

100%|██████████| 1/1 [01:55<00:00, 115.03s/it][A100%|██████████| 1/1 [01:55<00:00, 115.03s/it]
 17%|█▋        | 907/5198 [7:54:11<128:18:22, 107.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.17s/it][A100%|██████████| 1/1 [01:55<00:00, 115.17s/it]
 17%|█▋        | 907/5198 [7:53:58<128:19:07, 107.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.20s/it][A100%|██████████| 1/1 [01:55<00:00, 115.20s/it]
 17%|█▋        | 907/5198 [7:54:17<128:20:23, 107.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.00s/it][A100%|██████████| 1/1 [01:55<00:00, 115.00s/it]
 17%|█▋        | 907/5198 [7:52:42<128:17:59, 107.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.14s/it][A100%|██████████| 1/1 [01:55<00:00, 115.14s/it]
 17%|█▋        | 907/5198 [7:53:05<128:20:03, 107.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.10s/it][A100%|██████████| 1/1 [01:55<00:00, 115.10s/it]
 17%|█▋        | 907/5198 [7:53:36<128:19:54, 107.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.11s/it][A100%|██████████| 1/1 [01:55<00:00, 115.11s/it]
 17%|█▋        | 907/5198 [7:53:38<128:19:36, 107.66s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_851
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.64s/it][A100%|██████████| 1/1 [01:41<00:00, 101.64s/it]
 17%|█▋        | 908/5198 [7:55:38<126:01:51, 105.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:45:33,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=896, skipped=0, lr=[1.9169918312431894e-05], mom=[(0.9, 0.999)]
steps: 896 loss: 0.5195 iter time (s): 100.275 samples/sec: 1.276

100%|██████████| 1/1 [01:41<00:00, 101.23s/it][A100%|██████████| 1/1 [01:41<00:00, 101.24s/it]
 17%|█▋        | 908/5198 [7:55:52<125:59:23, 105.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.18s/it][A100%|██████████| 1/1 [01:41<00:00, 101.18s/it]
 17%|█▋        | 908/5198 [7:55:39<125:58:30, 105.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.14s/it][A100%|██████████| 1/1 [01:41<00:00, 101.14s/it]
 17%|█▋        | 908/5198 [7:55:59<125:58:35, 105.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.26s/it][A100%|██████████| 1/1 [01:41<00:00, 101.26s/it]
 17%|█▋        | 908/5198 [7:54:23<125:59:34, 105.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.19s/it][A100%|██████████| 1/1 [01:41<00:00, 101.19s/it]
 17%|█▋        | 908/5198 [7:54:46<125:59:24, 105.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.21s/it][A100%|██████████| 1/1 [01:41<00:00, 101.21s/it]
 17%|█▋        | 908/5198 [7:55:17<125:59:46, 105.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.22s/it][A100%|██████████| 1/1 [01:41<00:00, 101.22s/it]
 17%|█▋        | 908/5198 [7:55:19<125:59:42, 105.73s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_852
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.67s/it][A100%|██████████| 1/1 [01:31<00:00, 91.67s/it]
 17%|█▋        | 909/5198 [7:57:10<121:03:01, 101.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:47:05,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=897, skipped=0, lr=[1.9167657500248292e-05], mom=[(0.9, 0.999)]
steps: 897 loss: 0.5822 iter time (s): 90.872 samples/sec: 1.409

100%|██████████| 1/1 [01:31<00:00, 91.81s/it][A100%|██████████| 1/1 [01:31<00:00, 91.81s/it]
 17%|█▋        | 909/5198 [7:57:24<120:59:23, 101.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.81s/it][A100%|██████████| 1/1 [01:31<00:00, 91.81s/it]
 17%|█▋        | 909/5198 [7:57:11<120:58:56, 101.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.83s/it][A100%|██████████| 1/1 [01:31<00:00, 91.83s/it]
 17%|█▋        | 909/5198 [7:57:30<120:59:10, 101.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.83s/it][A100%|██████████| 1/1 [01:31<00:00, 91.83s/it]
 17%|█▋        | 909/5198 [7:55:55<120:59:57, 101.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.77s/it][A100%|██████████| 1/1 [01:31<00:00, 91.78s/it]
 17%|█▋        | 909/5198 [7:56:18<120:58:38, 101.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.78s/it][A100%|██████████| 1/1 [01:31<00:00, 91.78s/it]
 17%|█▋        | 909/5198 [7:56:49<120:59:04, 101.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.79s/it][A100%|██████████| 1/1 [01:31<00:00, 91.79s/it]
 17%|█▋        | 909/5198 [7:56:51<120:59:13, 101.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_853
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.22s/it][A100%|██████████| 1/1 [01:28<00:00, 88.22s/it]
 18%|█▊        | 910/5198 [7:58:38<116:17:36, 97.63s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:48:33,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=898, skipped=0, lr=[1.9165393747173423e-05], mom=[(0.9, 0.999)]
steps: 898 loss: 0.5603 iter time (s): 87.005 samples/sec: 1.471

100%|██████████| 1/1 [01:27<00:00, 87.89s/it][A100%|██████████| 1/1 [01:27<00:00, 87.89s/it]
 18%|█▊        | 910/5198 [7:58:52<116:04:55, 97.46s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.88s/it][A100%|██████████| 1/1 [01:27<00:00, 87.88s/it]
 18%|█▊        | 910/5198 [7:58:39<116:04:29, 97.45s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.96s/it][A100%|██████████| 1/1 [01:27<00:00, 87.96s/it]
 18%|█▊        | 910/5198 [7:58:58<116:06:19, 97.48s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.85s/it][A100%|██████████| 1/1 [01:27<00:00, 87.85s/it]
 18%|█▊        | 910/5198 [7:57:22<116:04:23, 97.45s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.91s/it][A100%|██████████| 1/1 [01:27<00:00, 87.92s/it]
 18%|█▊        | 910/5198 [7:57:46<116:04:56, 97.46s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.88s/it][A100%|██████████| 1/1 [01:27<00:00, 87.88s/it]
 18%|█▊        | 910/5198 [7:58:17<116:04:31, 97.45s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.88s/it][A100%|██████████| 1/1 [01:27<00:00, 87.88s/it]
 18%|█▊        | 910/5198 [7:58:19<116:04:30, 97.45s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_854
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.19s/it][A100%|██████████| 1/1 [01:33<00:00, 93.19s/it]
 18%|█▊        | 911/5198 [8:00:12<114:44:54, 96.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:50:07,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=899, skipped=0, lr=[1.916312705393355e-05], mom=[(0.9, 0.999)]
steps: 899 loss: 0.5511 iter time (s): 92.947 samples/sec: 1.377

100%|██████████| 1/1 [01:33<00:00, 93.85s/it][A100%|██████████| 1/1 [01:33<00:00, 93.85s/it]
 18%|█▊        | 911/5198 [8:00:26<114:46:17, 96.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.91s/it][A100%|██████████| 1/1 [01:33<00:00, 93.91s/it]
 18%|█▊        | 911/5198 [8:00:13<114:47:15, 96.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.74s/it][A100%|██████████| 1/1 [01:33<00:00, 93.74s/it]
 18%|█▊        | 911/5198 [8:00:32<114:44:46, 96.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.86s/it][A100%|██████████| 1/1 [01:33<00:00, 93.86s/it]
 18%|█▊        | 911/5198 [7:58:56<114:46:00, 96.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.80s/it][A100%|██████████| 1/1 [01:33<00:00, 93.80s/it]
 18%|█▊        | 911/5198 [7:59:20<114:45:09, 96.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.82s/it][A100%|██████████| 1/1 [01:33<00:00, 93.82s/it]
 18%|█▊        | 911/5198 [7:59:51<114:45:25, 96.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.83s/it][A100%|██████████| 1/1 [01:33<00:00, 93.83s/it]
 18%|█▊        | 911/5198 [7:59:53<114:45:24, 96.37s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_56
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.09s/it][A100%|██████████| 1/1 [01:50<00:00, 110.09s/it]
 18%|█▊        | 912/5198 [8:02:02<119:40:30, 100.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:51:57,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.9160857421255892e-05], mom=[(0.9, 0.999)]
steps: 900 loss: 0.8055 iter time (s): 109.914 samples/sec: 1.165

100%|██████████| 1/1 [01:50<00:00, 110.74s/it][A100%|██████████| 1/1 [01:50<00:00, 110.74s/it]
 18%|█▊        | 912/5198 [8:02:17<119:52:51, 100.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.84s/it][A100%|██████████| 1/1 [01:50<00:00, 110.84s/it]
 18%|█▊        | 912/5198 [8:02:04<119:55:37, 100.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.91s/it][A100%|██████████| 1/1 [01:50<00:00, 110.91s/it]
 18%|█▊        | 912/5198 [8:02:23<119:55:06, 100.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.85s/it][A100%|██████████| 1/1 [01:50<00:00, 110.85s/it]
 18%|█▊        | 912/5198 [8:00:47<119:54:48, 100.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.84s/it][A100%|██████████| 1/1 [01:50<00:00, 110.84s/it]
 18%|█▊        | 912/5198 [8:01:42<119:54:11, 100.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.89s/it][A100%|██████████| 1/1 [01:50<00:00, 110.89s/it]
 18%|█▊        | 912/5198 [8:01:10<119:55:07, 100.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.85s/it][A100%|██████████| 1/1 [01:50<00:00, 110.86s/it]
 18%|█▊        | 912/5198 [8:01:44<119:54:26, 100.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_855
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.78s/it][A100%|██████████| 1/1 [01:25<00:00, 85.78s/it]
 18%|█▊        | 913/5198 [8:03:28<114:26:13, 96.14s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:53:23,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=901, skipped=0, lr=[1.9158584849868596e-05], mom=[(0.9, 0.999)]
steps: 901 loss: 0.5523 iter time (s): 84.256 samples/sec: 1.519

100%|██████████| 1/1 [01:25<00:00, 85.24s/it][A100%|██████████| 1/1 [01:25<00:00, 85.24s/it]
 18%|█▊        | 913/5198 [8:03:42<114:20:22, 96.06s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.12s/it][A100%|██████████| 1/1 [01:25<00:00, 85.12s/it]
 18%|█▊        | 913/5198 [8:03:29<114:19:39, 96.05s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.07s/it][A100%|██████████| 1/1 [01:25<00:00, 85.07s/it]
 18%|█▊        | 913/5198 [8:03:48<114:18:21, 96.03s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.10s/it][A100%|██████████| 1/1 [01:25<00:00, 85.10s/it]
 18%|█▊        | 913/5198 [8:02:12<114:18:42, 96.04s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.08s/it][A100%|██████████| 1/1 [01:25<00:00, 85.08s/it]
 18%|█▊        | 913/5198 [8:02:36<114:18:27, 96.03s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.15s/it][A100%|██████████| 1/1 [01:25<00:00, 85.15s/it]
 18%|█▊        | 913/5198 [8:03:07<114:19:15, 96.05s/it] 
100%|██████████| 1/1 [01:25<00:00, 85.12s/it][A100%|██████████| 1/1 [01:25<00:00, 85.12s/it]
 18%|█▊        | 913/5198 [8:03:09<114:18:52, 96.04s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_856

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.68s/it][A100%|██████████| 1/1 [02:11<00:00, 131.68s/it]
 18%|█▊        | 914/5198 [8:05:40<127:08:56, 106.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:55:36,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=902, skipped=0, lr=[1.9156309340500763e-05], mom=[(0.9, 0.999)]
steps: 902 loss: 0.6007 iter time (s): 132.239 samples/sec: 0.968

100%|██████████| 1/1 [02:13<00:00, 133.06s/it][A100%|██████████| 1/1 [02:13<00:00, 133.06s/it]
 18%|█▊        | 914/5198 [8:05:55<127:31:21, 107.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.13s/it][A100%|██████████| 1/1 [02:13<00:00, 133.13s/it]
 18%|█▊        | 914/5198 [8:05:42<127:32:25, 107.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.19s/it][A100%|██████████| 1/1 [02:13<00:00, 133.19s/it]
 18%|█▊        | 914/5198 [8:06:01<127:32:50, 107.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.19s/it][A100%|██████████| 1/1 [02:13<00:00, 133.20s/it]
 18%|█▊        | 914/5198 [8:04:26<127:33:10, 107.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.20s/it][A100%|██████████| 1/1 [02:13<00:00, 133.20s/it]
 18%|█▊        | 914/5198 [8:04:49<127:33:03, 107.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.16s/it][A100%|██████████| 1/1 [02:13<00:00, 133.16s/it]
 18%|█▊        | 914/5198 [8:05:22<127:32:31, 107.18s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_857
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.18s/it][A100%|██████████| 1/1 [02:13<00:00, 133.18s/it]
 18%|█▊        | 914/5198 [8:05:20<127:33:23, 107.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.75s/it][A100%|██████████| 1/1 [02:11<00:00, 131.75s/it]
 18%|█▊        | 915/5198 [8:07:51<136:03:44, 114.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:57:48,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=903, skipped=0, lr=[1.9154030893882433e-05], mom=[(0.9, 0.999)]
steps: 903 loss: 0.5756 iter time (s): 130.917 samples/sec: 0.978

100%|██████████| 1/1 [02:11<00:00, 131.85s/it][A100%|██████████| 1/1 [02:11<00:00, 131.85s/it]
 18%|█▊        | 915/5198 [8:08:07<136:18:29, 114.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.80s/it][A100%|██████████| 1/1 [02:11<00:00, 131.80s/it]
 18%|█▊        | 915/5198 [8:07:54<136:18:02, 114.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.83s/it][A100%|██████████| 1/1 [02:11<00:00, 131.83s/it]
 18%|█▊        | 915/5198 [8:08:13<136:19:02, 114.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.74s/it][A100%|██████████| 1/1 [02:11<00:00, 131.75s/it]
 18%|█▊        | 915/5198 [8:06:37<136:17:27, 114.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.82s/it][A100%|██████████| 1/1 [02:11<00:00, 131.82s/it]
 18%|█▊        | 915/5198 [8:07:01<136:19:03, 114.58s/it]
100%|██████████| 1/1 [02:11<00:00, 131.78s/it][A100%|██████████| 1/1 [02:11<00:00, 131.78s/it]
 18%|█▊        | 915/5198 [8:07:32<136:18:26, 114.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.81s/it][A100%|██████████| 1/1 [02:11<00:00, 131.81s/it]
 18%|█▊        | 915/5198 [8:07:34<136:18:23, 114.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_858
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.06s/it][A100%|██████████| 1/1 [01:50<00:00, 110.06s/it]
 18%|█▊        | 916/5198 [8:09:42<134:33:33, 113.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 07:59:37,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=904, skipped=0, lr=[1.9151749510744583e-05], mom=[(0.9, 0.999)]
steps: 904 loss: 0.5974 iter time (s): 108.777 samples/sec: 1.177

100%|██████████| 1/1 [01:49<00:00, 109.62s/it][A100%|██████████| 1/1 [01:49<00:00, 109.62s/it]
 18%|█▊        | 916/5198 [8:09:57<134:30:51, 113.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.65s/it][A100%|██████████| 1/1 [01:49<00:00, 109.65s/it]
 18%|█▊        | 916/5198 [8:09:43<134:31:04, 113.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.64s/it][A100%|██████████| 1/1 [01:49<00:00, 109.64s/it]
 18%|█▊        | 916/5198 [8:10:03<134:31:33, 113.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.77s/it][A100%|██████████| 1/1 [01:49<00:00, 109.77s/it]
 18%|█▊        | 916/5198 [8:08:27<134:33:11, 113.12s/it]
100%|██████████| 1/1 [01:49<00:00, 109.64s/it][A100%|██████████| 1/1 [01:49<00:00, 109.64s/it]
 18%|█▊        | 916/5198 [8:08:50<134:31:38, 113.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.69s/it][A100%|██████████| 1/1 [01:49<00:00, 109.69s/it]
 18%|█▊        | 916/5198 [8:09:21<134:32:09, 113.11s/it]
100%|██████████| 1/1 [01:49<00:00, 109.69s/it][A100%|██████████| 1/1 [01:49<00:00, 109.69s/it]
 18%|█▊        | 916/5198 [8:09:23<134:32:06, 113.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_859

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.25s/it][A100%|██████████| 1/1 [01:50<00:00, 110.25s/it]
 18%|█▊        | 917/5198 [8:11:32<133:32:35, 112.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:01:28,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=905, skipped=0, lr=[1.9149465191819147e-05], mom=[(0.9, 0.999)]
steps: 905 loss: 0.5758 iter time (s): 109.353 samples/sec: 1.171

100%|██████████| 1/1 [01:50<00:00, 110.31s/it][A100%|██████████| 1/1 [01:50<00:00, 110.31s/it]
 18%|█▊        | 917/5198 [8:11:47<133:29:40, 112.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.19s/it][A100%|██████████| 1/1 [01:50<00:00, 110.19s/it]
 18%|█▊        | 917/5198 [8:11:33<133:27:17, 112.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.20s/it][A100%|██████████| 1/1 [01:50<00:00, 110.20s/it]
 18%|█▊        | 917/5198 [8:11:53<133:27:49, 112.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.24s/it][A100%|██████████| 1/1 [01:50<00:00, 110.24s/it]
 18%|█▊        | 917/5198 [8:10:17<133:29:40, 112.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.21s/it][A100%|██████████| 1/1 [01:50<00:00, 110.21s/it]
 18%|█▊        | 917/5198 [8:11:12<133:28:27, 112.24s/it]
100%|██████████| 1/1 [01:50<00:00, 110.27s/it][A100%|██████████| 1/1 [01:50<00:00, 110.27s/it]
 18%|█▊        | 917/5198 [8:10:41<133:29:18, 112.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.24s/it][A100%|██████████| 1/1 [01:50<00:00, 110.24s/it]
 18%|█▊        | 917/5198 [8:11:14<133:29:00, 112.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_860
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.44s/it][A100%|██████████| 1/1 [01:23<00:00, 83.44s/it]
 18%|█▊        | 918/5198 [8:12:56<123:16:10, 103.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:02:51,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=906, skipped=0, lr=[1.9147177937838985e-05], mom=[(0.9, 0.999)]
steps: 906 loss: 0.5442 iter time (s): 82.033 samples/sec: 1.560

100%|██████████| 1/1 [01:22<00:00, 82.84s/it][A100%|██████████| 1/1 [01:22<00:00, 82.84s/it]
 18%|█▊        | 918/5198 [8:13:10<122:58:30, 103.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.93s/it][A100%|██████████| 1/1 [01:22<00:00, 82.93s/it]
 18%|█▊        | 918/5198 [8:12:56<122:58:35, 103.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.01s/it][A100%|██████████| 1/1 [01:23<00:00, 83.01s/it]
 18%|█▊        | 918/5198 [8:13:16<123:00:47, 103.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.88s/it][A100%|██████████| 1/1 [01:22<00:00, 82.88s/it]
 18%|█▊        | 918/5198 [8:11:40<122:59:25, 103.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.89s/it][A100%|██████████| 1/1 [01:22<00:00, 82.89s/it]
 18%|█▊        | 918/5198 [8:12:03<122:59:10, 103.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.92s/it][A100%|██████████| 1/1 [01:22<00:00, 82.92s/it]
 18%|█▊        | 918/5198 [8:12:35<122:59:19, 103.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.93s/it][A100%|██████████| 1/1 [01:22<00:00, 82.93s/it]
 18%|█▊        | 918/5198 [8:12:37<122:59:50, 103.46s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_861
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.97s/it][A100%|██████████| 1/1 [01:59<00:00, 119.97s/it]
 18%|█▊        | 919/5198 [8:14:56<129:06:32, 108.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:04:52,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=907, skipped=0, lr=[1.9144887749537905e-05], mom=[(0.9, 0.999)]
steps: 907 loss: 0.6283 iter time (s): 120.287 samples/sec: 1.064

100%|██████████| 1/1 [02:01<00:00, 121.21s/it][A100%|██████████| 1/1 [02:01<00:00, 121.21s/it]
 18%|█▊        | 919/5198 [8:15:11<129:17:22, 108.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.23s/it][A100%|██████████| 1/1 [02:01<00:00, 121.23s/it]
 18%|█▊        | 919/5198 [8:14:58<129:17:50, 108.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.19s/it][A100%|██████████| 1/1 [02:01<00:00, 121.19s/it]
 18%|█▊        | 919/5198 [8:15:17<129:18:25, 108.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.16s/it][A100%|██████████| 1/1 [02:01<00:00, 121.16s/it]
 18%|█▊        | 919/5198 [8:13:41<129:16:40, 108.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.23s/it][A100%|██████████| 1/1 [02:01<00:00, 121.24s/it]
 18%|█▊        | 919/5198 [8:14:05<129:18:12, 108.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.25s/it][A100%|██████████| 1/1 [02:01<00:00, 121.25s/it]
 18%|█▊        | 919/5198 [8:14:36<129:18:39, 108.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.23s/it][A100%|██████████| 1/1 [02:01<00:00, 121.23s/it]
 18%|█▊        | 919/5198 [8:14:38<129:18:36, 108.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_862
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.18s/it][A100%|██████████| 1/1 [01:20<00:00, 80.18s/it]
[2024-06-30 08:06:11,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=908, skipped=0, lr=[1.9142594627650657e-05], mom=[(0.9, 0.999)]
steps: 908 loss: 0.5914 iter time (s): 78.222 samples/sec: 1.636

100%|██████████| 1/1 [01:19<00:00, 79.16s/it][A100%|██████████| 1/1 [01:19<00:00, 79.16s/it]

100%|██████████| 1/1 [01:19<00:00, 79.16s/it][A100%|██████████| 1/1 [01:19<00:00, 79.16s/it]

100%|██████████| 1/1 [01:19<00:00, 79.09s/it][A100%|██████████| 1/1 [01:19<00:00, 79.09s/it]

100%|██████████| 1/1 [01:19<00:00, 79.15s/it][A100%|██████████| 1/1 [01:19<00:00, 79.15s/it]

100%|██████████| 1/1 [01:19<00:00, 79.09s/it][A100%|██████████| 1/1 [01:19<00:00, 79.09s/it]

100%|██████████| 1/1 [01:19<00:00, 79.09s/it][A100%|██████████| 1/1 [01:19<00:00, 79.09s/it]

100%|██████████| 1/1 [01:19<00:00, 79.09s/it][A100%|██████████| 1/1 [01:19<00:00, 79.09s/it]
Checkpointing at shard 919
[2024-06-30 08:06:12,409] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step908 is about to be saved!
[2024-06-30 08:06:14,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_00-model_states.pt...
[2024-06-30 08:06:18,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_00-model_states.pt.
[2024-06-30 08:06:20,924] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_02-model_states.pt...
[2024-06-30 08:06:25,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_03-model_states.pt...
[2024-06-30 08:06:26,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_08-model_states.pt...
[2024-06-30 08:06:26,423] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_04-model_states.pt...
[2024-06-30 08:06:26,536] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_07-model_states.pt...
[2024-06-30 08:06:27,453] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_06-model_states.pt...
[2024-06-30 08:06:28,276] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_05-model_states.pt...
[2024-06-30 08:06:29,155] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_01-model_states.pt...
[2024-06-30 08:10:45,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_03-model_states.pt.
[2024-06-30 08:10:45,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_02_model_states.pt...
[2024-06-30 08:10:46,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_02_model_states.pt.
[2024-06-30 08:10:46,726] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
[2024-06-30 08:10:47,142] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_02-model_states.pt.
[2024-06-30 08:10:47,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_05-model_states.pt.
[2024-06-30 08:10:47,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_01-model_states.pt.
[2024-06-30 08:10:47,798] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_01_model_states.pt
[2024-06-30 08:10:47,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_01_model_states.pt...
[2024-06-30 08:10:47,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_06-model_states.pt.
[2024-06-30 08:10:47,826] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_08-model_states.pt.
[2024-06-30 08:10:47,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_04-model_states.pt.
[2024-06-30 08:10:47,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_01_model_states.pt.
[2024-06-30 08:10:47,856] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
[2024-06-30 08:10:48,479] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_04_model_states.pt...
[2024-06-30 08:10:48,531] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_00_model_states.pt
[2024-06-30 08:10:48,531] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_00_model_states.pt...
[2024-06-30 08:10:48,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_04_model_states.pt.
[2024-06-30 08:10:48,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
[2024-06-30 08:10:48,975] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_00_model_states.pt.
[2024-06-30 08:10:48,976] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_03_model_states.pt...
[2024-06-30 08:10:48,976] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
[2024-06-30 08:10:49,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_09-model_states.pt...
[2024-06-30 08:10:49,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_03_model_states.pt.
[2024-06-30 08:10:49,062] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
[2024-06-30 08:10:49,073] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_05_model_states.pt...
[2024-06-30 08:10:49,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_05_model_states.pt.
[2024-06-30 08:10:49,158] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
[2024-06-30 08:10:49,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_09-model_states.pt.
[2024-06-30 08:10:49,572] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_07_model_states.pt...
[2024-06-30 08:10:49,621] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_07_model_states.pt.
[2024-06-30 08:10:49,621] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
[2024-06-30 08:10:58,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/layer_07-model_states.pt.
[2024-06-30 08:10:59,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_06_model_states.pt...
[2024-06-30 08:11:00,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step908/mp_rank_06_model_states.pt.
[2024-06-30 08:11:00,593] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step908 is ready now!
 18%|█▊        | 920/5198 [8:21:07<222:40:09, 187.38s/it]Checkpoint saved using --- 288.18546772003174 seconds ---
 18%|█▊        | 920/5198 [8:21:05<221:34:55, 186.46s/it] 18%|█▊        | 920/5198 [8:21:19<221:37:22, 186.50s/it] 18%|█▊        | 920/5198 [8:19:49<221:28:35, 186.38s/it] 18%|█▊        | 920/5198 [8:20:43<221:26:29, 186.35s/it] 18%|█▊        | 920/5198 [8:20:45<221:25:58, 186.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_863
 18%|█▊        | 920/5198 [8:21:25<221:30:45, 186.41s/it] 18%|█▊        | 920/5198 [8:20:12<221:27:08, 186.36s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.64s/it][A100%|██████████| 1/1 [01:35<00:00, 95.64s/it]
 18%|█▊        | 921/5198 [8:22:43<190:03:31, 159.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:12:38,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=909, skipped=0, lr=[1.914029857291293e-05], mom=[(0.9, 0.999)]
steps: 909 loss: 0.5754 iter time (s): 97.877 samples/sec: 1.308

100%|██████████| 1/1 [01:38<00:00, 98.31s/it][A100%|██████████| 1/1 [01:38<00:00, 98.31s/it]
 18%|█▊        | 921/5198 [8:22:58<190:16:43, 160.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.45s/it][A100%|██████████| 1/1 [01:38<00:00, 98.45s/it]
 18%|█▊        | 921/5198 [8:22:44<190:17:56, 160.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.57s/it][A100%|██████████| 1/1 [01:38<00:00, 98.57s/it]
 18%|█▊        | 921/5198 [8:23:04<190:17:42, 160.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.68s/it][A100%|██████████| 1/1 [01:38<00:00, 98.68s/it]
 18%|█▊        | 921/5198 [8:21:28<190:18:30, 160.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.77s/it][A100%|██████████| 1/1 [01:38<00:00, 98.77s/it]
 18%|█▊        | 921/5198 [8:21:51<190:19:27, 160.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.81s/it][A100%|██████████| 1/1 [01:38<00:00, 98.81s/it]
 18%|█▊        | 921/5198 [8:22:22<190:19:49, 160.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.84s/it][A100%|██████████| 1/1 [01:38<00:00, 98.84s/it]
 18%|█▊        | 921/5198 [8:22:24<190:20:05, 160.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_864
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.41s/it][A100%|██████████| 1/1 [01:47<00:00, 107.41s/it]
 18%|█▊        | 922/5198 [8:24:31<171:21:44, 144.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:14:26,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.9137999586061364e-05], mom=[(0.9, 0.999)]
steps: 910 loss: 0.5534 iter time (s): 106.939 samples/sec: 1.197

100%|██████████| 1/1 [01:47<00:00, 107.90s/it][A100%|██████████| 1/1 [01:47<00:00, 107.90s/it]
 18%|█▊        | 922/5198 [8:24:46<171:36:59, 144.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.87s/it][A100%|██████████| 1/1 [01:47<00:00, 107.87s/it]
 18%|█▊        | 922/5198 [8:24:32<171:37:11, 144.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.89s/it][A100%|██████████| 1/1 [01:47<00:00, 107.89s/it]
 18%|█▊        | 922/5198 [8:24:52<171:37:25, 144.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.87s/it][A100%|██████████| 1/1 [01:47<00:00, 107.87s/it]
 18%|█▊        | 922/5198 [8:23:16<171:37:32, 144.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.86s/it][A100%|██████████| 1/1 [01:47<00:00, 107.86s/it]
 18%|█▊        | 922/5198 [8:23:39<171:37:53, 144.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.85s/it][A100%|██████████| 1/1 [01:47<00:00, 107.85s/it]
 18%|█▊        | 922/5198 [8:24:10<171:38:07, 144.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.86s/it][A100%|██████████| 1/1 [01:47<00:00, 107.86s/it]
 18%|█▊        | 922/5198 [8:24:12<171:38:25, 144.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_865
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.34s/it][A100%|██████████| 1/1 [01:39<00:00, 99.34s/it]
 18%|█▊        | 923/5198 [8:26:10<155:22:08, 130.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:16:06,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=911, skipped=0, lr=[1.9135697667833517e-05], mom=[(0.9, 0.999)]
steps: 911 loss: 0.5811 iter time (s): 98.310 samples/sec: 1.302

100%|██████████| 1/1 [01:39<00:00, 99.19s/it][A100%|██████████| 1/1 [01:39<00:00, 99.19s/it]
 18%|█▊        | 923/5198 [8:26:25<155:26:40, 130.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.19s/it][A100%|██████████| 1/1 [01:39<00:00, 99.19s/it]
 18%|█▊        | 923/5198 [8:26:11<155:26:45, 130.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.23s/it][A100%|██████████| 1/1 [01:39<00:00, 99.23s/it]
 18%|█▊        | 923/5198 [8:26:31<155:27:45, 130.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.27s/it][A100%|██████████| 1/1 [01:39<00:00, 99.27s/it]
 18%|█▊        | 923/5198 [8:24:55<155:28:41, 130.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.22s/it][A100%|██████████| 1/1 [01:39<00:00, 99.22s/it]
 18%|█▊        | 923/5198 [8:25:18<155:27:53, 130.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.28s/it][A100%|██████████| 1/1 [01:39<00:00, 99.28s/it]
 18%|█▊        | 923/5198 [8:25:49<155:29:10, 130.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.26s/it][A100%|██████████| 1/1 [01:39<00:00, 99.26s/it]
 18%|█▊        | 923/5198 [8:25:51<155:29:01, 130.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_866
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.80s/it][A100%|██████████| 1/1 [01:42<00:00, 102.80s/it]
 18%|█▊        | 924/5198 [8:27:53<145:24:05, 122.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:17:49,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=912, skipped=0, lr=[1.913339281896791e-05], mom=[(0.9, 0.999)]
steps: 912 loss: 0.5987 iter time (s): 102.147 samples/sec: 1.253

100%|██████████| 1/1 [01:43<00:00, 103.13s/it][A100%|██████████| 1/1 [01:43<00:00, 103.13s/it]
 18%|█▊        | 924/5198 [8:28:08<145:31:15, 122.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.05s/it][A100%|██████████| 1/1 [01:43<00:00, 103.05s/it]
 18%|█▊        | 924/5198 [8:27:54<145:29:35, 122.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.07s/it][A100%|██████████| 1/1 [01:43<00:00, 103.07s/it]
 18%|█▊        | 924/5198 [8:28:14<145:30:41, 122.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.07s/it][A100%|██████████| 1/1 [01:43<00:00, 103.07s/it]
 18%|█▊        | 924/5198 [8:27:01<145:30:52, 122.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.11s/it][A100%|██████████| 1/1 [01:43<00:00, 103.11s/it]
 18%|█▊        | 924/5198 [8:26:38<145:32:14, 122.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.04s/it][A100%|██████████| 1/1 [01:43<00:00, 103.04s/it]
 18%|█▊        | 924/5198 [8:27:32<145:31:03, 122.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.05s/it][A100%|██████████| 1/1 [01:43<00:00, 103.05s/it]
 18%|█▊        | 924/5198 [8:27:34<145:31:05, 122.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_867
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.40s/it][A100%|██████████| 1/1 [01:55<00:00, 115.40s/it]
 18%|█▊        | 925/5198 [8:29:49<142:54:54, 120.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:19:45,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=913, skipped=0, lr=[1.9131085040203995e-05], mom=[(0.9, 0.999)]
steps: 913 loss: 0.5593 iter time (s): 115.020 samples/sec: 1.113

100%|██████████| 1/1 [01:55<00:00, 115.89s/it][A100%|██████████| 1/1 [01:55<00:00, 115.89s/it]
 18%|█▊        | 925/5198 [8:30:04<143:06:43, 120.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.03s/it][A100%|██████████| 1/1 [01:56<00:00, 116.03s/it]
 18%|█▊        | 925/5198 [8:29:50<143:08:26, 120.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.10s/it][A100%|██████████| 1/1 [01:56<00:00, 116.10s/it]
 18%|█▊        | 925/5198 [8:30:10<143:10:44, 120.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.02s/it][A100%|██████████| 1/1 [01:56<00:00, 116.02s/it]
 18%|█▊        | 925/5198 [8:28:34<143:10:07, 120.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.05s/it][A100%|██████████| 1/1 [01:56<00:00, 116.05s/it]
 18%|█▊        | 925/5198 [8:28:57<143:09:45, 120.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.04s/it][A100%|██████████| 1/1 [01:56<00:00, 116.04s/it]
 18%|█▊        | 925/5198 [8:29:29<143:09:43, 120.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.03s/it][A100%|██████████| 1/1 [01:56<00:00, 116.03s/it]
 18%|█▊        | 925/5198 [8:29:30<143:09:34, 120.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_868
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.16s/it][A100%|██████████| 1/1 [01:33<00:00, 93.16s/it]
 18%|█▊        | 926/5198 [8:31:22<133:15:38, 112.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:21:17,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=914, skipped=0, lr=[1.9128774332282163e-05], mom=[(0.9, 0.999)]
steps: 914 loss: 0.5574 iter time (s): 91.735 samples/sec: 1.395

100%|██████████| 1/1 [01:32<00:00, 92.76s/it][A100%|██████████| 1/1 [01:32<00:00, 92.76s/it]
 18%|█▊        | 926/5198 [8:31:37<133:10:45, 112.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.62s/it][A100%|██████████| 1/1 [01:32<00:00, 92.62s/it]
 18%|█▊        | 926/5198 [8:31:23<133:08:58, 112.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.56s/it][A100%|██████████| 1/1 [01:32<00:00, 92.56s/it]
 18%|█▊        | 926/5198 [8:31:43<133:09:18, 112.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.51s/it][A100%|██████████| 1/1 [01:32<00:00, 92.51s/it]
 18%|█▊        | 926/5198 [8:30:07<133:07:46, 112.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.63s/it][A100%|██████████| 1/1 [01:32<00:00, 92.63s/it]
 18%|█▊        | 926/5198 [8:30:30<133:10:10, 112.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.61s/it][A100%|██████████| 1/1 [01:32<00:00, 92.61s/it]
 18%|█▊        | 926/5198 [8:31:01<133:09:43, 112.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.60s/it][A100%|██████████| 1/1 [01:32<00:00, 92.60s/it]
 18%|█▊        | 926/5198 [8:31:03<133:09:26, 112.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_869
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.52s/it][A100%|██████████| 1/1 [01:18<00:00, 78.52s/it]
 18%|█▊        | 927/5198 [8:32:41<121:17:10, 102.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:22:36,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=915, skipped=0, lr=[1.9126460695943748e-05], mom=[(0.9, 0.999)]
steps: 915 loss: 0.5665 iter time (s): 77.465 samples/sec: 1.652

100%|██████████| 1/1 [01:18<00:00, 78.31s/it][A100%|██████████| 1/1 [01:18<00:00, 78.31s/it]
 18%|█▊        | 927/5198 [8:32:55<121:04:48, 102.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.38s/it][A100%|██████████| 1/1 [01:18<00:00, 78.38s/it]
 18%|█▊        | 927/5198 [8:32:41<121:05:00, 102.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.39s/it][A100%|██████████| 1/1 [01:18<00:00, 78.40s/it]
 18%|█▊        | 927/5198 [8:33:01<121:05:30, 102.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.42s/it][A100%|██████████| 1/1 [01:18<00:00, 78.42s/it]
 18%|█▊        | 927/5198 [8:31:25<121:04:55, 102.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.34s/it][A100%|██████████| 1/1 [01:18<00:00, 78.34s/it]
 18%|█▊        | 927/5198 [8:31:48<121:04:54, 102.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.33s/it][A100%|██████████| 1/1 [01:18<00:00, 78.33s/it]
 18%|█▊        | 927/5198 [8:32:19<121:04:26, 102.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.35s/it][A100%|██████████| 1/1 [01:18<00:00, 78.35s/it]
 18%|█▊        | 927/5198 [8:32:21<121:04:41, 102.06s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_57
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.11s/it][A100%|██████████| 1/1 [02:03<00:00, 123.11s/it]
 18%|█▊        | 928/5198 [8:34:44<128:43:32, 108.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:24:40,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=916, skipped=0, lr=[1.912414413193102e-05], mom=[(0.9, 0.999)]
steps: 916 loss: 0.7706 iter time (s): 123.611 samples/sec: 1.036

100%|██████████| 1/1 [02:04<00:00, 124.47s/it][A100%|██████████| 1/1 [02:04<00:00, 124.47s/it]
 18%|█▊        | 928/5198 [8:34:59<129:01:44, 108.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.58s/it][A100%|██████████| 1/1 [02:04<00:00, 124.58s/it]
 18%|█▊        | 928/5198 [8:34:46<129:04:10, 108.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.43s/it][A100%|██████████| 1/1 [02:04<00:00, 124.43s/it]
 18%|█▊        | 928/5198 [8:35:06<129:01:19, 108.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.52s/it][A100%|██████████| 1/1 [02:04<00:00, 124.52s/it]
 18%|█▊        | 928/5198 [8:33:30<129:03:00, 108.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.56s/it][A100%|██████████| 1/1 [02:04<00:00, 124.56s/it]
 18%|█▊        | 928/5198 [8:33:53<129:03:49, 108.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.60s/it][A100%|██████████| 1/1 [02:04<00:00, 124.60s/it]
 18%|█▊        | 928/5198 [8:34:24<129:04:20, 108.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.58s/it][A100%|██████████| 1/1 [02:04<00:00, 124.59s/it]
 18%|█▊        | 928/5198 [8:34:26<129:04:06, 108.82s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_870
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.02s/it][A100%|██████████| 1/1 [01:37<00:00, 97.02s/it]
 18%|█▊        | 929/5198 [8:36:21<124:38:28, 105.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:26:16,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=917, skipped=0, lr=[1.912182464098719e-05], mom=[(0.9, 0.999)]
steps: 917 loss: 0.5455 iter time (s): 95.351 samples/sec: 1.342

100%|██████████| 1/1 [01:36<00:00, 96.34s/it][A100%|██████████| 1/1 [01:36<00:00, 96.34s/it]
 18%|█▊        | 929/5198 [8:36:36<124:34:27, 105.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.31s/it][A100%|██████████| 1/1 [01:36<00:00, 96.31s/it]
 18%|█▊        | 929/5198 [8:36:22<124:35:38, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.42s/it][A100%|██████████| 1/1 [01:36<00:00, 96.42s/it]
 18%|█▊        | 929/5198 [8:36:42<124:35:53, 105.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.42s/it][A100%|██████████| 1/1 [01:36<00:00, 96.42s/it]
 18%|█▊        | 929/5198 [8:35:06<124:37:00, 105.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.33s/it][A100%|██████████| 1/1 [01:36<00:00, 96.33s/it]
 18%|█▊        | 929/5198 [8:35:29<124:35:46, 105.07s/it]
100%|██████████| 1/1 [01:36<00:00, 96.28s/it][A100%|██████████| 1/1 [01:36<00:00, 96.28s/it]
 18%|█▊        | 929/5198 [8:36:00<124:35:00, 105.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.31s/it][A100%|██████████| 1/1 [01:36<00:00, 96.31s/it]
 18%|█▊        | 929/5198 [8:36:02<124:35:36, 105.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_871
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.30s/it][A100%|██████████| 1/1 [01:39<00:00, 99.30s/it]
 18%|█▊        | 930/5198 [8:38:01<122:36:31, 103.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:27:56,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=918, skipped=0, lr=[1.9119502223856413e-05], mom=[(0.9, 0.999)]
steps: 918 loss: 0.6099 iter time (s): 98.617 samples/sec: 1.298

100%|██████████| 1/1 [01:39<00:00, 99.53s/it][A100%|██████████| 1/1 [01:39<00:00, 99.53s/it]
 18%|█▊        | 930/5198 [8:38:15<122:35:05, 103.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.49s/it][A100%|██████████| 1/1 [01:39<00:00, 99.50s/it]
 18%|█▊        | 930/5198 [8:38:02<122:35:15, 103.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.51s/it][A100%|██████████| 1/1 [01:39<00:00, 99.51s/it]
 18%|█▊        | 930/5198 [8:38:21<122:35:52, 103.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.43s/it][A100%|██████████| 1/1 [01:39<00:00, 99.43s/it]
 18%|█▊        | 930/5198 [8:36:46<122:34:40, 103.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.49s/it][A100%|██████████| 1/1 [01:39<00:00, 99.49s/it]
 18%|█▊        | 930/5198 [8:37:09<122:35:13, 103.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.54s/it][A100%|██████████| 1/1 [01:39<00:00, 99.54s/it]
 18%|█▊        | 930/5198 [8:37:40<122:35:43, 103.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.51s/it][A100%|██████████| 1/1 [01:39<00:00, 99.51s/it]
 18%|█▊        | 930/5198 [8:37:42<122:35:27, 103.40s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_872
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.45s/it][A100%|██████████| 1/1 [01:26<00:00, 86.45s/it]
 18%|█▊        | 931/5198 [8:39:27<116:36:55, 98.39s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:29:22,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=919, skipped=0, lr=[1.9117176881283776e-05], mom=[(0.9, 0.999)]
steps: 919 loss: 0.5707 iter time (s): 85.328 samples/sec: 1.500

100%|██████████| 1/1 [01:26<00:00, 86.28s/it][A100%|██████████| 1/1 [01:26<00:00, 86.28s/it]
 18%|█▊        | 931/5198 [8:39:41<116:28:20, 98.27s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.29s/it][A100%|██████████| 1/1 [01:26<00:00, 86.29s/it]
 18%|█▊        | 931/5198 [8:39:28<116:28:34, 98.27s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.30s/it][A100%|██████████| 1/1 [01:26<00:00, 86.30s/it]
 18%|█▊        | 931/5198 [8:39:48<116:29:25, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.33s/it][A100%|██████████| 1/1 [01:26<00:00, 86.33s/it]
 18%|█▊        | 931/5198 [8:38:12<116:29:01, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.35s/it][A100%|██████████| 1/1 [01:26<00:00, 86.35s/it]
 18%|█▊        | 931/5198 [8:38:35<116:29:59, 98.29s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.31s/it][A100%|██████████| 1/1 [01:26<00:00, 86.31s/it]
 18%|█▊        | 931/5198 [8:39:06<116:29:20, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.33s/it][A100%|██████████| 1/1 [01:26<00:00, 86.33s/it]
 18%|█▊        | 931/5198 [8:39:08<116:29:32, 98.28s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_873
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.09s/it][A100%|██████████| 1/1 [01:54<00:00, 114.09s/it]
 18%|█▊        | 932/5198 [8:41:21<122:13:42, 103.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:31:17,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[1.911484861401531e-05], mom=[(0.9, 0.999)]
steps: 920 loss: 0.6120 iter time (s): 114.090 samples/sec: 1.122

100%|██████████| 1/1 [01:55<00:00, 115.09s/it][A100%|██████████| 1/1 [01:55<00:00, 115.09s/it]
 18%|█▊        | 932/5198 [8:41:37<122:25:47, 103.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.06s/it][A100%|██████████| 1/1 [01:55<00:00, 115.06s/it]
 18%|█▊        | 932/5198 [8:41:23<122:25:18, 103.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 115.00s/it][A100%|██████████| 1/1 [01:54<00:00, 115.00s/it]
 18%|█▊        | 932/5198 [8:41:43<122:24:36, 103.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.99s/it][A100%|██████████| 1/1 [01:54<00:00, 114.99s/it]
 18%|█▊        | 932/5198 [8:40:07<122:24:05, 103.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.94s/it][A100%|██████████| 1/1 [01:54<00:00, 114.94s/it]
 18%|█▊        | 932/5198 [8:40:30<122:23:37, 103.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.94s/it][A100%|██████████| 1/1 [01:54<00:00, 114.94s/it]
 18%|█▊        | 932/5198 [8:41:01<122:23:14, 103.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.95s/it][A100%|██████████| 1/1 [01:54<00:00, 114.95s/it]
 18%|█▊        | 932/5198 [8:41:03<122:23:36, 103.29s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_874
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.11s/it][A100%|██████████| 1/1 [01:23<00:00, 83.11s/it]
 18%|█▊        | 933/5198 [8:42:45<115:07:20, 97.17s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:32:40,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=921, skipped=0, lr=[1.9112517422797966e-05], mom=[(0.9, 0.999)]
steps: 921 loss: 0.5409 iter time (s): 81.420 samples/sec: 1.572

100%|██████████| 1/1 [01:22<00:00, 82.29s/it][A100%|██████████| 1/1 [01:22<00:00, 82.29s/it]
 18%|█▊        | 933/5198 [8:42:59<114:55:57, 97.01s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.36s/it][A100%|██████████| 1/1 [01:22<00:00, 82.36s/it]
 18%|█▊        | 933/5198 [8:42:46<114:56:58, 97.03s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.26s/it][A100%|██████████| 1/1 [01:22<00:00, 82.26s/it]
 18%|█▊        | 933/5198 [8:43:05<114:54:20, 96.99s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.34s/it][A100%|██████████| 1/1 [01:22<00:00, 82.34s/it]
 18%|█▊        | 933/5198 [8:41:29<114:55:39, 97.01s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.35s/it][A100%|██████████| 1/1 [01:22<00:00, 82.35s/it]
 18%|█▊        | 933/5198 [8:41:53<114:55:42, 97.01s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.38s/it][A100%|██████████| 1/1 [01:22<00:00, 82.38s/it]
 18%|█▊        | 933/5198 [8:42:24<114:55:52, 97.01s/it] 
100%|██████████| 1/1 [01:22<00:00, 82.34s/it][A100%|██████████| 1/1 [01:22<00:00, 82.34s/it]
 18%|█▊        | 933/5198 [8:42:26<114:55:20, 97.00s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_875

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.07s/it][A100%|██████████| 1/1 [01:26<00:00, 86.07s/it]
 18%|█▊        | 934/5198 [8:44:11<111:13:18, 93.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:34:06,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=922, skipped=0, lr=[1.9110183308379663e-05], mom=[(0.9, 0.999)]
steps: 922 loss: 0.5846 iter time (s): 85.484 samples/sec: 1.497

100%|██████████| 1/1 [01:26<00:00, 86.31s/it][A100%|██████████| 1/1 [01:26<00:00, 86.31s/it]
 18%|█▊        | 934/5198 [8:44:25<111:06:15, 93.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.25s/it][A100%|██████████| 1/1 [01:26<00:00, 86.25s/it]
 18%|█▊        | 934/5198 [8:44:12<111:05:47, 93.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.38s/it][A100%|██████████| 1/1 [01:26<00:00, 86.38s/it]
 18%|█▊        | 934/5198 [8:44:31<111:06:48, 93.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.31s/it][A100%|██████████| 1/1 [01:26<00:00, 86.31s/it]
 18%|█▊        | 934/5198 [8:42:56<111:06:02, 93.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.33s/it][A100%|██████████| 1/1 [01:26<00:00, 86.33s/it]
 18%|█▊        | 934/5198 [8:43:19<111:06:32, 93.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.34s/it][A100%|██████████| 1/1 [01:26<00:00, 86.34s/it]
 18%|█▊        | 934/5198 [8:43:50<111:06:49, 93.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.36s/it][A100%|██████████| 1/1 [01:26<00:00, 86.36s/it]
 18%|█▊        | 934/5198 [8:43:52<111:06:59, 93.81s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_876
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.67s/it][A100%|██████████| 1/1 [01:43<00:00, 103.67s/it]
 18%|█▊        | 935/5198 [8:45:55<114:43:41, 96.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:35:50,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=923, skipped=0, lr=[1.910784627150924e-05], mom=[(0.9, 0.999)]
steps: 923 loss: 0.5463 iter time (s): 103.440 samples/sec: 1.237

100%|██████████| 1/1 [01:44<00:00, 104.42s/it][A100%|██████████| 1/1 [01:44<00:00, 104.42s/it]
 18%|█▊        | 935/5198 [8:46:10<114:51:11, 96.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.45s/it][A100%|██████████| 1/1 [01:44<00:00, 104.45s/it]
 18%|█▊        | 935/5198 [8:45:56<114:51:23, 96.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.32s/it][A100%|██████████| 1/1 [01:44<00:00, 104.32s/it]
 18%|█▊        | 935/5198 [8:46:16<114:49:25, 96.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.43s/it][A100%|██████████| 1/1 [01:44<00:00, 104.43s/it]
 18%|█▊        | 935/5198 [8:44:40<114:51:13, 96.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.40s/it][A100%|██████████| 1/1 [01:44<00:00, 104.40s/it]
 18%|█▊        | 935/5198 [8:45:03<114:50:50, 96.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.39s/it][A100%|██████████| 1/1 [01:44<00:00, 104.39s/it]
 18%|█▊        | 935/5198 [8:45:34<114:50:49, 96.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.38s/it][A100%|██████████| 1/1 [01:44<00:00, 104.38s/it]
 18%|█▊        | 935/5198 [8:45:36<114:50:52, 96.99s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_877
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.71s/it][A100%|██████████| 1/1 [01:33<00:00, 93.71s/it]
 18%|█▊        | 936/5198 [8:47:29<113:36:57, 95.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:37:24,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=924, skipped=0, lr=[1.9105506312936473e-05], mom=[(0.9, 0.999)]
steps: 924 loss: 0.5546 iter time (s): 92.594 samples/sec: 1.382

100%|██████████| 1/1 [01:33<00:00, 93.41s/it][A100%|██████████| 1/1 [01:33<00:00, 93.41s/it]
 18%|█▊        | 936/5198 [8:47:43<113:33:32, 95.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.44s/it][A100%|██████████| 1/1 [01:33<00:00, 93.44s/it]
 18%|█▊        | 936/5198 [8:47:30<113:34:23, 95.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.50s/it][A100%|██████████| 1/1 [01:33<00:00, 93.50s/it]
 18%|█▊        | 936/5198 [8:47:49<113:34:09, 95.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.44s/it][A100%|██████████| 1/1 [01:33<00:00, 93.44s/it]
 18%|█▊        | 936/5198 [8:46:13<113:34:09, 95.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.59s/it][A100%|██████████| 1/1 [01:33<00:00, 93.59s/it]
 18%|█▊        | 936/5198 [8:47:08<113:37:04, 95.97s/it]
100%|██████████| 1/1 [01:33<00:00, 93.63s/it][A100%|██████████| 1/1 [01:33<00:00, 93.63s/it]
 18%|█▊        | 936/5198 [8:46:37<113:37:49, 95.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.60s/it][A100%|██████████| 1/1 [01:33<00:00, 93.60s/it]
 18%|█▊        | 936/5198 [8:47:10<113:37:18, 95.97s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_878
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.63s/it][A100%|██████████| 1/1 [01:28<00:00, 88.63s/it]
 18%|█▊        | 937/5198 [8:48:57<111:02:30, 93.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:38:53,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=925, skipped=0, lr=[1.910316343341208e-05], mom=[(0.9, 0.999)]
steps: 925 loss: 0.5527 iter time (s): 87.616 samples/sec: 1.461

100%|██████████| 1/1 [01:28<00:00, 88.71s/it][A100%|██████████| 1/1 [01:28<00:00, 88.71s/it]
 18%|█▊        | 937/5198 [8:49:12<110:58:42, 93.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.56s/it][A100%|██████████| 1/1 [01:28<00:00, 88.56s/it]
 18%|█▊        | 937/5198 [8:48:58<110:55:50, 93.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.63s/it][A100%|██████████| 1/1 [01:28<00:00, 88.63s/it]
 18%|█▊        | 937/5198 [8:49:18<110:57:13, 93.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.61s/it][A100%|██████████| 1/1 [01:28<00:00, 88.61s/it]
 18%|█▊        | 937/5198 [8:47:42<110:56:58, 93.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.47s/it][A100%|██████████| 1/1 [01:28<00:00, 88.47s/it]
 18%|█▊        | 937/5198 [8:48:05<110:56:17, 93.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.51s/it][A100%|██████████| 1/1 [01:28<00:00, 88.51s/it]
 18%|█▊        | 937/5198 [8:48:36<110:56:36, 93.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.49s/it][A100%|██████████| 1/1 [01:28<00:00, 88.49s/it]
 18%|█▊        | 937/5198 [8:48:38<110:56:20, 93.73s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_879
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.73s/it][A100%|██████████| 1/1 [01:45<00:00, 105.73s/it]
 18%|█▊        | 938/5198 [8:50:43<115:18:07, 97.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:40:39,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=926, skipped=0, lr=[1.9100817633687715e-05], mom=[(0.9, 0.999)]
steps: 926 loss: 0.5583 iter time (s): 105.535 samples/sec: 1.213

100%|██████████| 1/1 [01:46<00:00, 106.40s/it][A100%|██████████| 1/1 [01:46<00:00, 106.40s/it]
 18%|█▊        | 938/5198 [8:50:58<115:26:33, 97.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.44s/it][A100%|██████████| 1/1 [01:46<00:00, 106.44s/it]
 18%|█▊        | 938/5198 [8:50:45<115:25:20, 97.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.48s/it][A100%|██████████| 1/1 [01:46<00:00, 106.48s/it]
 18%|█▊        | 938/5198 [8:51:04<115:27:08, 97.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.38s/it][A100%|██████████| 1/1 [01:46<00:00, 106.38s/it]
 18%|█▊        | 938/5198 [8:49:28<115:24:56, 97.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.48s/it][A100%|██████████| 1/1 [01:46<00:00, 106.48s/it]
 18%|█▊        | 938/5198 [8:49:52<115:26:32, 97.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.48s/it][A100%|██████████| 1/1 [01:46<00:00, 106.48s/it]
 18%|█▊        | 938/5198 [8:50:23<115:26:39, 97.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.47s/it][A100%|██████████| 1/1 [01:46<00:00, 106.47s/it]
 18%|█▊        | 938/5198 [8:50:25<115:26:19, 97.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_880
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.61s/it][A100%|██████████| 1/1 [01:39<00:00, 99.61s/it]
 18%|█▊        | 939/5198 [8:52:23<116:05:43, 98.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:42:19,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=927, skipped=0, lr=[1.909846891451597e-05], mom=[(0.9, 0.999)]
steps: 927 loss: 0.5612 iter time (s): 98.624 samples/sec: 1.298

100%|██████████| 1/1 [01:39<00:00, 99.56s/it][A100%|██████████| 1/1 [01:39<00:00, 99.56s/it]
 18%|█▊        | 939/5198 [8:52:38<116:07:51, 98.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.62s/it][A100%|██████████| 1/1 [01:39<00:00, 99.62s/it]
 18%|█▊        | 939/5198 [8:52:24<116:08:12, 98.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.59s/it][A100%|██████████| 1/1 [01:39<00:00, 99.59s/it]
 18%|█▊        | 939/5198 [8:52:44<116:08:51, 98.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.63s/it][A100%|██████████| 1/1 [01:39<00:00, 99.63s/it]
 18%|█▊        | 939/5198 [8:51:08<116:08:06, 98.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.52s/it][A100%|██████████| 1/1 [01:39<00:00, 99.52s/it]
 18%|█▊        | 939/5198 [8:52:02<116:06:56, 98.15s/it]
100%|██████████| 1/1 [01:39<00:00, 99.55s/it][A100%|██████████| 1/1 [01:39<00:00, 99.55s/it]
 18%|█▊        | 939/5198 [8:51:31<116:07:35, 98.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.57s/it][A100%|██████████| 1/1 [01:39<00:00, 99.57s/it]
 18%|█▊        | 939/5198 [8:52:04<116:07:45, 98.16s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_881
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.32s/it][A100%|██████████| 1/1 [01:43<00:00, 103.32s/it]
[2024-06-30 08:44:02,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=928, skipped=0, lr=[1.9096117276650374e-05], mom=[(0.9, 0.999)]
steps: 928 loss: 0.5654 iter time (s): 102.664 samples/sec: 1.247

100%|██████████| 1/1 [01:43<00:00, 103.53s/it][A100%|██████████| 1/1 [01:43<00:00, 103.53s/it]

100%|██████████| 1/1 [01:43<00:00, 103.60s/it][A100%|██████████| 1/1 [01:43<00:00, 103.60s/it]

100%|██████████| 1/1 [01:43<00:00, 103.50s/it][A100%|██████████| 1/1 [01:43<00:00, 103.50s/it]

100%|██████████| 1/1 [01:43<00:00, 103.57s/it][A100%|██████████| 1/1 [01:43<00:00, 103.57s/it]

100%|██████████| 1/1 [01:43<00:00, 103.53s/it][A100%|██████████| 1/1 [01:43<00:00, 103.53s/it]

100%|██████████| 1/1 [01:43<00:00, 103.54s/it][A100%|██████████| 1/1 [01:43<00:00, 103.54s/it]

100%|██████████| 1/1 [01:43<00:00, 103.53s/it][A100%|██████████| 1/1 [01:43<00:00, 103.53s/it]
Checkpointing at shard 939
[2024-06-30 08:44:03,558] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step928 is about to be saved!
[2024-06-30 08:44:05,156] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_00-model_states.pt...
[2024-06-30 08:44:08,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_02-model_states.pt...
[2024-06-30 08:44:10,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_00-model_states.pt.
[2024-06-30 08:44:15,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_08-model_states.pt...
[2024-06-30 08:44:16,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_03-model_states.pt...
[2024-06-30 08:44:16,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_07-model_states.pt...
[2024-06-30 08:44:17,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_04-model_states.pt...
[2024-06-30 08:44:18,272] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_06-model_states.pt...
[2024-06-30 08:44:19,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_05-model_states.pt...
[2024-06-30 08:44:19,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_01-model_states.pt...
[2024-06-30 08:46:22,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_02-model_states.pt.
[2024-06-30 08:46:22,674] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_01_model_states.pt
[2024-06-30 08:46:22,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_01_model_states.pt...
[2024-06-30 08:46:22,802] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_01_model_states.pt.
[2024-06-30 08:46:22,803] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
[2024-06-30 08:46:55,222] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_08-model_states.pt.
[2024-06-30 08:46:56,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_09-model_states.pt...
[2024-06-30 08:46:57,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_09-model_states.pt.
[2024-06-30 08:46:57,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_07_model_states.pt...
[2024-06-30 08:46:57,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_07_model_states.pt.
[2024-06-30 08:46:57,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
[2024-06-30 08:47:01,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_05-model_states.pt.
[2024-06-30 08:47:02,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_04_model_states.pt...
[2024-06-30 08:47:02,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_04_model_states.pt.
[2024-06-30 08:47:02,871] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
[2024-06-30 08:47:07,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_01-model_states.pt.
[2024-06-30 08:47:08,424] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_00_model_states.pt
[2024-06-30 08:47:08,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_00_model_states.pt...
[2024-06-30 08:47:08,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_00_model_states.pt.
[2024-06-30 08:47:08,979] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
[2024-06-30 08:47:15,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_06-model_states.pt.
[2024-06-30 08:47:16,014] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_05_model_states.pt...
[2024-06-30 08:47:16,095] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_05_model_states.pt.
[2024-06-30 08:47:16,095] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
[2024-06-30 08:47:16,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_03-model_states.pt.
[2024-06-30 08:47:17,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_02_model_states.pt...
[2024-06-30 08:47:17,577] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_07-model_states.pt.
[2024-06-30 08:47:17,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_02_model_states.pt.
[2024-06-30 08:47:17,585] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
[2024-06-30 08:47:17,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/layer_04-model_states.pt.
[2024-06-30 08:47:18,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_03_model_states.pt...
[2024-06-30 08:47:18,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_03_model_states.pt.
[2024-06-30 08:47:18,343] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
[2024-06-30 08:47:18,460] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_06_model_states.pt...
[2024-06-30 08:47:18,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step928/mp_rank_06_model_states.pt.
[2024-06-30 08:47:18,536] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step928 is ready now!
Checkpoint saved using --- 194.97906041145325 seconds --- 18%|█▊        | 940/5198 [8:56:30<187:12:50, 158.28s/it]
 18%|█▊        | 940/5198 [8:56:07<187:15:39, 158.32s/it] 18%|█▊        | 940/5198 [8:57:23<187:21:13, 158.40s/it] 18%|█▊        | 940/5198 [8:57:25<188:20:03, 159.23s/it] 18%|█▊        | 940/5198 [8:57:03<187:11:46, 158.27s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_882
 18%|█▊        | 940/5198 [8:57:37<187:23:20, 158.43s/it] 18%|█▊        | 940/5198 [8:57:01<187:12:26, 158.28s/it] 18%|█▊        | 940/5198 [8:57:43<187:17:18, 158.35s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.19s/it][A100%|██████████| 1/1 [01:34<00:00, 94.20s/it]
 18%|█▊        | 941/5198 [8:58:59<165:14:21, 139.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:48:55,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=929, skipped=0, lr=[1.9093762720845387e-05], mom=[(0.9, 0.999)]
steps: 929 loss: 0.5619 iter time (s): 96.481 samples/sec: 1.327

100%|██████████| 1/1 [01:36<00:00, 96.83s/it][A100%|██████████| 1/1 [01:36<00:00, 96.83s/it]
 18%|█▊        | 941/5198 [8:59:14<165:30:42, 139.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.95s/it][A100%|██████████| 1/1 [01:36<00:00, 96.95s/it]
 18%|█▊        | 941/5198 [8:59:00<165:31:38, 139.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.20s/it][A100%|██████████| 1/1 [01:37<00:00, 97.20s/it]
 18%|█▊        | 941/5198 [8:59:20<165:34:17, 140.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.27s/it][A100%|██████████| 1/1 [01:37<00:00, 97.27s/it]
 18%|█▊        | 941/5198 [8:57:44<165:34:43, 140.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.38s/it][A100%|██████████| 1/1 [01:37<00:00, 97.38s/it]
 18%|█▊        | 941/5198 [8:58:07<165:35:10, 140.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.40s/it][A100%|██████████| 1/1 [01:37<00:00, 97.40s/it]
 18%|█▊        | 941/5198 [8:58:38<165:35:10, 140.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.42s/it][A100%|██████████| 1/1 [01:37<00:00, 97.42s/it]
 18%|█▊        | 941/5198 [8:58:40<165:35:14, 140.03s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_883
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.91s/it][A100%|██████████| 1/1 [01:38<00:00, 98.91s/it]
 18%|█▊        | 942/5198 [9:00:38<150:45:02, 127.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:50:33,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[1.9091405247856408e-05], mom=[(0.9, 0.999)]
steps: 930 loss: 0.5267 iter time (s): 97.711 samples/sec: 1.310

100%|██████████| 1/1 [01:38<00:00, 98.67s/it][A100%|██████████| 1/1 [01:38<00:00, 98.67s/it]
 18%|█▊        | 942/5198 [9:00:52<150:49:49, 127.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.68s/it][A100%|██████████| 1/1 [01:38<00:00, 98.68s/it]
 18%|█▊        | 942/5198 [9:00:39<150:50:29, 127.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.60s/it][A100%|██████████| 1/1 [01:38<00:00, 98.60s/it]
 18%|█▊        | 942/5198 [8:59:23<150:51:07, 127.60s/it]
100%|██████████| 1/1 [01:38<00:00, 98.68s/it][A
  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [01:38<00:00, 98.69s/it]
 18%|█▊        | 942/5198 [9:00:59<150:52:34, 127.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.59s/it][A100%|██████████| 1/1 [01:38<00:00, 98.59s/it]
 18%|█▊        | 942/5198 [8:59:46<150:51:07, 127.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.64s/it][A100%|██████████| 1/1 [01:38<00:00, 98.64s/it]
 18%|█▊        | 942/5198 [9:00:17<150:52:06, 127.61s/it]
100%|██████████| 1/1 [01:38<00:00, 98.62s/it][A100%|██████████| 1/1 [01:38<00:00, 98.62s/it]
 18%|█▊        | 942/5198 [9:00:19<150:51:46, 127.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_884

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.64s/it][A100%|██████████| 1/1 [01:25<00:00, 85.64s/it]
 18%|█▊        | 943/5198 [9:02:04<135:53:09, 114.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:51:59,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=931, skipped=0, lr=[1.9089044858439777e-05], mom=[(0.9, 0.999)]
steps: 931 loss: 0.5676 iter time (s): 84.689 samples/sec: 1.511

100%|██████████| 1/1 [01:25<00:00, 85.59s/it][A100%|██████████| 1/1 [01:25<00:00, 85.59s/it]
 18%|█▊        | 943/5198 [9:02:18<135:54:26, 114.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.57s/it][A100%|██████████| 1/1 [01:25<00:00, 85.57s/it]
 18%|█▊        | 943/5198 [9:02:05<135:54:33, 114.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.47s/it][A100%|██████████| 1/1 [01:25<00:00, 85.47s/it]
 18%|█▊        | 943/5198 [9:02:24<135:53:53, 114.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.59s/it][A100%|██████████| 1/1 [01:25<00:00, 85.59s/it]
 18%|█▊        | 943/5198 [9:00:48<135:55:18, 115.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.55s/it][A100%|██████████| 1/1 [01:25<00:00, 85.55s/it]
 18%|█▊        | 943/5198 [9:01:12<135:54:31, 114.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.56s/it][A100%|██████████| 1/1 [01:25<00:00, 85.56s/it]
 18%|█▊        | 943/5198 [9:01:43<135:55:31, 115.00s/it]
100%|██████████| 1/1 [01:25<00:00, 85.56s/it][A100%|██████████| 1/1 [01:25<00:00, 85.56s/it]
 18%|█▊        | 943/5198 [9:01:45<135:55:09, 115.00s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_58
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.89s/it][A100%|██████████| 1/1 [02:00<00:00, 120.89s/it]
 18%|█▊        | 944/5198 [9:04:05<137:58:26, 116.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:54:01,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=932, skipped=0, lr=[1.9086681553352766e-05], mom=[(0.9, 0.999)]
steps: 932 loss: 0.8434 iter time (s): 121.094 samples/sec: 1.057

100%|██████████| 1/1 [02:01<00:00, 121.98s/it][A100%|██████████| 1/1 [02:01<00:00, 121.98s/it]
 18%|█▊        | 944/5198 [9:04:20<138:21:31, 117.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.04s/it][A100%|██████████| 1/1 [02:02<00:00, 122.04s/it]
 18%|█▊        | 944/5198 [9:04:07<138:22:47, 117.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.97s/it][A100%|██████████| 1/1 [02:01<00:00, 121.97s/it]
 18%|█▊        | 944/5198 [9:04:26<138:21:00, 117.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.02s/it][A100%|██████████| 1/1 [02:02<00:00, 122.02s/it]
 18%|█▊        | 944/5198 [9:02:50<138:22:56, 117.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.00s/it][A100%|██████████| 1/1 [02:02<00:00, 122.00s/it]
 18%|█▊        | 944/5198 [9:03:14<138:21:56, 117.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.95s/it][A100%|██████████| 1/1 [02:01<00:00, 121.95s/it]
 18%|█▊        | 944/5198 [9:03:45<138:21:32, 117.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.98s/it][A100%|██████████| 1/1 [02:01<00:00, 121.98s/it]
 18%|█▊        | 944/5198 [9:03:47<138:21:56, 117.09s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_885
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.39s/it][A100%|██████████| 1/1 [01:30<00:00, 90.39s/it]
 18%|█▊        | 945/5198 [9:05:35<128:36:25, 108.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:55:30,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=933, skipped=0, lr=[1.9084315333353578e-05], mom=[(0.9, 0.999)]
steps: 933 loss: 0.5781 iter time (s): 88.666 samples/sec: 1.444

100%|██████████| 1/1 [01:29<00:00, 89.51s/it][A100%|██████████| 1/1 [01:29<00:00, 89.51s/it]
 18%|█▊        | 945/5198 [9:05:50<128:33:21, 108.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.43s/it][A100%|██████████| 1/1 [01:29<00:00, 89.43s/it]
 18%|█▊        | 945/5198 [9:05:36<128:32:29, 108.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.56s/it][A100%|██████████| 1/1 [01:29<00:00, 89.56s/it]
 18%|█▊        | 945/5198 [9:05:56<128:34:00, 108.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.61s/it][A100%|██████████| 1/1 [01:29<00:00, 89.61s/it]
 18%|█▊        | 945/5198 [9:04:20<128:36:28, 108.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.65s/it][A100%|██████████| 1/1 [01:29<00:00, 89.65s/it]
 18%|█▊        | 945/5198 [9:05:14<128:36:18, 108.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.79s/it][A100%|██████████| 1/1 [01:29<00:00, 89.79s/it]
 18%|█▊        | 945/5198 [9:05:16<128:39:30, 108.90s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_886
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.05s/it][A100%|██████████| 1/1 [01:30<00:00, 90.05s/it]
 18%|█▊        | 945/5198 [9:04:44<128:45:00, 108.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.86s/it][A100%|██████████| 1/1 [01:24<00:00, 84.86s/it]
 18%|█▊        | 946/5198 [9:07:00<120:05:28, 101.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:56:55,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=934, skipped=0, lr=[1.9081946199201356e-05], mom=[(0.9, 0.999)]
steps: 934 loss: 0.5847 iter time (s): 83.582 samples/sec: 1.531

100%|██████████| 1/1 [01:24<00:00, 84.72s/it][A100%|██████████| 1/1 [01:24<00:00, 84.72s/it]
 18%|█▊        | 946/5198 [9:07:14<119:59:51, 101.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.79s/it][A100%|██████████| 1/1 [01:24<00:00, 84.79s/it]
 18%|█▊        | 946/5198 [9:07:01<120:00:28, 101.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.69s/it][A100%|██████████| 1/1 [01:24<00:00, 84.69s/it]
 18%|█▊        | 946/5198 [9:07:20<119:59:14, 101.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.59s/it][A100%|██████████| 1/1 [01:24<00:00, 84.59s/it]
 18%|█▊        | 946/5198 [9:05:45<119:59:04, 101.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.18s/it][A100%|██████████| 1/1 [01:24<00:00, 84.18s/it]
 18%|█▊        | 946/5198 [9:06:08<119:56:11, 101.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.61s/it][A100%|██████████| 1/1 [01:24<00:00, 84.61s/it]
 18%|█▊        | 946/5198 [9:06:39<119:59:08, 101.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.45s/it][A100%|██████████| 1/1 [01:24<00:00, 84.45s/it]
 18%|█▊        | 946/5198 [9:06:41<119:58:01, 101.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_887
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.07s/it][A100%|██████████| 1/1 [01:42<00:00, 102.07s/it]
 18%|█▊        | 947/5198 [9:08:42<120:13:18, 101.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 08:58:38,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=935, skipped=0, lr=[1.907957415165618e-05], mom=[(0.9, 0.999)]
steps: 935 loss: 0.5709 iter time (s): 101.797 samples/sec: 1.257

100%|██████████| 1/1 [01:42<00:00, 102.71s/it][A100%|██████████| 1/1 [01:42<00:00, 102.71s/it]
 18%|█▊        | 947/5198 [9:08:57<120:21:54, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.62s/it][A100%|██████████| 1/1 [01:42<00:00, 102.62s/it]
 18%|█▊        | 947/5198 [9:08:44<120:20:34, 101.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.68s/it][A100%|██████████| 1/1 [01:42<00:00, 102.68s/it]
 18%|█▊        | 947/5198 [9:09:03<120:20:59, 101.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.70s/it][A100%|██████████| 1/1 [01:42<00:00, 102.70s/it]
 18%|█▊        | 947/5198 [9:07:27<120:21:20, 101.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.70s/it][A100%|██████████| 1/1 [01:42<00:00, 102.70s/it]
 18%|█▊        | 947/5198 [9:07:51<120:19:24, 101.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.71s/it][A100%|██████████| 1/1 [01:42<00:00, 102.71s/it]
 18%|█▊        | 947/5198 [9:08:22<120:21:26, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.71s/it][A100%|██████████| 1/1 [01:42<00:00, 102.71s/it]
 18%|█▊        | 947/5198 [9:08:24<120:20:43, 101.92s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_888
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.94s/it][A100%|██████████| 1/1 [01:24<00:00, 84.94s/it]
 18%|█▊        | 948/5198 [9:10:07<114:13:44, 96.76s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:00:02,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=936, skipped=0, lr=[1.9077199191479057e-05], mom=[(0.9, 0.999)]
steps: 936 loss: 0.5664 iter time (s): 83.536 samples/sec: 1.532

100%|██████████| 1/1 [01:24<00:00, 84.41s/it][A100%|██████████| 1/1 [01:24<00:00, 84.41s/it]
 18%|█▊        | 948/5198 [9:10:21<114:08:23, 96.68s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.49s/it][A100%|██████████| 1/1 [01:24<00:00, 84.49s/it]
 18%|█▊        | 948/5198 [9:10:08<114:08:51, 96.69s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.48s/it][A100%|██████████| 1/1 [01:24<00:00, 84.48s/it]
 18%|█▊        | 948/5198 [9:10:28<114:08:50, 96.69s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.48s/it][A100%|██████████| 1/1 [01:24<00:00, 84.48s/it]
 18%|█▊        | 948/5198 [9:08:52<114:09:10, 96.69s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.45s/it][A100%|██████████| 1/1 [01:24<00:00, 84.45s/it]
 18%|█▊        | 948/5198 [9:09:15<114:07:13, 96.67s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.45s/it][A100%|██████████| 1/1 [01:24<00:00, 84.45s/it]
 18%|█▊        | 948/5198 [9:09:46<114:08:26, 96.68s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.45s/it][A100%|██████████| 1/1 [01:24<00:00, 84.45s/it]
 18%|█▊        | 948/5198 [9:09:48<114:08:03, 96.68s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_889
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.37s/it][A100%|██████████| 1/1 [01:53<00:00, 113.37s/it]
 18%|█▊        | 949/5198 [9:12:01<120:05:27, 101.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:01:57,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=937, skipped=0, lr=[1.9074821319431937e-05], mom=[(0.9, 0.999)]
steps: 937 loss: 0.5485 iter time (s): 113.412 samples/sec: 1.129

100%|██████████| 1/1 [01:54<00:00, 114.28s/it][A100%|██████████| 1/1 [01:54<00:00, 114.29s/it]
 18%|█▊        | 949/5198 [9:12:16<120:20:55, 101.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.30s/it][A100%|██████████| 1/1 [01:54<00:00, 114.30s/it]
 18%|█▊        | 949/5198 [9:12:02<120:21:30, 101.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.38s/it][A100%|██████████| 1/1 [01:54<00:00, 114.38s/it]
 18%|█▊        | 949/5198 [9:12:22<120:23:15, 102.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.32s/it][A100%|██████████| 1/1 [01:54<00:00, 114.32s/it]
 18%|█▊        | 949/5198 [9:10:46<120:22:07, 101.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.32s/it][A100%|██████████| 1/1 [01:54<00:00, 114.32s/it]
 18%|█▊        | 949/5198 [9:11:09<120:20:47, 101.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.30s/it][A100%|██████████| 1/1 [01:54<00:00, 114.30s/it]
 18%|█▊        | 949/5198 [9:11:40<120:21:13, 101.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.34s/it][A100%|██████████| 1/1 [01:54<00:00, 114.34s/it]
 18%|█▊        | 949/5198 [9:11:42<120:21:42, 101.98s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_890
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.20s/it][A100%|██████████| 1/1 [02:01<00:00, 121.20s/it]
 18%|█▊        | 950/5198 [9:14:02<126:59:06, 107.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:03:58,484] [INFO] [logging.py:96:log_dist] [Rank 0] step=938, skipped=0, lr=[1.9072440536277697e-05], mom=[(0.9, 0.999)]
steps: 938 loss: 0.5846 iter time (s): 120.492 samples/sec: 1.062

100%|██████████| 1/1 [02:01<00:00, 121.42s/it][A100%|██████████| 1/1 [02:01<00:00, 121.42s/it]
 18%|█▊        | 950/5198 [9:14:17<127:12:46, 107.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.41s/it][A100%|██████████| 1/1 [02:01<00:00, 121.41s/it]
 18%|█▊        | 950/5198 [9:14:04<127:12:52, 107.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.42s/it][A100%|██████████| 1/1 [02:01<00:00, 121.42s/it]
 18%|█▊        | 950/5198 [9:14:23<127:14:17, 107.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.39s/it][A100%|██████████| 1/1 [02:01<00:00, 121.39s/it]
 18%|█▊        | 950/5198 [9:12:48<127:12:53, 107.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.42s/it][A100%|██████████| 1/1 [02:01<00:00, 121.42s/it]
 18%|█▊        | 950/5198 [9:13:11<127:12:33, 107.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.44s/it][A100%|██████████| 1/1 [02:01<00:00, 121.44s/it]
 18%|█▊        | 950/5198 [9:13:42<127:13:15, 107.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.42s/it][A100%|██████████| 1/1 [02:01<00:00, 121.42s/it]
 18%|█▊        | 950/5198 [9:13:44<127:13:00, 107.81s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_891
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.10s/it][A100%|██████████| 1/1 [01:37<00:00, 97.11s/it]
 18%|█▊        | 951/5198 [9:15:39<123:16:05, 104.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:05:34,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=939, skipped=0, lr=[1.907005684278016e-05], mom=[(0.9, 0.999)]
steps: 939 loss: 0.6229 iter time (s): 95.156 samples/sec: 1.345

100%|██████████| 1/1 [01:36<00:00, 96.22s/it][A100%|██████████| 1/1 [01:36<00:00, 96.22s/it]
 18%|█▊        | 951/5198 [9:15:53<123:05:15, 104.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.17s/it][A100%|██████████| 1/1 [01:36<00:00, 96.17s/it]
 18%|█▊        | 951/5198 [9:15:40<123:04:02, 104.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.08s/it][A100%|██████████| 1/1 [01:36<00:00, 96.08s/it]
 18%|█▊        | 951/5198 [9:16:00<123:03:14, 104.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.15s/it][A100%|██████████| 1/1 [01:36<00:00, 96.15s/it]
 18%|█▊        | 951/5198 [9:14:24<123:03:41, 104.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.11s/it][A100%|██████████| 1/1 [01:36<00:00, 96.11s/it]
 18%|█▊        | 951/5198 [9:14:47<123:02:41, 104.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.12s/it][A100%|██████████| 1/1 [01:36<00:00, 96.12s/it]
 18%|█▊        | 951/5198 [9:15:18<123:03:15, 104.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.14s/it][A100%|██████████| 1/1 [01:36<00:00, 96.14s/it]
 18%|█▊        | 951/5198 [9:15:20<123:03:28, 104.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_892
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.33s/it][A100%|██████████| 1/1 [01:23<00:00, 83.33s/it]
 18%|█▊        | 952/5198 [9:17:02<115:46:25, 98.16s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:06:57,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[1.906767023970406e-05], mom=[(0.9, 0.999)]
steps: 940 loss: 0.6156 iter time (s): 82.489 samples/sec: 1.552

100%|██████████| 1/1 [01:23<00:00, 83.28s/it][A100%|██████████| 1/1 [01:23<00:00, 83.29s/it]
 18%|█▊        | 952/5198 [9:17:17<115:36:46, 98.02s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.44s/it][A100%|██████████| 1/1 [01:23<00:00, 83.44s/it]
 18%|█▊        | 952/5198 [9:17:03<115:39:14, 98.06s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.39s/it][A100%|██████████| 1/1 [01:23<00:00, 83.39s/it]
 18%|█▊        | 952/5198 [9:17:23<115:37:33, 98.03s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.43s/it][A100%|██████████| 1/1 [01:23<00:00, 83.44s/it]
 18%|█▊        | 952/5198 [9:15:47<115:38:51, 98.05s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.44s/it][A100%|██████████| 1/1 [01:23<00:00, 83.44s/it]
 18%|█▊        | 952/5198 [9:16:10<115:38:11, 98.04s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.41s/it][A100%|██████████| 1/1 [01:23<00:00, 83.42s/it]
 18%|█▊        | 952/5198 [9:16:41<115:38:08, 98.04s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.41s/it][A100%|██████████| 1/1 [01:23<00:00, 83.41s/it]
 18%|█▊        | 952/5198 [9:16:43<115:38:11, 98.04s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_893
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.86s/it][A100%|██████████| 1/1 [01:21<00:00, 81.86s/it]
 18%|█▊        | 953/5198 [9:18:24<109:59:46, 93.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:08:19,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=941, skipped=0, lr=[1.906528072781509e-05], mom=[(0.9, 0.999)]
steps: 941 loss: 0.5701 iter time (s): 80.931 samples/sec: 1.582

100%|██████████| 1/1 [01:21<00:00, 81.89s/it][A100%|██████████| 1/1 [01:21<00:00, 81.89s/it]
 18%|█▊        | 953/5198 [9:18:39<109:52:53, 93.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.77s/it][A100%|██████████| 1/1 [01:21<00:00, 81.77s/it]
 18%|█▊        | 953/5198 [9:18:25<109:52:03, 93.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.85s/it][A100%|██████████| 1/1 [01:21<00:00, 81.85s/it]
 18%|█▊        | 953/5198 [9:18:45<109:52:36, 93.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.71s/it][A100%|██████████| 1/1 [01:21<00:00, 81.71s/it]
 18%|█▊        | 953/5198 [9:17:09<109:50:33, 93.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.77s/it][A100%|██████████| 1/1 [01:21<00:00, 81.77s/it]
 18%|█▊        | 953/5198 [9:17:32<109:51:24, 93.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.82s/it][A100%|██████████| 1/1 [01:21<00:00, 81.83s/it]
 18%|█▊        | 953/5198 [9:18:03<109:52:27, 93.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.79s/it][A100%|██████████| 1/1 [01:21<00:00, 81.79s/it]
 18%|█▊        | 953/5198 [9:18:05<109:51:49, 93.17s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_894
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.27s/it][A100%|██████████| 1/1 [01:22<00:00, 82.27s/it]
 18%|█▊        | 954/5198 [9:19:47<106:05:11, 89.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:09:42,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=942, skipped=0, lr=[1.9062888307879858e-05], mom=[(0.9, 0.999)]
steps: 942 loss: 0.6324 iter time (s): 81.404 samples/sec: 1.572

100%|██████████| 1/1 [01:22<00:00, 82.39s/it][A100%|██████████| 1/1 [01:22<00:00, 82.39s/it]
 18%|█▊        | 954/5198 [9:20:01<106:02:31, 89.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.35s/it][A100%|██████████| 1/1 [01:22<00:00, 82.35s/it]
 18%|█▊        | 954/5198 [9:19:48<106:01:06, 89.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.36s/it][A100%|██████████| 1/1 [01:22<00:00, 82.36s/it]
 18%|█▊        | 954/5198 [9:20:07<106:01:40, 89.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.45s/it][A100%|██████████| 1/1 [01:22<00:00, 82.45s/it]
 18%|█▊        | 954/5198 [9:18:31<106:02:03, 89.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.38s/it][A100%|██████████| 1/1 [01:22<00:00, 82.38s/it]
 18%|█▊        | 954/5198 [9:18:55<106:01:08, 89.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.37s/it][A100%|██████████| 1/1 [01:22<00:00, 82.37s/it]
 18%|█▊        | 954/5198 [9:19:26<106:01:35, 89.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.39s/it][A100%|██████████| 1/1 [01:22<00:00, 82.39s/it]
 18%|█▊        | 954/5198 [9:19:28<106:01:34, 89.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_895
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.02s/it][A100%|██████████| 1/1 [01:45<00:00, 105.02s/it]
 18%|█▊        | 955/5198 [9:21:32<111:23:45, 94.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:11:27,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=943, skipped=0, lr=[1.906049298066592e-05], mom=[(0.9, 0.999)]
steps: 943 loss: 0.5713 iter time (s): 104.771 samples/sec: 1.222

100%|██████████| 1/1 [01:45<00:00, 105.72s/it][A100%|██████████| 1/1 [01:45<00:00, 105.72s/it]
 18%|█▊        | 955/5198 [9:21:47<111:35:56, 94.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.77s/it][A100%|██████████| 1/1 [01:45<00:00, 105.77s/it]
 18%|█▊        | 955/5198 [9:21:33<111:35:44, 94.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.63s/it][A100%|██████████| 1/1 [01:45<00:00, 105.63s/it]
 18%|█▊        | 955/5198 [9:21:53<111:33:09, 94.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.69s/it][A100%|██████████| 1/1 [01:45<00:00, 105.69s/it]
 18%|█▊        | 955/5198 [9:20:17<111:34:43, 94.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.74s/it][A100%|██████████| 1/1 [01:45<00:00, 105.74s/it]
 18%|█▊        | 955/5198 [9:20:40<111:35:10, 94.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.71s/it][A100%|██████████| 1/1 [01:45<00:00, 105.71s/it]
 18%|█▊        | 955/5198 [9:21:11<111:34:47, 94.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.71s/it][A100%|██████████| 1/1 [01:45<00:00, 105.71s/it]
 18%|█▊        | 955/5198 [9:21:13<111:34:47, 94.67s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_896
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.24s/it][A100%|██████████| 1/1 [01:22<00:00, 82.24s/it]
 18%|█▊        | 956/5198 [9:22:54<107:03:15, 90.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:12:49,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=944, skipped=0, lr=[1.9058094746941747e-05], mom=[(0.9, 0.999)]
steps: 944 loss: 0.5950 iter time (s): 80.669 samples/sec: 1.587

100%|██████████| 1/1 [01:21<00:00, 81.57s/it][A100%|██████████| 1/1 [01:21<00:00, 81.57s/it]
 18%|█▊        | 956/5198 [9:23:08<106:56:24, 90.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.59s/it][A100%|██████████| 1/1 [01:21<00:00, 81.59s/it]
 18%|█▊        | 956/5198 [9:22:55<106:56:36, 90.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.63s/it][A100%|██████████| 1/1 [01:21<00:00, 81.63s/it]
 18%|█▊        | 956/5198 [9:23:14<106:55:36, 90.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.56s/it][A100%|██████████| 1/1 [01:21<00:00, 81.56s/it]
 18%|█▊        | 956/5198 [9:21:39<106:55:24, 90.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.51s/it][A100%|██████████| 1/1 [01:21<00:00, 81.52s/it]
 18%|█▊        | 956/5198 [9:22:02<106:54:37, 90.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.54s/it][A100%|██████████| 1/1 [01:21<00:00, 81.54s/it]
 18%|█▊        | 956/5198 [9:22:33<106:54:49, 90.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.55s/it][A100%|██████████| 1/1 [01:21<00:00, 81.55s/it]
 18%|█▊        | 956/5198 [9:22:35<106:54:59, 90.74s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_897
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.55s/it][A100%|██████████| 1/1 [01:50<00:00, 110.55s/it]
 18%|█▊        | 957/5198 [9:24:45<114:00:41, 96.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:14:40,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=945, skipped=0, lr=[1.905569360747675e-05], mom=[(0.9, 0.999)]
steps: 945 loss: 0.5677 iter time (s): 110.558 samples/sec: 1.158

100%|██████████| 1/1 [01:51<00:00, 111.46s/it][A100%|██████████| 1/1 [01:51<00:00, 111.46s/it]
 18%|█▊        | 957/5198 [9:25:00<114:14:00, 96.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.45s/it][A100%|██████████| 1/1 [01:51<00:00, 111.45s/it]
 18%|█▊        | 957/5198 [9:24:46<114:14:05, 96.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.38s/it][A100%|██████████| 1/1 [01:51<00:00, 111.38s/it]
 18%|█▊        | 957/5198 [9:25:06<114:12:01, 96.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.43s/it][A100%|██████████| 1/1 [01:51<00:00, 111.43s/it]
 18%|█▊        | 957/5198 [9:23:30<114:12:52, 96.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.50s/it][A100%|██████████| 1/1 [01:51<00:00, 111.50s/it]
 18%|█▊        | 957/5198 [9:23:53<114:13:42, 96.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.47s/it][A100%|██████████| 1/1 [01:51<00:00, 111.47s/it]
 18%|█▊        | 957/5198 [9:24:24<114:13:06, 96.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.47s/it][A100%|██████████| 1/1 [01:51<00:00, 111.47s/it]
 18%|█▊        | 957/5198 [9:24:26<114:13:20, 96.96s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_898
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 18%|█▊        | 958/5198 [9:26:13<111:01:06, 94.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:16:08,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=946, skipped=0, lr=[1.9053289563041292e-05], mom=[(0.9, 0.999)]
steps: 946 loss: 0.5116 iter time (s): 86.827 samples/sec: 1.474

100%|██████████| 1/1 [01:27<00:00, 87.73s/it][A100%|██████████| 1/1 [01:27<00:00, 87.73s/it]
 18%|█▊        | 958/5198 [9:26:28<110:56:49, 94.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.76s/it][A100%|██████████| 1/1 [01:27<00:00, 87.76s/it]
 18%|█▊        | 958/5198 [9:26:14<110:57:30, 94.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.85s/it][A100%|██████████| 1/1 [01:27<00:00, 87.85s/it]
 18%|█▊        | 958/5198 [9:26:34<110:58:02, 94.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.80s/it][A100%|██████████| 1/1 [01:27<00:00, 87.80s/it]
 18%|█▊        | 958/5198 [9:24:58<110:57:23, 94.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.78s/it][A100%|██████████| 1/1 [01:27<00:00, 87.78s/it]
 18%|█▊        | 958/5198 [9:25:21<110:57:40, 94.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.79s/it][A100%|██████████| 1/1 [01:27<00:00, 87.79s/it]
 18%|█▊        | 958/5198 [9:25:52<110:57:25, 94.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.79s/it][A100%|██████████| 1/1 [01:27<00:00, 87.79s/it]
 18%|█▊        | 958/5198 [9:25:54<110:57:35, 94.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_899
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.02s/it][A100%|██████████| 1/1 [01:26<00:00, 86.02s/it]
 18%|█▊        | 959/5198 [9:27:39<108:05:33, 91.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:17:34,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=947, skipped=0, lr=[1.9050882614406632e-05], mom=[(0.9, 0.999)]
steps: 947 loss: 0.5538 iter time (s): 84.678 samples/sec: 1.512

100%|██████████| 1/1 [01:25<00:00, 85.57s/it][A100%|██████████| 1/1 [01:25<00:00, 85.57s/it]
 18%|█▊        | 959/5198 [9:27:53<107:52:33, 91.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.62s/it][A100%|██████████| 1/1 [01:25<00:00, 85.62s/it]
 18%|█▊        | 959/5198 [9:27:40<107:53:58, 91.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.56s/it][A100%|██████████| 1/1 [01:25<00:00, 85.56s/it]
 18%|█▊        | 959/5198 [9:27:59<107:53:09, 91.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.56s/it][A100%|██████████| 1/1 [01:25<00:00, 85.56s/it]
 18%|█▊        | 959/5198 [9:26:23<107:52:34, 91.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.59s/it][A100%|██████████| 1/1 [01:25<00:00, 85.59s/it]
 18%|█▊        | 959/5198 [9:26:47<107:53:30, 91.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.61s/it][A100%|██████████| 1/1 [01:25<00:00, 85.61s/it]
 18%|█▊        | 959/5198 [9:27:18<107:53:47, 91.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.59s/it][A100%|██████████| 1/1 [01:25<00:00, 85.59s/it]
 18%|█▊        | 959/5198 [9:27:20<107:53:24, 91.63s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_59
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.18s/it][A100%|██████████| 1/1 [02:03<00:00, 123.18s/it]
[2024-06-30 09:19:38,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=948, skipped=0, lr=[1.9048472762344986e-05], mom=[(0.9, 0.999)]
steps: 948 loss: 0.8604 iter time (s): 123.739 samples/sec: 1.034

100%|██████████| 1/1 [02:04<00:00, 124.72s/it][A100%|██████████| 1/1 [02:04<00:00, 124.72s/it]

100%|██████████| 1/1 [02:04<00:00, 124.73s/it][A100%|██████████| 1/1 [02:04<00:00, 124.73s/it]

100%|██████████| 1/1 [02:04<00:00, 124.78s/it][A100%|██████████| 1/1 [02:04<00:00, 124.78s/it]

100%|██████████| 1/1 [02:04<00:00, 124.79s/it][A100%|██████████| 1/1 [02:04<00:00, 124.79s/it]

100%|██████████| 1/1 [02:04<00:00, 124.83s/it][A100%|██████████| 1/1 [02:04<00:00, 124.83s/it]

100%|██████████| 1/1 [02:04<00:00, 124.80s/it][A100%|██████████| 1/1 [02:04<00:00, 124.80s/it]

100%|██████████| 1/1 [02:04<00:00, 124.79s/it][A100%|██████████| 1/1 [02:04<00:00, 124.79s/it]
Checkpointing at shard 959
[2024-06-30 09:19:40,091] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step948 is about to be saved!
[2024-06-30 09:19:41,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_00-model_states.pt...
[2024-06-30 09:19:45,238] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_02-model_states.pt...
[2024-06-30 09:19:47,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_00-model_states.pt.
[2024-06-30 09:19:52,256] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_08-model_states.pt...
[2024-06-30 09:19:53,085] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_07-model_states.pt...
[2024-06-30 09:19:53,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_03-model_states.pt...
[2024-06-30 09:19:54,102] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_04-model_states.pt...
[2024-06-30 09:19:55,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_06-model_states.pt...
[2024-06-30 09:19:56,146] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_05-model_states.pt...
[2024-06-30 09:19:56,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_01-model_states.pt...
[2024-06-30 09:22:16,083] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_05-model_states.pt.
[2024-06-30 09:22:16,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_02-model_states.pt.
[2024-06-30 09:22:17,054] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_01_model_states.pt
[2024-06-30 09:22:17,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_01_model_states.pt...
[2024-06-30 09:22:17,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_04_model_states.pt...
[2024-06-30 09:22:17,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_01_model_states.pt.
[2024-06-30 09:22:17,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
[2024-06-30 09:22:17,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_04_model_states.pt.
[2024-06-30 09:22:17,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
[2024-06-30 09:22:19,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_01-model_states.pt.
[2024-06-30 09:22:19,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_06-model_states.pt.
[2024-06-30 09:22:20,057] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_00_model_states.pt
[2024-06-30 09:22:20,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_00_model_states.pt...
[2024-06-30 09:22:20,462] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_00_model_states.pt.
[2024-06-30 09:22:20,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
[2024-06-30 09:22:20,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_05_model_states.pt...
[2024-06-30 09:22:20,645] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_05_model_states.pt.
[2024-06-30 09:22:20,645] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
[2024-06-30 09:22:23,122] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_03-model_states.pt.
[2024-06-30 09:22:23,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_04-model_states.pt.
[2024-06-30 09:22:23,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_02_model_states.pt...
[2024-06-30 09:22:23,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_02_model_states.pt.
[2024-06-30 09:22:23,975] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
[2024-06-30 09:22:24,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_08-model_states.pt.
[2024-06-30 09:22:24,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_03_model_states.pt...
[2024-06-30 09:22:24,222] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_03_model_states.pt.
[2024-06-30 09:22:24,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
[2024-06-30 09:22:24,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_07-model_states.pt.
[2024-06-30 09:22:25,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_06_model_states.pt...
[2024-06-30 09:22:25,141] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_06_model_states.pt.
[2024-06-30 09:22:25,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
[2024-06-30 09:22:25,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_09-model_states.pt...
[2024-06-30 09:22:25,928] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/layer_09-model_states.pt.
[2024-06-30 09:22:25,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_07_model_states.pt...
[2024-06-30 09:22:26,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step948/mp_rank_07_model_states.pt.
[2024-06-30 09:22:26,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step948 is ready now!
Checkpoint saved using --- 165.92819452285767 seconds ---
 18%|█▊        | 960/5198 [9:31:14<178:13:48, 151.40s/it] 18%|█▊        | 960/5198 [9:32:44<178:21:04, 151.50s/it] 18%|█▊        | 960/5198 [9:32:32<179:12:41, 152.23s/it] 18%|█▊        | 960/5198 [9:31:37<178:12:13, 151.38s/it] 18%|█▊        | 960/5198 [9:32:31<178:18:29, 151.47s/it] 18%|█▊        | 960/5198 [9:32:09<178:11:30, 151.37s/it] 18%|█▊        | 960/5198 [9:32:10<178:10:47, 151.36s/it] 18%|█▊        | 960/5198 [9:32:50<178:16:05, 151.43s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_900
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s]  0%|          | 0/1 [00:00<?, ?it/s][A[A
100%|██████████| 1/1 [01:18<00:00, 78.69s/it][A100%|██████████| 1/1 [01:18<00:00, 78.69s/it]
 18%|█▊        | 961/5198 [9:33:51<153:14:48, 130.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:23:46,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=949, skipped=0, lr=[1.904606000762949e-05], mom=[(0.9, 0.999)]
steps: 949 loss: 0.5677 iter time (s): 80.580 samples/sec: 1.588

100%|██████████| 1/1 [01:20<00:00, 81.00s/it][A100%|██████████| 1/1 [01:20<00:00, 81.00s/it]
 18%|█▊        | 961/5198 [9:34:05<153:27:26, 130.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.08s/it][A100%|██████████| 1/1 [01:21<00:00, 81.09s/it]
 18%|█▊        | 961/5198 [9:33:52<153:27:31, 130.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.17s/it][A100%|██████████| 1/1 [01:21<00:00, 81.17s/it]
 18%|█▊        | 961/5198 [9:34:12<153:27:32, 130.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.41s/it][A100%|██████████| 1/1 [01:21<00:00, 81.41s/it]
 18%|█▊        | 961/5198 [9:32:36<153:30:58, 130.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.44s/it][A100%|██████████| 1/1 [01:21<00:00, 81.44s/it]
 18%|█▊        | 961/5198 [9:32:59<153:30:46, 130.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.50s/it][A100%|██████████| 1/1 [01:21<00:00, 81.50s/it]
 18%|█▊        | 961/5198 [9:33:30<153:31:17, 130.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.51s/it][A100%|██████████| 1/1 [01:21<00:00, 81.51s/it]
 18%|█▊        | 961/5198 [9:33:32<153:31:01, 130.44s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_901
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.15s/it][A100%|██████████| 1/1 [02:06<00:00, 126.15s/it]
 19%|█▊        | 962/5198 [9:35:57<151:48:16, 129.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:25:54,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[1.9043644351034223e-05], mom=[(0.9, 0.999)]
steps: 950 loss: 0.5657 iter time (s): 126.516 samples/sec: 1.012

100%|██████████| 1/1 [02:07<00:00, 127.46s/it][A100%|██████████| 1/1 [02:07<00:00, 127.46s/it]
 19%|█▊        | 962/5198 [9:36:13<152:23:27, 129.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.40s/it][A100%|██████████| 1/1 [02:07<00:00, 127.40s/it]
 19%|█▊        | 962/5198 [9:35:59<152:22:20, 129.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.53s/it][A100%|██████████| 1/1 [02:07<00:00, 127.53s/it]
 19%|█▊        | 962/5198 [9:36:19<152:25:02, 129.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.35s/it][A100%|██████████| 1/1 [02:07<00:00, 127.35s/it]
 19%|█▊        | 962/5198 [9:34:43<152:23:40, 129.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.40s/it][A100%|██████████| 1/1 [02:07<00:00, 127.40s/it]
 19%|█▊        | 962/5198 [9:35:38<152:24:54, 129.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.48s/it][A100%|██████████| 1/1 [02:07<00:00, 127.48s/it]
 19%|█▊        | 962/5198 [9:35:07<152:26:20, 129.55s/it]
100%|██████████| 1/1 [02:07<00:00, 127.44s/it][A100%|██████████| 1/1 [02:07<00:00, 127.44s/it]
 19%|█▊        | 962/5198 [9:35:40<152:25:26, 129.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_902

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.50s/it][A100%|██████████| 1/1 [01:32<00:00, 92.50s/it]
 19%|█▊        | 963/5198 [9:37:30<138:54:51, 118.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:27:25,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=951, skipped=0, lr=[1.9041225793334184e-05], mom=[(0.9, 0.999)]
steps: 951 loss: 0.5786 iter time (s): 90.606 samples/sec: 1.413

100%|██████████| 1/1 [01:31<00:00, 91.48s/it][A100%|██████████| 1/1 [01:31<00:00, 91.48s/it]
 19%|█▊        | 963/5198 [9:37:44<138:56:18, 118.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.62s/it][A100%|██████████| 1/1 [01:31<00:00, 91.62s/it]
 19%|█▊        | 963/5198 [9:37:31<138:58:26, 118.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.52s/it][A100%|██████████| 1/1 [01:31<00:00, 91.52s/it]
 19%|█▊        | 963/5198 [9:37:51<138:58:04, 118.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.65s/it][A100%|██████████| 1/1 [01:31<00:00, 91.65s/it]
 19%|█▊        | 963/5198 [9:36:15<139:00:03, 118.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.50s/it][A100%|██████████| 1/1 [01:31<00:00, 91.50s/it]
 19%|█▊        | 963/5198 [9:36:38<138:58:40, 118.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.56s/it][A100%|██████████| 1/1 [01:31<00:00, 91.56s/it]
 19%|█▊        | 963/5198 [9:37:09<138:58:59, 118.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.55s/it][A100%|██████████| 1/1 [01:31<00:00, 91.55s/it]
 19%|█▊        | 963/5198 [9:37:11<138:59:08, 118.15s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_903
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.79s/it][A100%|██████████| 1/1 [01:24<00:00, 84.79s/it]
 19%|█▊        | 964/5198 [9:38:55<127:09:25, 108.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:28:50,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=952, skipped=0, lr=[1.90388043353053e-05], mom=[(0.9, 0.999)]
steps: 952 loss: 0.6371 iter time (s): 83.731 samples/sec: 1.529

100%|██████████| 1/1 [01:24<00:00, 84.67s/it][A100%|██████████| 1/1 [01:24<00:00, 84.67s/it]
 19%|█▊        | 964/5198 [9:39:09<127:06:45, 108.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.64s/it][A100%|██████████| 1/1 [01:24<00:00, 84.64s/it]
 19%|█▊        | 964/5198 [9:38:56<127:07:30, 108.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.70s/it][A100%|██████████| 1/1 [01:24<00:00, 84.70s/it]
 19%|█▊        | 964/5198 [9:39:15<127:08:36, 108.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.58s/it][A100%|██████████| 1/1 [01:24<00:00, 84.58s/it]
 19%|█▊        | 964/5198 [9:37:39<127:07:22, 108.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.68s/it][A100%|██████████| 1/1 [01:24<00:00, 84.68s/it]
 19%|█▊        | 964/5198 [9:38:03<127:08:34, 108.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.69s/it][A100%|██████████| 1/1 [01:24<00:00, 84.70s/it]
 19%|█▊        | 964/5198 [9:38:34<127:09:04, 108.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.68s/it][A100%|██████████| 1/1 [01:24<00:00, 84.68s/it]
 19%|█▊        | 964/5198 [9:38:36<127:08:53, 108.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_904
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.21s/it][A100%|██████████| 1/1 [01:28<00:00, 88.21s/it]
 19%|█▊        | 965/5198 [9:40:23<120:06:53, 102.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:30:18,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=953, skipped=0, lr=[1.9036379977724448e-05], mom=[(0.9, 0.999)]
steps: 953 loss: 0.6142 iter time (s): 87.407 samples/sec: 1.464

100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 19%|█▊        | 965/5198 [9:40:37<120:06:42, 102.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.28s/it][A100%|██████████| 1/1 [01:28<00:00, 88.28s/it]
 19%|█▊        | 965/5198 [9:40:24<120:06:43, 102.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.30s/it][A100%|██████████| 1/1 [01:28<00:00, 88.30s/it]
 19%|█▊        | 965/5198 [9:40:44<120:07:54, 102.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.42s/it][A100%|██████████| 1/1 [01:28<00:00, 88.42s/it]

 19%|█▊        | 965/5198 [9:39:08<120:09:24, 102.19s/it]100%|██████████| 1/1 [01:28<00:00, 88.29s/it][A100%|██████████| 1/1 [01:28<00:00, 88.29s/it]
 19%|█▊        | 965/5198 [9:39:31<120:07:37, 102.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.27s/it][A100%|██████████| 1/1 [01:28<00:00, 88.27s/it]
 19%|█▊        | 965/5198 [9:40:02<120:07:28, 102.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.30s/it][A100%|██████████| 1/1 [01:28<00:00, 88.30s/it]
 19%|█▊        | 965/5198 [9:40:04<120:07:55, 102.17s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_905
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.13s/it][A100%|██████████| 1/1 [01:23<00:00, 83.13s/it]
 19%|█▊        | 966/5198 [9:41:46<113:23:20, 96.46s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:31:41,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=954, skipped=0, lr=[1.903395272136941e-05], mom=[(0.9, 0.999)]
steps: 954 loss: 0.6030 iter time (s): 82.119 samples/sec: 1.559

100%|██████████| 1/1 [01:23<00:00, 83.07s/it][A100%|██████████| 1/1 [01:23<00:00, 83.07s/it]
 19%|█▊        | 966/5198 [9:42:00<113:21:33, 96.43s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.11s/it][A100%|██████████| 1/1 [01:23<00:00, 83.11s/it]
 19%|█▊        | 966/5198 [9:41:47<113:22:16, 96.44s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.02s/it][A100%|██████████| 1/1 [01:23<00:00, 83.02s/it]
 19%|█▊        | 966/5198 [9:42:07<113:21:21, 96.43s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.98s/it][A100%|██████████| 1/1 [01:22<00:00, 82.98s/it]
 19%|█▊        | 966/5198 [9:40:31<113:21:18, 96.43s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.01s/it][A100%|██████████| 1/1 [01:23<00:00, 83.01s/it]
 19%|█▊        | 966/5198 [9:40:54<113:20:46, 96.42s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.07s/it][A100%|██████████| 1/1 [01:23<00:00, 83.07s/it]
 19%|█▊        | 966/5198 [9:41:25<113:21:53, 96.44s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.05s/it][A100%|██████████| 1/1 [01:23<00:00, 83.05s/it]
 19%|█▊        | 966/5198 [9:41:27<113:21:46, 96.43s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_906
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.49s/it][A100%|██████████| 1/1 [02:17<00:00, 137.49s/it]
 19%|█▊        | 967/5198 [9:44:04<127:51:40, 108.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:34:00,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=955, skipped=0, lr=[1.9031522567018918e-05], mom=[(0.9, 0.999)]
steps: 955 loss: 0.5730 iter time (s): 138.194 samples/sec: 0.926

100%|██████████| 1/1 [02:19<00:00, 139.05s/it][A100%|██████████| 1/1 [02:19<00:00, 139.05s/it]
 19%|█▊        | 967/5198 [9:44:20<128:21:49, 109.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.11s/it][A100%|██████████| 1/1 [02:19<00:00, 139.11s/it]
 19%|█▊        | 967/5198 [9:44:06<128:23:26, 109.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.08s/it][A100%|██████████| 1/1 [02:19<00:00, 139.08s/it]
 19%|█▊        | 967/5198 [9:44:26<128:22:10, 109.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.06s/it][A100%|██████████| 1/1 [02:19<00:00, 139.06s/it]
 19%|█▊        | 967/5198 [9:42:50<128:21:47, 109.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.11s/it][A100%|██████████| 1/1 [02:19<00:00, 139.11s/it]
 19%|█▊        | 967/5198 [9:43:13<128:22:38, 109.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.06s/it][A100%|██████████| 1/1 [02:19<00:00, 139.06s/it]
 19%|█▊        | 967/5198 [9:43:44<128:22:05, 109.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.07s/it][A100%|██████████| 1/1 [02:19<00:00, 139.07s/it]
 19%|█▊        | 967/5198 [9:43:46<128:22:15, 109.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_907
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.72s/it][A100%|██████████| 1/1 [01:23<00:00, 83.72s/it]
 19%|█▊        | 968/5198 [9:45:28<119:00:44, 101.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:35:23,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=956, skipped=0, lr=[1.902908951545262e-05], mom=[(0.9, 0.999)]
steps: 956 loss: 0.5817 iter time (s): 81.283 samples/sec: 1.575

100%|██████████| 1/1 [01:22<00:00, 82.20s/it][A100%|██████████| 1/1 [01:22<00:00, 82.20s/it]
 19%|█▊        | 968/5198 [9:45:42<118:48:48, 101.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.08s/it][A100%|██████████| 1/1 [01:22<00:00, 82.08s/it]
 19%|█▊        | 968/5198 [9:45:28<118:47:22, 101.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.23s/it][A100%|██████████| 1/1 [01:22<00:00, 82.23s/it]
 19%|█▊        | 968/5198 [9:45:48<118:49:39, 101.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.16s/it][A100%|██████████| 1/1 [01:22<00:00, 82.16s/it]
 19%|█▊        | 968/5198 [9:44:12<118:47:58, 101.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.21s/it][A100%|██████████| 1/1 [01:22<00:00, 82.21s/it]
 19%|█▊        | 968/5198 [9:44:35<118:49:34, 101.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.23s/it][A100%|██████████| 1/1 [01:22<00:00, 82.23s/it]
 19%|█▊        | 968/5198 [9:45:06<118:49:32, 101.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.21s/it][A100%|██████████| 1/1 [01:22<00:00, 82.21s/it]
 19%|█▊        | 968/5198 [9:45:08<118:49:07, 101.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_908
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.28s/it][A100%|██████████| 1/1 [01:31<00:00, 91.28s/it]
 19%|█▊        | 969/5198 [9:46:59<115:29:18, 98.31s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:36:54,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=957, skipped=0, lr=[1.9026653567451102e-05], mom=[(0.9, 0.999)]
steps: 957 loss: 0.5658 iter time (s): 90.667 samples/sec: 1.412

100%|██████████| 1/1 [01:31<00:00, 91.68s/it][A100%|██████████| 1/1 [01:31<00:00, 91.68s/it]
 19%|█▊        | 969/5198 [9:47:13<115:27:50, 98.29s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.83s/it][A100%|██████████| 1/1 [01:31<00:00, 91.83s/it]
 19%|█▊        | 969/5198 [9:47:00<115:29:54, 98.32s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.70s/it][A100%|██████████| 1/1 [01:31<00:00, 91.70s/it]
 19%|█▊        | 969/5198 [9:47:20<115:28:43, 98.30s/it] 
100%|██████████| 1/1 [01:31<00:00, 91.66s/it][A100%|██████████| 1/1 [01:31<00:00, 91.66s/it]
 19%|█▊        | 969/5198 [9:45:44<115:26:39, 98.27s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.60s/it][A100%|██████████| 1/1 [01:31<00:00, 91.60s/it]
 19%|█▊        | 969/5198 [9:46:07<115:26:32, 98.27s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.64s/it][A100%|██████████| 1/1 [01:31<00:00, 91.64s/it]
 19%|█▊        | 969/5198 [9:46:38<115:27:22, 98.28s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.66s/it][A100%|██████████| 1/1 [01:31<00:00, 91.66s/it]
 19%|█▊        | 969/5198 [9:46:40<115:27:34, 98.29s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_909
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.16s/it][A100%|██████████| 1/1 [01:33<00:00, 93.16s/it]
 19%|█▊        | 970/5198 [9:48:32<113:41:42, 96.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:38:28,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=958, skipped=0, lr=[1.902421472379588e-05], mom=[(0.9, 0.999)]
steps: 958 loss: 0.5293 iter time (s): 92.414 samples/sec: 1.385

100%|██████████| 1/1 [01:33<00:00, 93.41s/it][A100%|██████████| 1/1 [01:33<00:00, 93.41s/it]
 19%|█▊        | 970/5198 [9:48:47<113:43:10, 96.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.30s/it][A100%|██████████| 1/1 [01:33<00:00, 93.30s/it]
 19%|█▊        | 970/5198 [9:48:33<113:42:23, 96.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.28s/it][A100%|██████████| 1/1 [01:33<00:00, 93.28s/it]
 19%|█▊        | 970/5198 [9:48:53<113:41:01, 96.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.41s/it][A100%|██████████| 1/1 [01:33<00:00, 93.41s/it]
 19%|█▊        | 970/5198 [9:47:17<113:42:25, 96.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.41s/it][A100%|██████████| 1/1 [01:33<00:00, 93.41s/it]
 19%|█▊        | 970/5198 [9:48:12<113:42:46, 96.82s/it]
100%|██████████| 1/1 [01:33<00:00, 93.47s/it][A100%|██████████| 1/1 [01:33<00:00, 93.47s/it]
 19%|█▊        | 970/5198 [9:47:40<113:43:38, 96.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.41s/it][A100%|██████████| 1/1 [01:33<00:00, 93.41s/it]
 19%|█▊        | 970/5198 [9:48:14<113:43:00, 96.83s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_910
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.14s/it][A100%|██████████| 1/1 [01:42<00:00, 102.14s/it]
 19%|█▊        | 971/5198 [9:50:14<115:33:17, 98.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:40:10,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=959, skipped=0, lr=[1.902177298526939e-05], mom=[(0.9, 0.999)]
steps: 959 loss: 0.5458 iter time (s): 101.385 samples/sec: 1.263

100%|██████████| 1/1 [01:42<00:00, 102.27s/it][A100%|██████████| 1/1 [01:42<00:00, 102.27s/it]
 19%|█▊        | 971/5198 [9:50:29<115:36:47, 98.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.31s/it][A100%|██████████| 1/1 [01:42<00:00, 102.31s/it]
 19%|█▊        | 971/5198 [9:50:16<115:37:04, 98.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.33s/it][A100%|██████████| 1/1 [01:42<00:00, 102.33s/it]
 19%|█▊        | 971/5198 [9:50:35<115:36:33, 98.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.32s/it][A100%|██████████| 1/1 [01:42<00:00, 102.32s/it]
 19%|█▊        | 971/5198 [9:48:59<115:37:17, 98.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.31s/it][A100%|██████████| 1/1 [01:42<00:00, 102.31s/it]
 19%|█▊        | 971/5198 [9:49:23<115:37:55, 98.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.34s/it][A100%|██████████| 1/1 [01:42<00:00, 102.34s/it]
 19%|█▊        | 971/5198 [9:49:54<115:37:56, 98.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.33s/it][A100%|██████████| 1/1 [01:42<00:00, 102.33s/it]
 19%|█▊        | 971/5198 [9:49:56<115:37:52, 98.48s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_911
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.24s/it][A100%|██████████| 1/1 [01:46<00:00, 106.24s/it]
 19%|█▊        | 972/5198 [9:52:01<118:18:41, 100.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:41:56,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[1.9019328352655004e-05], mom=[(0.9, 0.999)]
steps: 960 loss: 0.5518 iter time (s): 105.480 samples/sec: 1.214

100%|██████████| 1/1 [01:46<00:00, 106.46s/it][A100%|██████████| 1/1 [01:46<00:00, 106.46s/it]
 19%|█▊        | 972/5198 [9:52:16<118:24:25, 100.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.40s/it][A100%|██████████| 1/1 [01:46<00:00, 106.40s/it]
 19%|█▊        | 972/5198 [9:52:02<118:23:17, 100.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.55s/it][A100%|██████████| 1/1 [01:46<00:00, 106.55s/it]
 19%|█▊        | 972/5198 [9:52:22<118:25:56, 100.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.43s/it][A100%|██████████| 1/1 [01:46<00:00, 106.43s/it]
 19%|█▊        | 972/5198 [9:50:46<118:23:53, 100.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.41s/it][A100%|██████████| 1/1 [01:46<00:00, 106.41s/it]
 19%|█▊        | 972/5198 [9:51:40<118:24:06, 100.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.45s/it][A100%|██████████| 1/1 [01:46<00:00, 106.45s/it]
 19%|█▊        | 972/5198 [9:51:09<118:24:57, 100.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.42s/it][A100%|██████████| 1/1 [01:46<00:00, 106.42s/it]
 19%|█▊        | 972/5198 [9:51:42<118:24:12, 100.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_912
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.60s/it][A100%|██████████| 1/1 [01:24<00:00, 84.60s/it]
 19%|█▊        | 973/5198 [9:53:25<112:36:08, 95.95s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:43:20,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=961, skipped=0, lr=[1.9016880826737024e-05], mom=[(0.9, 0.999)]
steps: 961 loss: 0.5483 iter time (s): 83.042 samples/sec: 1.541

100%|██████████| 1/1 [01:23<00:00, 83.98s/it][A100%|██████████| 1/1 [01:23<00:00, 83.98s/it]
 19%|█▊        | 973/5198 [9:53:40<112:26:18, 95.81s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.01s/it][A100%|██████████| 1/1 [01:24<00:00, 84.01s/it]
 19%|█▊        | 973/5198 [9:53:26<112:26:04, 95.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.86s/it][A100%|██████████| 1/1 [01:23<00:00, 83.86s/it]
 19%|█▊        | 973/5198 [9:53:46<112:24:44, 95.78s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.94s/it][A100%|██████████| 1/1 [01:23<00:00, 83.94s/it]
 19%|█▊        | 973/5198 [9:52:10<112:24:54, 95.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.87s/it][A100%|██████████| 1/1 [01:23<00:00, 83.87s/it]
 19%|█▊        | 973/5198 [9:52:33<112:24:11, 95.78s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.89s/it][A100%|██████████| 1/1 [01:23<00:00, 83.89s/it]
 19%|█▊        | 973/5198 [9:53:04<112:24:06, 95.77s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.90s/it][A100%|██████████| 1/1 [01:23<00:00, 83.90s/it]
 19%|█▊        | 973/5198 [9:53:06<112:24:12, 95.78s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_913
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.62s/it][A100%|██████████| 1/1 [01:53<00:00, 113.62s/it]
 19%|█▊        | 974/5198 [9:55:19<118:48:54, 101.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:45:15,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=962, skipped=0, lr=[1.901443040830067e-05], mom=[(0.9, 0.999)]
steps: 962 loss: 0.5284 iter time (s): 113.711 samples/sec: 1.126

100%|██████████| 1/1 [01:54<00:00, 114.50s/it][A100%|██████████| 1/1 [01:54<00:00, 114.50s/it]
 19%|█▊        | 974/5198 [9:55:34<118:59:44, 101.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.58s/it][A100%|██████████| 1/1 [01:54<00:00, 114.58s/it]
 19%|█▊        | 974/5198 [9:55:21<119:01:18, 101.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.64s/it][A100%|██████████| 1/1 [01:54<00:00, 114.64s/it]
 19%|█▊        | 974/5198 [9:55:40<119:01:39, 101.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.60s/it][A100%|██████████| 1/1 [01:54<00:00, 114.60s/it]
 19%|█▊        | 974/5198 [9:54:04<119:00:50, 101.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.58s/it][A100%|██████████| 1/1 [01:54<00:00, 114.58s/it]
 19%|█▊        | 974/5198 [9:54:28<118:59:53, 101.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.58s/it][A100%|██████████| 1/1 [01:54<00:00, 114.58s/it]
 19%|█▊        | 974/5198 [9:54:59<118:59:55, 101.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.58s/it][A100%|██████████| 1/1 [01:54<00:00, 114.58s/it]
 19%|█▊        | 974/5198 [9:55:01<118:59:49, 101.42s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_914
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.70s/it][A100%|██████████| 1/1 [01:51<00:00, 111.70s/it]
 19%|█▉        | 975/5198 [9:57:11<122:28:36, 104.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:47:07,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=963, skipped=0, lr=[1.90119770981321e-05], mom=[(0.9, 0.999)]
steps: 963 loss: 0.5888 iter time (s): 110.765 samples/sec: 1.156

100%|██████████| 1/1 [01:51<00:00, 111.65s/it][A100%|██████████| 1/1 [01:51<00:00, 111.65s/it]
 19%|█▉        | 975/5198 [9:57:26<122:34:16, 104.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.66s/it][A100%|██████████| 1/1 [01:51<00:00, 111.66s/it]
 19%|█▉        | 975/5198 [9:57:13<122:35:40, 104.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.66s/it][A100%|██████████| 1/1 [01:51<00:00, 111.66s/it]
 19%|█▉        | 975/5198 [9:57:32<122:35:51, 104.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.80s/it][A100%|██████████| 1/1 [01:51<00:00, 111.80s/it]
 19%|█▉        | 975/5198 [9:55:56<122:38:18, 104.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.76s/it][A100%|██████████| 1/1 [01:51<00:00, 111.76s/it]
 19%|█▉        | 975/5198 [9:56:19<122:36:40, 104.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.77s/it][A100%|██████████| 1/1 [01:51<00:00, 111.77s/it]
 19%|█▉        | 975/5198 [9:56:51<122:36:57, 104.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.77s/it][A100%|██████████| 1/1 [01:51<00:00, 111.77s/it]
 19%|█▉        | 975/5198 [9:56:53<122:36:52, 104.53s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_60
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.88s/it][A100%|██████████| 1/1 [01:59<00:00, 119.88s/it]
 19%|█▉        | 976/5198 [9:59:11<127:54:46, 109.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:49:07,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=964, skipped=0, lr=[1.9009520897018404e-05], mom=[(0.9, 0.999)]
steps: 964 loss: 0.8127 iter time (s): 119.223 samples/sec: 1.074

100%|██████████| 1/1 [02:00<00:00, 120.22s/it][A100%|██████████| 1/1 [02:00<00:00, 120.22s/it]
 19%|█▉        | 976/5198 [9:59:26<128:04:48, 109.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.07s/it][A100%|██████████| 1/1 [02:00<00:00, 120.07s/it]
 19%|█▉        | 976/5198 [9:59:13<128:02:39, 109.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.18s/it][A100%|██████████| 1/1 [02:00<00:00, 120.18s/it]
 19%|█▉        | 976/5198 [9:59:32<128:05:03, 109.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.04s/it][A100%|██████████| 1/1 [02:00<00:00, 120.04s/it]
 19%|█▉        | 976/5198 [9:57:56<128:03:46, 109.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.10s/it][A100%|██████████| 1/1 [02:00<00:00, 120.10s/it]
 19%|█▉        | 976/5198 [9:58:20<128:03:56, 109.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.10s/it][A100%|██████████| 1/1 [02:00<00:00, 120.10s/it]
 19%|█▉        | 976/5198 [9:58:53<128:03:56, 109.20s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_915
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.14s/it][A100%|██████████| 1/1 [02:00<00:00, 120.14s/it]
 19%|█▉        | 976/5198 [9:58:51<128:05:03, 109.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.02s/it][A100%|██████████| 1/1 [01:34<00:00, 94.02s/it]
 19%|█▉        | 977/5198 [10:00:45<122:37:11, 104.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:50:40,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=965, skipped=0, lr=[1.900706180574759e-05], mom=[(0.9, 0.999)]
steps: 965 loss: 0.5516 iter time (s): 92.401 samples/sec: 1.385

100%|██████████| 1/1 [01:33<00:00, 93.42s/it][A100%|██████████| 1/1 [01:33<00:00, 93.43s/it]
 19%|█▉        | 977/5198 [10:00:59<122:30:08, 104.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.41s/it][A100%|██████████| 1/1 [01:33<00:00, 93.41s/it]
 19%|█▉        | 977/5198 [10:00:46<122:28:09, 104.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.33s/it][A100%|██████████| 1/1 [01:33<00:00, 93.33s/it]
 19%|█▉        | 977/5198 [10:01:06<122:28:14, 104.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.49s/it][A100%|██████████| 1/1 [01:33<00:00, 93.49s/it]
 19%|█▉        | 977/5198 [9:59:30<122:30:46, 104.49s/it]
100%|██████████| 1/1 [01:33<00:00, 93.41s/it][A100%|██████████| 1/1 [01:33<00:00, 93.41s/it]
 19%|█▉        | 977/5198 [9:59:53<122:29:06, 104.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.38s/it][A100%|██████████| 1/1 [01:33<00:00, 93.38s/it]
 19%|█▉        | 977/5198 [10:00:24<122:29:10, 104.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.42s/it][A100%|██████████| 1/1 [01:33<00:00, 93.42s/it]
 19%|█▉        | 977/5198 [10:00:26<122:29:15, 104.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_916
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.01s/it][A100%|██████████| 1/1 [02:04<00:00, 124.01s/it]
 19%|█▉        | 978/5198 [10:02:49<129:26:45, 110.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:52:45,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=966, skipped=0, lr=[1.9004599825108585e-05], mom=[(0.9, 0.999)]
steps: 966 loss: 0.5485 iter time (s): 123.915 samples/sec: 1.033

100%|██████████| 1/1 [02:04<00:00, 124.90s/it][A100%|██████████| 1/1 [02:04<00:00, 124.90s/it]
 19%|█▉        | 978/5198 [10:03:04<129:39:31, 110.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.88s/it][A100%|██████████| 1/1 [02:04<00:00, 124.88s/it]
 19%|█▉        | 978/5198 [10:02:51<129:37:49, 110.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.89s/it][A100%|██████████| 1/1 [02:04<00:00, 124.89s/it]
 19%|█▉        | 978/5198 [10:03:10<129:37:53, 110.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.72s/it][A100%|██████████| 1/1 [02:04<00:00, 124.72s/it]
 19%|█▉        | 978/5198 [10:01:35<129:36:04, 110.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.83s/it][A100%|██████████| 1/1 [02:04<00:00, 124.83s/it]
 19%|█▉        | 978/5198 [10:01:58<129:37:17, 110.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.84s/it][A100%|██████████| 1/1 [02:04<00:00, 124.84s/it]
 19%|█▉        | 978/5198 [10:02:29<129:37:34, 110.58s/it]
100%|██████████| 1/1 [02:04<00:00, 124.83s/it][A100%|██████████| 1/1 [02:04<00:00, 124.83s/it]
 19%|█▉        | 978/5198 [10:02:31<129:37:17, 110.58s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_917

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.20s/it][A100%|██████████| 1/1 [01:48<00:00, 108.20s/it]
 19%|█▉        | 979/5198 [10:04:37<128:39:01, 109.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 09:54:33,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=967, skipped=0, lr=[1.9002134955891262e-05], mom=[(0.9, 0.999)]
steps: 967 loss: 0.5620 iter time (s): 106.905 samples/sec: 1.197

100%|██████████| 1/1 [01:47<00:00, 107.70s/it][A100%|██████████| 1/1 [01:47<00:00, 107.70s/it]
 19%|█▉        | 979/5198 [10:04:52<128:36:33, 109.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.87s/it][A100%|██████████| 1/1 [01:47<00:00, 107.87s/it]
 19%|█▉        | 979/5198 [10:04:39<128:39:07, 109.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.79s/it][A100%|██████████| 1/1 [01:47<00:00, 107.79s/it]
 19%|█▉        | 979/5198 [10:04:58<128:37:15, 109.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.81s/it][A100%|██████████| 1/1 [01:47<00:00, 107.81s/it]
 19%|█▉        | 979/5198 [10:03:22<128:36:28, 109.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.85s/it][A100%|██████████| 1/1 [01:47<00:00, 107.85s/it]
 19%|█▉        | 979/5198 [10:03:46<128:38:06, 109.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.85s/it][A100%|██████████| 1/1 [01:47<00:00, 107.85s/it]
 19%|█▉        | 979/5198 [10:04:19<128:37:56, 109.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_918

100%|██████████| 1/1 [01:47<00:00, 107.85s/it][A100%|██████████| 1/1 [01:47<00:00, 107.85s/it]
 19%|█▉        | 979/5198 [10:04:17<128:38:22, 109.77s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.01s/it][A100%|██████████| 1/1 [01:27<00:00, 87.01s/it]
[2024-06-30 09:55:59,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=968, skipped=0, lr=[1.8999667198886413e-05], mom=[(0.9, 0.999)]
steps: 968 loss: 0.5517 iter time (s): 85.514 samples/sec: 1.497

100%|██████████| 1/1 [01:26<00:00, 86.43s/it][A100%|██████████| 1/1 [01:26<00:00, 86.43s/it]

100%|██████████| 1/1 [01:26<00:00, 86.31s/it][A100%|██████████| 1/1 [01:26<00:00, 86.31s/it]

100%|██████████| 1/1 [01:26<00:00, 86.47s/it][A100%|██████████| 1/1 [01:26<00:00, 86.47s/it]

100%|██████████| 1/1 [01:26<00:00, 86.45s/it][A100%|██████████| 1/1 [01:26<00:00, 86.45s/it]

100%|██████████| 1/1 [01:26<00:00, 86.38s/it][A100%|██████████| 1/1 [01:26<00:00, 86.38s/it]

100%|██████████| 1/1 [01:26<00:00, 86.38s/it][A100%|██████████| 1/1 [01:26<00:00, 86.38s/it]

100%|██████████| 1/1 [01:26<00:00, 86.41s/it][A100%|██████████| 1/1 [01:26<00:00, 86.41s/it]
Checkpointing at shard 979
[2024-06-30 09:56:00,740] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step968 is about to be saved!
[2024-06-30 09:56:02,602] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_00-model_states.pt...
[2024-06-30 09:56:06,071] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_02-model_states.pt...
[2024-06-30 09:56:09,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_00-model_states.pt.
[2024-06-30 09:56:14,296] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_03-model_states.pt...
[2024-06-30 09:56:14,386] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_04-model_states.pt...
[2024-06-30 09:56:16,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_06-model_states.pt...
[2024-06-30 09:56:16,922] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_08-model_states.pt...
[2024-06-30 09:56:16,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_05-model_states.pt...
[2024-06-30 09:56:17,396] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_07-model_states.pt...
[2024-06-30 09:56:17,914] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_01-model_states.pt...
[2024-06-30 09:59:24,397] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_02-model_states.pt.
[2024-06-30 09:59:24,411] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_05-model_states.pt.
[2024-06-30 09:59:24,591] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_01_model_states.pt
[2024-06-30 09:59:24,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_01_model_states.pt...
[2024-06-30 09:59:24,852] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_01_model_states.pt.
[2024-06-30 09:59:24,852] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
[2024-06-30 09:59:25,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_07-model_states.pt.
[2024-06-30 09:59:25,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_04_model_states.pt...
[2024-06-30 09:59:25,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_04_model_states.pt.
[2024-06-30 09:59:25,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
[2024-06-30 09:59:25,985] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_06_model_states.pt...
[2024-06-30 09:59:26,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_08-model_states.pt.
[2024-06-30 09:59:26,121] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_06_model_states.pt.
[2024-06-30 09:59:26,121] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
[2024-06-30 09:59:27,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_09-model_states.pt...
[2024-06-30 09:59:28,800] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_09-model_states.pt.
[2024-06-30 09:59:28,837] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_07_model_states.pt...
[2024-06-30 09:59:28,948] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_07_model_states.pt.
[2024-06-30 09:59:28,948] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
[2024-06-30 09:59:29,396] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_06-model_states.pt.
[2024-06-30 09:59:30,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_05_model_states.pt...
[2024-06-30 09:59:30,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_05_model_states.pt.
[2024-06-30 09:59:30,428] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
[2024-06-30 09:59:41,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_04-model_states.pt.
[2024-06-30 09:59:41,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_03_model_states.pt...
[2024-06-30 09:59:41,989] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_03_model_states.pt.
[2024-06-30 09:59:41,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
[2024-06-30 09:59:47,926] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_03-model_states.pt.
[2024-06-30 09:59:48,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_02_model_states.pt...
[2024-06-30 09:59:49,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_02_model_states.pt.
[2024-06-30 09:59:49,299] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
[2024-06-30 10:00:00,366] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/layer_01-model_states.pt.
[2024-06-30 10:00:01,196] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_00_model_states.pt
[2024-06-30 10:00:01,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_00_model_states.pt...
[2024-06-30 10:00:01,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step968/mp_rank_00_model_states.pt.
[2024-06-30 10:00:01,456] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step968 is ready now!
Checkpoint saved using --- 240.72242403030396 seconds ---
 19%|█▉        | 980/5198 [10:08:50<205:03:53, 175.02s/it] 19%|█▉        | 980/5198 [10:09:46<205:00:42, 174.97s/it] 19%|█▉        | 980/5198 [10:09:13<205:01:47, 174.99s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_919
 19%|█▉        | 980/5198 [10:10:06<205:08:36, 175.09s/it] 19%|█▉        | 980/5198 [10:09:44<205:00:52, 174.98s/it] 19%|█▉        | 980/5198 [10:10:20<205:11:11, 175.12s/it] 19%|█▉        | 980/5198 [10:10:26<205:05:40, 175.05s/it] 19%|█▉        | 980/5198 [10:10:08<206:14:42, 176.03s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.04s/it][A100%|██████████| 1/1 [01:41<00:00, 101.04s/it]
 19%|█▉        | 981/5198 [10:11:49<179:53:10, 153.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:01:45,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=969, skipped=0, lr=[1.899719655488575e-05], mom=[(0.9, 0.999)]
steps: 969 loss: 0.5749 iter time (s): 103.530 samples/sec: 1.236

100%|██████████| 1/1 [01:43<00:00, 103.87s/it][A100%|██████████| 1/1 [01:43<00:00, 103.87s/it]
 19%|█▉        | 981/5198 [10:12:04<180:08:19, 153.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.99s/it][A100%|██████████| 1/1 [01:43<00:00, 103.99s/it]
 19%|█▉        | 981/5198 [10:11:50<180:09:10, 153.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.16s/it][A100%|██████████| 1/1 [01:44<00:00, 104.16s/it]
 19%|█▉        | 981/5198 [10:12:10<180:10:40, 153.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.26s/it][A100%|██████████| 1/1 [01:44<00:00, 104.26s/it]
 19%|█▉        | 981/5198 [10:10:34<180:11:27, 153.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.36s/it][A100%|██████████| 1/1 [01:44<00:00, 104.36s/it]
 19%|█▉        | 981/5198 [10:10:57<180:12:11, 153.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.41s/it][A100%|██████████| 1/1 [01:44<00:00, 104.41s/it]
 19%|█▉        | 981/5198 [10:11:28<180:12:34, 153.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.43s/it][A100%|██████████| 1/1 [01:44<00:00, 104.43s/it]
 19%|█▉        | 981/5198 [10:11:30<180:12:53, 153.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_920
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.84s/it][A100%|██████████| 1/1 [01:34<00:00, 94.84s/it]
 19%|█▉        | 982/5198 [10:13:24<159:13:51, 135.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:03:19,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[1.8994723024681926e-05], mom=[(0.9, 0.999)]
steps: 970 loss: 0.6080 iter time (s): 93.693 samples/sec: 1.366

100%|██████████| 1/1 [01:34<00:00, 94.64s/it][A100%|██████████| 1/1 [01:34<00:00, 94.64s/it]
 19%|█▉        | 982/5198 [10:13:38<159:19:13, 136.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.72s/it][A100%|██████████| 1/1 [01:34<00:00, 94.72s/it]
 19%|█▉        | 982/5198 [10:13:25<159:21:35, 136.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.69s/it][A100%|██████████| 1/1 [01:34<00:00, 94.69s/it]
 19%|█▉        | 982/5198 [10:13:45<159:21:59, 136.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.68s/it][A100%|██████████| 1/1 [01:34<00:00, 94.68s/it]
 19%|█▉        | 982/5198 [10:12:09<159:22:15, 136.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.64s/it][A100%|██████████| 1/1 [01:34<00:00, 94.64s/it]
 19%|█▉        | 982/5198 [10:12:32<159:21:58, 136.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.63s/it][A100%|██████████| 1/1 [01:34<00:00, 94.63s/it]
 19%|█▉        | 982/5198 [10:13:03<159:21:52, 136.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.64s/it][A100%|██████████| 1/1 [01:34<00:00, 94.64s/it]
 19%|█▉        | 982/5198 [10:13:05<159:22:26, 136.09s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_921
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.41s/it][A100%|██████████| 1/1 [01:21<00:00, 81.41s/it]
 19%|█▉        | 983/5198 [10:14:45<140:03:36, 119.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:04:40,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=971, skipped=0, lr=[1.89922466090685e-05], mom=[(0.9, 0.999)]
steps: 971 loss: 0.5904 iter time (s): 80.157 samples/sec: 1.597

100%|██████████| 1/1 [01:21<00:00, 81.19s/it][A100%|██████████| 1/1 [01:21<00:00, 81.19s/it]
 19%|█▉        | 983/5198 [10:15:00<140:01:12, 119.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.03s/it][A100%|██████████| 1/1 [01:21<00:00, 81.03s/it]
 19%|█▉        | 983/5198 [10:14:46<139:59:17, 119.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.98s/it][A100%|██████████| 1/1 [01:20<00:00, 80.98s/it]
 19%|█▉        | 983/5198 [10:15:06<139:58:41, 119.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.00s/it][A100%|██████████| 1/1 [01:21<00:00, 81.01s/it]
 19%|█▉        | 983/5198 [10:13:30<139:59:19, 119.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.04s/it][A100%|██████████| 1/1 [01:21<00:00, 81.04s/it]
 19%|█▉        | 983/5198 [10:13:53<139:59:54, 119.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.04s/it][A100%|██████████| 1/1 [01:21<00:00, 81.05s/it]
 19%|█▉        | 983/5198 [10:14:24<139:59:55, 119.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.03s/it][A100%|██████████| 1/1 [01:21<00:00, 81.03s/it]
 19%|█▉        | 983/5198 [10:14:26<139:59:56, 119.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_922
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.44s/it][A100%|██████████| 1/1 [01:22<00:00, 82.44s/it]
 19%|█▉        | 984/5198 [10:16:08<126:59:16, 108.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:06:03,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=972, skipped=0, lr=[1.8989767308839978e-05], mom=[(0.9, 0.999)]
steps: 972 loss: 0.5802 iter time (s): 81.658 samples/sec: 1.568

100%|██████████| 1/1 [01:22<00:00, 82.40s/it][A100%|██████████| 1/1 [01:22<00:00, 82.41s/it]
 19%|█▉        | 984/5198 [10:16:22<126:55:53, 108.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.45s/it][A100%|██████████| 1/1 [01:22<00:00, 82.45s/it]
 19%|█▉        | 984/5198 [10:16:09<126:55:32, 108.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.54s/it][A100%|██████████| 1/1 [01:22<00:00, 82.55s/it]
 19%|█▉        | 984/5198 [10:16:28<126:57:10, 108.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.50s/it][A100%|██████████| 1/1 [01:22<00:00, 82.50s/it]
 19%|█▉        | 984/5198 [10:14:52<126:56:33, 108.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.52s/it][A100%|██████████| 1/1 [01:22<00:00, 82.52s/it]
 19%|█▉        | 984/5198 [10:15:47<126:57:24, 108.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.62s/it][A100%|██████████| 1/1 [01:22<00:00, 82.62s/it]
 19%|█▉        | 984/5198 [10:15:16<126:59:26, 108.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.60s/it][A100%|██████████| 1/1 [01:22<00:00, 82.60s/it]
 19%|█▉        | 984/5198 [10:15:49<126:59:08, 108.48s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_923
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.78s/it][A100%|██████████| 1/1 [01:32<00:00, 92.79s/it]
 19%|█▉        | 985/5198 [10:17:41<121:28:08, 103.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:07:36,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=973, skipped=0, lr=[1.8987285124791774e-05], mom=[(0.9, 0.999)]
steps: 973 loss: 0.5834 iter time (s): 92.158 samples/sec: 1.389

100%|██████████| 1/1 [01:33<00:00, 93.17s/it][A100%|██████████| 1/1 [01:33<00:00, 93.17s/it]
 19%|█▉        | 985/5198 [10:17:55<121:32:49, 103.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.12s/it][A100%|██████████| 1/1 [01:33<00:00, 93.12s/it]
 19%|█▉        | 985/5198 [10:17:42<121:31:29, 103.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.10s/it][A100%|██████████| 1/1 [01:33<00:00, 93.11s/it]
 19%|█▉        | 985/5198 [10:18:01<121:32:09, 103.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.18s/it][A100%|██████████| 1/1 [01:33<00:00, 93.19s/it]
 19%|█▉        | 985/5198 [10:16:25<121:33:26, 103.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.00s/it][A100%|██████████| 1/1 [01:33<00:00, 93.00s/it]
 19%|█▉        | 985/5198 [10:16:49<121:31:39, 103.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.12s/it][A100%|██████████| 1/1 [01:33<00:00, 93.12s/it]
 19%|█▉        | 985/5198 [10:17:20<121:32:44, 103.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.05s/it][A100%|██████████| 1/1 [01:33<00:00, 93.05s/it]
 19%|█▉        | 985/5198 [10:17:22<121:32:27, 103.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_924
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.74s/it][A100%|██████████| 1/1 [01:23<00:00, 83.74s/it]
 19%|█▉        | 986/5198 [10:19:04<114:24:36, 97.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:08:59,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=974, skipped=0, lr=[1.8984800057720235e-05], mom=[(0.9, 0.999)]
steps: 974 loss: 0.5457 iter time (s): 82.598 samples/sec: 1.550

100%|██████████| 1/1 [01:23<00:00, 83.47s/it][A100%|██████████| 1/1 [01:23<00:00, 83.47s/it]
 19%|█▉        | 986/5198 [10:19:19<114:21:55, 97.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.53s/it][A100%|██████████| 1/1 [01:23<00:00, 83.53s/it]
 19%|█▉        | 986/5198 [10:19:05<114:22:08, 97.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.53s/it][A100%|██████████| 1/1 [01:23<00:00, 83.53s/it]
 19%|█▉        | 986/5198 [10:19:25<114:22:36, 97.76s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.51s/it][A100%|██████████| 1/1 [01:23<00:00, 83.51s/it]
 19%|█▉        | 986/5198 [10:17:49<114:23:07, 97.77s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.51s/it][A100%|██████████| 1/1 [01:23<00:00, 83.51s/it]
 19%|█▉        | 986/5198 [10:18:12<114:21:46, 97.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.49s/it][A100%|██████████| 1/1 [01:23<00:00, 83.49s/it]
 19%|█▉        | 986/5198 [10:18:43<114:22:15, 97.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.49s/it][A100%|██████████| 1/1 [01:23<00:00, 83.49s/it]
 19%|█▉        | 986/5198 [10:18:45<114:21:50, 97.75s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_925
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.25s/it][A100%|██████████| 1/1 [01:49<00:00, 109.25s/it]
 19%|█▉        | 987/5198 [10:20:54<118:25:04, 101.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:10:50,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=975, skipped=0, lr=[1.8982312108422632e-05], mom=[(0.9, 0.999)]
steps: 975 loss: 0.5869 iter time (s): 109.129 samples/sec: 1.173

100%|██████████| 1/1 [01:50<00:00, 110.03s/it][A100%|██████████| 1/1 [01:50<00:00, 110.03s/it]
 19%|█▉        | 987/5198 [10:21:09<118:39:01, 101.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.02s/it][A100%|██████████| 1/1 [01:50<00:00, 110.02s/it]
 19%|█▉        | 987/5198 [10:20:55<118:39:12, 101.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.04s/it][A100%|██████████| 1/1 [01:50<00:00, 110.05s/it]
 19%|█▉        | 987/5198 [10:21:15<118:40:04, 101.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.05s/it][A100%|██████████| 1/1 [01:50<00:00, 110.05s/it]
 19%|█▉        | 987/5198 [10:19:39<118:40:13, 101.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.04s/it][A100%|██████████| 1/1 [01:50<00:00, 110.04s/it]
 19%|█▉        | 987/5198 [10:20:02<118:39:13, 101.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.08s/it][A100%|██████████| 1/1 [01:50<00:00, 110.08s/it]
 19%|█▉        | 987/5198 [10:20:33<118:40:21, 101.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.08s/it][A100%|██████████| 1/1 [01:50<00:00, 110.08s/it]
 19%|█▉        | 987/5198 [10:20:35<118:39:56, 101.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_926
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.11s/it][A100%|██████████| 1/1 [01:33<00:00, 93.11s/it]
 19%|█▉        | 988/5198 [10:22:27<115:33:39, 98.82s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:12:22,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=976, skipped=0, lr=[1.8979821277697165e-05], mom=[(0.9, 0.999)]
steps: 976 loss: 0.5688 iter time (s): 91.702 samples/sec: 1.396

100%|██████████| 1/1 [01:32<00:00, 92.61s/it][A100%|██████████| 1/1 [01:32<00:00, 92.61s/it]
 19%|█▉        | 988/5198 [10:22:41<115:31:50, 98.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.62s/it][A100%|██████████| 1/1 [01:32<00:00, 92.62s/it]
 19%|█▉        | 988/5198 [10:22:28<115:32:13, 98.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.61s/it][A100%|██████████| 1/1 [01:32<00:00, 92.61s/it]
 19%|█▉        | 988/5198 [10:22:48<115:32:28, 98.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.57s/it][A100%|██████████| 1/1 [01:32<00:00, 92.57s/it]
 19%|█▉        | 988/5198 [10:21:12<115:31:42, 98.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.66s/it][A100%|██████████| 1/1 [01:32<00:00, 92.66s/it]
 19%|█▉        | 988/5198 [10:21:35<115:32:52, 98.81s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.61s/it][A100%|██████████| 1/1 [01:32<00:00, 92.61s/it]
 19%|█▉        | 988/5198 [10:22:06<115:32:45, 98.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.62s/it][A100%|██████████| 1/1 [01:32<00:00, 92.62s/it]
 19%|█▉        | 988/5198 [10:22:08<115:32:37, 98.80s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_927
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.52s/it][A100%|██████████| 1/1 [01:27<00:00, 87.52s/it]
 19%|█▉        | 989/5198 [10:23:55<111:35:52, 95.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:13:50,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=977, skipped=0, lr=[1.897732756634295e-05], mom=[(0.9, 0.999)]
steps: 977 loss: 0.5898 iter time (s): 86.539 samples/sec: 1.479

100%|██████████| 1/1 [01:27<00:00, 87.46s/it][A100%|██████████| 1/1 [01:27<00:00, 87.47s/it]
 19%|█▉        | 989/5198 [10:24:09<111:32:00, 95.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.51s/it][A100%|██████████| 1/1 [01:27<00:00, 87.51s/it]
 19%|█▉        | 989/5198 [10:23:55<111:33:13, 95.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.43s/it][A100%|██████████| 1/1 [01:27<00:00, 87.43s/it]
 19%|█▉        | 989/5198 [10:24:15<111:31:37, 95.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.50s/it][A100%|██████████| 1/1 [01:27<00:00, 87.50s/it]
 19%|█▉        | 989/5198 [10:22:39<111:32:37, 95.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.41s/it][A100%|██████████| 1/1 [01:27<00:00, 87.41s/it]
 19%|█▉        | 989/5198 [10:23:02<111:31:39, 95.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.40s/it][A100%|██████████| 1/1 [01:27<00:00, 87.40s/it]
 19%|█▉        | 989/5198 [10:23:33<111:31:16, 95.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.42s/it][A100%|██████████| 1/1 [01:27<00:00, 87.42s/it]
 19%|█▉        | 989/5198 [10:23:35<111:31:31, 95.39s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_928
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.95s/it][A100%|██████████| 1/1 [01:28<00:00, 88.95s/it]
 19%|█▉        | 990/5198 [10:25:24<109:19:21, 93.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:15:19,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=978, skipped=0, lr=[1.8974830975160035e-05], mom=[(0.9, 0.999)]
steps: 978 loss: 0.5955 iter time (s): 88.215 samples/sec: 1.451

100%|██████████| 1/1 [01:29<00:00, 89.05s/it][A100%|██████████| 1/1 [01:29<00:00, 89.05s/it]
 19%|█▉        | 990/5198 [10:25:38<109:17:05, 93.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.10s/it][A100%|██████████| 1/1 [01:29<00:00, 89.10s/it]
 19%|█▉        | 990/5198 [10:25:25<109:18:56, 93.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.16s/it][A100%|██████████| 1/1 [01:29<00:00, 89.16s/it]
 19%|█▉        | 990/5198 [10:25:44<109:19:02, 93.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.14s/it][A100%|██████████| 1/1 [01:29<00:00, 89.14s/it]
 19%|█▉        | 990/5198 [10:24:08<109:19:19, 93.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.16s/it][A100%|██████████| 1/1 [01:29<00:00, 89.16s/it]
 19%|█▉        | 990/5198 [10:24:32<109:19:07, 93.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.18s/it][A100%|██████████| 1/1 [01:29<00:00, 89.18s/it]
 19%|█▉        | 990/5198 [10:25:03<109:19:16, 93.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.16s/it][A100%|██████████| 1/1 [01:29<00:00, 89.16s/it]
 19%|█▉        | 990/5198 [10:25:05<109:19:02, 93.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_929
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.54s/it][A100%|██████████| 1/1 [01:33<00:00, 93.54s/it]
 19%|█▉        | 991/5198 [10:26:57<109:19:27, 93.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:16:52,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=979, skipped=0, lr=[1.8972331504949387e-05], mom=[(0.9, 0.999)]
steps: 979 loss: 0.5623 iter time (s): 92.766 samples/sec: 1.380

100%|██████████| 1/1 [01:33<00:00, 93.66s/it][A100%|██████████| 1/1 [01:33<00:00, 93.66s/it]
 19%|█▉        | 991/5198 [10:26:58<109:20:24, 93.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.87s/it][A100%|██████████| 1/1 [01:33<00:00, 93.87s/it]
 19%|█▉        | 991/5198 [10:27:12<109:23:43, 93.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.76s/it][A100%|██████████| 1/1 [01:33<00:00, 93.76s/it]
 19%|█▉        | 991/5198 [10:27:18<109:22:40, 93.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.72s/it][A100%|██████████| 1/1 [01:33<00:00, 93.73s/it]
 19%|█▉        | 991/5198 [10:25:42<109:22:21, 93.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.72s/it][A100%|██████████| 1/1 [01:33<00:00, 93.72s/it]
 19%|█▉        | 991/5198 [10:26:05<109:21:57, 93.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.75s/it][A100%|██████████| 1/1 [01:33<00:00, 93.75s/it]
 19%|█▉        | 991/5198 [10:26:36<109:22:42, 93.60s/it]
100%|██████████| 1/1 [01:33<00:00, 93.74s/it][A100%|██████████| 1/1 [01:33<00:00, 93.74s/it]
 19%|█▉        | 991/5198 [10:26:38<109:22:14, 93.59s/it]Shard 991 in [76, 158, 182, 242, 293, 363, 418, 421, 664, 752, 814, 842, 991, 1266, 1366, 1425, 1464, 1574, 1728, 2166, 2441, 2563, 2739, 2854, 2894, 3089, 3181, 3395, 3576, 3831, 4300, 4589, 4947, 4950]: file /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_61 skipped to avoid exceeding cuda memory
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_930

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.53s/it][A100%|██████████| 1/1 [01:52<00:00, 112.53s/it]
 19%|█▉        | 993/5198 [10:28:50<89:11:08, 76.35s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:18:46,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[1.89698291565129e-05], mom=[(0.9, 0.999)]
steps: 980 loss: 0.5990 iter time (s): 112.160 samples/sec: 1.141

100%|██████████| 1/1 [01:53<00:00, 113.02s/it][A100%|██████████| 1/1 [01:53<00:00, 113.02s/it]
 19%|█▉        | 993/5198 [10:29:05<89:20:44, 76.49s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.14s/it][A100%|██████████| 1/1 [01:53<00:00, 113.14s/it]
 19%|█▉        | 993/5198 [10:28:51<89:20:47, 76.49s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.04s/it][A100%|██████████| 1/1 [01:53<00:00, 113.04s/it]
 19%|█▉        | 993/5198 [10:29:11<89:20:29, 76.49s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.05s/it][A100%|██████████| 1/1 [01:53<00:00, 113.05s/it]
 19%|█▉        | 993/5198 [10:27:35<89:20:27, 76.49s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.20s/it][A100%|██████████| 1/1 [01:53<00:00, 113.20s/it]
 19%|█▉        | 993/5198 [10:27:58<89:22:40, 76.52s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.14s/it][A100%|██████████| 1/1 [01:53<00:00, 113.14s/it]
 19%|█▉        | 993/5198 [10:28:31<89:21:43, 76.50s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_931
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.16s/it][A100%|██████████| 1/1 [01:53<00:00, 113.16s/it]
 19%|█▉        | 993/5198 [10:28:30<89:22:25, 76.52s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.97s/it][A100%|██████████| 1/1 [01:53<00:00, 113.98s/it]
 19%|█▉        | 994/5198 [10:30:44<100:05:02, 85.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:20:40,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=981, skipped=0, lr=[1.896732393065338e-05], mom=[(0.9, 0.999)]
steps: 981 loss: 0.6047 iter time (s): 113.160 samples/sec: 1.131

100%|██████████| 1/1 [01:54<00:00, 114.18s/it][A100%|██████████| 1/1 [01:54<00:00, 114.18s/it]
 19%|█▉        | 994/5198 [10:30:59<100:14:16, 85.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.20s/it][A100%|██████████| 1/1 [01:54<00:00, 114.20s/it]
 19%|█▉        | 994/5198 [10:30:46<100:14:38, 85.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.16s/it][A100%|██████████| 1/1 [01:54<00:00, 114.16s/it]
 19%|█▉        | 994/5198 [10:31:05<100:14:03, 85.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.14s/it][A100%|██████████| 1/1 [01:54<00:00, 114.14s/it]
 19%|█▉        | 994/5198 [10:29:29<100:13:28, 85.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.03s/it][A100%|██████████| 1/1 [01:54<00:00, 114.03s/it]
 19%|█▉        | 994/5198 [10:29:53<100:13:09, 85.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.07s/it][A100%|██████████| 1/1 [01:54<00:00, 114.07s/it]
 19%|█▉        | 994/5198 [10:30:24<100:13:39, 85.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.12s/it][A100%|██████████| 1/1 [01:54<00:00, 114.12s/it]
 19%|█▉        | 994/5198 [10:30:26<100:13:55, 85.83s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_932
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.94s/it][A100%|██████████| 1/1 [01:32<00:00, 92.94s/it]
 19%|█▉        | 995/5198 [10:32:17<102:17:55, 87.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:22:12,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=982, skipped=0, lr=[1.8964815828174578e-05], mom=[(0.9, 0.999)]
steps: 982 loss: 0.5291 iter time (s): 91.420 samples/sec: 1.400

100%|██████████| 1/1 [01:32<00:00, 92.42s/it][A100%|██████████| 1/1 [01:32<00:00, 92.42s/it]
 19%|█▉        | 995/5198 [10:32:31<102:13:45, 87.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.39s/it][A100%|██████████| 1/1 [01:32<00:00, 92.39s/it]
 19%|█▉        | 995/5198 [10:32:18<102:13:16, 87.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.38s/it][A100%|██████████| 1/1 [01:32<00:00, 92.38s/it]
 19%|█▉        | 995/5198 [10:32:38<102:12:44, 87.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.34s/it][A100%|██████████| 1/1 [01:32<00:00, 92.34s/it]
 19%|█▉        | 995/5198 [10:31:02<102:11:35, 87.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 19%|█▉        | 995/5198 [10:31:25<102:12:40, 87.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.35s/it][A100%|██████████| 1/1 [01:32<00:00, 92.35s/it]
 19%|█▉        | 995/5198 [10:31:56<102:11:53, 87.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.37s/it][A100%|██████████| 1/1 [01:32<00:00, 92.37s/it]
 19%|█▉        | 995/5198 [10:31:58<102:12:20, 87.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_933
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.68s/it][A100%|██████████| 1/1 [01:25<00:00, 85.68s/it]
 19%|█▉        | 996/5198 [10:33:43<101:40:36, 87.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:23:38,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=983, skipped=0, lr=[1.896230484988115e-05], mom=[(0.9, 0.999)]
steps: 983 loss: 0.5793 iter time (s): 84.591 samples/sec: 1.513

100%|██████████| 1/1 [01:25<00:00, 85.37s/it][A100%|██████████| 1/1 [01:25<00:00, 85.37s/it]
 19%|█▉        | 996/5198 [10:33:57<101:30:42, 86.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.53s/it][A100%|██████████| 1/1 [01:25<00:00, 85.53s/it]
 19%|█▉        | 996/5198 [10:33:44<101:33:28, 87.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.62s/it][A100%|██████████| 1/1 [01:25<00:00, 85.62s/it]
 19%|█▉        | 996/5198 [10:32:27<101:33:56, 87.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.66s/it][A100%|██████████| 1/1 [01:25<00:00, 85.67s/it]
 19%|█▉        | 996/5198 [10:34:03<101:35:45, 87.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.60s/it][A100%|██████████| 1/1 [01:25<00:00, 85.60s/it]
 19%|█▉        | 996/5198 [10:32:51<101:34:15, 87.02s/it]
100%|██████████| 1/1 [01:25<00:00, 85.59s/it][A100%|██████████| 1/1 [01:25<00:00, 85.59s/it]
 19%|█▉        | 996/5198 [10:33:22<101:33:32, 87.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.59s/it][A100%|██████████| 1/1 [01:25<00:00, 85.59s/it]
 19%|█▉        | 996/5198 [10:33:24<101:33:51, 87.01s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_934
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.75s/it][A100%|██████████| 1/1 [02:16<00:00, 136.75s/it]
 19%|█▉        | 997/5198 [10:35:59<117:52:54, 101.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:25:56,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=984, skipped=0, lr=[1.895979099657868e-05], mom=[(0.9, 0.999)]
steps: 984 loss: 0.5992 iter time (s): 137.283 samples/sec: 0.932

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
 19%|█▉        | 997/5198 [10:36:15<118:16:09, 101.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.26s/it][A100%|██████████| 1/1 [02:18<00:00, 138.26s/it]
 19%|█▉        | 997/5198 [10:36:02<118:16:22, 101.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.09s/it][A100%|██████████| 1/1 [02:18<00:00, 138.09s/it]
 19%|█▉        | 997/5198 [10:36:21<118:14:45, 101.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.18s/it][A100%|██████████| 1/1 [02:18<00:00, 138.18s/it]
 19%|█▉        | 997/5198 [10:34:45<118:15:10, 101.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.16s/it][A100%|██████████| 1/1 [02:18<00:00, 138.16s/it]
 19%|█▉        | 997/5198 [10:35:40<118:14:27, 101.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.17s/it][A100%|██████████| 1/1 [02:18<00:00, 138.17s/it]
 19%|█▉        | 997/5198 [10:35:09<118:15:12, 101.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.16s/it][A100%|██████████| 1/1 [02:18<00:00, 138.16s/it]
 19%|█▉        | 997/5198 [10:35:42<118:14:38, 101.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_935
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.43s/it][A100%|██████████| 1/1 [01:49<00:00, 109.43s/it]
 19%|█▉        | 998/5198 [10:37:49<120:40:51, 103.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:27:45,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=985, skipped=0, lr=[1.8957274269073674e-05], mom=[(0.9, 0.999)]
steps: 985 loss: 0.6038 iter time (s): 107.750 samples/sec: 1.188

100%|██████████| 1/1 [01:48<00:00, 108.62s/it][A100%|██████████| 1/1 [01:48<00:00, 108.62s/it]
 19%|█▉        | 998/5198 [10:38:04<120:39:57, 103.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.65s/it][A100%|██████████| 1/1 [01:48<00:00, 108.65s/it]
 19%|█▉        | 998/5198 [10:37:50<120:40:41, 103.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.61s/it][A100%|██████████| 1/1 [01:48<00:00, 108.61s/it]
 19%|█▉        | 998/5198 [10:38:10<120:38:51, 103.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.67s/it][A100%|██████████| 1/1 [01:48<00:00, 108.67s/it]
 19%|█▉        | 998/5198 [10:36:34<120:40:19, 103.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.60s/it][A100%|██████████| 1/1 [01:48<00:00, 108.60s/it]
 19%|█▉        | 998/5198 [10:36:57<120:38:51, 103.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.67s/it][A100%|██████████| 1/1 [01:48<00:00, 108.67s/it]
 19%|█▉        | 998/5198 [10:37:28<120:39:47, 103.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.66s/it][A100%|██████████| 1/1 [01:48<00:00, 108.66s/it]
 19%|█▉        | 998/5198 [10:37:30<120:39:45, 103.43s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_936
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.11s/it][A100%|██████████| 1/1 [01:31<00:00, 91.11s/it]
 19%|█▉        | 999/5198 [10:39:20<116:31:04, 99.90s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:29:15,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=986, skipped=0, lr=[1.8954754668173566e-05], mom=[(0.9, 0.999)]
steps: 986 loss: 0.5911 iter time (s): 89.826 samples/sec: 1.425

100%|██████████| 1/1 [01:30<00:00, 90.75s/it][A100%|██████████| 1/1 [01:30<00:00, 90.75s/it]
 19%|█▉        | 999/5198 [10:39:35<116:21:16, 99.76s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.70s/it][A100%|██████████| 1/1 [01:30<00:00, 90.70s/it]
 19%|█▉        | 999/5198 [10:39:21<116:20:42, 99.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.78s/it][A100%|██████████| 1/1 [01:30<00:00, 90.78s/it]
 19%|█▉        | 999/5198 [10:39:41<116:21:11, 99.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.73s/it][A100%|██████████| 1/1 [01:30<00:00, 90.73s/it]
 19%|█▉        | 999/5198 [10:38:05<116:21:11, 99.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.80s/it][A100%|██████████| 1/1 [01:30<00:00, 90.80s/it]
 19%|█▉        | 999/5198 [10:38:28<116:21:29, 99.76s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.79s/it][A100%|██████████| 1/1 [01:30<00:00, 90.79s/it]
 19%|█▉        | 999/5198 [10:38:59<116:21:56, 99.77s/it] 
100%|██████████| 1/1 [01:30<00:00, 90.77s/it][A100%|██████████| 1/1 [01:30<00:00, 90.77s/it]
 19%|█▉        | 999/5198 [10:39:01<116:21:28, 99.76s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_937

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.74s/it][A100%|██████████| 1/1 [01:23<00:00, 83.74s/it]
[2024-06-30 10:30:39,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=987, skipped=0, lr=[1.8952232194686696e-05], mom=[(0.9, 0.999)]
steps: 987 loss: 0.5677 iter time (s): 82.688 samples/sec: 1.548

100%|██████████| 1/1 [01:23<00:00, 83.65s/it][A100%|██████████| 1/1 [01:23<00:00, 83.65s/it]

100%|██████████| 1/1 [01:23<00:00, 83.62s/it][A100%|██████████| 1/1 [01:23<00:00, 83.62s/it]

100%|██████████| 1/1 [01:23<00:00, 83.61s/it][A100%|██████████| 1/1 [01:23<00:00, 83.61s/it]

100%|██████████| 1/1 [01:23<00:00, 83.63s/it][A100%|██████████| 1/1 [01:23<00:00, 83.63s/it]

100%|██████████| 1/1 [01:23<00:00, 83.70s/it][A100%|██████████| 1/1 [01:23<00:00, 83.70s/it]

100%|██████████| 1/1 [01:23<00:00, 83.65s/it][A100%|██████████| 1/1 [01:23<00:00, 83.65s/it]

100%|██████████| 1/1 [01:23<00:00, 83.67s/it][A100%|██████████| 1/1 [01:23<00:00, 83.67s/it]
Checkpointing at shard 999
[2024-06-30 10:30:40,437] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step987 is about to be saved!
[2024-06-30 10:30:42,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_00-model_states.pt...
[2024-06-30 10:30:45,505] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_02-model_states.pt...
[2024-06-30 10:30:47,384] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_00-model_states.pt.
[2024-06-30 10:30:53,995] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_04-model_states.pt...
[2024-06-30 10:30:53,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_03-model_states.pt...
[2024-06-30 10:30:54,605] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_08-model_states.pt...
[2024-06-30 10:30:55,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_07-model_states.pt...
[2024-06-30 10:30:56,041] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_06-model_states.pt...
[2024-06-30 10:30:56,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_01-model_states.pt...
[2024-06-30 10:30:56,430] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_05-model_states.pt...
[2024-06-30 10:33:48,051] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_02-model_states.pt.
[2024-06-30 10:33:48,190] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_01_model_states.pt
[2024-06-30 10:33:48,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_01_model_states.pt...
[2024-06-30 10:33:48,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_01_model_states.pt.
[2024-06-30 10:33:48,434] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
[2024-06-30 10:34:01,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_06-model_states.pt.
[2024-06-30 10:34:01,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_01-model_states.pt.
[2024-06-30 10:34:01,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_04-model_states.pt.
[2024-06-30 10:34:01,791] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_07-model_states.pt.
[2024-06-30 10:34:01,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_03-model_states.pt.
[2024-06-30 10:34:01,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_08-model_states.pt.
[2024-06-30 10:34:01,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_05-model_states.pt.
[2024-06-30 10:34:02,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_03_model_states.pt...
[2024-06-30 10:34:02,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_02_model_states.pt...
[2024-06-30 10:34:02,694] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_02_model_states.pt.
[2024-06-30 10:34:02,694] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
[2024-06-30 10:34:02,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_03_model_states.pt.
[2024-06-30 10:34:02,701] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
[2024-06-30 10:34:02,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_05_model_states.pt...
[2024-06-30 10:34:02,747] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_00_model_states.pt
[2024-06-30 10:34:02,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_00_model_states.pt...
[2024-06-30 10:34:02,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_05_model_states.pt.
[2024-06-30 10:34:02,758] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
[2024-06-30 10:34:02,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_06_model_states.pt...
[2024-06-30 10:34:03,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_04_model_states.pt...
[2024-06-30 10:34:03,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_06_model_states.pt.
[2024-06-30 10:34:03,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
[2024-06-30 10:34:03,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_00_model_states.pt.
[2024-06-30 10:34:03,075] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
[2024-06-30 10:34:03,085] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_04_model_states.pt.
[2024-06-30 10:34:03,085] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
[2024-06-30 10:34:03,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_09-model_states.pt...
[2024-06-30 10:34:03,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/layer_09-model_states.pt.
[2024-06-30 10:34:03,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_07_model_states.pt...
[2024-06-30 10:34:04,022] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step987/mp_rank_07_model_states.pt.
[2024-06-30 10:34:04,022] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step987 is ready now!
Checkpoint saved using --- 203.5869140625 seconds ---
 19%|█▉        | 1000/5198 [10:44:22<180:31:18, 154.81s/it] 19%|█▉        | 1000/5198 [10:44:09<180:28:30, 154.77s/it] 19%|█▉        | 1000/5198 [10:44:28<180:25:26, 154.72s/it] 19%|█▉        | 1000/5198 [10:42:52<180:23:55, 154.70s/it] 19%|█▉        | 1000/5198 [10:43:15<180:21:49, 154.67s/it] 19%|█▉        | 1000/5198 [10:44:10<181:28:42, 155.63s/it] 19%|█▉        | 1000/5198 [10:43:47<180:21:02, 154.66s/it] 19%|█▉        | 1000/5198 [10:43:48<180:20:39, 154.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_938
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.59s/it][A100%|██████████| 1/1 [01:34<00:00, 94.59s/it]
 19%|█▉        | 1001/5198 [10:45:45<160:29:14, 137.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:35:41,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=988, skipped=0, lr=[1.8949706849422348e-05], mom=[(0.9, 0.999)]
steps: 988 loss: 0.5386 iter time (s): 96.888 samples/sec: 1.321

100%|██████████| 1/1 [01:37<00:00, 97.24s/it][A100%|██████████| 1/1 [01:37<00:00, 97.24s/it]
 19%|█▉        | 1001/5198 [10:46:00<160:43:25, 137.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.36s/it][A100%|██████████| 1/1 [01:37<00:00, 97.36s/it]
 19%|█▉        | 1001/5198 [10:45:46<160:44:03, 137.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.61s/it][A100%|██████████| 1/1 [01:37<00:00, 97.61s/it]
 19%|█▉        | 1001/5198 [10:46:06<160:47:02, 137.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.67s/it][A100%|██████████| 1/1 [01:37<00:00, 97.67s/it]
 19%|█▉        | 1001/5198 [10:44:30<160:47:12, 137.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.71s/it][A100%|██████████| 1/1 [01:37<00:00, 97.72s/it]
 19%|█▉        | 1001/5198 [10:44:53<160:46:34, 137.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.77s/it][A100%|██████████| 1/1 [01:37<00:00, 97.77s/it]
 19%|█▉        | 1001/5198 [10:45:24<160:47:06, 137.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.78s/it][A100%|██████████| 1/1 [01:37<00:00, 97.78s/it]
 19%|█▉        | 1001/5198 [10:45:26<160:47:08, 137.91s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_939
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.52s/it][A100%|██████████| 1/1 [01:34<00:00, 94.52s/it]
 19%|█▉        | 1002/5198 [10:47:20<145:34:23, 124.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:37:15,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=989, skipped=0, lr=[1.8947178633190708e-05], mom=[(0.9, 0.999)]
steps: 989 loss: 0.5593 iter time (s): 93.582 samples/sec: 1.368

100%|██████████| 1/1 [01:34<00:00, 94.56s/it][A100%|██████████| 1/1 [01:34<00:00, 94.56s/it]
 19%|█▉        | 1002/5198 [10:47:34<145:43:40, 125.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.59s/it][A100%|██████████| 1/1 [01:34<00:00, 94.59s/it]
 19%|█▉        | 1002/5198 [10:47:21<145:44:45, 125.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.46s/it][A100%|██████████| 1/1 [01:34<00:00, 94.46s/it]
 19%|█▉        | 1002/5198 [10:47:40<145:44:23, 125.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.55s/it][A100%|██████████| 1/1 [01:34<00:00, 94.55s/it]
 19%|█▉        | 1002/5198 [10:46:05<145:46:21, 125.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.58s/it][A100%|██████████| 1/1 [01:34<00:00, 94.58s/it]
 19%|█▉        | 1002/5198 [10:46:28<145:46:25, 125.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.55s/it][A100%|██████████| 1/1 [01:34<00:00, 94.55s/it]
 19%|█▉        | 1002/5198 [10:46:59<145:46:03, 125.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.56s/it][A100%|██████████| 1/1 [01:34<00:00, 94.56s/it]
 19%|█▉        | 1002/5198 [10:47:01<145:46:20, 125.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_940
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.75s/it][A100%|██████████| 1/1 [01:23<00:00, 83.75s/it]
 19%|█▉        | 1003/5198 [10:48:44<131:18:39, 112.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:38:39,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[1.894464754680289e-05], mom=[(0.9, 0.999)]
steps: 990 loss: 0.5465 iter time (s): 82.553 samples/sec: 1.551

100%|██████████| 1/1 [01:23<00:00, 83.54s/it][A100%|██████████| 1/1 [01:23<00:00, 83.54s/it]
 19%|█▉        | 1003/5198 [10:48:58<131:18:55, 112.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.64s/it][A100%|██████████| 1/1 [01:23<00:00, 83.64s/it]
 19%|█▉        | 1003/5198 [10:48:45<131:21:39, 112.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.60s/it][A100%|██████████| 1/1 [01:23<00:00, 83.60s/it]
 19%|█▉        | 1003/5198 [10:49:04<131:20:33, 112.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.55s/it][A100%|██████████| 1/1 [01:23<00:00, 83.55s/it]
 19%|█▉        | 1003/5198 [10:47:28<131:20:59, 112.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.52s/it][A100%|██████████| 1/1 [01:23<00:00, 83.52s/it]
 19%|█▉        | 1003/5198 [10:47:51<131:20:21, 112.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.60s/it][A100%|██████████| 1/1 [01:23<00:00, 83.60s/it]
 19%|█▉        | 1003/5198 [10:48:23<131:21:41, 112.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.58s/it][A100%|██████████| 1/1 [01:23<00:00, 83.58s/it]
 19%|█▉        | 1003/5198 [10:48:25<131:21:32, 112.73s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_941
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:24<00:00, 144.37s/it][A100%|██████████| 1/1 [02:24<00:00, 144.37s/it]
 19%|█▉        | 1004/5198 [10:51:08<142:18:52, 122.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:41:05,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=991, skipped=0, lr=[1.8942113591070936e-05], mom=[(0.9, 0.999)]
steps: 991 loss: 0.5612 iter time (s): 145.050 samples/sec: 0.882

100%|██████████| 1/1 [02:26<00:00, 146.08s/it][A100%|██████████| 1/1 [02:26<00:00, 146.08s/it]
 19%|█▉        | 1004/5198 [10:51:24<142:53:11, 122.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:25<00:00, 145.88s/it][A100%|██████████| 1/1 [02:25<00:00, 145.88s/it]
 19%|█▉        | 1004/5198 [10:51:10<142:50:59, 122.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:25<00:00, 145.96s/it][A100%|██████████| 1/1 [02:25<00:00, 145.96s/it]
 19%|█▉        | 1004/5198 [10:51:30<142:52:08, 122.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:25<00:00, 145.94s/it][A100%|██████████| 1/1 [02:25<00:00, 145.94s/it]
 19%|█▉        | 1004/5198 [10:49:54<142:51:49, 122.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:25<00:00, 145.89s/it][A100%|██████████| 1/1 [02:25<00:00, 145.89s/it]
 19%|█▉        | 1004/5198 [10:50:48<142:51:11, 122.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:25<00:00, 146.00s/it][A100%|██████████| 1/1 [02:25<00:00, 146.00s/it]
 19%|█▉        | 1004/5198 [10:50:17<142:52:34, 122.64s/it]
100%|██████████| 1/1 [02:25<00:00, 145.90s/it][A100%|██████████| 1/1 [02:25<00:00, 145.90s/it]
 19%|█▉        | 1004/5198 [10:50:50<142:51:22, 122.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_942

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.56s/it][A100%|██████████| 1/1 [01:39<00:00, 99.56s/it]
 19%|█▉        | 1005/5198 [10:52:48<134:27:37, 115.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:42:43,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=992, skipped=0, lr=[1.8939576766807796e-05], mom=[(0.9, 0.999)]
steps: 992 loss: 0.6192 iter time (s): 97.630 samples/sec: 1.311

100%|██████████| 1/1 [01:38<00:00, 98.44s/it][A100%|██████████| 1/1 [01:38<00:00, 98.44s/it]
 19%|█▉        | 1005/5198 [10:53:02<134:25:54, 115.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.42s/it][A100%|██████████| 1/1 [01:38<00:00, 98.42s/it]
 19%|█▉        | 1005/5198 [10:52:49<134:24:01, 115.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.44s/it][A100%|██████████| 1/1 [01:38<00:00, 98.44s/it]
 19%|█▉        | 1005/5198 [10:53:08<134:25:09, 115.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.49s/it][A100%|██████████| 1/1 [01:38<00:00, 98.49s/it]
 19%|█▉        | 1005/5198 [10:51:33<134:25:55, 115.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.46s/it][A100%|██████████| 1/1 [01:38<00:00, 98.46s/it]
 19%|█▉        | 1005/5198 [10:51:56<134:25:50, 115.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.51s/it][A100%|██████████| 1/1 [01:38<00:00, 98.51s/it]
 19%|█▉        | 1005/5198 [10:52:27<134:25:50, 115.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.50s/it][A100%|██████████| 1/1 [01:38<00:00, 98.50s/it]
 19%|█▉        | 1005/5198 [10:52:29<134:25:48, 115.42s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_943
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.96s/it][A100%|██████████| 1/1 [01:39<00:00, 99.96s/it]
 19%|█▉        | 1006/5198 [10:54:28<129:03:59, 110.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:44:23,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=993, skipped=0, lr=[1.8937037074827344e-05], mom=[(0.9, 0.999)]
steps: 993 loss: 0.5962 iter time (s): 99.410 samples/sec: 1.288

100%|██████████| 1/1 [01:40<00:00, 100.20s/it][A100%|██████████| 1/1 [01:40<00:00, 100.20s/it]
 19%|█▉        | 1006/5198 [10:54:43<129:06:04, 110.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.34s/it][A100%|██████████| 1/1 [01:40<00:00, 100.34s/it]
 19%|█▉        | 1006/5198 [10:54:29<129:07:44, 110.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.34s/it][A100%|██████████| 1/1 [01:40<00:00, 100.34s/it]
 19%|█▉        | 1006/5198 [10:54:49<129:08:42, 110.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.29s/it][A100%|██████████| 1/1 [01:40<00:00, 100.29s/it]
 19%|█▉        | 1006/5198 [10:53:13<129:07:55, 110.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.33s/it][A100%|██████████| 1/1 [01:40<00:00, 100.33s/it]
 19%|█▉        | 1006/5198 [10:53:36<129:08:40, 110.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.34s/it][A100%|██████████| 1/1 [01:40<00:00, 100.34s/it]
 19%|█▉        | 1006/5198 [10:54:07<129:08:57, 110.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.35s/it][A100%|██████████| 1/1 [01:40<00:00, 100.35s/it]
 19%|█▉        | 1006/5198 [10:54:09<129:09:07, 110.91s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_944
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.06s/it][A100%|██████████| 1/1 [01:53<00:00, 113.06s/it]
 19%|█▉        | 1007/5198 [10:56:21<129:50:31, 111.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:46:17,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=994, skipped=0, lr=[1.8934494515944387e-05], mom=[(0.9, 0.999)]
steps: 994 loss: 0.5450 iter time (s): 112.390 samples/sec: 1.139

100%|██████████| 1/1 [01:53<00:00, 113.37s/it][A100%|██████████| 1/1 [01:53<00:00, 113.37s/it]
 19%|█▉        | 1007/5198 [10:56:36<129:56:46, 111.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.35s/it][A100%|██████████| 1/1 [01:53<00:00, 113.35s/it]
 19%|█▉        | 1007/5198 [10:56:23<129:57:37, 111.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.29s/it][A100%|██████████| 1/1 [01:53<00:00, 113.29s/it]
 19%|█▉        | 1007/5198 [10:56:42<129:57:02, 111.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.38s/it][A100%|██████████| 1/1 [01:53<00:00, 113.38s/it]
 19%|█▉        | 1007/5198 [10:55:06<129:58:19, 111.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.30s/it][A100%|██████████| 1/1 [01:53<00:00, 113.31s/it]
 19%|█▉        | 1007/5198 [10:55:30<129:57:09, 111.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.32s/it][A100%|██████████| 1/1 [01:53<00:00, 113.32s/it]
 19%|█▉        | 1007/5198 [10:56:01<129:57:36, 111.63s/it]
100%|██████████| 1/1 [01:53<00:00, 113.29s/it][A100%|██████████| 1/1 [01:53<00:00, 113.30s/it]
 19%|█▉        | 1007/5198 [10:56:03<129:57:20, 111.63s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_62

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.97s/it][A100%|██████████| 1/1 [01:51<00:00, 111.97s/it]
 19%|█▉        | 1008/5198 [10:58:13<129:58:17, 111.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:48:08,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=995, skipped=0, lr=[1.893194909097463e-05], mom=[(0.9, 0.999)]
steps: 995 loss: 0.8165 iter time (s): 110.965 samples/sec: 1.154

100%|██████████| 1/1 [01:52<00:00, 112.08s/it][A100%|██████████| 1/1 [01:52<00:00, 112.08s/it]
 19%|█▉        | 1008/5198 [10:58:28<130:04:46, 111.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.09s/it][A100%|██████████| 1/1 [01:52<00:00, 112.09s/it]
 19%|█▉        | 1008/5198 [10:58:15<130:05:29, 111.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.05s/it][A100%|██████████| 1/1 [01:52<00:00, 112.05s/it]
 19%|█▉        | 1008/5198 [10:58:34<130:04:14, 111.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.98s/it][A100%|██████████| 1/1 [01:51<00:00, 111.98s/it]
 19%|█▉        | 1008/5198 [10:56:58<130:03:35, 111.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.93s/it][A100%|██████████| 1/1 [01:51<00:00, 111.93s/it]
 19%|█▉        | 1008/5198 [10:57:53<130:02:02, 111.72s/it]
100%|██████████| 1/1 [01:51<00:00, 111.99s/it][A100%|██████████| 1/1 [01:51<00:00, 111.99s/it]
 19%|█▉        | 1008/5198 [10:57:22<130:03:04, 111.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.98s/it][A100%|██████████| 1/1 [01:51<00:00, 111.98s/it]
 19%|█▉        | 1008/5198 [10:57:55<130:02:56, 111.74s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_945
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.62s/it][A100%|██████████| 1/1 [01:23<00:00, 83.62s/it]
 19%|█▉        | 1009/5198 [10:59:37<120:12:52, 103.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:49:32,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=996, skipped=0, lr=[1.892940080073471e-05], mom=[(0.9, 0.999)]
steps: 996 loss: 0.6141 iter time (s): 81.973 samples/sec: 1.561

100%|██████████| 1/1 [01:22<00:00, 82.98s/it][A100%|██████████| 1/1 [01:22<00:00, 82.98s/it]
 19%|█▉        | 1009/5198 [10:59:51<120:00:49, 103.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.96s/it][A100%|██████████| 1/1 [01:22<00:00, 82.97s/it]
 19%|█▉        | 1009/5198 [10:59:38<120:01:03, 103.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.07s/it][A100%|██████████| 1/1 [01:23<00:00, 83.07s/it]
 19%|█▉        | 1009/5198 [10:59:57<120:02:22, 103.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.07s/it][A100%|██████████| 1/1 [01:23<00:00, 83.07s/it]
 19%|█▉        | 1009/5198 [10:58:21<120:01:54, 103.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.16s/it][A100%|██████████| 1/1 [01:23<00:00, 83.16s/it]
 19%|█▉        | 1009/5198 [10:59:16<120:02:45, 103.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.21s/it][A100%|██████████| 1/1 [01:23<00:00, 83.21s/it]
 19%|█▉        | 1009/5198 [10:59:18<120:04:24, 103.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_946
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.43s/it][A100%|██████████| 1/1 [01:23<00:00, 83.43s/it]
 19%|█▉        | 1009/5198 [10:58:45<120:08:56, 103.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.04s/it][A100%|██████████| 1/1 [01:37<00:00, 97.04s/it]
 19%|█▉        | 1010/5198 [11:01:14<118:01:56, 101.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:51:09,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=997, skipped=0, lr=[1.892684964604219e-05], mom=[(0.9, 0.999)]
steps: 997 loss: 0.5313 iter time (s): 96.388 samples/sec: 1.328

100%|██████████| 1/1 [01:37<00:00, 97.44s/it][A100%|██████████| 1/1 [01:37<00:00, 97.44s/it]
 19%|█▉        | 1010/5198 [11:01:28<118:00:20, 101.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.40s/it][A100%|██████████| 1/1 [01:37<00:00, 97.41s/it]
 19%|█▉        | 1010/5198 [11:01:15<117:59:30, 101.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.34s/it][A100%|██████████| 1/1 [01:37<00:00, 97.34s/it]
 19%|█▉        | 1010/5198 [11:01:35<117:58:57, 101.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.37s/it][A100%|██████████| 1/1 [01:37<00:00, 97.37s/it]
 19%|█▉        | 1010/5198 [10:59:59<117:59:25, 101.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.02s/it][A100%|██████████| 1/1 [01:37<00:00, 97.02s/it]
 19%|█▉        | 1010/5198 [11:00:22<117:57:06, 101.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.35s/it][A100%|██████████| 1/1 [01:37<00:00, 97.35s/it]
 19%|█▉        | 1010/5198 [11:00:53<117:59:44, 101.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.26s/it][A100%|██████████| 1/1 [01:37<00:00, 97.26s/it]
 19%|█▉        | 1010/5198 [11:00:55<117:58:53, 101.42s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_947
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.46s/it][A100%|██████████| 1/1 [01:40<00:00, 100.46s/it]
 19%|█▉        | 1011/5198 [11:02:54<117:41:06, 101.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:52:50,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=998, skipped=0, lr=[1.8924295627715535e-05], mom=[(0.9, 0.999)]
steps: 998 loss: 0.5580 iter time (s): 99.661 samples/sec: 1.284

100%|██████████| 1/1 [01:40<00:00, 100.54s/it][A100%|██████████| 1/1 [01:40<00:00, 100.54s/it]
 19%|█▉        | 1011/5198 [11:03:09<117:40:05, 101.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.52s/it][A100%|██████████| 1/1 [01:40<00:00, 100.52s/it]
 19%|█▉        | 1011/5198 [11:02:56<117:38:59, 101.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.48s/it][A100%|██████████| 1/1 [01:40<00:00, 100.48s/it]
 19%|█▉        | 1011/5198 [11:03:15<117:37:51, 101.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.52s/it][A100%|██████████| 1/1 [01:40<00:00, 100.52s/it]
 19%|█▉        | 1011/5198 [11:01:39<117:38:58, 101.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.53s/it][A100%|██████████| 1/1 [01:40<00:00, 100.53s/it]
 19%|█▉        | 1011/5198 [11:02:03<117:37:37, 101.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.55s/it][A100%|██████████| 1/1 [01:40<00:00, 100.55s/it]
 19%|█▉        | 1011/5198 [11:02:36<117:39:09, 101.16s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_948

100%|██████████| 1/1 [01:40<00:00, 100.56s/it][A100%|██████████| 1/1 [01:40<00:00, 100.56s/it]
 19%|█▉        | 1011/5198 [11:02:34<117:40:01, 101.17s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.86s/it][A100%|██████████| 1/1 [01:20<00:00, 80.86s/it]
 19%|█▉        | 1012/5198 [11:04:15<110:35:10, 95.11s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:54:10,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=999, skipped=0, lr=[1.8921738746574138e-05], mom=[(0.9, 0.999)]
steps: 999 loss: 0.5751 iter time (s): 79.450 samples/sec: 1.611

100%|██████████| 1/1 [01:20<00:00, 80.33s/it][A100%|██████████| 1/1 [01:20<00:00, 80.33s/it]
 19%|█▉        | 1012/5198 [11:04:29<110:22:27, 94.92s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.27s/it][A100%|██████████| 1/1 [01:20<00:00, 80.27s/it]
 19%|█▉        | 1012/5198 [11:04:16<110:20:32, 94.90s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.33s/it][A100%|██████████| 1/1 [01:20<00:00, 80.33s/it]
 19%|█▉        | 1012/5198 [11:04:35<110:20:51, 94.90s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.33s/it][A100%|██████████| 1/1 [01:20<00:00, 80.33s/it]
 19%|█▉        | 1012/5198 [11:03:00<110:21:47, 94.91s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.35s/it][A100%|██████████| 1/1 [01:20<00:00, 80.35s/it]
 19%|█▉        | 1012/5198 [11:03:23<110:21:15, 94.91s/it] 
100%|██████████| 1/1 [01:20<00:00, 80.27s/it][A100%|██████████| 1/1 [01:20<00:00, 80.27s/it]
 19%|█▉        | 1012/5198 [11:03:54<110:21:06, 94.90s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.32s/it][A100%|██████████| 1/1 [01:20<00:00, 80.32s/it]
 19%|█▉        | 1012/5198 [11:03:56<110:21:36, 94.91s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_949
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.12s/it][A100%|██████████| 1/1 [01:42<00:00, 102.12s/it]
 19%|█▉        | 1013/5198 [11:05:57<113:01:18, 97.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:55:53,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.8919179003438316e-05], mom=[(0.9, 0.999)]
steps: 1000 loss: 0.5714 iter time (s): 101.906 samples/sec: 1.256

100%|██████████| 1/1 [01:42<00:00, 102.74s/it][A100%|██████████| 1/1 [01:42<00:00, 102.74s/it]
 19%|█▉        | 1013/5198 [11:06:12<113:04:38, 97.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.91s/it][A100%|██████████| 1/1 [01:42<00:00, 102.91s/it]
 19%|█▉        | 1013/5198 [11:05:59<113:06:43, 97.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.78s/it][A100%|██████████| 1/1 [01:42<00:00, 102.78s/it]
 19%|█▉        | 1013/5198 [11:06:18<113:04:19, 97.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.78s/it][A100%|██████████| 1/1 [01:42<00:00, 102.78s/it]
 19%|█▉        | 1013/5198 [11:04:42<113:05:07, 97.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.78s/it][A100%|██████████| 1/1 [01:42<00:00, 102.78s/it]
 19%|█▉        | 1013/5198 [11:05:06<113:04:39, 97.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.90s/it][A100%|██████████| 1/1 [01:42<00:00, 102.90s/it]
 19%|█▉        | 1013/5198 [11:05:37<113:06:51, 97.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.86s/it][A100%|██████████| 1/1 [01:42<00:00, 102.86s/it]
 19%|█▉        | 1013/5198 [11:05:39<113:06:28, 97.30s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_950
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.77s/it][A100%|██████████| 1/1 [01:28<00:00, 88.77s/it]
 20%|█▉        | 1014/5198 [11:07:26<110:04:04, 94.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:57:21,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=1001, skipped=0, lr=[1.8916616399129296e-05], mom=[(0.9, 0.999)]
steps: 1001 loss: 0.5581 iter time (s): 87.480 samples/sec: 1.463

100%|██████████| 1/1 [01:28<00:00, 88.51s/it][A100%|██████████| 1/1 [01:28<00:00, 88.51s/it]
 20%|█▉        | 1014/5198 [11:07:41<109:59:58, 94.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.33s/it][A100%|██████████| 1/1 [01:28<00:00, 88.33s/it]
 20%|█▉        | 1014/5198 [11:07:27<109:57:37, 94.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.53s/it][A100%|██████████| 1/1 [01:28<00:00, 88.53s/it]
 20%|█▉        | 1014/5198 [11:07:47<110:00:10, 94.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.38s/it][A100%|██████████| 1/1 [01:28<00:00, 88.38s/it]
 20%|█▉        | 1014/5198 [11:06:11<109:57:37, 94.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.44s/it][A100%|██████████| 1/1 [01:28<00:00, 88.44s/it]
 20%|█▉        | 1014/5198 [11:06:34<109:58:32, 94.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.35s/it][A100%|██████████| 1/1 [01:28<00:00, 88.36s/it]
 20%|█▉        | 1014/5198 [11:07:05<109:58:17, 94.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.36s/it][A100%|██████████| 1/1 [01:28<00:00, 88.37s/it]
 20%|█▉        | 1014/5198 [11:07:07<109:58:12, 94.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_951
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.23s/it][A100%|██████████| 1/1 [01:41<00:00, 101.23s/it]
 20%|█▉        | 1015/5198 [11:09:07<112:19:38, 96.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 10:59:03,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=1002, skipped=0, lr=[1.891405093446922e-05], mom=[(0.9, 0.999)]
steps: 1002 loss: 0.5818 iter time (s): 100.750 samples/sec: 1.270

100%|██████████| 1/1 [01:41<00:00, 101.57s/it][A100%|██████████| 1/1 [01:41<00:00, 101.57s/it]
 20%|█▉        | 1015/5198 [11:09:22<112:23:26, 96.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.69s/it][A100%|██████████| 1/1 [01:41<00:00, 101.69s/it]
 20%|█▉        | 1015/5198 [11:09:09<112:24:18, 96.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.59s/it][A100%|██████████| 1/1 [01:41<00:00, 101.59s/it]
 20%|█▉        | 1015/5198 [11:09:28<112:23:59, 96.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.60s/it][A100%|██████████| 1/1 [01:41<00:00, 101.60s/it]
 20%|█▉        | 1015/5198 [11:07:52<112:22:25, 96.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.56s/it][A100%|██████████| 1/1 [01:41<00:00, 101.56s/it]
 20%|█▉        | 1015/5198 [11:08:16<112:22:07, 96.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.62s/it][A100%|██████████| 1/1 [01:41<00:00, 101.62s/it]
 20%|█▉        | 1015/5198 [11:08:47<112:23:09, 96.72s/it]
100%|██████████| 1/1 [01:41<00:00, 101.59s/it][A100%|██████████| 1/1 [01:41<00:00, 101.59s/it]
 20%|█▉        | 1015/5198 [11:08:49<112:22:38, 96.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_952

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.06s/it][A100%|██████████| 1/1 [01:36<00:00, 96.06s/it]
 20%|█▉        | 1016/5198 [11:10:44<112:05:45, 96.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:00:39,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=1003, skipped=0, lr=[1.8911482610281158e-05], mom=[(0.9, 0.999)]
steps: 1003 loss: 0.5761 iter time (s): 95.064 samples/sec: 1.346

100%|██████████| 1/1 [01:35<00:00, 95.93s/it][A100%|██████████| 1/1 [01:35<00:00, 95.93s/it]
 20%|█▉        | 1016/5198 [11:10:58<112:05:15, 96.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.97s/it][A100%|██████████| 1/1 [01:35<00:00, 95.97s/it]
 20%|█▉        | 1016/5198 [11:10:45<112:06:54, 96.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.97s/it][A100%|██████████| 1/1 [01:35<00:00, 95.97s/it]
 20%|█▉        | 1016/5198 [11:11:04<112:06:35, 96.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.99s/it][A100%|██████████| 1/1 [01:35<00:00, 95.99s/it]
 20%|█▉        | 1016/5198 [11:09:28<112:05:55, 96.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.99s/it][A100%|██████████| 1/1 [01:35<00:00, 95.99s/it]
 20%|█▉        | 1016/5198 [11:09:52<112:05:49, 96.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.97s/it][A100%|██████████| 1/1 [01:35<00:00, 95.97s/it]
 20%|█▉        | 1016/5198 [11:10:23<112:06:00, 96.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.98s/it][A100%|██████████| 1/1 [01:35<00:00, 95.99s/it]
 20%|█▉        | 1016/5198 [11:10:25<112:05:54, 96.50s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_953
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.47s/it][A100%|██████████| 1/1 [01:28<00:00, 88.47s/it]
 20%|█▉        | 1017/5198 [11:12:12<109:16:38, 94.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:02:07,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=1004, skipped=0, lr=[1.8908911427389093e-05], mom=[(0.9, 0.999)]
steps: 1004 loss: 0.6031 iter time (s): 87.377 samples/sec: 1.465

100%|██████████| 1/1 [01:28<00:00, 88.25s/it][A100%|██████████| 1/1 [01:28<00:00, 88.25s/it]
 20%|█▉        | 1017/5198 [11:12:26<109:11:46, 94.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.25s/it][A100%|██████████| 1/1 [01:28<00:00, 88.25s/it]
 20%|█▉        | 1017/5198 [11:12:13<109:12:46, 94.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.21s/it][A100%|██████████| 1/1 [01:28<00:00, 88.21s/it]
 20%|█▉        | 1017/5198 [11:12:33<109:11:39, 94.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.33s/it][A100%|██████████| 1/1 [01:28<00:00, 88.33s/it]
 20%|█▉        | 1017/5198 [11:10:57<109:13:39, 94.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.28s/it][A100%|██████████| 1/1 [01:28<00:00, 88.28s/it]
 20%|█▉        | 1017/5198 [11:11:20<109:12:42, 94.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 20%|█▉        | 1017/5198 [11:11:51<109:13:20, 94.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.33s/it][A100%|██████████| 1/1 [01:28<00:00, 88.33s/it]
 20%|█▉        | 1017/5198 [11:11:53<109:13:44, 94.05s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_954
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.57s/it][A100%|██████████| 1/1 [01:53<00:00, 113.57s/it]
 20%|█▉        | 1018/5198 [11:14:06<116:03:01, 99.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:04:02,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=1005, skipped=0, lr=[1.8906337386617916e-05], mom=[(0.9, 0.999)]
steps: 1005 loss: 0.5477 iter time (s): 113.346 samples/sec: 1.129

100%|██████████| 1/1 [01:54<00:00, 114.39s/it][A100%|██████████| 1/1 [01:54<00:00, 114.39s/it]
 20%|█▉        | 1018/5198 [11:14:21<116:16:02, 100.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.26s/it][A100%|██████████| 1/1 [01:54<00:00, 114.26s/it]
 20%|█▉        | 1018/5198 [11:14:07<116:14:07, 100.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.28s/it][A100%|██████████| 1/1 [01:54<00:00, 114.28s/it]
 20%|█▉        | 1018/5198 [11:14:27<116:13:35, 100.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.29s/it][A100%|██████████| 1/1 [01:54<00:00, 114.29s/it]
 20%|█▉        | 1018/5198 [11:12:51<116:15:20, 100.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.29s/it][A100%|██████████| 1/1 [01:54<00:00, 114.29s/it]
 20%|█▉        | 1018/5198 [11:13:14<116:14:41, 100.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.25s/it][A100%|██████████| 1/1 [01:54<00:00, 114.25s/it]
 20%|█▉        | 1018/5198 [11:13:45<116:14:12, 100.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.24s/it][A100%|██████████| 1/1 [01:54<00:00, 114.24s/it]
 20%|█▉        | 1018/5198 [11:13:47<116:14:17, 100.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_955
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.78s/it][A100%|██████████| 1/1 [01:40<00:00, 100.78s/it]
 20%|█▉        | 1019/5198 [11:15:46<116:19:35, 100.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:05:42,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=1006, skipped=0, lr=[1.890376048879345e-05], mom=[(0.9, 0.999)]
steps: 1006 loss: 0.5569 iter time (s): 99.561 samples/sec: 1.286

100%|██████████| 1/1 [01:40<00:00, 100.45s/it][A100%|██████████| 1/1 [01:40<00:00, 100.45s/it]
 20%|█▉        | 1019/5198 [11:16:01<116:21:06, 100.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.55s/it][A100%|██████████| 1/1 [01:40<00:00, 100.55s/it]
 20%|█▉        | 1019/5198 [11:15:48<116:21:53, 100.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.59s/it][A100%|██████████| 1/1 [01:40<00:00, 100.59s/it]
 20%|█▉        | 1019/5198 [11:16:07<116:22:23, 100.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.47s/it][A100%|██████████| 1/1 [01:40<00:00, 100.47s/it]
 20%|█▉        | 1019/5198 [11:14:32<116:20:56, 100.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.50s/it][A100%|██████████| 1/1 [01:40<00:00, 100.50s/it]
 20%|█▉        | 1019/5198 [11:14:55<116:21:12, 100.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.52s/it][A100%|██████████| 1/1 [01:40<00:00, 100.52s/it]
 20%|█▉        | 1019/5198 [11:15:26<116:21:18, 100.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.50s/it][A100%|██████████| 1/1 [01:40<00:00, 100.50s/it]
 20%|█▉        | 1019/5198 [11:15:28<116:20:57, 100.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_956
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.43s/it][A100%|██████████| 1/1 [01:55<00:00, 115.43s/it]
[2024-06-30 11:07:38,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=1007, skipped=0, lr=[1.8901180734742436e-05], mom=[(0.9, 0.999)]
steps: 1007 loss: 0.5162 iter time (s): 114.783 samples/sec: 1.115

100%|██████████| 1/1 [01:55<00:00, 115.75s/it][A100%|██████████| 1/1 [01:55<00:00, 115.75s/it]

100%|██████████| 1/1 [01:55<00:00, 115.69s/it][A100%|██████████| 1/1 [01:55<00:00, 115.69s/it]

100%|██████████| 1/1 [01:55<00:00, 115.73s/it][A100%|██████████| 1/1 [01:55<00:00, 115.73s/it]

100%|██████████| 1/1 [01:55<00:00, 115.74s/it][A100%|██████████| 1/1 [01:55<00:00, 115.75s/it]

100%|██████████| 1/1 [01:55<00:00, 115.70s/it][A100%|██████████| 1/1 [01:55<00:00, 115.70s/it]

100%|██████████| 1/1 [01:55<00:00, 115.73s/it][A100%|██████████| 1/1 [01:55<00:00, 115.73s/it]

100%|██████████| 1/1 [01:55<00:00, 115.73s/it][A100%|██████████| 1/1 [01:55<00:00, 115.73s/it]
Checkpointing at shard 1019
[2024-06-30 11:07:39,184] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1007 is about to be saved!
[2024-06-30 11:07:40,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_00-model_states.pt...
[2024-06-30 11:07:45,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_02-model_states.pt...
[2024-06-30 11:07:46,141] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_00-model_states.pt.
[2024-06-30 11:07:52,723] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_04-model_states.pt...
[2024-06-30 11:07:52,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_03-model_states.pt...
[2024-06-30 11:07:53,251] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_08-model_states.pt...
[2024-06-30 11:07:53,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_06-model_states.pt...
[2024-06-30 11:07:53,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_07-model_states.pt...
[2024-06-30 11:07:53,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_05-model_states.pt...
[2024-06-30 11:07:55,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_01-model_states.pt...
[2024-06-30 11:11:43,234] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_01-model_states.pt.
[2024-06-30 11:11:45,298] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_00_model_states.pt
[2024-06-30 11:11:45,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_00_model_states.pt...
[2024-06-30 11:11:49,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_00_model_states.pt.
[2024-06-30 11:11:49,181] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
[2024-06-30 11:11:57,224] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_05-model_states.pt.
[2024-06-30 11:11:57,274] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_02-model_states.pt.
[2024-06-30 11:11:57,558] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_01_model_states.pt
[2024-06-30 11:11:57,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_01_model_states.pt...
[2024-06-30 11:11:57,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_01_model_states.pt.
[2024-06-30 11:11:57,676] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
[2024-06-30 11:11:58,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_04_model_states.pt...
[2024-06-30 11:11:58,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_04_model_states.pt.
[2024-06-30 11:11:58,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
[2024-06-30 11:11:59,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_04-model_states.pt.
[2024-06-30 11:12:00,423] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_03-model_states.pt.
[2024-06-30 11:12:00,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_03_model_states.pt...
[2024-06-30 11:12:00,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_03_model_states.pt.
[2024-06-30 11:12:00,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
[2024-06-30 11:12:00,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_08-model_states.pt.
[2024-06-30 11:12:01,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_06-model_states.pt.
[2024-06-30 11:12:01,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_07-model_states.pt.
[2024-06-30 11:12:01,390] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_02_model_states.pt...
[2024-06-30 11:12:01,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_02_model_states.pt.
[2024-06-30 11:12:01,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
[2024-06-30 11:12:01,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_09-model_states.pt...
[2024-06-30 11:12:01,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_05_model_states.pt...
[2024-06-30 11:12:01,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_06_model_states.pt...
[2024-06-30 11:12:01,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_05_model_states.pt.
[2024-06-30 11:12:01,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
[2024-06-30 11:12:01,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_06_model_states.pt.
[2024-06-30 11:12:01,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
[2024-06-30 11:12:02,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/layer_09-model_states.pt.
[2024-06-30 11:12:02,486] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_07_model_states.pt...
[2024-06-30 11:12:02,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1007/mp_rank_07_model_states.pt.
[2024-06-30 11:12:02,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1007 is ready now!
Checkpoint saved using --- 263.3777434825897 seconds ---
 20%|█▉        | 1020/5198 [11:22:07<213:32:53, 184.01s/it] 20%|█▉        | 1020/5198 [11:22:21<213:35:31, 184.04s/it] 20%|█▉        | 1020/5198 [11:21:45<213:25:35, 183.90s/it] 20%|█▉        | 1020/5198 [11:21:14<213:26:44, 183.92s/it] 20%|█▉        | 1020/5198 [11:20:51<213:28:15, 183.94s/it] 20%|█▉        | 1020/5198 [11:21:47<213:25:09, 183.89s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_957
 20%|█▉        | 1020/5198 [11:22:09<214:33:24, 184.87s/it] 20%|█▉        | 1020/5198 [11:22:27<213:30:20, 183.97s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.93s/it][A100%|██████████| 1/1 [01:43<00:00, 103.93s/it]
 20%|█▉        | 1021/5198 [11:23:53<186:21:40, 160.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:13:49,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=1008, skipped=0, lr=[1.8898598125292504e-05], mom=[(0.9, 0.999)]
steps: 1008 loss: 0.6028 iter time (s): 106.454 samples/sec: 1.202

100%|██████████| 1/1 [01:46<00:00, 106.87s/it][A100%|██████████| 1/1 [01:46<00:00, 106.87s/it]
 20%|█▉        | 1021/5198 [11:24:08<186:42:40, 160.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.02s/it][A100%|██████████| 1/1 [01:47<00:00, 107.02s/it]
 20%|█▉        | 1021/5198 [11:23:54<186:43:50, 160.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.07s/it][A100%|██████████| 1/1 [01:47<00:00, 107.07s/it]
 20%|█▉        | 1021/5198 [11:24:14<186:43:15, 160.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.19s/it][A100%|██████████| 1/1 [01:47<00:00, 107.19s/it]
 20%|█▉        | 1021/5198 [11:22:38<186:44:25, 160.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.25s/it][A100%|██████████| 1/1 [01:47<00:00, 107.25s/it]
 20%|█▉        | 1021/5198 [11:23:01<186:44:27, 160.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.31s/it][A100%|██████████| 1/1 [01:47<00:00, 107.31s/it]
 20%|█▉        | 1021/5198 [11:23:32<186:44:53, 160.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.32s/it][A100%|██████████| 1/1 [01:47<00:00, 107.32s/it]
 20%|█▉        | 1021/5198 [11:23:34<186:44:54, 160.95s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_958
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.08s/it][A100%|██████████| 1/1 [01:24<00:00, 84.08s/it]
 20%|█▉        | 1022/5198 [11:25:17<159:42:19, 137.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:15:12,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=1009, skipped=0, lr=[1.889601266127223e-05], mom=[(0.9, 0.999)]
steps: 1009 loss: 0.5895 iter time (s): 82.560 samples/sec: 1.550

100%|██████████| 1/1 [01:23<00:00, 83.46s/it][A100%|██████████| 1/1 [01:23<00:00, 83.46s/it]
 20%|█▉        | 1022/5198 [11:25:31<159:42:52, 137.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.42s/it][A100%|██████████| 1/1 [01:23<00:00, 83.42s/it]
 20%|█▉        | 1022/5198 [11:25:18<159:42:49, 137.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.45s/it][A100%|██████████| 1/1 [01:23<00:00, 83.45s/it]
 20%|█▉        | 1022/5198 [11:25:37<159:43:08, 137.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.36s/it][A100%|██████████| 1/1 [01:23<00:00, 83.36s/it]
 20%|█▉        | 1022/5198 [11:24:01<159:41:54, 137.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.43s/it][A100%|██████████| 1/1 [01:23<00:00, 83.43s/it]
 20%|█▉        | 1022/5198 [11:24:25<159:43:29, 137.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.41s/it][A100%|██████████| 1/1 [01:23<00:00, 83.41s/it]
 20%|█▉        | 1022/5198 [11:24:56<159:43:21, 137.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.42s/it][A100%|██████████| 1/1 [01:23<00:00, 83.42s/it]
 20%|█▉        | 1022/5198 [11:24:58<159:43:30, 137.69s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_959
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.75s/it][A100%|██████████| 1/1 [01:22<00:00, 82.75s/it]
 20%|█▉        | 1023/5198 [11:26:40<140:34:41, 121.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:16:35,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[1.8893424343511098e-05], mom=[(0.9, 0.999)]
steps: 1010 loss: 0.5825 iter time (s): 81.955 samples/sec: 1.562

100%|██████████| 1/1 [01:22<00:00, 82.81s/it][A100%|██████████| 1/1 [01:22<00:00, 82.81s/it]
 20%|█▉        | 1023/5198 [11:26:54<140:35:10, 121.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.75s/it][A100%|██████████| 1/1 [01:22<00:00, 82.75s/it]
 20%|█▉        | 1023/5198 [11:26:41<140:34:01, 121.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.80s/it][A100%|██████████| 1/1 [01:22<00:00, 82.80s/it]
 20%|█▉        | 1023/5198 [11:27:00<140:35:23, 121.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.82s/it][A100%|██████████| 1/1 [01:22<00:00, 82.82s/it]
 20%|█▉        | 1023/5198 [11:25:24<140:34:51, 121.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.84s/it][A100%|██████████| 1/1 [01:22<00:00, 82.84s/it]
 20%|█▉        | 1023/5198 [11:25:48<140:36:20, 121.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.83s/it][A100%|██████████| 1/1 [01:22<00:00, 82.83s/it]
 20%|█▉        | 1023/5198 [11:26:19<140:36:03, 121.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.86s/it][A100%|██████████| 1/1 [01:22<00:00, 82.86s/it]
 20%|█▉        | 1023/5198 [11:26:21<140:36:48, 121.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_63
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.56s/it][A100%|██████████| 1/1 [01:53<00:00, 113.56s/it]
 20%|█▉        | 1024/5198 [11:28:34<137:54:20, 118.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:18:29,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=1011, skipped=0, lr=[1.88908331728395e-05], mom=[(0.9, 0.999)]
steps: 1011 loss: 0.7680 iter time (s): 113.635 samples/sec: 1.126

100%|██████████| 1/1 [01:54<00:00, 114.65s/it][A100%|██████████| 1/1 [01:54<00:00, 114.65s/it]
 20%|█▉        | 1024/5198 [11:28:49<138:16:04, 119.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.96s/it][A100%|██████████| 1/1 [01:54<00:00, 114.96s/it]
 20%|█▉        | 1024/5198 [11:28:36<138:21:45, 119.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.84s/it][A100%|██████████| 1/1 [01:54<00:00, 114.84s/it]
 20%|█▉        | 1024/5198 [11:28:55<138:20:16, 119.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.79s/it][A100%|██████████| 1/1 [01:54<00:00, 114.79s/it]
 20%|█▉        | 1024/5198 [11:27:42<138:19:48, 119.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.97s/it][A100%|██████████| 1/1 [01:54<00:00, 114.97s/it]
 20%|█▉        | 1024/5198 [11:27:19<138:22:36, 119.35s/it]
100%|██████████| 1/1 [01:54<00:00, 114.75s/it][A100%|██████████| 1/1 [01:54<00:00, 114.75s/it]
 20%|█▉        | 1024/5198 [11:28:15<138:19:14, 119.30s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_960
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.81s/it][A100%|██████████| 1/1 [01:54<00:00, 114.81s/it]
 20%|█▉        | 1024/5198 [11:28:14<138:20:09, 119.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.72s/it][A100%|██████████| 1/1 [01:22<00:00, 82.72s/it]
 20%|█▉        | 1025/5198 [11:29:56<125:17:44, 108.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:19:51,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=1012, skipped=0, lr=[1.888823915008875e-05], mom=[(0.9, 0.999)]
steps: 1012 loss: 0.5793 iter time (s): 80.731 samples/sec: 1.586

100%|██████████| 1/1 [01:21<00:00, 81.63s/it][A100%|██████████| 1/1 [01:21<00:00, 81.63s/it]
 20%|█▉        | 1025/5198 [11:30:10<125:09:15, 107.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.39s/it][A100%|██████████| 1/1 [01:21<00:00, 81.40s/it]
 20%|█▉        | 1025/5198 [11:29:57<125:08:25, 107.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.45s/it][A100%|██████████| 1/1 [01:21<00:00, 81.45s/it]
 20%|█▉        | 1025/5198 [11:30:17<125:08:33, 107.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.45s/it][A100%|██████████| 1/1 [01:21<00:00, 81.45s/it]
 20%|█▉        | 1025/5198 [11:28:41<125:09:59, 107.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.51s/it][A100%|██████████| 1/1 [01:21<00:00, 81.51s/it]
 20%|█▉        | 1025/5198 [11:29:04<125:09:19, 107.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.51s/it][A100%|██████████| 1/1 [01:21<00:00, 81.51s/it]
 20%|█▉        | 1025/5198 [11:29:35<125:09:41, 107.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.56s/it][A100%|██████████| 1/1 [01:21<00:00, 81.56s/it]
 20%|█▉        | 1025/5198 [11:29:37<125:09:53, 107.98s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_961
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.30s/it][A100%|██████████| 1/1 [01:53<00:00, 113.30s/it]
 20%|█▉        | 1026/5198 [11:31:50<127:06:01, 109.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:21:46,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=1013, skipped=0, lr=[1.8885642276091073e-05], mom=[(0.9, 0.999)]
steps: 1013 loss: 0.6002 iter time (s): 113.458 samples/sec: 1.128

100%|██████████| 1/1 [01:54<00:00, 114.27s/it][A100%|██████████| 1/1 [01:54<00:00, 114.27s/it]
 20%|█▉        | 1026/5198 [11:32:05<127:19:07, 109.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.30s/it][A100%|██████████| 1/1 [01:54<00:00, 114.30s/it]
 20%|█▉        | 1026/5198 [11:31:51<127:19:10, 109.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.43s/it][A100%|██████████| 1/1 [01:54<00:00, 114.43s/it]
 20%|█▉        | 1026/5198 [11:32:11<127:22:07, 109.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.36s/it][A100%|██████████| 1/1 [01:54<00:00, 114.36s/it]
 20%|█▉        | 1026/5198 [11:30:35<127:21:30, 109.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.36s/it][A100%|██████████| 1/1 [01:54<00:00, 114.36s/it]
 20%|█▉        | 1026/5198 [11:30:58<127:20:57, 109.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.45s/it][A100%|██████████| 1/1 [01:54<00:00, 114.45s/it]
 20%|█▉        | 1026/5198 [11:31:30<127:23:12, 109.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.47s/it][A100%|██████████| 1/1 [01:54<00:00, 114.47s/it]
 20%|█▉        | 1026/5198 [11:31:32<127:23:44, 109.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_962
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.02s/it][A100%|██████████| 1/1 [01:46<00:00, 106.02s/it]
 20%|█▉        | 1027/5198 [11:33:36<125:49:14, 108.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:23:31,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=1014, skipped=0, lr=[1.8883042551679613e-05], mom=[(0.9, 0.999)]
steps: 1014 loss: 0.5844 iter time (s): 104.829 samples/sec: 1.221

100%|██████████| 1/1 [01:46<00:00, 106.16s/it][A100%|██████████| 1/1 [01:46<00:00, 106.16s/it]
 20%|█▉        | 1027/5198 [11:33:51<126:00:24, 108.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.85s/it][A100%|██████████| 1/1 [01:45<00:00, 105.85s/it]
 20%|█▉        | 1027/5198 [11:33:57<125:56:02, 108.69s/it]
100%|██████████| 1/1 [01:46<00:00, 106.10s/it][A100%|██████████| 1/1 [01:46<00:00, 106.10s/it]
 20%|█▉        | 1027/5198 [11:33:37<125:59:00, 108.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.97s/it][A100%|██████████| 1/1 [01:45<00:00, 105.97s/it]
 20%|█▉        | 1027/5198 [11:32:21<125:57:57, 108.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.96s/it][A100%|██████████| 1/1 [01:45<00:00, 105.96s/it]
 20%|█▉        | 1027/5198 [11:33:15<125:58:51, 108.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.13s/it][A100%|██████████| 1/1 [01:46<00:00, 106.13s/it]
 20%|█▉        | 1027/5198 [11:32:44<126:00:54, 108.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.13s/it][A100%|██████████| 1/1 [01:46<00:00, 106.13s/it]
 20%|█▉        | 1027/5198 [11:33:18<126:02:46, 108.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_963
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.83s/it][A100%|██████████| 1/1 [01:55<00:00, 115.83s/it]
 20%|█▉        | 1028/5198 [11:35:32<128:19:22, 110.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:25:28,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=1015, skipped=0, lr=[1.888043997768842e-05], mom=[(0.9, 0.999)]
steps: 1015 loss: 0.5703 iter time (s): 114.855 samples/sec: 1.114

100%|██████████| 1/1 [01:55<00:00, 115.89s/it][A100%|██████████| 1/1 [01:55<00:00, 115.89s/it]
 20%|█▉        | 1028/5198 [11:35:47<128:27:33, 110.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.92s/it][A100%|██████████| 1/1 [01:55<00:00, 115.93s/it]
 20%|█▉        | 1028/5198 [11:35:33<128:27:13, 110.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.01s/it][A100%|██████████| 1/1 [01:56<00:00, 116.01s/it]
 20%|█▉        | 1028/5198 [11:35:53<128:27:20, 110.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.99s/it][A100%|██████████| 1/1 [01:55<00:00, 115.99s/it]
 20%|█▉        | 1028/5198 [11:34:17<128:27:59, 110.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.88s/it][A100%|██████████| 1/1 [01:55<00:00, 115.88s/it]
 20%|█▉        | 1028/5198 [11:34:40<128:27:34, 110.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.72s/it][A100%|██████████| 1/1 [01:55<00:00, 115.72s/it]
 20%|█▉        | 1028/5198 [11:35:13<128:25:40, 110.87s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_964
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.94s/it][A100%|██████████| 1/1 [01:55<00:00, 115.94s/it]
 20%|█▉        | 1028/5198 [11:35:11<128:27:32, 110.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.59s/it][A100%|██████████| 1/1 [01:35<00:00, 95.59s/it]
 20%|█▉        | 1029/5198 [11:37:07<123:01:49, 106.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:27:03,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=1016, skipped=0, lr=[1.887783455495247e-05], mom=[(0.9, 0.999)]
steps: 1016 loss: 0.5362 iter time (s): 94.153 samples/sec: 1.359

100%|██████████| 1/1 [01:35<00:00, 95.06s/it][A100%|██████████| 1/1 [01:35<00:00, 95.06s/it]
 20%|█▉        | 1029/5198 [11:37:22<122:55:36, 106.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.01s/it][A100%|██████████| 1/1 [01:35<00:00, 95.01s/it]
 20%|█▉        | 1029/5198 [11:37:08<122:54:32, 106.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.05s/it][A100%|██████████| 1/1 [01:35<00:00, 95.05s/it]
 20%|█▉        | 1029/5198 [11:37:28<122:55:26, 106.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.98s/it][A100%|██████████| 1/1 [01:34<00:00, 94.98s/it]
 20%|█▉        | 1029/5198 [11:35:52<122:54:16, 106.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 95.00s/it][A100%|██████████| 1/1 [01:34<00:00, 95.00s/it]
 20%|█▉        | 1029/5198 [11:36:15<122:54:25, 106.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.00s/it][A100%|██████████| 1/1 [01:35<00:00, 95.00s/it]
 20%|█▉        | 1029/5198 [11:36:46<122:54:29, 106.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.03s/it][A100%|██████████| 1/1 [01:35<00:00, 95.03s/it]
 20%|█▉        | 1029/5198 [11:36:48<122:53:44, 106.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_965
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.64s/it][A100%|██████████| 1/1 [01:42<00:00, 102.64s/it]
 20%|█▉        | 1030/5198 [11:38:50<121:46:14, 105.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:28:46,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=1017, skipped=0, lr=[1.8875226284307645e-05], mom=[(0.9, 0.999)]
steps: 1017 loss: 0.6132 iter time (s): 102.035 samples/sec: 1.254

100%|██████████| 1/1 [01:42<00:00, 102.96s/it][A100%|██████████| 1/1 [01:42<00:00, 102.96s/it]
 20%|█▉        | 1030/5198 [11:39:05<121:47:28, 105.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.99s/it][A100%|██████████| 1/1 [01:42<00:00, 102.99s/it]
 20%|█▉        | 1030/5198 [11:38:51<121:47:22, 105.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.94s/it][A100%|██████████| 1/1 [01:42<00:00, 102.94s/it]
 20%|█▉        | 1030/5198 [11:39:11<121:47:02, 105.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.02s/it][A100%|██████████| 1/1 [01:43<00:00, 103.02s/it]
 20%|█▉        | 1030/5198 [11:37:35<121:47:49, 105.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 103.00s/it][A100%|██████████| 1/1 [01:42<00:00, 103.00s/it]
 20%|█▉        | 1030/5198 [11:37:58<121:47:27, 105.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.98s/it][A100%|██████████| 1/1 [01:42<00:00, 102.98s/it]
 20%|█▉        | 1030/5198 [11:38:29<121:47:11, 105.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.99s/it][A100%|██████████| 1/1 [01:42<00:00, 102.99s/it]
 20%|█▉        | 1030/5198 [11:38:31<121:46:55, 105.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_966
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.69s/it][A100%|██████████| 1/1 [01:37<00:00, 97.69s/it]
 20%|█▉        | 1031/5198 [11:40:28<119:09:54, 102.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:30:23,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=1018, skipped=0, lr=[1.8872615166590748e-05], mom=[(0.9, 0.999)]
steps: 1018 loss: 0.5687 iter time (s): 96.650 samples/sec: 1.324

100%|██████████| 1/1 [01:37<00:00, 97.55s/it][A100%|██████████| 1/1 [01:37<00:00, 97.55s/it]
 20%|█▉        | 1031/5198 [11:40:42<119:06:38, 102.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.54s/it][A100%|██████████| 1/1 [01:37<00:00, 97.54s/it]
 20%|█▉        | 1031/5198 [11:40:29<119:06:31, 102.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.65s/it][A100%|██████████| 1/1 [01:37<00:00, 97.65s/it]
 20%|█▉        | 1031/5198 [11:40:49<119:08:25, 102.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.51s/it][A100%|██████████| 1/1 [01:37<00:00, 97.51s/it]
 20%|█▉        | 1031/5198 [11:39:13<119:05:59, 102.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.49s/it][A100%|██████████| 1/1 [01:37<00:00, 97.49s/it]
 20%|█▉        | 1031/5198 [11:39:36<119:05:27, 102.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.53s/it][A100%|██████████| 1/1 [01:37<00:00, 97.53s/it]
 20%|█▉        | 1031/5198 [11:40:07<119:06:02, 102.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.53s/it][A100%|██████████| 1/1 [01:37<00:00, 97.53s/it]
 20%|█▉        | 1031/5198 [11:40:09<119:05:49, 102.89s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_967
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.02s/it][A100%|██████████| 1/1 [02:12<00:00, 132.02s/it]
 20%|█▉        | 1032/5198 [11:42:40<129:14:51, 111.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:32:36,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=1019, skipped=0, lr=[1.8870001202639483e-05], mom=[(0.9, 0.999)]
steps: 1019 loss: 0.5486 iter time (s): 132.118 samples/sec: 0.969

100%|██████████| 1/1 [02:12<00:00, 132.99s/it][A100%|██████████| 1/1 [02:12<00:00, 132.99s/it]
 20%|█▉        | 1032/5198 [11:42:55<129:31:46, 111.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.96s/it][A100%|██████████| 1/1 [02:12<00:00, 132.96s/it]
 20%|█▉        | 1032/5198 [11:42:42<129:31:16, 111.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.01s/it][A100%|██████████| 1/1 [02:13<00:00, 133.01s/it]
 20%|█▉        | 1032/5198 [11:43:02<129:33:24, 111.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.05s/it][A100%|██████████| 1/1 [02:13<00:00, 133.05s/it]
 20%|█▉        | 1032/5198 [11:41:26<129:32:36, 111.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.11s/it][A100%|██████████| 1/1 [02:13<00:00, 133.11s/it]
 20%|█▉        | 1032/5198 [11:41:49<129:33:26, 111.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.06s/it][A100%|██████████| 1/1 [02:13<00:00, 133.06s/it]
 20%|█▉        | 1032/5198 [11:42:20<129:32:54, 111.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.06s/it][A100%|██████████| 1/1 [02:13<00:00, 133.06s/it]
 20%|█▉        | 1032/5198 [11:42:22<129:32:33, 111.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_968
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.05s/it][A100%|██████████| 1/1 [01:46<00:00, 106.05s/it]
 20%|█▉        | 1033/5198 [11:44:26<127:17:57, 110.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:34:22,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[1.886738439329248e-05], mom=[(0.9, 0.999)]
steps: 1020 loss: 0.5958 iter time (s): 104.498 samples/sec: 1.225

100%|██████████| 1/1 [01:45<00:00, 105.45s/it][A100%|██████████| 1/1 [01:45<00:00, 105.45s/it]
 20%|█▉        | 1033/5198 [11:44:41<127:15:10, 109.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.46s/it][A100%|██████████| 1/1 [01:45<00:00, 105.46s/it]
 20%|█▉        | 1033/5198 [11:44:27<127:15:08, 109.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.44s/it][A100%|██████████| 1/1 [01:45<00:00, 105.44s/it]
 20%|█▉        | 1033/5198 [11:44:47<127:16:17, 110.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.46s/it][A100%|██████████| 1/1 [01:45<00:00, 105.46s/it]
 20%|█▉        | 1033/5198 [11:43:11<127:15:51, 110.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.44s/it][A100%|██████████| 1/1 [01:45<00:00, 105.44s/it]
 20%|█▉        | 1033/5198 [11:43:34<127:16:01, 110.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.45s/it][A100%|██████████| 1/1 [01:45<00:00, 105.45s/it]
 20%|█▉        | 1033/5198 [11:44:05<127:15:47, 110.00s/it]
100%|██████████| 1/1 [01:45<00:00, 105.43s/it][A100%|██████████| 1/1 [01:45<00:00, 105.43s/it]
 20%|█▉        | 1033/5198 [11:44:07<127:15:12, 109.99s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_969

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.61s/it][A100%|██████████| 1/1 [01:28<00:00, 88.61s/it]
 20%|█▉        | 1034/5198 [11:45:55<119:51:16, 103.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:35:50,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=1021, skipped=0, lr=[1.886476473938927e-05], mom=[(0.9, 0.999)]
steps: 1021 loss: 0.5705 iter time (s): 87.263 samples/sec: 1.467

100%|██████████| 1/1 [01:28<00:00, 88.11s/it][A100%|██████████| 1/1 [01:28<00:00, 88.12s/it]
 20%|█▉        | 1034/5198 [11:46:09<119:38:03, 103.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.20s/it][A100%|██████████| 1/1 [01:28<00:00, 88.20s/it]
 20%|█▉        | 1034/5198 [11:45:56<119:39:44, 103.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.09s/it][A100%|██████████| 1/1 [01:28<00:00, 88.09s/it]
 20%|█▉        | 1034/5198 [11:46:15<119:38:20, 103.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.20s/it][A100%|██████████| 1/1 [01:28<00:00, 88.20s/it]
 20%|█▉        | 1034/5198 [11:44:39<119:40:14, 103.46s/it]
100%|██████████| 1/1 [01:28<00:00, 88.12s/it][A100%|██████████| 1/1 [01:28<00:00, 88.12s/it]
 20%|█▉        | 1034/5198 [11:45:03<119:38:38, 103.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.15s/it][A100%|██████████| 1/1 [01:28<00:00, 88.15s/it]
 20%|█▉        | 1034/5198 [11:45:34<119:39:11, 103.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.17s/it][A100%|██████████| 1/1 [01:28<00:00, 88.17s/it]
 20%|█▉        | 1034/5198 [11:45:36<119:39:13, 103.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_970
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.91s/it][A100%|██████████| 1/1 [01:41<00:00, 101.91s/it]
 20%|█▉        | 1035/5198 [11:47:37<119:15:46, 103.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:37:32,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=1022, skipped=0, lr=[1.886214224177031e-05], mom=[(0.9, 0.999)]
steps: 1022 loss: 0.5663 iter time (s): 101.540 samples/sec: 1.261

100%|██████████| 1/1 [01:42<00:00, 102.60s/it][A100%|██████████| 1/1 [01:42<00:00, 102.60s/it]
 20%|█▉        | 1035/5198 [11:47:52<119:19:20, 103.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.50s/it][A100%|██████████| 1/1 [01:42<00:00, 102.50s/it]
 20%|█▉        | 1035/5198 [11:47:38<119:18:13, 103.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.48s/it][A100%|██████████| 1/1 [01:42<00:00, 102.48s/it]
 20%|█▉        | 1035/5198 [11:47:58<119:16:55, 103.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.44s/it][A100%|██████████| 1/1 [01:42<00:00, 102.44s/it]
 20%|█▉        | 1035/5198 [11:46:22<119:17:30, 103.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.49s/it][A100%|██████████| 1/1 [01:42<00:00, 102.49s/it]
 20%|█▉        | 1035/5198 [11:46:45<119:17:15, 103.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.48s/it][A100%|██████████| 1/1 [01:42<00:00, 102.48s/it]
 20%|█▉        | 1035/5198 [11:47:16<119:17:33, 103.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.47s/it][A100%|██████████| 1/1 [01:42<00:00, 102.47s/it]
 20%|█▉        | 1035/5198 [11:47:18<119:17:18, 103.16s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_971
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.95s/it][A100%|██████████| 1/1 [01:54<00:00, 114.95s/it]
 20%|█▉        | 1036/5198 [11:49:32<123:21:04, 106.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:39:28,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=1023, skipped=0, lr=[1.8859516901276963e-05], mom=[(0.9, 0.999)]
steps: 1023 loss: 0.5306 iter time (s): 114.357 samples/sec: 1.119

100%|██████████| 1/1 [01:55<00:00, 115.27s/it][A100%|██████████| 1/1 [01:55<00:00, 115.27s/it]
 20%|█▉        | 1036/5198 [11:49:47<123:29:22, 106.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.22s/it][A100%|██████████| 1/1 [01:55<00:00, 115.22s/it]
 20%|█▉        | 1036/5198 [11:49:33<123:27:32, 106.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.40s/it][A100%|██████████| 1/1 [01:55<00:00, 115.40s/it]
 20%|█▉        | 1036/5198 [11:49:53<123:30:17, 106.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.29s/it][A100%|██████████| 1/1 [01:55<00:00, 115.29s/it]
 20%|█▉        | 1036/5198 [11:48:17<123:28:28, 106.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.33s/it][A100%|██████████| 1/1 [01:55<00:00, 115.33s/it]
 20%|█▉        | 1036/5198 [11:48:40<123:28:59, 106.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.30s/it][A100%|██████████| 1/1 [01:55<00:00, 115.30s/it]
 20%|█▉        | 1036/5198 [11:49:11<123:28:33, 106.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.30s/it][A100%|██████████| 1/1 [01:55<00:00, 115.30s/it]
 20%|█▉        | 1036/5198 [11:49:13<123:28:22, 106.80s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_972
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.99s/it][A100%|██████████| 1/1 [01:22<00:00, 82.99s/it]
 20%|█▉        | 1037/5198 [11:50:55<115:07:09, 99.60s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:40:50,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=1024, skipped=0, lr=[1.8856888718751494e-05], mom=[(0.9, 0.999)]
steps: 1024 loss: 0.5113 iter time (s): 81.159 samples/sec: 1.577

100%|██████████| 1/1 [01:22<00:00, 82.11s/it][A100%|██████████| 1/1 [01:22<00:00, 82.11s/it]
 20%|█▉        | 1037/5198 [11:51:09<114:53:55, 99.41s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.16s/it][A100%|██████████| 1/1 [01:22<00:00, 82.16s/it]
 20%|█▉        | 1037/5198 [11:50:56<114:53:32, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.07s/it][A100%|██████████| 1/1 [01:22<00:00, 82.07s/it]
 20%|█▉        | 1037/5198 [11:51:15<114:53:34, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.09s/it][A100%|██████████| 1/1 [01:22<00:00, 82.09s/it]
 20%|█▉        | 1037/5198 [11:49:39<114:52:50, 99.39s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.11s/it][A100%|██████████| 1/1 [01:22<00:00, 82.11s/it]
 20%|█▉        | 1037/5198 [11:50:02<114:53:26, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.09s/it][A100%|██████████| 1/1 [01:22<00:00, 82.09s/it]
 20%|█▉        | 1037/5198 [11:50:34<114:52:43, 99.39s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.10s/it][A100%|██████████| 1/1 [01:22<00:00, 82.10s/it]
 20%|█▉        | 1037/5198 [11:50:36<114:52:50, 99.39s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_973
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.03s/it][A100%|██████████| 1/1 [01:38<00:00, 98.03s/it]
 20%|█▉        | 1038/5198 [11:52:33<114:34:01, 99.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:42:28,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=1025, skipped=0, lr=[1.88542576950371e-05], mom=[(0.9, 0.999)]
steps: 1025 loss: 0.5528 iter time (s): 97.575 samples/sec: 1.312

100%|██████████| 1/1 [01:38<00:00, 98.39s/it][A100%|██████████| 1/1 [01:38<00:00, 98.39s/it]
 20%|█▉        | 1038/5198 [11:52:47<114:31:19, 99.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.48s/it][A100%|██████████| 1/1 [01:38<00:00, 98.48s/it]
 20%|█▉        | 1038/5198 [11:52:34<114:33:01, 99.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.45s/it][A100%|██████████| 1/1 [01:38<00:00, 98.45s/it]
 20%|█▉        | 1038/5198 [11:52:54<114:32:21, 99.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.44s/it][A100%|██████████| 1/1 [01:38<00:00, 98.44s/it]
 20%|█▉        | 1038/5198 [11:51:18<114:31:35, 99.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.51s/it][A100%|██████████| 1/1 [01:38<00:00, 98.51s/it]
 20%|█▉        | 1038/5198 [11:51:41<114:33:21, 99.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.51s/it][A100%|██████████| 1/1 [01:38<00:00, 98.51s/it]
 20%|█▉        | 1038/5198 [11:52:12<114:32:50, 99.13s/it]
100%|██████████| 1/1 [01:38<00:00, 98.49s/it][A100%|██████████| 1/1 [01:38<00:00, 98.49s/it]
 20%|█▉        | 1038/5198 [11:52:14<114:32:33, 99.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_974

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.57s/it][A100%|██████████| 1/1 [01:29<00:00, 89.57s/it]
 20%|█▉        | 1039/5198 [11:54:02<111:14:22, 96.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:43:58,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=1026, skipped=0, lr=[1.8851623830977872e-05], mom=[(0.9, 0.999)]
steps: 1026 loss: 0.5999 iter time (s): 88.477 samples/sec: 1.447

100%|██████████| 1/1 [01:29<00:00, 89.36s/it][A100%|██████████| 1/1 [01:29<00:00, 89.36s/it]
 20%|█▉        | 1039/5198 [11:54:17<111:07:06, 96.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.42s/it][A100%|██████████| 1/1 [01:29<00:00, 89.42s/it]
 20%|█▉        | 1039/5198 [11:54:03<111:09:36, 96.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.33s/it][A100%|██████████| 1/1 [01:29<00:00, 89.33s/it]
 20%|█▉        | 1039/5198 [11:54:23<111:07:18, 96.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.47s/it][A100%|██████████| 1/1 [01:29<00:00, 89.47s/it]
 20%|█▉        | 1039/5198 [11:52:47<111:09:32, 96.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.35s/it][A100%|██████████| 1/1 [01:29<00:00, 89.35s/it]
 20%|█▉        | 1039/5198 [11:53:10<111:08:20, 96.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.38s/it][A100%|██████████| 1/1 [01:29<00:00, 89.38s/it]
 20%|█▉        | 1039/5198 [11:53:41<111:08:42, 96.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.42s/it][A100%|██████████| 1/1 [01:29<00:00, 89.42s/it]
 20%|█▉        | 1039/5198 [11:53:43<111:09:15, 96.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_64
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.42s/it][A100%|██████████| 1/1 [02:05<00:00, 125.42s/it]
[2024-06-30 11:46:04,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=1027, skipped=0, lr=[1.8848987127418817e-05], mom=[(0.9, 0.999)]
steps: 1027 loss: 0.7841 iter time (s): 125.578 samples/sec: 1.019

100%|██████████| 1/1 [02:06<00:00, 126.73s/it][A100%|██████████| 1/1 [02:06<00:00, 126.73s/it]

100%|██████████| 1/1 [02:06<00:00, 126.70s/it][A100%|██████████| 1/1 [02:06<00:00, 126.70s/it]

100%|██████████| 1/1 [02:06<00:00, 126.86s/it][A100%|██████████| 1/1 [02:06<00:00, 126.86s/it]

100%|██████████| 1/1 [02:06<00:00, 126.72s/it][A100%|██████████| 1/1 [02:06<00:00, 126.72s/it]

100%|██████████| 1/1 [02:06<00:00, 126.63s/it][A100%|██████████| 1/1 [02:06<00:00, 126.63s/it]
Checkpointing at shard 1039

100%|██████████| 1/1 [02:06<00:00, 126.77s/it][A100%|██████████| 1/1 [02:06<00:00, 126.77s/it]

100%|██████████| 1/1 [02:06<00:00, 126.74s/it][A100%|██████████| 1/1 [02:06<00:00, 126.74s/it]
[2024-06-30 11:46:05,706] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1027 is about to be saved!
[2024-06-30 11:46:07,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_00-model_states.pt...
[2024-06-30 11:46:10,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_02-model_states.pt...
[2024-06-30 11:46:12,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_00-model_states.pt.
[2024-06-30 11:46:18,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_03-model_states.pt...
[2024-06-30 11:46:18,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_07-model_states.pt...
[2024-06-30 11:46:19,090] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_08-model_states.pt...
[2024-06-30 11:46:19,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_04-model_states.pt...
[2024-06-30 11:46:19,626] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_05-model_states.pt...
[2024-06-30 11:46:19,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_06-model_states.pt...
[2024-06-30 11:46:22,064] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_01-model_states.pt...
[2024-06-30 11:48:54,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_06-model_states.pt.
[2024-06-30 11:48:55,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_02-model_states.pt.
[2024-06-30 11:48:55,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_05_model_states.pt...
[2024-06-30 11:48:55,505] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_01_model_states.pt
[2024-06-30 11:48:55,505] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_01_model_states.pt...
[2024-06-30 11:48:55,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_03-model_states.pt.
[2024-06-30 11:48:55,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_05_model_states.pt.
[2024-06-30 11:48:55,696] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
[2024-06-30 11:48:55,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_01_model_states.pt.
[2024-06-30 11:48:55,713] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
[2024-06-30 11:48:55,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_04-model_states.pt.
[2024-06-30 11:48:55,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_05-model_states.pt.
[2024-06-30 11:48:55,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_08-model_states.pt.
[2024-06-30 11:48:56,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_01-model_states.pt.
[2024-06-30 11:48:56,642] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_02_model_states.pt...
[2024-06-30 11:48:56,697] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_02_model_states.pt.
[2024-06-30 11:48:56,697] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
[2024-06-30 11:48:57,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_03_model_states.pt...
[2024-06-30 11:48:57,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_03_model_states.pt.
[2024-06-30 11:48:57,157] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
[2024-06-30 11:48:57,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_04_model_states.pt...
[2024-06-30 11:48:57,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_04_model_states.pt.
[2024-06-30 11:48:57,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
[2024-06-30 11:48:57,997] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_00_model_states.pt
[2024-06-30 11:48:57,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_00_model_states.pt...
[2024-06-30 11:48:58,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_09-model_states.pt...
[2024-06-30 11:48:58,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_00_model_states.pt.
[2024-06-30 11:48:58,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
[2024-06-30 11:48:58,686] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_09-model_states.pt.
[2024-06-30 11:48:58,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_07_model_states.pt...
[2024-06-30 11:48:58,774] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_07_model_states.pt.
[2024-06-30 11:48:58,774] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
[2024-06-30 11:49:06,363] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/layer_07-model_states.pt.
[2024-06-30 11:49:07,237] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_06_model_states.pt...
[2024-06-30 11:49:07,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1027/mp_rank_06_model_states.pt.
[2024-06-30 11:49:07,315] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1027 is ready now!
Checkpoint saved using --- 181.6801073551178 seconds ---
 20%|██        | 1040/5198 [11:57:56<184:40:01, 159.89s/it] 20%|██        | 1040/5198 [11:59:14<185:42:46, 160.79s/it] 20%|██        | 1040/5198 [11:59:12<184:44:48, 159.95s/it] 20%|██        | 1040/5198 [11:58:52<184:37:10, 159.84s/it] 20%|██        | 1040/5198 [11:58:50<184:37:38, 159.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_975
 20%|██        | 1040/5198 [11:58:19<184:38:28, 159.86s/it] 20%|██        | 1040/5198 [11:59:26<184:47:48, 160.00s/it] 20%|██        | 1040/5198 [11:59:32<184:42:14, 159.92s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s]
[A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s]
[A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.60s/it][A100%|██████████| 1/1 [01:28<00:00, 88.60s/it]
 20%|██        | 1041/5198 [12:00:42<160:42:19, 139.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:50:38,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=1028, skipped=0, lr=[1.8846347585205866e-05], mom=[(0.9, 0.999)]
steps: 1028 loss: 0.5945 iter time (s): 90.756 samples/sec: 1.410

100%|██████████| 1/1 [01:31<00:00, 91.15s/it][A100%|██████████| 1/1 [01:31<00:00, 91.15s/it]
 20%|██        | 1041/5198 [12:00:57<160:56:50, 139.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.26s/it][A100%|██████████| 1/1 [01:31<00:00, 91.26s/it]
 20%|██        | 1041/5198 [12:00:44<160:57:00, 139.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.53s/it][A100%|██████████| 1/1 [01:31<00:00, 91.53s/it]
 20%|██        | 1041/5198 [12:01:03<161:00:56, 139.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.55s/it][A100%|██████████| 1/1 [01:31<00:00, 91.55s/it]
 20%|██        | 1041/5198 [11:59:27<160:59:42, 139.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.67s/it][A100%|██████████| 1/1 [01:31<00:00, 91.67s/it]
 20%|██        | 1041/5198 [11:59:51<161:01:12, 139.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.72s/it][A100%|██████████| 1/1 [01:31<00:00, 91.72s/it]
 20%|██        | 1041/5198 [12:00:24<161:01:21, 139.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_976

100%|██████████| 1/1 [01:31<00:00, 91.73s/it][A100%|██████████| 1/1 [01:31<00:00, 91.73s/it]
 20%|██        | 1041/5198 [12:00:22<161:01:46, 139.45s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.57s/it][A100%|██████████| 1/1 [01:35<00:00, 95.57s/it]
 20%|██        | 1042/5198 [12:02:18<145:35:41, 126.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:52:13,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=1029, skipped=0, lr=[1.8843705205185838e-05], mom=[(0.9, 0.999)]
steps: 1029 loss: 0.5420 iter time (s): 94.752 samples/sec: 1.351

100%|██████████| 1/1 [01:35<00:00, 95.81s/it][A100%|██████████| 1/1 [01:35<00:00, 95.81s/it]
 20%|██        | 1042/5198 [12:02:33<145:49:21, 126.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.86s/it][A100%|██████████| 1/1 [01:35<00:00, 95.86s/it]
 20%|██        | 1042/5198 [12:02:19<145:50:25, 126.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.80s/it][A100%|██████████| 1/1 [01:35<00:00, 95.80s/it]
 20%|██        | 1042/5198 [12:02:39<145:51:50, 126.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.82s/it][A100%|██████████| 1/1 [01:35<00:00, 95.82s/it]
 20%|██        | 1042/5198 [12:01:03<145:51:29, 126.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.75s/it][A100%|██████████| 1/1 [01:35<00:00, 95.75s/it]
 20%|██        | 1042/5198 [12:01:26<145:51:06, 126.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.74s/it][A100%|██████████| 1/1 [01:35<00:00, 95.74s/it]
 20%|██        | 1042/5198 [12:01:57<145:51:14, 126.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.79s/it][A100%|██████████| 1/1 [01:35<00:00, 95.79s/it]
 20%|██        | 1042/5198 [12:01:59<145:51:58, 126.35s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_977
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.08s/it][A100%|██████████| 1/1 [01:38<00:00, 98.08s/it]
 20%|██        | 1043/5198 [12:03:56<135:52:17, 117.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:53:52,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[1.884105998820648e-05], mom=[(0.9, 0.999)]
steps: 1030 loss: 0.6143 iter time (s): 97.118 samples/sec: 1.318

100%|██████████| 1/1 [01:38<00:00, 98.07s/it][A100%|██████████| 1/1 [01:38<00:00, 98.07s/it]
 20%|██        | 1043/5198 [12:04:11<136:00:35, 117.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.11s/it][A100%|██████████| 1/1 [01:38<00:00, 98.11s/it]
 20%|██        | 1043/5198 [12:03:57<136:02:15, 117.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.01s/it][A100%|██████████| 1/1 [01:38<00:00, 98.01s/it]
 20%|██        | 1043/5198 [12:04:17<136:01:08, 117.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.05s/it][A100%|██████████| 1/1 [01:38<00:00, 98.05s/it]
 20%|██        | 1043/5198 [12:02:41<136:01:44, 117.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.07s/it][A100%|██████████| 1/1 [01:38<00:00, 98.07s/it]
 20%|██        | 1043/5198 [12:03:04<136:01:45, 117.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.06s/it][A100%|██████████| 1/1 [01:38<00:00, 98.06s/it]
 20%|██        | 1043/5198 [12:03:35<136:01:48, 117.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.03s/it][A100%|██████████| 1/1 [01:38<00:00, 98.03s/it]
 20%|██        | 1043/5198 [12:03:37<136:01:44, 117.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_978
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.08s/it][A100%|██████████| 1/1 [01:25<00:00, 85.08s/it]
 20%|██        | 1044/5198 [12:05:21<124:34:10, 107.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:55:16,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=1031, skipped=0, lr=[1.883841193511644e-05], mom=[(0.9, 0.999)]
steps: 1031 loss: 0.5513 iter time (s): 83.845 samples/sec: 1.527

100%|██████████| 1/1 [01:24<00:00, 84.69s/it][A100%|██████████| 1/1 [01:24<00:00, 84.69s/it]
 20%|██        | 1044/5198 [12:05:35<124:30:21, 107.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.67s/it][A100%|██████████| 1/1 [01:24<00:00, 84.67s/it]
 20%|██        | 1044/5198 [12:05:22<124:31:01, 107.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.76s/it][A100%|██████████| 1/1 [01:24<00:00, 84.76s/it]
 20%|██        | 1044/5198 [12:05:42<124:32:08, 107.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.74s/it][A100%|██████████| 1/1 [01:24<00:00, 84.74s/it]
 20%|██        | 1044/5198 [12:04:06<124:32:14, 107.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.73s/it][A100%|██████████| 1/1 [01:24<00:00, 84.73s/it]
 20%|██        | 1044/5198 [12:04:29<124:31:57, 107.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.75s/it][A100%|██████████| 1/1 [01:24<00:00, 84.75s/it]
 20%|██        | 1044/5198 [12:05:00<124:32:20, 107.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [01:24<00:00, 84.74s/it][A100%|██████████| 1/1 [01:24<00:00, 84.74s/it]
 20%|██        | 1044/5198 [12:05:02<124:32:05, 107.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_979
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.43s/it][A100%|██████████| 1/1 [01:40<00:00, 100.43s/it]
 20%|██        | 1045/5198 [12:07:02<121:57:39, 105.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:56:57,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=1032, skipped=0, lr=[1.8835761046765285e-05], mom=[(0.9, 0.999)]
steps: 1032 loss: 0.5758 iter time (s): 100.121 samples/sec: 1.278

100%|██████████| 1/1 [01:41<00:00, 101.13s/it][A100%|██████████| 1/1 [01:41<00:00, 101.13s/it]
 20%|██        | 1045/5198 [12:07:17<122:08:12, 105.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.05s/it][A100%|██████████| 1/1 [01:41<00:00, 101.05s/it]
 20%|██        | 1045/5198 [12:07:03<122:06:55, 105.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.96s/it][A100%|██████████| 1/1 [01:40<00:00, 100.96s/it]
 20%|██        | 1045/5198 [12:07:23<122:05:48, 105.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.01s/it][A100%|██████████| 1/1 [01:41<00:00, 101.02s/it]
 20%|██        | 1045/5198 [12:05:47<122:07:02, 105.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 101.00s/it][A100%|██████████| 1/1 [01:40<00:00, 101.00s/it]
 20%|██        | 1045/5198 [12:06:10<122:06:27, 105.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 101.00s/it][A100%|██████████| 1/1 [01:40<00:00, 101.00s/it]
 20%|██        | 1045/5198 [12:06:41<122:06:44, 105.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.01s/it][A100%|██████████| 1/1 [01:41<00:00, 101.01s/it]
 20%|██        | 1045/5198 [12:06:43<122:06:47, 105.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_980
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.00s/it][A100%|██████████| 1/1 [01:40<00:00, 100.00s/it]
 20%|██        | 1046/5198 [12:08:42<119:58:24, 104.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 11:58:37,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=1033, skipped=0, lr=[1.8833107324003475e-05], mom=[(0.9, 0.999)]
steps: 1033 loss: 0.5256 iter time (s): 99.094 samples/sec: 1.292

100%|██████████| 1/1 [01:40<00:00, 100.04s/it][A100%|██████████| 1/1 [01:40<00:00, 100.04s/it]
 20%|██        | 1046/5198 [12:08:57<120:05:38, 104.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.03s/it][A100%|██████████| 1/1 [01:40<00:00, 100.03s/it]
 20%|██        | 1046/5198 [12:08:43<120:04:25, 104.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.07s/it][A100%|██████████| 1/1 [01:40<00:00, 100.07s/it]
 20%|██        | 1046/5198 [12:09:03<120:04:29, 104.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.06s/it][A100%|██████████| 1/1 [01:40<00:00, 100.06s/it]
 20%|██        | 1046/5198 [12:07:27<120:05:05, 104.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.16s/it][A100%|██████████| 1/1 [01:40<00:00, 100.16s/it]
 20%|██        | 1046/5198 [12:07:50<120:06:45, 104.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.11s/it][A100%|██████████| 1/1 [01:40<00:00, 100.11s/it]
 20%|██        | 1046/5198 [12:08:21<120:06:00, 104.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.11s/it][A100%|██████████| 1/1 [01:40<00:00, 100.11s/it]
 20%|██        | 1046/5198 [12:08:23<120:05:53, 104.13s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_981
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.30s/it][A100%|██████████| 1/1 [01:46<00:00, 106.30s/it]
 20%|██        | 1047/5198 [12:10:28<120:45:02, 104.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:00:24,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=1034, skipped=0, lr=[1.8830450767682405e-05], mom=[(0.9, 0.999)]
steps: 1034 loss: 0.5576 iter time (s): 105.565 samples/sec: 1.213

100%|██████████| 1/1 [01:46<00:00, 106.43s/it][A100%|██████████| 1/1 [01:46<00:00, 106.43s/it]
 20%|██        | 1047/5198 [12:10:43<120:52:00, 104.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.48s/it][A100%|██████████| 1/1 [01:46<00:00, 106.48s/it]
 20%|██        | 1047/5198 [12:10:30<120:51:59, 104.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.49s/it][A100%|██████████| 1/1 [01:46<00:00, 106.49s/it]
 20%|██        | 1047/5198 [12:10:49<120:52:25, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.46s/it][A100%|██████████| 1/1 [01:46<00:00, 106.46s/it]
 20%|██        | 1047/5198 [12:09:13<120:52:03, 104.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.42s/it][A100%|██████████| 1/1 [01:46<00:00, 106.42s/it]
 20%|██        | 1047/5198 [12:09:37<120:52:29, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.45s/it][A100%|██████████| 1/1 [01:46<00:00, 106.45s/it]
 20%|██        | 1047/5198 [12:10:08<120:52:30, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.47s/it][A100%|██████████| 1/1 [01:46<00:00, 106.47s/it]
 20%|██        | 1047/5198 [12:10:10<120:52:47, 104.83s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_982
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.10s/it][A100%|██████████| 1/1 [01:50<00:00, 110.10s/it]
 20%|██        | 1048/5198 [12:12:18<122:36:30, 106.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:02:14,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=1035, skipped=0, lr=[1.882779137865435e-05], mom=[(0.9, 0.999)]
steps: 1035 loss: 0.5667 iter time (s): 109.367 samples/sec: 1.170

100%|██████████| 1/1 [01:50<00:00, 110.35s/it][A100%|██████████| 1/1 [01:50<00:00, 110.35s/it]
 20%|██        | 1048/5198 [12:12:34<122:45:21, 106.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.31s/it][A100%|██████████| 1/1 [01:50<00:00, 110.31s/it]
 20%|██        | 1048/5198 [12:12:20<122:44:17, 106.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.29s/it][A100%|██████████| 1/1 [01:50<00:00, 110.29s/it]
 20%|██        | 1048/5198 [12:12:40<122:44:12, 106.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.29s/it][A100%|██████████| 1/1 [01:50<00:00, 110.29s/it]
 20%|██        | 1048/5198 [12:11:04<122:43:58, 106.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.28s/it][A100%|██████████| 1/1 [01:50<00:00, 110.28s/it]
 20%|██        | 1048/5198 [12:11:27<122:43:58, 106.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.32s/it][A100%|██████████| 1/1 [01:50<00:00, 110.32s/it]
 20%|██        | 1048/5198 [12:11:58<122:44:45, 106.48s/it]
100%|██████████| 1/1 [01:50<00:00, 110.29s/it][A100%|██████████| 1/1 [01:50<00:00, 110.29s/it]
 20%|██        | 1048/5198 [12:12:00<122:44:26, 106.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_983

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.38s/it][A100%|██████████| 1/1 [01:43<00:00, 103.38s/it]
 20%|██        | 1049/5198 [12:14:02<121:34:47, 105.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:03:57,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=1036, skipped=0, lr=[1.882512915777252e-05], mom=[(0.9, 0.999)]
steps: 1036 loss: 0.5367 iter time (s): 102.308 samples/sec: 1.251

100%|██████████| 1/1 [01:43<00:00, 103.19s/it][A100%|██████████| 1/1 [01:43<00:00, 103.19s/it]
 20%|██        | 1049/5198 [12:14:17<121:35:27, 105.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.33s/it][A100%|██████████| 1/1 [01:43<00:00, 103.34s/it]
 20%|██        | 1049/5198 [12:14:03<121:37:37, 105.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.30s/it][A100%|██████████| 1/1 [01:43<00:00, 103.30s/it]
 20%|██        | 1049/5198 [12:14:23<121:36:46, 105.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.32s/it][A100%|██████████| 1/1 [01:43<00:00, 103.32s/it]
 20%|██        | 1049/5198 [12:12:47<121:37:06, 105.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.30s/it][A100%|██████████| 1/1 [01:43<00:00, 103.30s/it]
 20%|██        | 1049/5198 [12:13:10<121:36:38, 105.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.28s/it][A100%|██████████| 1/1 [01:43<00:00, 103.28s/it]
 20%|██        | 1049/5198 [12:13:41<121:36:45, 105.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.28s/it][A100%|██████████| 1/1 [01:43<00:00, 103.28s/it]
 20%|██        | 1049/5198 [12:13:43<121:36:39, 105.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_984
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.18s/it][A100%|██████████| 1/1 [01:43<00:00, 103.18s/it]
 20%|██        | 1050/5198 [12:15:45<120:45:52, 104.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:05:41,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=1037, skipped=0, lr=[1.882246410589101e-05], mom=[(0.9, 0.999)]
steps: 1037 loss: 0.5714 iter time (s): 102.233 samples/sec: 1.252

100%|██████████| 1/1 [01:43<00:00, 103.17s/it][A100%|██████████| 1/1 [01:43<00:00, 103.17s/it]
 20%|██        | 1050/5198 [12:16:00<120:45:37, 104.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.14s/it][A100%|██████████| 1/1 [01:43<00:00, 103.14s/it]
 20%|██        | 1050/5198 [12:15:47<120:46:27, 104.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.23s/it][A100%|██████████| 1/1 [01:43<00:00, 103.24s/it]
 20%|██        | 1050/5198 [12:16:06<120:47:46, 104.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.17s/it][A100%|██████████| 1/1 [01:43<00:00, 103.17s/it]
 20%|██        | 1050/5198 [12:14:30<120:46:48, 104.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.22s/it][A100%|██████████| 1/1 [01:43<00:00, 103.23s/it]
 20%|██        | 1050/5198 [12:14:54<120:47:27, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.20s/it][A100%|██████████| 1/1 [01:43<00:00, 103.20s/it]
 20%|██        | 1050/5198 [12:15:25<120:47:07, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.22s/it][A100%|██████████| 1/1 [01:43<00:00, 103.22s/it]
 20%|██        | 1050/5198 [12:15:27<120:47:25, 104.83s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_985
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.72s/it][A100%|██████████| 1/1 [01:26<00:00, 86.72s/it]
 20%|██        | 1051/5198 [12:17:12<114:30:29, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:07:07,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=1038, skipped=0, lr=[1.8819796223864843e-05], mom=[(0.9, 0.999)]
steps: 1038 loss: 0.5486 iter time (s): 85.346 samples/sec: 1.500

100%|██████████| 1/1 [01:26<00:00, 86.29s/it][A100%|██████████| 1/1 [01:26<00:00, 86.30s/it]
 20%|██        | 1051/5198 [12:17:26<114:20:16, 99.26s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.20s/it][A100%|██████████| 1/1 [01:26<00:00, 86.20s/it]
 20%|██        | 1051/5198 [12:17:13<114:18:59, 99.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.18s/it][A100%|██████████| 1/1 [01:26<00:00, 86.18s/it]
 20%|██        | 1051/5198 [12:17:32<114:19:21, 99.24s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.25s/it][A100%|██████████| 1/1 [01:26<00:00, 86.25s/it]
 20%|██        | 1051/5198 [12:15:57<114:20:01, 99.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.22s/it][A100%|██████████| 1/1 [01:26<00:00, 86.22s/it]
 20%|██        | 1051/5198 [12:16:20<114:19:58, 99.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.22s/it][A100%|██████████| 1/1 [01:26<00:00, 86.22s/it]
 20%|██        | 1051/5198 [12:16:51<114:19:47, 99.25s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.23s/it][A100%|██████████| 1/1 [01:26<00:00, 86.24s/it]
 20%|██        | 1051/5198 [12:16:53<114:20:11, 99.26s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_986
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.32s/it][A100%|██████████| 1/1 [01:39<00:00, 99.32s/it]
 20%|██        | 1052/5198 [12:18:51<114:28:01, 99.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:08:47,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=1039, skipped=0, lr=[1.881712551254994e-05], mom=[(0.9, 0.999)]
steps: 1039 loss: 0.6051 iter time (s): 98.810 samples/sec: 1.295

100%|██████████| 1/1 [01:39<00:00, 99.68s/it][A100%|██████████| 1/1 [01:39<00:00, 99.68s/it]
 20%|██        | 1052/5198 [12:19:06<114:27:44, 99.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.79s/it][A100%|██████████| 1/1 [01:39<00:00, 99.79s/it]
 20%|██        | 1052/5198 [12:18:53<114:29:03, 99.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.77s/it][A100%|██████████| 1/1 [01:39<00:00, 99.77s/it]
 20%|██        | 1052/5198 [12:19:12<114:28:45, 99.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.77s/it][A100%|██████████| 1/1 [01:39<00:00, 99.77s/it]
 20%|██        | 1052/5198 [12:17:36<114:29:18, 99.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.79s/it][A100%|██████████| 1/1 [01:39<00:00, 99.79s/it]
 20%|██        | 1052/5198 [12:18:00<114:29:40, 99.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.79s/it][A100%|██████████| 1/1 [01:39<00:00, 99.79s/it]
 20%|██        | 1052/5198 [12:18:31<114:29:30, 99.41s/it]
100%|██████████| 1/1 [01:39<00:00, 99.76s/it][A100%|██████████| 1/1 [01:39<00:00, 99.76s/it]
 20%|██        | 1052/5198 [12:18:33<114:29:09, 99.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_987

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.74s/it][A100%|██████████| 1/1 [01:29<00:00, 89.75s/it]
 20%|██        | 1053/5198 [12:20:21<111:06:58, 96.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:10:16,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[1.881445197280313e-05], mom=[(0.9, 0.999)]
steps: 1040 loss: 0.5958 iter time (s): 88.622 samples/sec: 1.444

100%|██████████| 1/1 [01:29<00:00, 89.60s/it][A100%|██████████| 1/1 [01:29<00:00, 89.60s/it]
 20%|██        | 1053/5198 [12:20:36<111:03:23, 96.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.57s/it][A100%|██████████| 1/1 [01:29<00:00, 89.57s/it]
 20%|██        | 1053/5198 [12:20:22<111:03:41, 96.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.54s/it][A100%|██████████| 1/1 [01:29<00:00, 89.54s/it]
 20%|██        | 1053/5198 [12:20:42<111:02:57, 96.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.53s/it][A100%|██████████| 1/1 [01:29<00:00, 89.53s/it]
 20%|██        | 1053/5198 [12:19:06<111:02:58, 96.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.62s/it][A100%|██████████| 1/1 [01:29<00:00, 89.62s/it]
 20%|██        | 1053/5198 [12:19:29<111:05:13, 96.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.64s/it][A100%|██████████| 1/1 [01:29<00:00, 89.64s/it]
 20%|██        | 1053/5198 [12:20:02<111:05:05, 96.48s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_988

100%|██████████| 1/1 [01:29<00:00, 89.65s/it][A100%|██████████| 1/1 [01:29<00:00, 89.65s/it]
 20%|██        | 1053/5198 [12:20:00<111:05:39, 96.49s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.65s/it][A100%|██████████| 1/1 [01:43<00:00, 103.65s/it]
 20%|██        | 1054/5198 [12:22:05<113:35:36, 98.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:12:00,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=1041, skipped=0, lr=[1.8811775605482153e-05], mom=[(0.9, 0.999)]
steps: 1041 loss: 0.4986 iter time (s): 103.035 samples/sec: 1.242

100%|██████████| 1/1 [01:44<00:00, 104.20s/it][A100%|██████████| 1/1 [01:44<00:00, 104.20s/it]
 20%|██        | 1054/5198 [12:22:20<113:42:35, 98.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.12s/it][A100%|██████████| 1/1 [01:44<00:00, 104.12s/it]
 20%|██        | 1054/5198 [12:22:06<113:41:09, 98.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.17s/it][A100%|██████████| 1/1 [01:44<00:00, 104.17s/it]
 20%|██        | 1054/5198 [12:22:26<113:41:40, 98.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.15s/it][A100%|██████████| 1/1 [01:44<00:00, 104.15s/it]
 20%|██        | 1054/5198 [12:20:50<113:41:04, 98.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.04s/it][A100%|██████████| 1/1 [01:44<00:00, 104.04s/it]
 20%|██        | 1054/5198 [12:21:13<113:40:17, 98.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.02s/it][A100%|██████████| 1/1 [01:44<00:00, 104.02s/it]
 20%|██        | 1054/5198 [12:21:44<113:40:13, 98.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.04s/it][A100%|██████████| 1/1 [01:44<00:00, 104.04s/it]
 20%|██        | 1054/5198 [12:21:46<113:40:25, 98.75s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_989
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.89s/it][A100%|██████████| 1/1 [01:38<00:00, 98.89s/it]
 20%|██        | 1055/5198 [12:23:44<113:39:19, 98.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:13:39,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=1042, skipped=0, lr=[1.880909641144565e-05], mom=[(0.9, 0.999)]
steps: 1042 loss: 0.6120 iter time (s): 97.840 samples/sec: 1.308

100%|██████████| 1/1 [01:38<00:00, 98.73s/it][A100%|██████████| 1/1 [01:38<00:00, 98.73s/it]
 20%|██        | 1055/5198 [12:23:58<113:40:14, 98.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.83s/it][A100%|██████████| 1/1 [01:38<00:00, 98.83s/it]
 20%|██        | 1055/5198 [12:23:45<113:41:12, 98.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.84s/it][A100%|██████████| 1/1 [01:38<00:00, 98.84s/it]
 20%|██        | 1055/5198 [12:24:05<113:41:42, 98.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.90s/it][A100%|██████████| 1/1 [01:38<00:00, 98.90s/it]
 20%|██        | 1055/5198 [12:22:29<113:42:27, 98.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.84s/it][A100%|██████████| 1/1 [01:38<00:00, 98.84s/it]
 20%|██        | 1055/5198 [12:22:52<113:40:44, 98.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.85s/it][A100%|██████████| 1/1 [01:38<00:00, 98.85s/it]
 20%|██        | 1055/5198 [12:23:23<113:40:51, 98.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.85s/it][A100%|██████████| 1/1 [01:38<00:00, 98.85s/it]
 20%|██        | 1055/5198 [12:23:25<113:41:02, 98.78s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_65
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.34s/it][A100%|██████████| 1/1 [02:01<00:00, 121.34s/it]
 20%|██        | 1056/5198 [12:25:45<121:26:45, 105.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:15:41,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=1043, skipped=0, lr=[1.880641439155318e-05], mom=[(0.9, 0.999)]
steps: 1043 loss: 0.7816 iter time (s): 120.939 samples/sec: 1.058

100%|██████████| 1/1 [02:01<00:00, 121.92s/it][A100%|██████████| 1/1 [02:01<00:00, 121.92s/it]
 20%|██        | 1056/5198 [12:26:00<121:38:08, 105.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.01s/it][A100%|██████████| 1/1 [02:02<00:00, 122.01s/it]
 20%|██        | 1056/5198 [12:25:47<121:40:42, 105.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.94s/it][A100%|██████████| 1/1 [02:01<00:00, 121.94s/it]
 20%|██        | 1056/5198 [12:26:07<121:39:40, 105.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.96s/it][A100%|██████████| 1/1 [02:01<00:00, 121.96s/it]
 20%|██        | 1056/5198 [12:24:31<121:40:30, 105.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.99s/it][A100%|██████████| 1/1 [02:01<00:00, 121.99s/it]
 20%|██        | 1056/5198 [12:24:54<121:39:50, 105.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.96s/it][A100%|██████████| 1/1 [02:01<00:00, 121.96s/it]
 20%|██        | 1056/5198 [12:25:27<121:39:29, 105.74s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_990

100%|██████████| 1/1 [02:01<00:00, 121.99s/it][A100%|██████████| 1/1 [02:01<00:00, 121.99s/it]
 20%|██        | 1056/5198 [12:25:25<121:39:56, 105.75s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.40s/it][A100%|██████████| 1/1 [01:54<00:00, 114.40s/it]
 20%|██        | 1057/5198 [12:27:40<124:30:43, 108.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:17:36,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=1044, skipped=0, lr=[1.880372954666519e-05], mom=[(0.9, 0.999)]
steps: 1044 loss: 0.5773 iter time (s): 113.369 samples/sec: 1.129

100%|██████████| 1/1 [01:54<00:00, 114.40s/it][A100%|██████████| 1/1 [01:54<00:00, 114.40s/it]
 20%|██        | 1057/5198 [12:27:55<124:36:23, 108.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.39s/it][A100%|██████████| 1/1 [01:54<00:00, 114.39s/it]
 20%|██        | 1057/5198 [12:27:42<124:37:55, 108.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.40s/it][A100%|██████████| 1/1 [01:54<00:00, 114.40s/it]
 20%|██        | 1057/5198 [12:28:01<124:37:26, 108.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.31s/it][A100%|██████████| 1/1 [01:54<00:00, 114.31s/it]
 20%|██        | 1057/5198 [12:26:25<124:36:07, 108.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.31s/it][A100%|██████████| 1/1 [01:54<00:00, 114.31s/it]
 20%|██        | 1057/5198 [12:26:48<124:35:40, 108.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.37s/it][A100%|██████████| 1/1 [01:54<00:00, 114.37s/it]
 20%|██        | 1057/5198 [12:27:22<124:36:41, 108.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_991

100%|██████████| 1/1 [01:54<00:00, 114.37s/it][A100%|██████████| 1/1 [01:54<00:00, 114.37s/it]
 20%|██        | 1057/5198 [12:27:20<124:36:59, 108.34s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.69s/it][A100%|██████████| 1/1 [02:03<00:00, 123.69s/it]
 20%|██        | 1058/5198 [12:29:43<129:50:32, 112.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:19:40,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=1045, skipped=0, lr=[1.880104187764306e-05], mom=[(0.9, 0.999)]
steps: 1045 loss: 0.5785 iter time (s): 123.030 samples/sec: 1.040

100%|██████████| 1/1 [02:04<00:00, 124.13s/it][A100%|██████████| 1/1 [02:04<00:00, 124.13s/it]
 20%|██        | 1058/5198 [12:29:59<130:01:54, 113.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.00s/it][A100%|██████████| 1/1 [02:04<00:00, 124.00s/it]
 20%|██        | 1058/5198 [12:29:46<130:00:20, 113.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.97s/it][A100%|██████████| 1/1 [02:03<00:00, 123.97s/it]
 20%|██        | 1058/5198 [12:30:05<129:59:16, 113.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.04s/it][A100%|██████████| 1/1 [02:04<00:00, 124.04s/it]
 20%|██        | 1058/5198 [12:28:29<129:59:43, 113.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.07s/it][A100%|██████████| 1/1 [02:04<00:00, 124.07s/it]
 20%|██        | 1058/5198 [12:28:53<130:00:14, 113.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.01s/it][A100%|██████████| 1/1 [02:04<00:00, 124.01s/it]
 20%|██        | 1058/5198 [12:29:26<129:59:32, 113.04s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_992

100%|██████████| 1/1 [02:04<00:00, 124.01s/it][A100%|██████████| 1/1 [02:04<00:00, 124.01s/it]
 20%|██        | 1058/5198 [12:29:24<129:59:42, 113.04s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.29s/it][A100%|██████████| 1/1 [01:33<00:00, 93.29s/it]
 20%|██        | 1059/5198 [12:31:17<123:03:38, 107.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:21:12,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=1046, skipped=0, lr=[1.8798351385349046e-05], mom=[(0.9, 0.999)]
steps: 1046 loss: 0.5829 iter time (s): 91.488 samples/sec: 1.399

100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 20%|██        | 1059/5198 [12:31:31<122:52:31, 106.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.38s/it][A100%|██████████| 1/1 [01:32<00:00, 92.38s/it]
 20%|██        | 1059/5198 [12:31:18<122:51:00, 106.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.52s/it][A100%|██████████| 1/1 [01:32<00:00, 92.52s/it]
 20%|██        | 1059/5198 [12:31:38<122:53:01, 106.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.45s/it][A100%|██████████| 1/1 [01:32<00:00, 92.45s/it]
 20%|██        | 1059/5198 [12:30:02<122:51:56, 106.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.43s/it][A100%|██████████| 1/1 [01:32<00:00, 92.43s/it]
 20%|██        | 1059/5198 [12:30:25<122:51:56, 106.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.42s/it][A100%|██████████| 1/1 [01:32<00:00, 92.42s/it]
 20%|██        | 1059/5198 [12:30:56<122:51:20, 106.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.45s/it][A100%|██████████| 1/1 [01:32<00:00, 92.45s/it]
 20%|██        | 1059/5198 [12:30:58<122:51:51, 106.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_993
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.99s/it][A100%|██████████| 1/1 [01:30<00:00, 90.99s/it]
[2024-06-30 12:22:43,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=1047, skipped=0, lr=[1.8795658070646338e-05], mom=[(0.9, 0.999)]
steps: 1047 loss: 0.6253 iter time (s): 89.989 samples/sec: 1.422

100%|██████████| 1/1 [01:30<00:00, 90.88s/it][A100%|██████████| 1/1 [01:30<00:00, 90.88s/it]

100%|██████████| 1/1 [01:30<00:00, 90.86s/it][A100%|██████████| 1/1 [01:30<00:00, 90.86s/it]

100%|██████████| 1/1 [01:30<00:00, 90.89s/it][A100%|██████████| 1/1 [01:30<00:00, 90.89s/it]

100%|██████████| 1/1 [01:30<00:00, 90.90s/it][A100%|██████████| 1/1 [01:30<00:00, 90.90s/it]

100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]

100%|██████████| 1/1 [01:30<00:00, 90.92s/it][A100%|██████████| 1/1 [01:30<00:00, 90.92s/it]

100%|██████████| 1/1 [01:30<00:00, 90.93s/it][A100%|██████████| 1/1 [01:30<00:00, 90.93s/it]
Checkpointing at shard 1059
[2024-06-30 12:22:44,513] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1047 is about to be saved!
[2024-06-30 12:22:46,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_00-model_states.pt...
[2024-06-30 12:22:50,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_02-model_states.pt...
[2024-06-30 12:22:51,442] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_00-model_states.pt.
[2024-06-30 12:22:58,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_04-model_states.pt...
[2024-06-30 12:22:58,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_03-model_states.pt...
[2024-06-30 12:22:58,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_08-model_states.pt...
[2024-06-30 12:22:58,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_06-model_states.pt...
[2024-06-30 12:22:59,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_07-model_states.pt...
[2024-06-30 12:22:59,501] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_05-model_states.pt...
[2024-06-30 12:23:00,599] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_01-model_states.pt...
[2024-06-30 12:26:46,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_02-model_states.pt.
[2024-06-30 12:26:46,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_01-model_states.pt.
[2024-06-30 12:26:46,914] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_01_model_states.pt
[2024-06-30 12:26:46,914] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_01_model_states.pt...
[2024-06-30 12:26:47,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_01_model_states.pt.
[2024-06-30 12:26:47,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
[2024-06-30 12:26:47,285] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_00_model_states.pt
[2024-06-30 12:26:47,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_00_model_states.pt...
[2024-06-30 12:26:49,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_00_model_states.pt.
[2024-06-30 12:26:49,741] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
[2024-06-30 12:26:54,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_07-model_states.pt.
[2024-06-30 12:26:54,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_06_model_states.pt...
[2024-06-30 12:26:56,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_06_model_states.pt.
[2024-06-30 12:26:56,040] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
[2024-06-30 12:26:56,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_05-model_states.pt.
[2024-06-30 12:26:57,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_04_model_states.pt...
[2024-06-30 12:26:58,402] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_04_model_states.pt.
[2024-06-30 12:26:58,402] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
[2024-06-30 12:26:59,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_04-model_states.pt.
[2024-06-30 12:26:59,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_03-model_states.pt.
[2024-06-30 12:26:59,818] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_03_model_states.pt...
[2024-06-30 12:27:00,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_03_model_states.pt.
[2024-06-30 12:27:00,424] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
[2024-06-30 12:27:00,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_02_model_states.pt...
[2024-06-30 12:27:01,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_02_model_states.pt.
[2024-06-30 12:27:01,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
[2024-06-30 12:27:01,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_08-model_states.pt.
[2024-06-30 12:27:02,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_06-model_states.pt.
[2024-06-30 12:27:02,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_09-model_states.pt...
[2024-06-30 12:27:03,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_05_model_states.pt...
[2024-06-30 12:27:03,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_05_model_states.pt.
[2024-06-30 12:27:03,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
[2024-06-30 12:27:03,684] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/layer_09-model_states.pt.
[2024-06-30 12:27:03,686] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_07_model_states.pt...
[2024-06-30 12:27:03,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_4_checkpoint/global_step1047/mp_rank_07_model_states.pt.
[2024-06-30 12:27:03,747] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1047 is ready now!
Checkpoint saved using --- 259.2351117134094 seconds ---
 20%|██        | 1060/5198 [12:37:09<206:51:50, 179.97s/it] 20%|██        | 1060/5198 [12:37:10<207:56:38, 180.91s/it] 20%|██        | 1060/5198 [12:35:52<206:47:02, 179.90s/it] 20%|██        | 1060/5198 [12:37:22<206:55:05, 180.02s/it] 20%|██        | 1060/5198 [12:36:48<206:44:06, 179.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_994
 20%|██        | 1060/5198 [12:36:15<206:45:22, 179.88s/it] 20%|██        | 1060/5198 [12:36:46<206:44:18, 179.86s/it] 20%|██        | 1060/5198 [12:37:28<206:48:35, 179.92s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.58s/it][A100%|██████████| 1/1 [01:30<00:00, 90.58s/it]
 20%|██        | 1061/5198 [12:38:41<176:47:33, 153.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-30 12:28:36,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=1048, skipped=0, lr=[1.8792961934399006e-05], mom=[(0.9, 0.999)]
steps: 1048 loss: 0.5620 iter time (s): 92.725 samples/sec: 1.380

100%|██████████| 1/1 [01:33<00:00, 93.14s/it][A100%|██████████| 1/1 [01:33<00:00, 93.14s/it]
 20%|██        | 1061/5198 [12:38:55<176:57:26, 153.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.21s/it][A100%|██████████| 1/1 [01:33<00:00, 93.21s/it]
 20%|██        | 1061/5198 [12:38:42<176:56:36, 153.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.42s/it][A100%|██████████| 1/1 [01:33<00:00, 93.42s/it]
 20%|██        | 1061/5198 [12:39:02<176:58:48, 154.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.49s/it][A100%|██████████| 1/1 [01:33<00:00, 93.49s/it]
 20%|██        | 1061/5198 [12:37:26<176:59:03, 154.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.54s/it][A100%|██████████| 1/1 [01:33<00:00, 93.54s/it]
 20%|██        | 1061/5198 [12:37:49<176:59:03, 154.01s/it]
100%|██████████| 1/1 [01:33<00:00, 93.55s/it][A100%|██████████| 1/1 [01:33<00:00, 93.55s/it]
 20%|██        | 1061/5198 [12:38:20<176:58:24, 154.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.60s/it][A100%|██████████| 1/1 [01:33<00:00, 93.60s/it]
 20%|██        | 1061/5198 [12:38:22<176:59:15, 154.01s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_995
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A