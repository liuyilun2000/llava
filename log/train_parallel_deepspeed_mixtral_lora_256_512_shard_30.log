[2024-05-29 13:30:13,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 13:30:16,896] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 13:30:16,896] [INFO] [runner.py:568:main] cmd = /home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train_parallel_deepspeed_mixtral_lora.py --num_stages=8 --lora_r=256 --lora_alpha=512 --save_model_shard=6 --skip_shard=30 --checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint
[2024-05-29 13:30:19,768] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 13:30:21,145] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-05-29 13:30:21,145] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-05-29 13:30:21,145] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-05-29 13:30:21,145] [INFO] [launch.py:163:main] dist_world_size=8
[2024-05-29 13:30:21,145] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-05-29 13:30:21,155] [INFO] [launch.py:253:main] process 1751098 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=0', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:21,165] [INFO] [launch.py:253:main] process 1751099 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=1', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:21,175] [INFO] [launch.py:253:main] process 1751100 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=2', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:21,192] [INFO] [launch.py:253:main] process 1751101 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=3', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:21,209] [INFO] [launch.py:253:main] process 1751102 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=4', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:21,225] [INFO] [launch.py:253:main] process 1751103 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=5', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:21,240] [INFO] [launch.py:253:main] process 1751104 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=6', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:21,256] [INFO] [launch.py:253:main] process 1751105 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=7', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint']
[2024-05-29 13:30:26,859] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 13:30:27,024] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 13:30:27,069] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 13:30:27,520] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 13:30:27,520] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 13:30:27,658] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 13:30:27,697] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:12,  1.50it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:14,  1.33it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:01<00:13,  1.32it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:01<00:15,  1.13it/s]Loading checkpoint shards:  15%|█▌        | 3/20 [00:02<00:13,  1.22it/s]Loading checkpoint shards:  15%|█▌        | 3/20 [00:02<00:14,  1.19it/s]Loading checkpoint shards:  20%|██        | 4/20 [00:03<00:12,  1.29it/s]Loading checkpoint shards:  20%|██        | 4/20 [00:03<00:12,  1.26it/s][2024-05-29 13:30:33,199] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  25%|██▌       | 5/20 [00:03<00:11,  1.36it/s]Loading checkpoint shards:  25%|██▌       | 5/20 [00:03<00:11,  1.34it/s][2024-05-29 13:30:33,688] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  30%|███       | 6/20 [00:04<00:10,  1.37it/s][2024-05-29 13:30:34,245] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  30%|███       | 6/20 [00:04<00:10,  1.31it/s][2024-05-29 13:30:34,814] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  35%|███▌      | 7/20 [00:05<00:09,  1.34it/s]Loading checkpoint shards:  35%|███▌      | 7/20 [00:05<00:10,  1.23it/s]Loading checkpoint shards:  40%|████      | 8/20 [00:06<00:10,  1.11it/s]Loading checkpoint shards:  40%|████      | 8/20 [00:06<00:11,  1.04it/s]Loading checkpoint shards:  45%|████▌     | 9/20 [00:07<00:09,  1.12it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  45%|████▌     | 9/20 [00:07<00:10,  1.09it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:17,  1.09it/s]Loading checkpoint shards:  50%|█████     | 10/20 [00:08<00:09,  1.08it/s]Loading checkpoint shards:  50%|█████     | 10/20 [00:08<00:09,  1.01it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:18,  1.01it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:01<00:18,  1.01s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:09<00:09,  1.01s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:09<00:09,  1.03s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:02<00:16,  1.03it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:20,  1.13s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:10<00:08,  1.06s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:11<00:08,  1.12s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:03<00:15,  1.01it/s]Loading checkpoint shards:  15%|█▌        | 3/20 [00:03<00:20,  1.20s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:11<00:07,  1.03s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:12<00:08,  1.15s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:05<00:15,  1.07s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:12<00:06,  1.02s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:04<00:19,  1.21s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:13<00:06,  1.15s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:06<00:14,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:13<00:05,  1.01s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:05<00:18,  1.21s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:14<00:05,  1.08s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:07<00:13,  1.04s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:14<00:04,  1.05s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:15<00:04,  1.00s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:07<00:17,  1.24s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:08<00:13,  1.11s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:15<00:03,  1.06s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:16<00:02,  1.01it/s]Loading checkpoint shards:  35%|███▌      | 7/20 [00:08<00:16,  1.24s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:16<00:02,  1.04s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:09<00:12,  1.12s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:17<00:02,  1.01s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:09<00:14,  1.20s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:10<00:10,  1.09s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:18<00:01,  1.05s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:18<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:18<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:18<00:00,  1.08it/s]
Loading checkpoint shards:  45%|████▌     | 9/20 [00:10<00:12,  1.11s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:18<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:18<00:00,  1.06it/s]
Loading checkpoint shards:  55%|█████▌    | 11/20 [00:11<00:09,  1.10s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:11<00:10,  1.04s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:12<00:08,  1.12s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:12<00:10,  1.13s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:13<00:07,  1.11s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:13<00:08,  1.12s/it][2024-05-29 13:30:51,980] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  70%|███████   | 14/20 [00:15<00:06,  1.12s/it][2024-05-29 13:30:52,675] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  65%|██████▌   | 13/20 [00:14<00:07,  1.13s/it]Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
Loading checkpoint shards:  75%|███████▌  | 15/20 [00:16<00:05,  1.12s/it]Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
Loading checkpoint shards:  70%|███████   | 14/20 [00:15<00:06,  1.03s/it][2024-05-29 13:30:53,712] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 13:30:54,168] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  80%|████████  | 16/20 [00:17<00:04,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:16<00:05,  1.01s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:18<00:03,  1.08s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:17<00:03,  1.03it/s]Loading checkpoint shards:  90%|█████████ | 18/20 [00:19<00:02,  1.07s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:18<00:02,  1.02it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  90%|█████████ | 18/20 [00:19<00:01,  1.03it/s]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:20<00:01,  1.06s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:16,  1.12it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:15,  1.26it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:21<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:21<00:00,  1.06s/it]
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:20<00:00,  1.02it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:01<00:15,  1.15it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:01<00:16,  1.12it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:21<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:21<00:00,  1.05s/it]
Rank 2 initialized with CUDA_MEM (60717531136, 84974239744)
Deepspeed engine initializing at --- RANK 2 --- ...
Loading checkpoint shards:  15%|█▌        | 3/20 [00:02<00:17,  1.02s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:02<00:15,  1.07it/s]Rank 7 initialized with CUDA_MEM (60191145984, 84974239744)
Deepspeed engine initializing at --- RANK 7 --- ...
Loading checkpoint shards:  20%|██        | 4/20 [00:03<00:16,  1.02s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:04<00:17,  1.08s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:05<00:15,  1.07s/it]Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  25%|██▌       | 5/20 [00:05<00:16,  1.11s/it]Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  30%|███       | 6/20 [00:06<00:15,  1.09s/it]Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
Loading checkpoint shards:  30%|███       | 6/20 [00:06<00:15,  1.10s/it]Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading checkpoint shards:  35%|███▌      | 7/20 [00:07<00:12,  1.02it/s]Loading checkpoint shards:  35%|███▌      | 7/20 [00:06<00:12,  1.03it/s]Loading checkpoint shards:  40%|████      | 8/20 [00:07<00:10,  1.15it/s]ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 2.57080078125 seconds
[2024-05-29 13:31:04,517] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 1.707103967666626 seconds
[2024-05-29 13:31:04,597] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading checkpoint shards:  40%|████      | 8/20 [00:07<00:10,  1.15it/s]Loading checkpoint shards:  45%|████▌     | 9/20 [00:08<00:08,  1.26it/s]Loading checkpoint shards:  45%|████▌     | 9/20 [00:08<00:08,  1.24it/s]Loading checkpoint shards:  50%|█████     | 10/20 [00:08<00:07,  1.32it/s]Loading checkpoint shards:  50%|█████     | 10/20 [00:08<00:07,  1.31it/s]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:09<00:06,  1.43it/s]Loading checkpoint shards:  60%|██████    | 12/20 [00:09<00:05,  1.58it/s]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:09<00:06,  1.35it/s]Loading checkpoint shards:  60%|██████    | 12/20 [00:10<00:05,  1.41it/s]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:10<00:04,  1.57it/s][2024-05-29 13:31:07,954] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  70%|███████   | 14/20 [00:11<00:03,  1.54it/s]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:10<00:04,  1.42it/s]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:11<00:03,  1.54it/s]Loading checkpoint shards:  70%|███████   | 14/20 [00:11<00:04,  1.46it/s][2024-05-29 13:31:09,001] [INFO] [comm.py:637:init_distributed] cdb=None
Rank 5 initialized with CUDA_MEM (60717531136, 84974239744)
Deepspeed engine initializing at --- RANK 5 --- ...
Loading checkpoint shards:  75%|███████▌  | 15/20 [00:12<00:03,  1.46it/s]Loading checkpoint shards:  80%|████████  | 16/20 [00:12<00:02,  1.50it/s]Loading checkpoint shards:  80%|████████  | 16/20 [00:13<00:03,  1.08it/s]Rank 3 initialized with CUDA_MEM (60717531136, 84974239744)
Loading checkpoint shards:  85%|████████▌ | 17/20 [00:14<00:03,  1.07s/it]Deepspeed engine initializing at --- RANK 3 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  85%|████████▌ | 17/20 [00:14<00:02,  1.01it/s]Loading checkpoint shards:  90%|█████████ | 18/20 [00:15<00:02,  1.00s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  90%|█████████ | 18/20 [00:16<00:02,  1.02s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:17,  1.09it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:16,  1.13it/s]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:16<00:01,  1.01s/it]Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.16it/s]
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 2.1624531745910645 seconds
[2024-05-29 13:31:14,034] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:16<00:00,  1.01it/s]Loading extension module fused_adam...
Time to load fused_adam op: 0.40833353996276855 seconds
[2024-05-29 13:31:14,107] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]
Loading checkpoint shards:  10%|█         | 2/20 [00:01<00:16,  1.09it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:01<00:17,  1.06it/s]Loading checkpoint shards:  15%|█▌        | 3/20 [00:02<00:16,  1.06it/s]Loading checkpoint shards:  15%|█▌        | 3/20 [00:02<00:16,  1.01it/s]Loading checkpoint shards:  20%|██        | 4/20 [00:03<00:15,  1.01it/s]Loading checkpoint shards:  20%|██        | 4/20 [00:04<00:18,  1.16s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:04<00:15,  1.01s/it]Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
Loading checkpoint shards:  25%|██▌       | 5/20 [00:05<00:17,  1.19s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:05<00:13,  1.03it/s]Loading checkpoint shards:  30%|███       | 6/20 [00:06<00:15,  1.12s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:06<00:12,  1.02it/s]Loading checkpoint shards:  35%|███▌      | 7/20 [00:07<00:13,  1.01s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:07<00:11,  1.06it/s]Loading checkpoint shards:  40%|████      | 8/20 [00:07<00:10,  1.12it/s]Loading checkpoint shards:  45%|████▌     | 9/20 [00:08<00:10,  1.08it/s]Loading checkpoint shards:  45%|████▌     | 9/20 [00:08<00:09,  1.13it/s]Loading checkpoint shards:  50%|█████     | 10/20 [00:09<00:08,  1.19it/s]Loading checkpoint shards:  50%|█████     | 10/20 [00:09<00:08,  1.11it/s]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:10<00:07,  1.18it/s]Rank 4 initialized with CUDA_MEM (60717531136, 84974239744)
Deepspeed engine initializing at --- RANK 4 --- ...
Loading checkpoint shards:  55%|█████▌    | 11/20 [00:10<00:07,  1.19it/s]Loading checkpoint shards:  60%|██████    | 12/20 [00:10<00:05,  1.36it/s]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:11<00:05,  1.36it/s]Loading checkpoint shards:  60%|██████    | 12/20 [00:11<00:06,  1.18it/s]Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 1 initialized with CUDA_MEM (60717531136, 84974239744)
Deepspeed engine initializing at --- RANK 1 --- ...
Loading checkpoint shards:  70%|███████   | 14/20 [00:11<00:04,  1.41it/s]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:12<00:05,  1.23it/s]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:12<00:03,  1.47it/s]Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading checkpoint shards:  70%|███████   | 14/20 [00:12<00:04,  1.28it/s]Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  80%|████████  | 16/20 [00:13<00:02,  1.50it/s]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:13<00:03,  1.32it/s]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:13<00:01,  1.52it/s]ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 2.2547008991241455 seconds
[2024-05-29 13:31:26,205] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 0.8089754581451416 seconds
[2024-05-29 13:31:26,281] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading checkpoint shards:  80%|████████  | 16/20 [00:14<00:02,  1.41it/s]Loading checkpoint shards:  90%|█████████ | 18/20 [00:14<00:01,  1.57it/s]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:14<00:02,  1.49it/s]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:14<00:00,  1.70it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:15<00:00,  2.14it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:15<00:00,  1.33it/s]
Loading checkpoint shards:  90%|█████████ | 18/20 [00:15<00:01,  1.50it/s]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:15<00:00,  1.53it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:16<00:00,  1.83it/s]Loading checkpoint shards: 100%|██████████| 20/20 [00:16<00:00,  1.23it/s]
Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
Print trainable params: 151011328 || all params: 47182493696 || trainable%: 0.32
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1, ProcessCoord(pipe=2, data=0): 2, ProcessCoord(pipe=3, data=0): 3, ProcessCoord(pipe=4, data=0): 4, ProcessCoord(pipe=5, data=0): 5, ProcessCoord(pipe=6, data=0): 6, ProcessCoord(pipe=7, data=0): 7}
[2024-05-29 13:31:30,706] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:LanguageModelLayerWrapper
stage=0 layers=2
     0: LlavaMultiModalModuleWrapper
     1: LanguageModelLayerWrapper
stage=1 layers=1
     2: LanguageModelLayerWrapper
stage=2 layers=1
     3: LanguageModelLayerWrapper
stage=3 layers=1
     4: LanguageModelLayerWrapper
stage=4 layers=1
     5: LanguageModelLayerWrapper
stage=5 layers=1
     6: LanguageModelLayerWrapper
stage=6 layers=1
     7: LanguageModelLayerWrapper
stage=7 layers=2
     8: LanguageModelLayerWrapper
     9: LanguageModelFinalWrapper
  loss: loss_fn
Rank 6 initialized with CUDA_MEM (60717531136, 84974239744)
Deepspeed engine initializing at --- RANK 6 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 0 initialized with CUDA_MEM (58697973760, 84974239744)
Deepspeed engine initializing at --- RANK 0 --- ...
[2024-05-29 13:31:33,198] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-05-29 13:31:33,589] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading extension module fused_adam...
Time to load fused_adam op: 3.016695737838745 seconds
[2024-05-29 13:31:37,293] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 13:31:37,293] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 13:31:37,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-05-29 13:31:37,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-05-29 13:31:37,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupCosineLR
[2024-05-29 13:31:37,300] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupCosineLR object at 0x1475b0169eb0>
[2024-05-29 13:31:37,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-05-29 13:31:37,301] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-05-29 13:31:37,301] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-05-29 13:31:37,301] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-05-29 13:31:37,301] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-05-29 13:31:37,301] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1475b0169a00>
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 128
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=True, output_path='/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint', job_name='deepspeed_monitor_logs') enabled=True
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   optimizer_name ............... adam
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 2e-05}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True, 'use_reentrant': False}
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-05-29 13:31:37,302] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupCosineLR
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 5718, 'warmup_min_ratio': 0.1, 'warmup_num_steps': 171.54}
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   steps_per_print .............. 1
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-05-29 13:31:37,303] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-05-29 13:31:37,303] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 128, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupCosineLR", 
        "params": {
            "total_num_steps": 5.718000e+03, 
            "warmup_min_ratio": 0.1, 
            "warmup_num_steps": 171.54
        }
    }, 
    "pipeline": {
        "use_reentrant": false
    }, 
    "csv_monitor": {
        "enabled": true, 
        "output_path": "/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint", 
        "job_name": "deepspeed_monitor_logs"
    }
}
[2024-05-29 13:31:37,303] [INFO] [engine.py:101:__init__] CONFIG: micro_batches=128 micro_batch_size=1
[2024-05-29 13:31:37,303] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 4.418950319290161 seconds
[2024-05-29 13:31:37,318] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=1 STAGE=1 LAYERS=1 [2, 3) STAGE_PARAMS=13631488 (13.631M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=6 STAGE=6 LAYERS=1 [7, 8) STAGE_PARAMS=13631488 (13.631M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=7 STAGE=7 LAYERS=2 [8, 10) STAGE_PARAMS=13631488 (13.631M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=0 STAGE=0 LAYERS=2 [0, 2) STAGE_PARAMS=55590912 (55.591M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=3 STAGE=3 LAYERS=1 [4, 5) STAGE_PARAMS=13631488 (13.631M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=4 STAGE=4 LAYERS=1 [5, 6) STAGE_PARAMS=13631488 (13.631M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=2 STAGE=2 LAYERS=1 [3, 4) STAGE_PARAMS=13631488 (13.631M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
[2024-05-29 13:31:41,785] [INFO] [engine.py:160:__init__] RANK=5 STAGE=5 LAYERS=1 [6, 7) STAGE_PARAMS=13631488 (13.631M) TOTAL_PARAMS=151011328 (151.011M) UNIQUE_PARAMS=151011328 (151.011M)
Deepspeed engine successfully initialized at --- RANK 0 --- hosting 24 of 136 trainable parameters
Loading latest model checkpoint at shard 30
[2024-05-29 13:31:44,567] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:44,705] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:44,706] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:44,786] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:44,794] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_00-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 7 --- hosting 16 of 136 trainable parameters
[2024-05-29 13:31:45,246] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_00-model_states.pt.
[2024-05-29 13:31:45,248] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_00-model_states.pt...
[2024-05-29 13:31:45,267] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:45,377] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:45,379] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_07_model_states.pt...
[2024-05-29 13:31:45,428] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_07_model_states.pt.
[2024-05-29 13:31:45,439] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_08-model_states.pt...
[2024-05-29 13:31:45,657] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_00-model_states.pt.
[2024-05-29 13:31:45,842] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_01-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 1 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 2 --- hosting 16 of 136 trainable parameters
[2024-05-29 13:31:46,969] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:46,984] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 3 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 4 --- hosting 16 of 136 trainable parameters
[2024-05-29 13:31:47,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
Deepspeed engine successfully initialized at --- RANK 6 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 5 --- hosting 16 of 136 trainable parameters
[2024-05-29 13:31:47,118] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:47,134] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_02_model_states.pt...
[2024-05-29 13:31:47,152] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:47,235] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:47,300] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_01_model_states.pt...
[2024-05-29 13:31:47,318] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_02_model_states.pt.
[2024-05-29 13:31:47,324] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:47,349] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:47,383] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_03-model_states.pt...
[2024-05-29 13:31:47,399] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_04_model_states.pt...
[2024-05-29 13:31:47,416] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt...
[2024-05-29 13:31:47,433] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:47,483] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_01_model_states.pt.
[2024-05-29 13:31:47,529] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_03_model_states.pt...
[2024-05-29 13:31:47,542] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:47,547] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_02-model_states.pt...
[2024-05-29 13:31:47,575] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_04_model_states.pt.
[2024-05-29 13:31:47,587] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_05_model_states.pt...
[2024-05-29 13:31:47,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_05-model_states.pt...
[2024-05-29 13:31:47,624] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_03_model_states.pt.
[2024-05-29 13:31:47,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_04-model_states.pt...
[2024-05-29 13:31:47,670] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_00_model_states.pt.
[2024-05-29 13:31:47,680] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_05_model_states.pt.
[2024-05-29 13:31:47,694] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_06_model_states.pt...
[2024-05-29 13:31:47,699] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_06-model_states.pt...
[2024-05-29 13:31:47,732] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/mp_rank_06_model_states.pt.
[2024-05-29 13:31:47,736] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_07-model_states.pt...
[2024-05-29 13:31:50,270] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_01-model_states.pt.
[2024-05-29 13:31:50,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_01-model_states.pt...
[2024-05-29 13:31:51,895] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_03-model_states.pt.
[2024-05-29 13:31:51,938] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_03-model_states.pt...
[2024-05-29 13:31:52,231] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_06-model_states.pt.
[2024-05-29 13:31:52,276] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_06-model_states.pt...
[2024-05-29 13:31:52,328] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_04-model_states.pt.
[2024-05-29 13:31:52,376] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_04-model_states.pt...
[2024-05-29 13:31:52,632] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_05-model_states.pt.
[2024-05-29 13:31:52,693] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_05-model_states.pt...
[2024-05-29 13:31:54,639] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_01-model_states.pt.
[2024-05-29 13:31:54,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_08-model_states.pt.
[2024-05-29 13:31:55,315] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_08-model_states.pt...
[2024-05-29 13:31:56,160] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_02-model_states.pt.
[2024-05-29 13:31:56,191] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_03-model_states.pt.
  0%|          | 0/520 [00:00<?, ?it/s]Shard 0 / 30 skipped
Shard 1 / 30 skipped
Shard 2 / 30 skipped
Shard 3 / 30 skipped
Shard 4 / 30 skipped
Shard 5 / 30 skipped
Shard 6 / 30 skipped
Shard 7 / 30 skipped
Shard 8 / 30 skipped
Shard 9 / 30 skipped
Shard 10 / 30 skipped
Shard 11 / 30 skipped
Shard 12 / 30 skipped
Shard 13 / 30 skipped
Shard 14 / 30 skipped
Shard 15 / 30 skipped
Shard 16 / 30 skipped
Shard 17 / 30 skipped
Shard 18 / 30 skipped
Shard 19 / 30 skipped
Shard 20 / 30 skipped
Shard 21 / 30 skipped
Shard 22 / 30 skipped
Shard 23 / 30 skipped
Shard 24 / 30 skipped
Shard 25 / 30 skipped
Shard 26 / 30 skipped
Shard 27 / 30 skipped
Shard 28 / 30 skipped
Shard 29 / 30 skipped
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_293
[2024-05-29 13:31:56,704] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_02-model_states.pt...
[2024-05-29 13:31:56,894] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_04-model_states.pt.
Training on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 13:31:57,162] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_05-model_states.pt.
[2024-05-29 13:31:57,185] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_06-model_states.pt.
  0%|          | 0/520 [00:00<?, ?it/s]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 13:31:59,777] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_07-model_states.pt.
  0%|          | 0/520 [00:00<?, ?it/s]
  0%|          | 0/10 [00:00<?, ?it/s][A  0%|          | 0/520 [00:00<?, ?it/s][2024-05-29 13:32:00,464] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_07-model_states.pt...

  0%|          | 0/10 [00:00<?, ?it/s][A  0%|          | 0/520 [00:00<?, ?it/s][2024-05-29 13:32:01,237] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_02-model_states.pt.

  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 13:32:01,505] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_08-model_states.pt.
  0%|          | 0/520 [00:00<?, ?it/s][2024-05-29 13:32:04,001] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_09-model_states.pt...

  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 13:32:04,321] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_09-model_states.pt.
[2024-05-29 13:32:04,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_09-model_states.pt...
[2024-05-29 13:32:04,600] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_09-model_states.pt.
  0%|          | 0/520 [00:00<?, ?it/s]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 13:32:08,519] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step300/layer_07-model_states.pt.
  0%|          | 0/520 [00:00<?, ?it/s]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 13:33:30,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=301, skipped=0, lr=[1.9973129784813498e-05], mom=[(0.9, 0.999)]
steps: 301 loss: 0.6420 iter time (s): 97.982 samples/sec: 1.306

 10%|█         | 1/10 [01:30<13:31, 90.12s/it][A
 10%|█         | 1/10 [01:24<12:36, 84.09s/it][A
 10%|█         | 1/10 [01:34<14:07, 94.11s/it][A
 10%|█         | 1/10 [01:36<14:27, 96.36s/it][A
 10%|█         | 1/10 [01:35<14:17, 95.24s/it][A
 10%|█         | 1/10 [01:35<14:20, 95.56s/it][A
 10%|█         | 1/10 [01:31<13:46, 91.79s/it][A
 10%|█         | 1/10 [01:38<14:48, 98.74s/it][A
 20%|██        | 2/10 [02:49<11:10, 83.75s/it][A[2024-05-29 13:34:56,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=302, skipped=0, lr=[1.9972713258965157e-05], mom=[(0.9, 0.999)]
steps: 302 loss: 0.6336 iter time (s): 80.650 samples/sec: 1.587

 20%|██        | 2/10 [02:45<10:59, 82.47s/it][A
 20%|██        | 2/10 [02:57<11:39, 87.48s/it][A
 20%|██        | 2/10 [02:55<11:33, 86.63s/it][A
 20%|██        | 2/10 [02:56<11:36, 87.11s/it][A
 20%|██        | 2/10 [02:56<11:37, 87.24s/it][A
 20%|██        | 2/10 [02:53<11:25, 85.69s/it][A
 20%|██        | 2/10 [03:00<11:48, 88.54s/it][A
 30%|███       | 3/10 [04:10<09:37, 82.46s/it][A[2024-05-29 13:36:17,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=303, skipped=0, lr=[1.9972293533943674e-05], mom=[(0.9, 0.999)]
steps: 303 loss: 0.6262 iter time (s): 80.118 samples/sec: 1.598

 30%|███       | 3/10 [04:06<09:32, 81.79s/it][A
 30%|███       | 3/10 [04:16<09:47, 83.98s/it][A
 30%|███       | 3/10 [04:18<09:51, 84.50s/it][A
 30%|███       | 3/10 [04:17<09:49, 84.21s/it][A
 30%|███       | 3/10 [04:17<09:50, 84.30s/it][A
 30%|███       | 3/10 [04:14<09:44, 83.47s/it][A
 30%|███       | 3/10 [04:20<09:55, 85.01s/it][A
 40%|████      | 4/10 [05:30<08:10, 81.73s/it][A[2024-05-29 13:37:37,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=304, skipped=0, lr=[1.9971870609883714e-05], mom=[(0.9, 0.999)]
steps: 304 loss: 0.6316 iter time (s): 79.891 samples/sec: 1.602

 40%|████      | 4/10 [05:26<08:07, 81.31s/it][A
 40%|████      | 4/10 [05:36<08:15, 82.65s/it][A
 40%|████      | 4/10 [05:39<08:17, 82.98s/it][A
 40%|████      | 4/10 [05:38<08:16, 82.82s/it][A
 40%|████      | 4/10 [05:38<08:17, 82.85s/it][A
 40%|████      | 4/10 [05:34<08:14, 82.35s/it][A
 40%|████      | 4/10 [05:41<08:19, 83.29s/it][A
 50%|█████     | 5/10 [06:51<06:46, 81.30s/it][A[2024-05-29 13:38:58,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=305, skipped=0, lr=[1.9971444486920952e-05], mom=[(0.9, 0.999)]
steps: 305 loss: 0.6224 iter time (s): 79.961 samples/sec: 1.601

 50%|█████     | 5/10 [06:47<06:45, 81.08s/it][A
 50%|█████     | 5/10 [06:57<06:49, 81.95s/it][A
 50%|█████     | 5/10 [06:59<06:50, 82.16s/it][A
 50%|█████     | 5/10 [06:58<06:50, 82.05s/it][A
 50%|█████     | 5/10 [06:59<06:50, 82.08s/it][A
 50%|█████     | 5/10 [06:55<06:48, 81.76s/it][A
 50%|█████     | 5/10 [07:02<06:51, 82.36s/it][A
 60%|██████    | 6/10 [08:11<05:23, 80.95s/it][A[2024-05-29 13:40:18,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=306, skipped=0, lr=[1.9971015165192106e-05], mom=[(0.9, 0.999)]
steps: 306 loss: 0.6374 iter time (s): 79.326 samples/sec: 1.614

 60%|██████    | 6/10 [08:07<05:23, 80.77s/it][A
 60%|██████    | 6/10 [08:19<05:25, 81.44s/it][A
 60%|██████    | 6/10 [08:17<05:25, 81.34s/it][A
 60%|██████    | 6/10 [08:18<05:25, 81.40s/it][A
 60%|██████    | 6/10 [08:19<05:25, 81.41s/it][A
 60%|██████    | 6/10 [08:15<05:24, 81.20s/it][A
 60%|██████    | 6/10 [08:22<05:26, 81.59s/it][A
 70%|███████   | 7/10 [09:32<04:02, 81.00s/it][A[2024-05-29 13:41:39,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=307, skipped=0, lr=[1.997058264483491e-05], mom=[(0.9, 0.999)]
steps: 307 loss: 0.6095 iter time (s): 80.278 samples/sec: 1.594

 70%|███████   | 7/10 [09:28<04:02, 80.86s/it][A
 70%|███████   | 7/10 [09:38<04:03, 81.22s/it][A
 70%|███████   | 7/10 [09:41<04:03, 81.33s/it][A
 70%|███████   | 7/10 [09:39<04:03, 81.28s/it][A
 70%|███████   | 7/10 [09:40<04:03, 81.29s/it][A
 70%|███████   | 7/10 [09:36<04:03, 81.15s/it][A
 70%|███████   | 7/10 [09:43<04:04, 81.41s/it][A
 80%|████████  | 8/10 [10:53<02:41, 80.81s/it][A[2024-05-29 13:43:00,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=308, skipped=0, lr=[1.9970146925988127e-05], mom=[(0.9, 0.999)]
steps: 308 loss: 0.6080 iter time (s): 79.705 samples/sec: 1.606

 80%|████████  | 8/10 [10:49<02:41, 80.72s/it][A
 80%|████████  | 8/10 [10:59<02:41, 80.98s/it][A
 80%|████████  | 8/10 [11:01<02:42, 81.05s/it][A
 80%|████████  | 8/10 [11:00<02:42, 81.02s/it][A
 80%|████████  | 8/10 [11:00<02:42, 81.03s/it][A
 80%|████████  | 8/10 [10:57<02:41, 80.94s/it][A
 80%|████████  | 8/10 [11:03<02:42, 81.12s/it][A
 90%|█████████ | 9/10 [12:13<01:20, 80.50s/it][A[2024-05-29 13:44:20,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=309, skipped=0, lr=[1.9969708008791543e-05], mom=[(0.9, 0.999)]
steps: 309 loss: 0.6079 iter time (s): 79.065 samples/sec: 1.619

 90%|█████████ | 9/10 [12:09<01:20, 80.46s/it][A
 90%|█████████ | 9/10 [12:19<01:20, 80.61s/it][A
 90%|█████████ | 9/10 [12:21<01:20, 80.67s/it][A
 90%|█████████ | 9/10 [12:20<01:20, 80.66s/it][A
 90%|█████████ | 9/10 [12:20<01:20, 80.66s/it][A
 90%|█████████ | 9/10 [12:16<01:20, 80.58s/it][A
 90%|█████████ | 9/10 [12:23<01:20, 80.71s/it][A
100%|██████████| 10/10 [13:32<00:00, 80.13s/it][A100%|██████████| 10/10 [13:32<00:00, 81.24s/it]
  6%|▌         | 31/520 [13:32<3:33:35, 26.21s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 13:45:39,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[1.996926589338598e-05], mom=[(0.9, 0.999)]
steps: 310 loss: 0.6098 iter time (s): 78.509 samples/sec: 1.630

100%|██████████| 10/10 [13:28<00:00, 80.08s/it][A100%|██████████| 10/10 [13:28<00:00, 80.84s/it]
  6%|▌         | 31/520 [13:28<3:32:32, 26.08s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:38<00:00, 80.21s/it][A100%|██████████| 10/10 [13:38<00:00, 81.84s/it]
  6%|▌         | 31/520 [13:38<3:35:16, 26.41s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:40<00:00, 80.24s/it][A100%|██████████| 10/10 [13:40<00:00, 82.07s/it]
  6%|▌         | 31/520 [13:40<3:35:50, 26.48s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:39<00:00, 80.22s/it][A100%|██████████| 10/10 [13:39<00:00, 81.95s/it]
  6%|▌         | 31/520 [13:39<3:35:29, 26.44s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:39<00:00, 80.24s/it][A100%|██████████| 10/10 [13:39<00:00, 81.99s/it]
  6%|▌         | 31/520 [13:39<3:35:34, 26.45s/it]
100%|██████████| 10/10 [13:36<00:00, 80.18s/it][A100%|██████████| 10/10 [13:36<00:00, 81.61s/it]
  6%|▌         | 31/520 [13:36<3:34:36, 26.33s/it]
100%|██████████| 10/10 [13:43<00:00, 80.27s/it][A100%|██████████| 10/10 [13:43<00:00, 82.31s/it]
  6%|▌         | 31/520 [13:43<3:36:33, 26.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_15

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][ATraining on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [02:05<18:51, 125.75s/it][A[2024-05-29 13:47:46,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=311, skipped=0, lr=[1.9968820579913283e-05], mom=[(0.9, 0.999)]
steps: 311 loss: 0.8790 iter time (s): 126.058 samples/sec: 1.015

 10%|█         | 1/10 [02:07<19:03, 127.08s/it][A
 10%|█         | 1/10 [02:07<19:04, 127.17s/it][A
 10%|█         | 1/10 [02:07<19:05, 127.23s/it][A
 10%|█         | 1/10 [02:07<19:04, 127.22s/it][A
 10%|█         | 1/10 [02:07<19:03, 127.08s/it][A
 10%|█         | 1/10 [02:07<19:04, 127.11s/it][A
 10%|█         | 1/10 [02:07<19:03, 127.06s/it][A
 20%|██        | 2/10 [04:12<16:53, 126.63s/it][A[2024-05-29 13:49:53,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=312, skipped=0, lr=[1.9968372068516306e-05], mom=[(0.9, 0.999)]
steps: 312 loss: 0.8408 iter time (s): 126.380 samples/sec: 1.013

 20%|██        | 2/10 [04:14<16:58, 127.26s/it][A
 20%|██        | 2/10 [04:14<16:57, 127.14s/it][A
 20%|██        | 2/10 [04:14<16:57, 127.18s/it][A
 20%|██        | 2/10 [04:14<16:57, 127.19s/it][A
 20%|██        | 2/10 [04:14<16:57, 127.17s/it][A
 20%|██        | 2/10 [04:14<16:57, 127.14s/it][A
 20%|██        | 2/10 [04:14<16:57, 127.18s/it][A
 30%|███       | 3/10 [06:19<14:47, 126.72s/it][A[2024-05-29 13:52:00,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=313, skipped=0, lr=[1.9967920359338956e-05], mom=[(0.9, 0.999)]
steps: 313 loss: 0.9209 iter time (s): 125.991 samples/sec: 1.016

 30%|███       | 3/10 [06:21<14:48, 127.00s/it][A
 30%|███       | 3/10 [06:21<14:48, 126.98s/it][A
 30%|███       | 3/10 [06:21<14:49, 127.04s/it][A
 30%|███       | 3/10 [06:21<14:49, 127.05s/it][A
 30%|███       | 3/10 [06:21<14:48, 127.00s/it][A
 30%|███       | 3/10 [06:21<14:49, 127.01s/it][A
 30%|███       | 3/10 [06:21<14:49, 127.00s/it][A
 40%|████      | 4/10 [08:27<12:42, 127.02s/it][A[2024-05-29 13:54:07,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=314, skipped=0, lr=[1.996746545252614e-05], mom=[(0.9, 0.999)]
steps: 314 loss: 0.9474 iter time (s): 126.108 samples/sec: 1.015

 40%|████      | 4/10 [08:28<12:41, 126.98s/it][A
 40%|████      | 4/10 [08:27<12:41, 126.94s/it][A
 40%|████      | 4/10 [08:28<12:42, 127.00s/it][A
 40%|████      | 4/10 [08:28<12:42, 127.02s/it][A
 40%|████      | 4/10 [08:28<12:42, 127.07s/it][A
 40%|████      | 4/10 [08:28<12:42, 127.02s/it][A
 40%|████      | 4/10 [08:28<12:42, 127.04s/it][A
 50%|█████     | 5/10 [10:33<10:34, 126.84s/it][A[2024-05-29 13:56:14,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=315, skipped=0, lr=[1.9967007348223816e-05], mom=[(0.9, 0.999)]
steps: 315 loss: 0.8829 iter time (s): 126.044 samples/sec: 1.016

 50%|█████     | 5/10 [10:35<10:34, 126.97s/it][A
 50%|█████     | 5/10 [10:35<10:34, 126.99s/it][A
 50%|█████     | 5/10 [10:35<10:34, 126.95s/it][A
 50%|█████     | 5/10 [10:35<10:34, 126.95s/it][A
 50%|█████     | 5/10 [10:34<10:34, 126.93s/it][A
 50%|█████     | 5/10 [10:34<10:34, 126.93s/it][A
 50%|█████     | 5/10 [10:34<10:34, 126.93s/it][A
 60%|██████    | 6/10 [12:40<08:27, 126.83s/it][A[2024-05-29 13:58:21,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=316, skipped=0, lr=[1.996654604657895e-05], mom=[(0.9, 0.999)]
steps: 316 loss: 0.8972 iter time (s): 126.095 samples/sec: 1.015

 60%|██████    | 6/10 [12:42<08:27, 127.00s/it][A
 60%|██████    | 6/10 [12:41<08:27, 126.96s/it][A
 60%|██████    | 6/10 [12:42<08:27, 126.98s/it][A
 60%|██████    | 6/10 [12:42<08:27, 126.94s/it][A
 60%|██████    | 6/10 [12:41<08:27, 126.95s/it][A
 60%|██████    | 6/10 [12:41<08:27, 126.94s/it][A
 60%|██████    | 6/10 [12:41<08:27, 126.96s/it][A
 70%|███████   | 7/10 [14:47<06:20, 126.95s/it][A[2024-05-29 14:00:28,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=317, skipped=0, lr=[1.9966081547739534e-05], mom=[(0.9, 0.999)]
steps: 317 loss: 0.8761 iter time (s): 126.369 samples/sec: 1.013

 70%|███████   | 7/10 [14:49<06:21, 127.05s/it][A
 70%|███████   | 7/10 [14:49<06:21, 127.03s/it][A
 70%|███████   | 7/10 [14:49<06:21, 127.02s/it][A
 70%|███████   | 7/10 [14:49<06:21, 127.01s/it][A
 70%|███████   | 7/10 [14:49<06:21, 127.00s/it][A
 70%|███████   | 7/10 [14:49<06:21, 127.01s/it][A
 70%|███████   | 7/10 [14:49<06:21, 127.01s/it][A
 80%|████████  | 8/10 [16:54<04:13, 126.94s/it][A[2024-05-29 14:02:35,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=318, skipped=0, lr=[1.99656138518546e-05], mom=[(0.9, 0.999)]
steps: 318 loss: 0.8901 iter time (s): 126.094 samples/sec: 1.015

 80%|████████  | 8/10 [16:56<04:13, 126.98s/it][A
 80%|████████  | 8/10 [16:55<04:13, 126.97s/it][A
 80%|████████  | 8/10 [16:56<04:13, 126.99s/it][A
 80%|████████  | 8/10 [16:56<04:13, 126.99s/it][A
 80%|████████  | 8/10 [16:55<04:13, 126.99s/it][A
 80%|████████  | 8/10 [16:56<04:13, 126.99s/it][A
 80%|████████  | 8/10 [16:55<04:13, 126.99s/it][A
 90%|█████████ | 9/10 [19:01<02:06, 126.95s/it][A[2024-05-29 14:04:42,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=319, skipped=0, lr=[1.9965142959074188e-05], mom=[(0.9, 0.999)]
steps: 319 loss: 0.8851 iter time (s): 126.131 samples/sec: 1.015

 90%|█████████ | 9/10 [19:03<02:06, 126.97s/it][A
 90%|█████████ | 9/10 [19:02<02:06, 126.98s/it][A
 90%|█████████ | 9/10 [19:02<02:06, 126.95s/it][A
 90%|█████████ | 9/10 [19:02<02:06, 126.94s/it][A
 90%|█████████ | 9/10 [19:02<02:06, 126.96s/it][A
 90%|█████████ | 9/10 [19:02<02:06, 126.96s/it][A
 90%|█████████ | 9/10 [19:02<02:06, 126.95s/it][A
100%|██████████| 10/10 [21:08<00:00, 126.90s/it][A100%|██████████| 10/10 [21:08<00:00, 126.85s/it]
  6%|▌         | 32/520 [34:41<10:58:18, 80.94s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 14:06:48,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[1.9964668869549378e-05], mom=[(0.9, 0.999)]
steps: 320 loss: 0.8668 iter time (s): 126.087 samples/sec: 1.015

100%|██████████| 10/10 [21:09<00:00, 126.90s/it][A100%|██████████| 10/10 [21:09<00:00, 126.98s/it]
  6%|▌         | 32/520 [34:38<10:57:44, 80.87s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [21:09<00:00, 126.91s/it][A100%|██████████| 10/10 [21:09<00:00, 126.97s/it]
  6%|▌         | 32/520 [34:48<11:00:20, 81.19s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [21:09<00:00, 126.91s/it][A100%|██████████| 10/10 [21:09<00:00, 126.98s/it]
  6%|▌         | 32/520 [34:50<11:00:52, 81.25s/it]
100%|██████████| 10/10 [21:09<00:00, 126.91s/it][A100%|██████████| 10/10 [21:09<00:00, 126.98s/it]
  6%|▌         | 32/520 [34:49<11:00:32, 81.21s/it]
100%|██████████| 10/10 [21:09<00:00, 126.90s/it][A100%|██████████| 10/10 [21:09<00:00, 126.97s/it]
  6%|▌         | 32/520 [34:49<11:00:36, 81.22s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [21:09<00:00, 126.90s/it][A100%|██████████| 10/10 [21:09<00:00, 126.97s/it]
  6%|▌         | 32/520 [34:46<10:59:41, 81.11s/it]
100%|██████████| 10/10 [21:09<00:00, 126.91s/it][A100%|██████████| 10/10 [21:09<00:00, 126.97s/it]
  6%|▌         | 32/520 [34:53<11:01:33, 81.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_203

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][ATraining on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [00:57<08:37, 57.54s/it][A[2024-05-29 14:07:44,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=321, skipped=0, lr=[1.9964191583432265e-05], mom=[(0.9, 0.999)]
steps: 321 loss: 0.6828 iter time (s): 54.915 samples/sec: 2.331

 10%|█         | 1/10 [00:55<08:21, 55.70s/it][A
 10%|█         | 1/10 [00:55<08:19, 55.55s/it][A
 10%|█         | 1/10 [00:55<08:18, 55.40s/it][A
 10%|█         | 1/10 [00:55<08:20, 55.57s/it][A
 10%|█         | 1/10 [00:55<08:19, 55.55s/it][A
 10%|█         | 1/10 [00:55<08:19, 55.50s/it][A
 10%|█         | 1/10 [00:55<08:19, 55.50s/it][A
 20%|██        | 2/10 [01:52<07:30, 56.28s/it][A[2024-05-29 14:08:40,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=322, skipped=0, lr=[1.9963711100875983e-05], mom=[(0.9, 0.999)]
steps: 322 loss: 0.6775 iter time (s): 54.888 samples/sec: 2.332

 20%|██        | 2/10 [01:51<07:24, 55.57s/it][A
 20%|██        | 2/10 [01:51<07:24, 55.59s/it][A
 20%|██        | 2/10 [01:51<07:24, 55.52s/it][A
 20%|██        | 2/10 [01:50<07:24, 55.51s/it][A
 20%|██        | 2/10 [01:51<07:24, 55.54s/it][A
 20%|██        | 2/10 [01:51<07:24, 55.55s/it][A
 20%|██        | 2/10 [01:51<07:24, 55.58s/it][A
 30%|███       | 3/10 [02:47<06:28, 55.51s/it][A[2024-05-29 14:09:34,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=323, skipped=0, lr=[1.996322742203467e-05], mom=[(0.9, 0.999)]
steps: 323 loss: 0.6857 iter time (s): 53.724 samples/sec: 2.383

 30%|███       | 3/10 [02:45<06:25, 55.07s/it][A
 30%|███       | 3/10 [02:45<06:25, 55.02s/it][A
 30%|███       | 3/10 [02:45<06:25, 55.01s/it][A
 30%|███       | 3/10 [02:45<06:25, 55.02s/it][A
 30%|███       | 3/10 [02:45<06:25, 55.02s/it][A
 30%|███       | 3/10 [02:45<06:25, 55.01s/it][A
 30%|███       | 3/10 [02:45<06:25, 55.00s/it][A
 40%|████      | 4/10 [03:43<05:33, 55.62s/it][A[2024-05-29 14:10:30,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=324, skipped=0, lr=[1.996274054706351e-05], mom=[(0.9, 0.999)]
steps: 324 loss: 0.6088 iter time (s): 55.200 samples/sec: 2.319

 40%|████      | 4/10 [03:41<05:31, 55.33s/it][A
 40%|████      | 4/10 [03:41<05:31, 55.33s/it][A
 40%|████      | 4/10 [03:41<05:31, 55.32s/it][A
 40%|████      | 4/10 [03:41<05:31, 55.32s/it][A
 40%|████      | 4/10 [03:41<05:31, 55.32s/it][A
 40%|████      | 4/10 [03:41<05:31, 55.33s/it][A
 40%|████      | 4/10 [03:41<05:32, 55.35s/it][A
 50%|█████     | 5/10 [04:38<04:37, 55.41s/it][A[2024-05-29 14:11:25,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=325, skipped=0, lr=[1.9962250476118704e-05], mom=[(0.9, 0.999)]
steps: 325 loss: 0.5874 iter time (s): 54.380 samples/sec: 2.354

 50%|█████     | 5/10 [04:36<04:36, 55.25s/it][A
 50%|█████     | 5/10 [04:36<04:36, 55.23s/it][A
 50%|█████     | 5/10 [04:36<04:36, 55.23s/it][A
 50%|█████     | 5/10 [04:36<04:36, 55.24s/it][A
 50%|█████     | 5/10 [04:36<04:36, 55.24s/it][A

 50%|█████     | 5/10 [04:36<04:36, 55.21s/it][A 50%|█████     | 5/10 [04:36<04:36, 55.22s/it][A
 60%|██████    | 6/10 [05:33<03:41, 55.35s/it][A[2024-05-29 14:12:20,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=326, skipped=0, lr=[1.9961757209357476e-05], mom=[(0.9, 0.999)]
steps: 326 loss: 0.6656 iter time (s): 54.700 samples/sec: 2.340

 60%|██████    | 6/10 [05:31<03:40, 55.24s/it][A
 60%|██████    | 6/10 [05:31<03:41, 55.26s/it][A
 60%|██████    | 6/10 [05:31<03:40, 55.23s/it][A
 60%|██████    | 6/10 [05:31<03:40, 55.23s/it][A
 60%|██████    | 6/10 [05:31<03:40, 55.23s/it][A
 60%|██████    | 6/10 [05:31<03:41, 55.28s/it][A
 60%|██████    | 6/10 [05:31<03:41, 55.27s/it][A
 70%|███████   | 7/10 [06:28<02:45, 55.25s/it][A[2024-05-29 14:13:16,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=327, skipped=0, lr=[1.9961260746938083e-05], mom=[(0.9, 0.999)]
steps: 327 loss: 0.6410 iter time (s): 54.350 samples/sec: 2.355

 70%|███████   | 7/10 [06:26<02:45, 55.19s/it][A
 70%|███████   | 7/10 [06:26<02:45, 55.18s/it][A
 70%|███████   | 7/10 [06:26<02:45, 55.17s/it][A
 70%|███████   | 7/10 [06:26<02:45, 55.16s/it][A
 70%|███████   | 7/10 [06:26<02:45, 55.17s/it][A
 70%|███████   | 7/10 [06:26<02:45, 55.15s/it][A
 70%|███████   | 7/10 [06:26<02:45, 55.15s/it][A
 80%|████████  | 8/10 [07:23<01:50, 55.01s/it][A[2024-05-29 14:14:10,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=328, skipped=0, lr=[1.9960761089019802e-05], mom=[(0.9, 0.999)]
steps: 328 loss: 0.5815 iter time (s): 53.909 samples/sec: 2.374

 80%|████████  | 8/10 [07:21<01:49, 54.95s/it][A
 80%|████████  | 8/10 [07:21<01:49, 54.96s/it][A
 80%|████████  | 8/10 [07:21<01:49, 54.96s/it][A
 80%|████████  | 8/10 [07:21<01:49, 54.95s/it][A
 80%|████████  | 8/10 [07:21<01:49, 54.97s/it][A
 80%|████████  | 8/10 [07:21<01:49, 54.95s/it][A
 80%|████████  | 8/10 [07:21<01:49, 54.95s/it][A
 90%|█████████ | 9/10 [08:17<00:54, 54.82s/it][A[2024-05-29 14:15:04,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=329, skipped=0, lr=[1.996025823576293e-05], mom=[(0.9, 0.999)]
steps: 329 loss: 0.6484 iter time (s): 53.855 samples/sec: 2.377

 90%|█████████ | 9/10 [08:15<00:54, 54.80s/it][A
 90%|█████████ | 9/10 [08:15<00:54, 54.85s/it][A
 90%|█████████ | 9/10 [08:15<00:54, 54.84s/it][A
 90%|█████████ | 9/10 [08:15<00:54, 54.82s/it][A
 90%|█████████ | 9/10 [08:15<00:54, 54.83s/it][A
 90%|█████████ | 9/10 [08:16<00:54, 54.95s/it][A
 90%|█████████ | 9/10 [08:16<00:55, 55.12s/it][A
100%|██████████| 10/10 [09:14<00:00, 55.36s/it][A100%|██████████| 10/10 [09:14<00:00, 55.41s/it]
  6%|▋         | 33/520 [43:55<14:44:23, 108.96s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 14:16:01,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[1.99597521873288e-05], mom=[(0.9, 0.999)]
steps: 330 loss: 0.6664 iter time (s): 54.899 samples/sec: 2.332

100%|██████████| 10/10 [09:12<00:00, 55.33s/it][A100%|██████████| 10/10 [09:12<00:00, 55.23s/it]
  6%|▋         | 33/520 [43:50<14:42:55, 108.78s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [09:12<00:00, 55.32s/it][A100%|██████████| 10/10 [09:12<00:00, 55.21s/it]
  6%|▋         | 33/520 [44:00<14:45:20, 109.08s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [09:12<00:00, 55.32s/it][A100%|██████████| 10/10 [09:12<00:00, 55.21s/it]
  6%|▋         | 33/520 [44:02<14:45:49, 109.14s/it]
100%|██████████| 10/10 [09:12<00:00, 55.32s/it][A100%|██████████| 10/10 [09:12<00:00, 55.20s/it]
  6%|▋         | 33/520 [44:01<14:45:32, 109.10s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [09:12<00:00, 55.28s/it][A100%|██████████| 10/10 [09:12<00:00, 55.21s/it]
  6%|▋         | 33/520 [43:58<14:44:43, 109.00s/it]
100%|██████████| 10/10 [09:12<00:00, 55.34s/it][A100%|██████████| 10/10 [09:12<00:00, 55.21s/it]
  6%|▋         | 33/520 [44:01<14:45:36, 109.11s/it]
100%|██████████| 10/10 [09:12<00:00, 55.24s/it][A100%|██████████| 10/10 [09:12<00:00, 55.21s/it]
  6%|▋         | 33/520 [44:05<14:46:28, 109.22s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_59

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][ATraining on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [01:03<09:31, 63.49s/it][A[2024-05-29 14:17:05,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=331, skipped=0, lr=[1.9959242943879762e-05], mom=[(0.9, 0.999)]
steps: 331 loss: 1.3191 iter time (s): 63.101 samples/sec: 2.028

 10%|█         | 1/10 [01:04<09:37, 64.11s/it][A
 10%|█         | 1/10 [01:04<09:37, 64.11s/it][A
 10%|█         | 1/10 [01:04<09:36, 64.05s/it][A
 10%|█         | 1/10 [01:03<09:34, 63.84s/it][A
 10%|█         | 1/10 [01:03<09:35, 63.93s/it][A
 10%|█         | 1/10 [01:04<09:38, 64.32s/it][A
 10%|█         | 1/10 [01:04<09:38, 64.32s/it][A
 20%|██        | 2/10 [02:07<08:31, 63.92s/it][A[2024-05-29 14:18:09,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=332, skipped=0, lr=[1.9958730505579195e-05], mom=[(0.9, 0.999)]
steps: 332 loss: 1.3025 iter time (s): 63.078 samples/sec: 2.029

 20%|██        | 2/10 [02:08<08:32, 64.09s/it][A
 20%|██        | 2/10 [02:08<08:33, 64.14s/it][A
 20%|██        | 2/10 [02:08<08:32, 64.06s/it][A
 20%|██        | 2/10 [02:07<08:31, 64.00s/it][A
 20%|██        | 2/10 [02:07<08:31, 63.97s/it][A
 20%|██        | 2/10 [02:08<08:32, 64.00s/it][A
 20%|██        | 2/10 [02:08<08:31, 63.97s/it][A
 30%|███       | 3/10 [03:11<07:26, 63.73s/it][A[2024-05-29 14:19:13,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=333, skipped=0, lr=[1.9958214872591502e-05], mom=[(0.9, 0.999)]
steps: 333 loss: 1.2952 iter time (s): 62.897 samples/sec: 2.035

 30%|███       | 3/10 [03:11<07:26, 63.83s/it][A
 30%|███       | 3/10 [03:11<07:26, 63.85s/it][A
 30%|███       | 3/10 [03:11<07:26, 63.82s/it][A
 30%|███       | 3/10 [03:11<07:26, 63.79s/it][A

 30%|███       | 3/10 [03:11<07:26, 63.79s/it][A 30%|███       | 3/10 [03:11<07:26, 63.80s/it][A
 30%|███       | 3/10 [03:11<07:26, 63.78s/it][A
 40%|████      | 4/10 [04:15<06:24, 64.10s/it][A[2024-05-29 14:20:17,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=334, skipped=0, lr=[1.995769604508211e-05], mom=[(0.9, 0.999)]
steps: 334 loss: 1.2931 iter time (s): 64.002 samples/sec: 2.000

 40%|████      | 4/10 [04:16<06:24, 64.16s/it][A
 40%|████      | 4/10 [04:16<06:25, 64.17s/it][A
 40%|████      | 4/10 [04:16<06:24, 64.16s/it][A
 40%|████      | 4/10 [04:16<06:24, 64.14s/it][A
 40%|████      | 4/10 [04:16<06:24, 64.14s/it][A
 40%|████      | 4/10 [04:16<06:24, 64.14s/it][A
 40%|████      | 4/10 [04:16<06:24, 64.13s/it][A
 50%|█████     | 5/10 [05:19<05:20, 64.01s/it][A[2024-05-29 14:21:21,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=335, skipped=0, lr=[1.9957174023217473e-05], mom=[(0.9, 0.999)]
steps: 335 loss: 1.2728 iter time (s): 63.155 samples/sec: 2.027

 50%|█████     | 5/10 [05:20<05:20, 64.04s/it][A
 50%|█████     | 5/10 [05:20<05:20, 64.05s/it][A
 50%|█████     | 5/10 [05:20<05:20, 64.02s/it][A
 50%|█████     | 5/10 [05:19<05:20, 64.01s/it][A
 50%|█████     | 5/10 [05:19<05:20, 64.01s/it][A
 50%|█████     | 5/10 [05:20<05:20, 64.02s/it][A
 50%|█████     | 5/10 [05:20<05:20, 64.01s/it][A
 60%|██████    | 6/10 [06:23<04:15, 63.80s/it][A[2024-05-29 14:22:25,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=336, skipped=0, lr=[1.9956648807165074e-05], mom=[(0.9, 0.999)]
steps: 336 loss: 1.2750 iter time (s): 62.690 samples/sec: 2.042

 60%|██████    | 6/10 [06:23<04:15, 63.81s/it][A
 60%|██████    | 6/10 [06:23<04:15, 63.81s/it][A
 60%|██████    | 6/10 [06:23<04:15, 63.80s/it][A
 60%|██████    | 6/10 [06:23<04:15, 63.79s/it][A
 60%|██████    | 6/10 [06:23<04:15, 63.79s/it][A
 60%|██████    | 6/10 [06:23<04:15, 63.79s/it][A
 60%|██████    | 6/10 [06:23<04:15, 63.78s/it][A
 70%|███████   | 7/10 [07:27<03:11, 63.91s/it][A[2024-05-29 14:23:29,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=337, skipped=0, lr=[1.99561203970934e-05], mom=[(0.9, 0.999)]
steps: 337 loss: 1.2746 iter time (s): 63.522 samples/sec: 2.015

 70%|███████   | 7/10 [07:27<03:11, 63.93s/it][A
 70%|███████   | 7/10 [07:27<03:11, 63.93s/it][A
 70%|███████   | 7/10 [07:27<03:11, 63.91s/it][A
 70%|███████   | 7/10 [07:27<03:11, 63.92s/it][A
 70%|███████   | 7/10 [07:27<03:11, 63.91s/it][A
 70%|███████   | 7/10 [07:27<03:11, 63.91s/it][A
 70%|███████   | 7/10 [07:27<03:11, 63.91s/it][A
 80%|████████  | 8/10 [08:31<02:08, 64.10s/it][A[2024-05-29 14:24:33,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=338, skipped=0, lr=[1.9955588793171995e-05], mom=[(0.9, 0.999)]
steps: 338 loss: 1.2635 iter time (s): 63.843 samples/sec: 2.005

 80%|████████  | 8/10 [08:32<02:08, 64.10s/it][A
 80%|████████  | 8/10 [08:32<02:08, 64.10s/it][A
 80%|████████  | 8/10 [08:32<02:08, 64.12s/it][A
 80%|████████  | 8/10 [08:32<02:08, 64.11s/it][A
 80%|████████  | 8/10 [08:31<02:08, 64.11s/it][A
 80%|████████  | 8/10 [08:32<02:08, 64.11s/it][A
 80%|████████  | 8/10 [08:32<02:08, 64.10s/it][A
 90%|█████████ | 9/10 [09:35<01:03, 63.93s/it][A[2024-05-29 14:25:37,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=339, skipped=0, lr=[1.9955053995571402e-05], mom=[(0.9, 0.999)]
steps: 339 loss: 1.2238 iter time (s): 62.917 samples/sec: 2.034

 90%|█████████ | 9/10 [09:35<01:03, 63.94s/it][A
 90%|█████████ | 9/10 [09:35<01:03, 63.93s/it][A
 90%|█████████ | 9/10 [09:35<01:03, 63.94s/it][A
 90%|█████████ | 9/10 [09:35<01:03, 63.94s/it][A
 90%|█████████ | 9/10 [09:35<01:03, 63.94s/it][A
 90%|█████████ | 9/10 [09:35<01:03, 63.93s/it][A
 90%|█████████ | 9/10 [09:35<01:03, 63.94s/it][A
100%|██████████| 10/10 [10:38<00:00, 63.61s/it][A100%|██████████| 10/10 [10:38<00:00, 63.82s/it]
  7%|▋         | 34/520 [54:33<20:17:04, 150.26s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 14:26:40,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[1.9954516004463197e-05], mom=[(0.9, 0.999)]
steps: 340 loss: 1.2510 iter time (s): 62.212 samples/sec: 2.057

100%|██████████| 10/10 [10:38<00:00, 63.62s/it][A100%|██████████| 10/10 [10:38<00:00, 63.87s/it]
  7%|▋         | 34/520 [54:29<20:15:52, 150.11s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [10:38<00:00, 63.59s/it][A100%|██████████| 10/10 [10:38<00:00, 63.85s/it]
  7%|▋         | 34/520 [54:41<20:18:30, 150.43s/it]
100%|██████████| 10/10 [10:38<00:00, 63.64s/it][A100%|██████████| 10/10 [10:38<00:00, 63.88s/it]
  7%|▋         | 34/520 [54:39<20:18:08, 150.39s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [10:38<00:00, 63.59s/it][A100%|██████████| 10/10 [10:38<00:00, 63.84s/it]
  7%|▋         | 34/520 [54:40<20:18:17, 150.41s/it]

100%|██████████| 10/10 [10:38<00:00, 63.61s/it][A  0%|          | 0/10 [00:00<?, ?it/s][A100%|██████████| 10/10 [10:38<00:00, 63.85s/it]
  7%|▋         | 34/520 [54:40<20:18:16, 150.40s/it]

  0%|          | 0/10 [00:00<?, ?it/s][A  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [10:38<00:00, 63.61s/it][A100%|██████████| 10/10 [10:38<00:00, 63.86s/it]
  7%|▋         | 34/520 [54:36<20:17:31, 150.31s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [10:38<00:00, 63.61s/it][A100%|██████████| 10/10 [10:38<00:00, 63.85s/it]
  7%|▋         | 34/520 [54:44<20:19:07, 150.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_284
Training on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [01:19<11:56, 79.67s/it][A[2024-05-29 14:28:00,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=341, skipped=0, lr=[1.9953974820019984e-05], mom=[(0.9, 0.999)]
steps: 341 loss: 0.5733 iter time (s): 79.450 samples/sec: 1.611

 10%|█         | 1/10 [01:20<12:01, 80.17s/it][A
 10%|█         | 1/10 [01:20<12:00, 80.11s/it][A
 10%|█         | 1/10 [01:20<12:02, 80.25s/it][A
 10%|█         | 1/10 [01:20<12:02, 80.27s/it][A
 10%|█         | 1/10 [01:20<12:03, 80.33s/it][A
 10%|█         | 1/10 [01:20<12:02, 80.31s/it][A
 10%|█         | 1/10 [01:20<12:01, 80.20s/it][A
 20%|██        | 2/10 [02:38<10:32, 79.10s/it][A[2024-05-29 14:29:19,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=342, skipped=0, lr=[1.9953430442415384e-05], mom=[(0.9, 0.999)]
steps: 342 loss: 0.5688 iter time (s): 78.004 samples/sec: 1.641

 20%|██        | 2/10 [02:38<10:34, 79.29s/it][A
 20%|██        | 2/10 [02:38<10:34, 79.29s/it][A
 20%|██        | 2/10 [02:38<10:34, 79.32s/it][A
 20%|██        | 2/10 [02:38<10:34, 79.34s/it][A
 20%|██        | 2/10 [02:38<10:34, 79.34s/it][A
 20%|██        | 2/10 [02:38<10:34, 79.33s/it][A
 20%|██        | 2/10 [02:38<10:34, 79.29s/it][A
 30%|███       | 3/10 [03:56<09:11, 78.82s/it][A[2024-05-29 14:30:37,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=343, skipped=0, lr=[1.9952882871824054e-05], mom=[(0.9, 0.999)]
steps: 343 loss: 0.5782 iter time (s): 77.793 samples/sec: 1.645

 30%|███       | 3/10 [03:57<09:12, 78.89s/it][A
 30%|███       | 3/10 [03:57<09:12, 78.89s/it][A
 30%|███       | 3/10 [03:57<09:12, 78.94s/it][A
 30%|███       | 3/10 [03:57<09:12, 78.94s/it][A
 30%|███       | 3/10 [03:57<09:12, 78.97s/it][A
 30%|███       | 3/10 [03:57<09:12, 78.95s/it][A
 30%|███       | 3/10 [03:57<09:12, 78.92s/it][A
 40%|████      | 4/10 [05:15<07:52, 78.78s/it][A[2024-05-29 14:31:56,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=344, skipped=0, lr=[1.995233210842166e-05], mom=[(0.9, 0.999)]
steps: 344 loss: 0.5589 iter time (s): 78.056 samples/sec: 1.640

 40%|████      | 4/10 [05:16<07:53, 78.85s/it][A
 40%|████      | 4/10 [05:16<07:53, 78.86s/it][A
 40%|████      | 4/10 [05:16<07:53, 78.86s/it][A
 40%|████      | 4/10 [05:16<07:53, 78.87s/it][A
 40%|████      | 4/10 [05:16<07:53, 78.86s/it][A
 40%|████      | 4/10 [05:16<07:53, 78.87s/it][A
 40%|████      | 4/10 [05:16<07:53, 78.85s/it][A
 50%|█████     | 5/10 [06:35<06:35, 79.14s/it][A[2024-05-29 14:33:16,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=345, skipped=0, lr=[1.9951778152384908e-05], mom=[(0.9, 0.999)]
steps: 345 loss: 0.5905 iter time (s): 79.144 samples/sec: 1.617

 50%|█████     | 5/10 [06:35<06:35, 79.19s/it][A
 50%|█████     | 5/10 [06:35<06:35, 79.17s/it][A
 50%|█████     | 5/10 [06:35<06:36, 79.20s/it][A
 50%|█████     | 5/10 [06:36<06:36, 79.21s/it][A
 50%|█████     | 5/10 [06:36<06:36, 79.21s/it][A
 50%|█████     | 5/10 [06:35<06:36, 79.21s/it][A
 50%|█████     | 5/10 [06:35<06:35, 79.20s/it][A
 60%|██████    | 6/10 [07:54<05:16, 79.20s/it][A[2024-05-29 14:34:35,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=346, skipped=0, lr=[1.9951221003891517e-05], mom=[(0.9, 0.999)]
steps: 346 loss: 0.5726 iter time (s): 78.598 samples/sec: 1.629

 60%|██████    | 6/10 [07:55<05:16, 79.24s/it][A
 60%|██████    | 6/10 [07:55<05:17, 79.25s/it][A
 60%|██████    | 6/10 [07:55<05:16, 79.25s/it][A
 60%|██████    | 6/10 [07:55<05:17, 79.25s/it][A
 60%|██████    | 6/10 [07:55<05:17, 79.27s/it][A
 60%|██████    | 6/10 [07:55<05:17, 79.25s/it][A
 60%|██████    | 6/10 [07:55<05:17, 79.25s/it][A
 70%|███████   | 7/10 [09:14<03:58, 79.37s/it][A[2024-05-29 14:35:55,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=347, skipped=0, lr=[1.9950660663120237e-05], mom=[(0.9, 0.999)]
steps: 347 loss: 0.5716 iter time (s): 78.985 samples/sec: 1.621

 70%|███████   | 7/10 [09:14<03:58, 79.41s/it][A
 70%|███████   | 7/10 [09:14<03:58, 79.41s/it][A
 70%|███████   | 7/10 [09:15<03:58, 79.42s/it][A
 70%|███████   | 7/10 [09:15<03:58, 79.41s/it][A
 70%|███████   | 7/10 [09:15<03:58, 79.42s/it][A
 70%|███████   | 7/10 [09:15<03:58, 79.43s/it][A
 70%|███████   | 7/10 [09:15<03:58, 79.42s/it][A
 80%|████████  | 8/10 [10:34<02:39, 79.66s/it][A[2024-05-29 14:37:15,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=348, skipped=0, lr=[1.995009713025083e-05], mom=[(0.9, 0.999)]
steps: 348 loss: 0.5818 iter time (s): 79.501 samples/sec: 1.610

 80%|████████  | 8/10 [10:35<02:39, 79.69s/it][A
 80%|████████  | 8/10 [10:35<02:39, 79.68s/it][A
 80%|████████  | 8/10 [10:35<02:39, 79.68s/it][A
 80%|████████  | 8/10 [10:35<02:39, 79.68s/it][A
 80%|████████  | 8/10 [10:35<02:39, 79.69s/it][A
 80%|████████  | 8/10 [10:35<02:39, 79.69s/it][A
 80%|████████  | 8/10 [10:35<02:39, 79.69s/it][A
 90%|█████████ | 9/10 [11:53<01:19, 79.52s/it][A[2024-05-29 14:38:34,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=349, skipped=0, lr=[1.9949530405464102e-05], mom=[(0.9, 0.999)]
steps: 349 loss: 0.5735 iter time (s): 78.503 samples/sec: 1.631

 90%|█████████ | 9/10 [11:54<01:19, 79.54s/it][A
 90%|█████████ | 9/10 [11:54<01:19, 79.54s/it][A
 90%|█████████ | 9/10 [11:54<01:19, 79.55s/it][A
 90%|█████████ | 9/10 [11:54<01:19, 79.55s/it][A
 90%|█████████ | 9/10 [11:54<01:19, 79.55s/it][A
 90%|█████████ | 9/10 [11:54<01:19, 79.55s/it][A
 90%|█████████ | 9/10 [11:54<01:19, 79.55s/it][A
100%|██████████| 10/10 [13:14<00:00, 79.79s/it][A100%|██████████| 10/10 [13:14<00:00, 79.43s/it]
  7%|▋         | 35/520 [1:07:48<28:56:30, 214.83s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 14:39:55,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[1.994896048894187e-05], mom=[(0.9, 0.999)]
steps: 350 loss: 0.5530 iter time (s): 79.547 samples/sec: 1.609

100%|██████████| 10/10 [13:14<00:00, 79.79s/it][A100%|██████████| 10/10 [13:14<00:00, 79.48s/it]
  7%|▋         | 35/520 [1:07:44<28:55:48, 214.74s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:14<00:00, 79.78s/it][A100%|██████████| 10/10 [13:14<00:00, 79.48s/it]
  7%|▋         | 35/520 [1:07:54<28:57:50, 214.99s/it]
100%|██████████| 10/10 [13:14<00:00, 79.79s/it][A100%|██████████| 10/10 [13:14<00:00, 79.49s/it]
  7%|▋         | 35/520 [1:07:56<28:58:16, 215.04s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:14<00:00, 79.80s/it][A100%|██████████| 10/10 [13:14<00:00, 79.50s/it]
  7%|▋         | 35/520 [1:07:55<28:58:05, 215.02s/it]
100%|██████████| 10/10 [13:14<00:00, 79.80s/it][A100%|██████████| 10/10 [13:14<00:00, 79.50s/it]
  7%|▋         | 35/520 [1:07:55<28:58:03, 215.02s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:14<00:00, 79.80s/it][A100%|██████████| 10/10 [13:14<00:00, 79.50s/it]
  7%|▋         | 35/520 [1:07:51<28:57:23, 214.93s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [13:14<00:00, 79.80s/it][A100%|██████████| 10/10 [13:14<00:00, 79.49s/it]
  7%|▋         | 35/520 [1:07:59<28:58:49, 215.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_397
Training on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [01:40<15:07, 100.83s/it][A[2024-05-29 14:41:36,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=351, skipped=0, lr=[1.9948387380866977e-05], mom=[(0.9, 0.999)]
steps: 351 loss: 0.8479 iter time (s): 100.538 samples/sec: 1.273

 10%|█         | 1/10 [01:41<15:12, 101.34s/it][A
 10%|█         | 1/10 [01:41<15:11, 101.33s/it][A
 10%|█         | 1/10 [01:41<15:12, 101.42s/it][A
 10%|█         | 1/10 [01:41<15:13, 101.50s/it][A
 10%|█         | 1/10 [01:41<15:13, 101.50s/it][A
 10%|█         | 1/10 [01:41<15:13, 101.49s/it][A
 10%|█         | 1/10 [01:41<15:12, 101.41s/it][A
 20%|██        | 2/10 [03:22<13:31, 101.42s/it][A[2024-05-29 14:43:18,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=352, skipped=0, lr=[1.9947811081423287e-05], mom=[(0.9, 0.999)]
steps: 352 loss: 0.8213 iter time (s): 101.091 samples/sec: 1.266

 20%|██        | 2/10 [03:23<13:33, 101.69s/it][A
 20%|██        | 2/10 [03:23<13:33, 101.69s/it][A
 20%|██        | 2/10 [03:23<13:33, 101.66s/it][A
 20%|██        | 2/10 [03:23<13:33, 101.66s/it][A
 20%|██        | 2/10 [03:23<13:33, 101.70s/it][A
 20%|██        | 2/10 [03:23<13:33, 101.71s/it][A
 20%|██        | 2/10 [03:23<13:33, 101.66s/it][A
 30%|███       | 3/10 [05:04<11:50, 101.43s/it][A[2024-05-29 14:44:59,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=353, skipped=0, lr=[1.994723159079569e-05], mom=[(0.9, 0.999)]
steps: 353 loss: 0.8047 iter time (s): 100.616 samples/sec: 1.272

 30%|███       | 3/10 [05:04<11:50, 101.50s/it][A
 30%|███       | 3/10 [05:04<11:50, 101.50s/it][A
 30%|███       | 3/10 [05:04<11:50, 101.50s/it][A
 30%|███       | 3/10 [05:04<11:50, 101.51s/it][A
 30%|███       | 3/10 [05:04<11:50, 101.54s/it][A
 30%|███       | 3/10 [05:04<11:50, 101.53s/it][A
 30%|███       | 3/10 [05:04<11:50, 101.51s/it][A
 40%|████      | 4/10 [06:46<10:10, 101.79s/it][A[2024-05-29 14:46:42,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=354, skipped=0, lr=[1.994664890917011e-05], mom=[(0.9, 0.999)]
steps: 354 loss: 0.7920 iter time (s): 101.648 samples/sec: 1.259

 40%|████      | 4/10 [06:46<10:11, 101.86s/it][A
 40%|████      | 4/10 [06:47<10:11, 101.90s/it][A
 40%|████      | 4/10 [06:47<10:11, 101.87s/it][A
 40%|████      | 4/10 [06:46<10:11, 101.86s/it][A
 40%|████      | 4/10 [06:47<10:11, 101.88s/it][A
 40%|████      | 4/10 [06:47<10:11, 101.86s/it][A
 40%|████      | 4/10 [06:46<10:11, 101.86s/it][A
 50%|█████     | 5/10 [08:29<08:30, 102.09s/it][A[2024-05-29 14:48:24,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=355, skipped=0, lr=[1.9946063036733475e-05], mom=[(0.9, 0.999)]
steps: 355 loss: 0.7548 iter time (s): 101.882 samples/sec: 1.256

 50%|█████     | 5/10 [08:29<08:30, 102.20s/it][A
 50%|█████     | 5/10 [08:29<08:30, 102.17s/it][A
 50%|█████     | 5/10 [08:29<08:31, 102.21s/it][A
 50%|█████     | 5/10 [08:29<08:31, 102.22s/it][A
 50%|█████     | 5/10 [08:29<08:31, 102.20s/it][A
 50%|█████     | 5/10 [08:29<08:30, 102.20s/it][A
 50%|█████     | 5/10 [08:29<08:30, 102.20s/it][A
 60%|██████    | 6/10 [10:11<06:49, 102.30s/it][A[2024-05-29 14:50:06,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=356, skipped=0, lr=[1.9945473973673758e-05], mom=[(0.9, 0.999)]
steps: 356 loss: 0.7675 iter time (s): 101.268 samples/sec: 1.264

 60%|██████    | 6/10 [10:11<06:48, 102.13s/it][A
 60%|██████    | 6/10 [10:11<06:48, 102.14s/it][A
 60%|██████    | 6/10 [10:11<06:48, 102.13s/it][A
 60%|██████    | 6/10 [10:11<06:48, 102.13s/it][A
 60%|██████    | 6/10 [10:11<06:48, 102.14s/it][A
 60%|██████    | 6/10 [10:11<06:48, 102.15s/it][A
 60%|██████    | 6/10 [10:11<06:48, 102.13s/it][A
 70%|███████   | 7/10 [11:53<05:06, 102.00s/it][A[2024-05-29 14:51:48,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=357, skipped=0, lr=[1.9944881720179935e-05], mom=[(0.9, 0.999)]
steps: 357 loss: 0.7346 iter time (s): 101.175 samples/sec: 1.265

 70%|███████   | 7/10 [11:53<05:06, 102.08s/it][A
 70%|███████   | 7/10 [11:53<05:06, 102.07s/it][A
 70%|███████   | 7/10 [11:53<05:06, 102.08s/it][A
 70%|███████   | 7/10 [11:53<05:06, 102.07s/it][A
 70%|███████   | 7/10 [11:53<05:06, 102.06s/it][A
 70%|███████   | 7/10 [11:53<05:06, 102.07s/it][A
 70%|███████   | 7/10 [11:53<05:06, 102.06s/it][A
 80%|████████  | 8/10 [13:35<03:24, 102.16s/it][A[2024-05-29 14:53:31,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=358, skipped=0, lr=[1.9944286276442023e-05], mom=[(0.9, 0.999)]
steps: 358 loss: 0.7387 iter time (s): 101.724 samples/sec: 1.258

 80%|████████  | 8/10 [13:36<03:24, 102.20s/it][A
 80%|████████  | 8/10 [13:36<03:24, 102.22s/it][A
 80%|████████  | 8/10 [13:36<03:24, 102.20s/it][A
 80%|████████  | 8/10 [13:36<03:24, 102.21s/it][A
 80%|████████  | 8/10 [13:36<03:24, 102.21s/it][A
 80%|████████  | 8/10 [13:36<03:24, 102.21s/it][A
 80%|████████  | 8/10 [13:36<03:24, 102.20s/it][A
 90%|█████████ | 9/10 [15:17<01:42, 102.06s/it][A[2024-05-29 14:55:13,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=359, skipped=0, lr=[1.994368764265105e-05], mom=[(0.9, 0.999)]
steps: 359 loss: 0.7218 iter time (s): 101.025 samples/sec: 1.267

 90%|█████████ | 9/10 [15:17<01:42, 102.05s/it][A
 90%|█████████ | 9/10 [15:17<01:42, 102.06s/it][A
 90%|█████████ | 9/10 [15:18<01:42, 102.08s/it][A
 90%|█████████ | 9/10 [15:18<01:42, 102.08s/it][A
 90%|█████████ | 9/10 [15:18<01:42, 102.08s/it][A
 90%|█████████ | 9/10 [15:18<01:42, 102.07s/it][A
 90%|█████████ | 9/10 [15:17<01:42, 102.07s/it][A
100%|██████████| 10/10 [16:59<00:00, 102.01s/it][A100%|██████████| 10/10 [16:59<00:00, 101.94s/it]
[2024-05-29 14:56:55,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[1.994308581899908e-05], mom=[(0.9, 0.999)]
steps: 360 loss: 0.7020 iter time (s): 101.219 samples/sec: 1.265

100%|██████████| 10/10 [16:59<00:00, 102.04s/it][A100%|██████████| 10/10 [16:59<00:00, 101.99s/it]

100%|██████████| 10/10 [16:59<00:00, 102.03s/it][A100%|██████████| 10/10 [16:59<00:00, 101.99s/it]

100%|██████████| 10/10 [16:59<00:00, 102.02s/it][A100%|██████████| 10/10 [16:59<00:00, 101.99s/it]

100%|██████████| 10/10 [16:59<00:00, 102.01s/it][A100%|██████████| 10/10 [16:59<00:00, 101.99s/it]

100%|██████████| 10/10 [17:00<00:00, 102.03s/it][A100%|██████████| 10/10 [17:00<00:00, 102.00s/it]

100%|██████████| 10/10 [16:59<00:00, 102.03s/it][A100%|██████████| 10/10 [16:59<00:00, 102.00s/it]

100%|██████████| 10/10 [16:59<00:00, 102.03s/it][A100%|██████████| 10/10 [16:59<00:00, 101.99s/it]
Checkpointing at shard 36
[2024-05-29 14:57:00,592] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step360 is about to be saved!
[2024-05-29 14:57:01,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_00-model_states.pt...
[2024-05-29 14:57:04,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_03-model_states.pt...
[2024-05-29 14:57:05,341] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_02-model_states.pt...
[2024-05-29 14:57:05,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_04-model_states.pt...
[2024-05-29 14:57:11,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_00-model_states.pt.
[2024-05-29 14:57:14,326] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_01-model_states.pt...
[2024-05-29 14:57:15,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_06-model_states.pt...
[2024-05-29 14:57:16,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_05-model_states.pt...
[2024-05-29 14:57:19,452] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_08-model_states.pt...
[2024-05-29 14:57:20,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_07-model_states.pt...
[2024-05-29 15:19:38,779] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_04-model_states.pt.
[2024-05-29 15:19:38,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_02-model_states.pt.
[2024-05-29 15:19:38,851] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_03_model_states.pt...
[2024-05-29 15:19:38,852] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_01_model_states.pt
[2024-05-29 15:19:38,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_01_model_states.pt...
[2024-05-29 15:19:40,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_03-model_states.pt.
[2024-05-29 15:19:40,246] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_02_model_states.pt...
[2024-05-29 15:19:41,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_03_model_states.pt.
[2024-05-29 15:19:41,062] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
[2024-05-29 15:19:41,101] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_01_model_states.pt.
[2024-05-29 15:19:41,101] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
[2024-05-29 15:19:42,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_02_model_states.pt.
[2024-05-29 15:19:42,407] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
[2024-05-29 15:19:44,364] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_07-model_states.pt.
[2024-05-29 15:19:44,741] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_06-model_states.pt.
[2024-05-29 15:19:44,805] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_06_model_states.pt...
[2024-05-29 15:19:44,864] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_08-model_states.pt.
[2024-05-29 15:19:44,889] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_05-model_states.pt.
[2024-05-29 15:19:44,993] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_05_model_states.pt...
[2024-05-29 15:19:45,046] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_04_model_states.pt...
[2024-05-29 15:19:45,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_09-model_states.pt...
[2024-05-29 15:19:45,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_01-model_states.pt.
[2024-05-29 15:19:45,374] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_00_model_states.pt
[2024-05-29 15:19:45,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_00_model_states.pt...
[2024-05-29 15:19:46,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_06_model_states.pt.
[2024-05-29 15:19:46,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
[2024-05-29 15:19:46,621] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_05_model_states.pt.
[2024-05-29 15:19:46,622] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
[2024-05-29 15:19:46,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_04_model_states.pt.
[2024-05-29 15:19:46,975] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
[2024-05-29 15:19:49,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_00_model_states.pt.
[2024-05-29 15:19:49,371] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
[2024-05-29 15:19:49,570] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/layer_09-model_states.pt.
[2024-05-29 15:19:49,575] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_07_model_states.pt...
[2024-05-29 15:19:50,151] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint/global_step360/mp_rank_07_model_states.pt.
[2024-05-29 15:19:50,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
Checkpoint saved using --- 1374.262281179428 seconds ---
  7%|▋         | 36/520 [1:47:46<65:35:56, 487.93s/it]  7%|▋         | 36/520 [1:47:51<65:36:55, 488.05s/it]  7%|▋         | 36/520 [1:47:50<65:36:36, 488.01s/it]  7%|▋         | 36/520 [1:47:45<65:37:47, 488.16s/it]  7%|▋         | 36/520 [1:47:49<65:36:35, 488.01s/it]  7%|▋         | 36/520 [1:47:38<65:34:59, 487.81s/it]  7%|▋         | 36/520 [1:47:53<65:37:10, 488.08s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_132
  7%|▋         | 36/520 [1:47:49<65:36:38, 488.01s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A

  0%|          | 0/10 [00:00<?, ?it/s][A  0%|          | 0/10 [00:00<?, ?it/s][ATraining on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [01:39<14:52, 99.14s/it][A[2024-05-29 15:21:32,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=361, skipped=0, lr=[1.9942480805679182e-05], mom=[(0.9, 0.999)]
steps: 361 loss: 0.6040 iter time (s): 101.572 samples/sec: 1.260

 10%|█         | 1/10 [01:41<15:16, 101.88s/it][A
 10%|█         | 1/10 [01:42<15:18, 102.06s/it][A
 10%|█         | 1/10 [01:42<15:19, 102.18s/it][A
 10%|█         | 1/10 [01:42<15:20, 102.23s/it][A
 10%|█         | 1/10 [01:42<15:21, 102.34s/it][A
 10%|█         | 1/10 [01:42<15:21, 102.36s/it][A
 10%|█         | 1/10 [01:42<15:21, 102.37s/it][A
 20%|██        | 2/10 [03:21<13:27, 100.90s/it][A[2024-05-29 15:23:14,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=362, skipped=0, lr=[1.9941872602885468e-05], mom=[(0.9, 0.999)]
steps: 362 loss: 0.6133 iter time (s): 101.336 samples/sec: 1.263

 20%|██        | 2/10 [03:23<13:36, 102.02s/it][A
 20%|██        | 2/10 [03:24<13:36, 102.07s/it][A
 20%|██        | 2/10 [03:24<13:37, 102.15s/it][A
 20%|██        | 2/10 [03:24<13:37, 102.16s/it][A
 20%|██        | 2/10 [03:24<13:37, 102.19s/it][A
 20%|██        | 2/10 [03:24<13:37, 102.20s/it][A
 20%|██        | 2/10 [03:24<13:37, 102.20s/it][A
 30%|███       | 3/10 [05:04<11:52, 101.79s/it][A[2024-05-29 15:24:57,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=363, skipped=0, lr=[1.9941261210813058e-05], mom=[(0.9, 0.999)]
steps: 363 loss: 0.5974 iter time (s): 102.111 samples/sec: 1.254

 30%|███       | 3/10 [05:06<11:56, 102.38s/it][A
 30%|███       | 3/10 [05:07<11:57, 102.45s/it][A
 30%|███       | 3/10 [05:07<11:57, 102.46s/it][A
 30%|███       | 3/10 [05:07<11:57, 102.49s/it][A
 30%|███       | 3/10 [05:07<11:57, 102.50s/it][A
 30%|███       | 3/10 [05:07<11:57, 102.50s/it][A
 30%|███       | 3/10 [05:07<11:57, 102.51s/it][A
 40%|████      | 4/10 [06:47<10:15, 102.50s/it][A[2024-05-29 15:26:40,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=364, skipped=0, lr=[1.9940646629658112e-05], mom=[(0.9, 0.999)]
steps: 364 loss: 0.6110 iter time (s): 102.839 samples/sec: 1.245

 40%|████      | 4/10 [06:50<10:17, 102.89s/it][A
 40%|████      | 4/10 [06:50<10:17, 102.89s/it][A
 40%|████      | 4/10 [06:50<10:17, 102.87s/it][A
 40%|████      | 4/10 [06:50<10:17, 102.89s/it][A
 40%|████      | 4/10 [06:50<10:17, 102.90s/it][A
 40%|████      | 4/10 [06:50<10:17, 102.92s/it][A
 40%|████      | 4/10 [06:50<10:17, 102.91s/it][A
 50%|█████     | 5/10 [08:32<08:36, 103.25s/it][A[2024-05-29 15:28:25,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=365, skipped=0, lr=[1.9940028859617792e-05], mom=[(0.9, 0.999)]
steps: 365 loss: 0.6168 iter time (s): 103.922 samples/sec: 1.232

 50%|█████     | 5/10 [08:35<08:37, 103.50s/it][A
 50%|█████     | 5/10 [08:35<08:37, 103.53s/it][A
 50%|█████     | 5/10 [08:35<08:37, 103.54s/it][A
 50%|█████     | 5/10 [08:35<08:37, 103.52s/it][A
 50%|█████     | 5/10 [08:35<08:37, 103.53s/it][A
 50%|█████     | 5/10 [08:35<08:37, 103.54s/it][A
 50%|█████     | 5/10 [08:35<08:37, 103.55s/it][A
 60%|██████    | 6/10 [10:14<06:51, 102.99s/it][A[2024-05-29 15:30:07,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=366, skipped=0, lr=[1.99394079008903e-05], mom=[(0.9, 0.999)]
steps: 366 loss: 0.5712 iter time (s): 101.285 samples/sec: 1.264

 60%|██████    | 6/10 [10:17<06:52, 103.02s/it][A
 60%|██████    | 6/10 [10:17<06:52, 103.01s/it][A
 60%|██████    | 6/10 [10:17<06:52, 103.01s/it][A
 60%|██████    | 6/10 [10:17<06:52, 103.03s/it][A
 60%|██████    | 6/10 [10:17<06:52, 103.02s/it][A
 60%|██████    | 6/10 [10:17<06:52, 103.04s/it][A
 60%|██████    | 6/10 [10:17<06:52, 103.04s/it][A
 70%|███████   | 7/10 [11:57<05:08, 102.98s/it][A[2024-05-29 15:31:50,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=367, skipped=0, lr=[1.993878375367485e-05], mom=[(0.9, 0.999)]
steps: 367 loss: 0.5800 iter time (s): 102.656 samples/sec: 1.247

 70%|███████   | 7/10 [12:00<05:09, 103.15s/it][A
 70%|███████   | 7/10 [12:00<05:09, 103.16s/it][A
 70%|███████   | 7/10 [12:00<05:09, 103.16s/it][A
 70%|███████   | 7/10 [12:00<05:09, 103.19s/it][A
 70%|███████   | 7/10 [12:01<05:09, 103.20s/it][A
 70%|███████   | 7/10 [12:01<05:09, 103.20s/it][A
 70%|███████   | 7/10 [12:01<05:09, 103.20s/it][A
 80%|████████  | 8/10 [13:49<03:31, 105.85s/it][A[2024-05-29 15:33:42,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=368, skipped=0, lr=[1.993815641817169e-05], mom=[(0.9, 0.999)]
steps: 368 loss: 0.5731 iter time (s): 111.094 samples/sec: 1.152

 80%|████████  | 8/10 [13:52<03:31, 105.95s/it][A
 80%|████████  | 8/10 [13:52<03:31, 105.95s/it][A
 80%|████████  | 8/10 [13:52<03:31, 105.95s/it][A
 80%|████████  | 8/10 [13:52<03:31, 105.95s/it][A
 80%|████████  | 8/10 [13:52<03:31, 105.95s/it][A
 80%|████████  | 8/10 [13:52<03:31, 105.95s/it][A
 80%|████████  | 8/10 [13:52<03:31, 105.95s/it][A
 90%|█████████ | 9/10 [15:39<01:47, 107.10s/it][A[2024-05-29 15:35:32,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=369, skipped=0, lr=[1.9937525894582082e-05], mom=[(0.9, 0.999)]
steps: 369 loss: 0.5903 iter time (s): 109.119 samples/sec: 1.173

 90%|█████████ | 9/10 [15:42<01:47, 107.19s/it][A
 90%|█████████ | 9/10 [15:42<01:47, 107.17s/it][A
 90%|█████████ | 9/10 [15:42<01:47, 107.18s/it][A
 90%|█████████ | 9/10 [15:42<01:47, 107.17s/it][A
 90%|█████████ | 9/10 [15:42<01:47, 107.17s/it][A
 90%|█████████ | 9/10 [15:42<01:47, 107.17s/it][A
 90%|█████████ | 9/10 [15:42<01:47, 107.16s/it][A
100%|██████████| 10/10 [17:29<00:00, 107.99s/it][A100%|██████████| 10/10 [17:29<00:00, 104.96s/it]
  7%|▋         | 37/520 [2:05:14<76:56:01, 573.42s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 15:37:22,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[1.9936892183108313e-05], mom=[(0.9, 0.999)]
steps: 370 loss: 0.5855 iter time (s): 109.285 samples/sec: 1.171

100%|██████████| 10/10 [17:32<00:00, 108.03s/it][A100%|██████████| 10/10 [17:32<00:00, 105.23s/it]
  7%|▋         | 37/520 [2:05:11<76:57:01, 573.54s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [17:32<00:00, 108.04s/it][A100%|██████████| 10/10 [17:32<00:00, 105.25s/it]
  7%|▋         | 37/520 [2:05:21<76:58:36, 573.74s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [17:32<00:00, 108.04s/it][A100%|██████████| 10/10 [17:32<00:00, 105.26s/it]
  7%|▋         | 37/520 [2:05:24<76:58:59, 573.79s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [17:32<00:00, 108.05s/it][A100%|██████████| 10/10 [17:32<00:00, 105.27s/it]
  7%|▋         | 37/520 [2:05:22<76:58:49, 573.77s/it]
100%|██████████| 10/10 [17:32<00:00, 108.03s/it][A100%|██████████| 10/10 [17:32<00:00, 105.27s/it]
  7%|▋         | 37/520 [2:05:23<76:58:53, 573.78s/it]
100%|██████████| 10/10 [17:32<00:00, 108.04s/it][A100%|██████████| 10/10 [17:32<00:00, 105.28s/it]
  7%|▋         | 37/520 [2:05:19<76:58:21, 573.71s/it]
100%|██████████| 10/10 [17:32<00:00, 108.04s/it][A100%|██████████| 10/10 [17:32<00:00, 105.28s/it]
  7%|▋         | 37/520 [2:05:26<76:59:25, 573.84s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_144

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][ATraining on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [02:47<25:05, 167.32s/it][A[2024-05-29 15:40:11,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=371, skipped=0, lr=[1.9936255283953695e-05], mom=[(0.9, 0.999)]
steps: 371 loss: 0.5903 iter time (s): 168.615 samples/sec: 0.759

 10%|█         | 1/10 [02:48<25:19, 168.81s/it][A
 10%|█         | 1/10 [02:49<25:24, 169.42s/it][A
 10%|█         | 1/10 [02:49<25:24, 169.37s/it][A
 10%|█         | 1/10 [02:49<25:23, 169.33s/it][A
 10%|█         | 1/10 [02:49<25:22, 169.20s/it][A
 10%|█         | 1/10 [02:49<25:23, 169.25s/it][A
 10%|█         | 1/10 [02:49<25:23, 169.27s/it][A
 20%|██        | 2/10 [05:31<22:03, 165.39s/it][A[2024-05-29 15:42:55,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=372, skipped=0, lr=[1.9935615197322563e-05], mom=[(0.9, 0.999)]
steps: 372 loss: 0.5807 iter time (s): 163.895 samples/sec: 0.781

 20%|██        | 2/10 [05:32<22:07, 165.96s/it][A
 20%|██        | 2/10 [05:33<22:09, 166.20s/it][A
 20%|██        | 2/10 [05:33<22:09, 166.15s/it][A
 20%|██        | 2/10 [05:33<22:09, 166.18s/it][A
 20%|██        | 2/10 [05:33<22:09, 166.13s/it][A
 20%|██        | 2/10 [05:33<22:09, 166.16s/it][A
 20%|██        | 2/10 [05:33<22:09, 166.17s/it][A
 30%|███       | 3/10 [08:16<19:15, 165.06s/it][A[2024-05-29 15:45:40,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=373, skipped=0, lr=[1.9934971923420264e-05], mom=[(0.9, 0.999)]
steps: 373 loss: 0.5616 iter time (s): 164.529 samples/sec: 0.778

 30%|███       | 3/10 [08:18<19:18, 165.49s/it][A
 30%|███       | 3/10 [08:17<19:18, 165.49s/it][A
 30%|███       | 3/10 [08:17<19:18, 165.43s/it][A
 30%|███       | 3/10 [08:17<19:18, 165.50s/it][A
 30%|███       | 3/10 [08:17<19:18, 165.50s/it][A
 30%|███       | 3/10 [08:17<19:18, 165.50s/it][A
 30%|███       | 3/10 [08:17<19:19, 165.62s/it][A
 40%|████      | 4/10 [10:59<16:26, 164.41s/it][A[2024-05-29 15:48:23,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=374, skipped=0, lr=[1.9934325462453184e-05], mom=[(0.9, 0.999)]
steps: 374 loss: 0.5502 iter time (s): 162.676 samples/sec: 0.787

 40%|████      | 4/10 [11:01<16:28, 164.75s/it][A
 40%|████      | 4/10 [11:01<16:28, 164.82s/it][A
 40%|████      | 4/10 [11:01<16:28, 164.74s/it][A
 40%|████      | 4/10 [11:01<16:28, 164.74s/it][A
 40%|████      | 4/10 [11:01<16:28, 164.75s/it][A
 40%|████      | 4/10 [11:01<16:28, 164.75s/it][A
 40%|████      | 4/10 [11:01<16:28, 164.82s/it][A
 50%|█████     | 5/10 [13:42<13:39, 163.82s/it][A[2024-05-29 15:51:06,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=375, skipped=0, lr=[1.9933675814628723e-05], mom=[(0.9, 0.999)]
steps: 375 loss: 0.5827 iter time (s): 161.845 samples/sec: 0.791

 50%|█████     | 5/10 [13:44<13:39, 163.92s/it][A
 50%|█████     | 5/10 [13:44<13:39, 163.96s/it][A
 50%|█████     | 5/10 [13:44<13:39, 163.95s/it][A
 50%|█████     | 5/10 [13:44<13:39, 163.95s/it][A
 50%|█████     | 5/10 [13:44<13:39, 163.96s/it][A
 50%|█████     | 5/10 [13:44<13:39, 163.96s/it][A
 50%|█████     | 5/10 [13:44<13:40, 164.01s/it][A
 60%|██████    | 6/10 [16:25<10:55, 163.78s/it][A[2024-05-29 15:53:50,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=376, skipped=0, lr=[1.9933022980155302e-05], mom=[(0.9, 0.999)]
steps: 376 loss: 0.5509 iter time (s): 163.004 samples/sec: 0.785

 60%|██████    | 6/10 [16:27<10:55, 163.87s/it][A
 60%|██████    | 6/10 [16:27<10:55, 163.85s/it][A
 60%|██████    | 6/10 [16:27<10:55, 163.86s/it][A
 60%|██████    | 6/10 [16:27<10:55, 163.85s/it][A
 60%|██████    | 6/10 [16:27<10:55, 163.87s/it][A
 60%|██████    | 6/10 [16:27<10:55, 163.89s/it][A
 60%|██████    | 6/10 [16:27<10:55, 163.88s/it][A
 70%|███████   | 7/10 [19:14<08:15, 165.30s/it][A[2024-05-29 15:56:38,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=377, skipped=0, lr=[1.9932366959242366e-05], mom=[(0.9, 0.999)]
steps: 377 loss: 0.5823 iter time (s): 167.746 samples/sec: 0.763

 70%|███████   | 7/10 [19:16<08:16, 165.38s/it][A
 70%|███████   | 7/10 [19:16<08:16, 165.37s/it][A
 70%|███████   | 7/10 [19:16<08:15, 165.33s/it][A
 70%|███████   | 7/10 [19:16<08:16, 165.36s/it][A
 70%|███████   | 7/10 [19:16<08:16, 165.36s/it][A
 70%|███████   | 7/10 [19:16<08:16, 165.37s/it][A
 70%|███████   | 7/10 [19:16<08:16, 165.36s/it][A
 80%|████████  | 8/10 [21:58<05:29, 164.94s/it][A[2024-05-29 15:59:23,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=378, skipped=0, lr=[1.9931707752100388e-05], mom=[(0.9, 0.999)]
steps: 378 loss: 0.5762 iter time (s): 163.607 samples/sec: 0.782

 80%|████████  | 8/10 [21:59<05:29, 164.85s/it][A
 80%|████████  | 8/10 [22:00<05:29, 164.99s/it][A
 80%|████████  | 8/10 [22:00<05:30, 165.01s/it][A
 80%|████████  | 8/10 [22:00<05:30, 165.02s/it][A
 80%|████████  | 8/10 [22:00<05:30, 165.02s/it][A
 80%|████████  | 8/10 [22:00<05:29, 164.99s/it][A
 80%|████████  | 8/10 [22:00<05:29, 164.99s/it][A
 90%|█████████ | 9/10 [24:41<02:44, 164.36s/it][A[2024-05-29 16:02:06,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=379, skipped=0, lr=[1.993104535894085e-05], mom=[(0.9, 0.999)]
steps: 379 loss: 0.5711 iter time (s): 162.788 samples/sec: 0.786

 90%|█████████ | 9/10 [24:43<02:44, 164.38s/it][A
 90%|█████████ | 9/10 [24:43<02:44, 164.37s/it][A
 90%|█████████ | 9/10 [24:43<02:44, 164.37s/it][A
 90%|█████████ | 9/10 [24:43<02:44, 164.37s/it][A
 90%|█████████ | 9/10 [24:43<02:44, 164.38s/it][A
 90%|█████████ | 9/10 [24:43<02:44, 164.43s/it][A
 90%|█████████ | 9/10 [24:43<02:44, 164.38s/it][A
100%|██████████| 10/10 [27:24<00:00, 164.06s/it][A100%|██████████| 10/10 [27:24<00:00, 164.50s/it]
  7%|▋         | 38/520 [2:32:40<102:21:00, 764.44s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 16:04:49,407] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[1.9930379779976267e-05], mom=[(0.9, 0.999)]
steps: 380 loss: 0.5904 iter time (s): 162.673 samples/sec: 0.787

100%|██████████| 10/10 [27:26<00:00, 164.05s/it][A100%|██████████| 10/10 [27:26<00:00, 164.69s/it]
  7%|▋         | 38/520 [2:32:38<102:24:06, 764.83s/it]
100%|██████████| 10/10 [27:26<00:00, 164.04s/it][A100%|██████████| 10/10 [27:26<00:00, 164.68s/it]
  7%|▋         | 38/520 [2:32:48<102:25:14, 764.97s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [27:26<00:00, 164.08s/it][A100%|██████████| 10/10 [27:26<00:00, 164.69s/it]
  7%|▋         | 38/520 [2:32:51<102:25:40, 765.02s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [27:26<00:00, 164.07s/it][A100%|██████████| 10/10 [27:26<00:00, 164.68s/it]
  7%|▋         | 38/520 [2:32:49<102:25:26, 764.99s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [27:26<00:00, 164.07s/it][A100%|██████████| 10/10 [27:26<00:00, 164.69s/it]
  7%|▋         | 38/520 [2:32:46<102:25:05, 764.95s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [27:26<00:00, 164.14s/it][A100%|██████████| 10/10 [27:26<00:00, 164.70s/it]
  7%|▋         | 38/520 [2:32:53<102:26:04, 765.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_451

100%|██████████| 10/10 [27:27<00:00, 164.12s/it][A100%|██████████| 10/10 [27:27<00:00, 164.70s/it]
  7%|▋         | 38/520 [2:32:50<102:25:46, 765.03s/it]Training on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [01:10<10:32, 70.26s/it][A[2024-05-29 16:05:57,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=381, skipped=0, lr=[1.9929711015420177e-05], mom=[(0.9, 0.999)]
steps: 381 loss: 0.4030 iter time (s): 67.099 samples/sec: 1.908

 10%|█         | 1/10 [01:07<10:11, 67.89s/it][A
 10%|█         | 1/10 [01:07<10:11, 67.97s/it][A
 10%|█         | 1/10 [01:07<10:11, 67.98s/it][A
 10%|█         | 1/10 [01:08<10:12, 68.02s/it][A
 10%|█         | 1/10 [01:07<10:09, 67.76s/it][A
 10%|█         | 1/10 [01:07<10:11, 67.96s/it][A
 10%|█         | 1/10 [01:07<10:10, 67.86s/it][A
 20%|██        | 2/10 [02:17<09:08, 68.51s/it][A[2024-05-29 16:07:04,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=382, skipped=0, lr=[1.9929039065487134e-05], mom=[(0.9, 0.999)]
steps: 382 loss: 0.3695 iter time (s): 66.552 samples/sec: 1.923

 20%|██        | 2/10 [02:15<09:00, 67.52s/it][A
 20%|██        | 2/10 [02:15<09:00, 67.57s/it][A
 20%|██        | 2/10 [02:15<09:00, 67.60s/it][A
 20%|██        | 2/10 [02:15<09:00, 67.58s/it][A
 20%|██        | 2/10 [02:15<09:00, 67.59s/it][A
 20%|██        | 2/10 [02:15<09:02, 67.87s/it][A
 20%|██        | 2/10 [02:15<09:03, 67.99s/it][A
 30%|███       | 3/10 [03:23<07:52, 67.49s/it][A[2024-05-29 16:08:10,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=383, skipped=0, lr=[1.9928363930392715e-05], mom=[(0.9, 0.999)]
steps: 383 loss: 0.3969 iter time (s): 64.971 samples/sec: 1.970

 30%|███       | 3/10 [03:21<07:48, 66.93s/it][A
 30%|███       | 3/10 [03:21<07:48, 66.94s/it][A
 30%|███       | 3/10 [03:21<07:48, 66.91s/it][A
 30%|███       | 3/10 [03:21<07:48, 66.95s/it][A
 30%|███       | 3/10 [03:21<07:47, 66.78s/it][A
 30%|███       | 3/10 [03:21<07:48, 66.94s/it][A
 30%|███       | 3/10 [03:21<07:47, 66.84s/it][A
 40%|████      | 4/10 [04:30<06:42, 67.10s/it][A[2024-05-29 16:09:17,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=384, skipped=0, lr=[1.9927685610353525e-05], mom=[(0.9, 0.999)]
steps: 384 loss: 0.3698 iter time (s): 65.856 samples/sec: 1.944

 40%|████      | 4/10 [04:27<06:40, 66.76s/it][A
 40%|████      | 4/10 [04:27<06:40, 66.76s/it][A
 40%|████      | 4/10 [04:27<06:40, 66.78s/it][A
 40%|████      | 4/10 [04:27<06:40, 66.77s/it][A
 40%|████      | 4/10 [04:27<06:40, 66.67s/it][A
 40%|████      | 4/10 [04:27<06:40, 66.77s/it][A
 40%|████      | 4/10 [04:27<06:40, 66.71s/it][A
 50%|█████     | 5/10 [05:38<05:37, 67.58s/it][A[2024-05-29 16:10:25,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=385, skipped=0, lr=[1.992700410558718e-05], mom=[(0.9, 0.999)]
steps: 385 loss: 0.3332 iter time (s): 67.817 samples/sec: 1.887

 50%|█████     | 5/10 [05:36<05:36, 67.37s/it][A
 50%|█████     | 5/10 [05:36<05:36, 67.37s/it][A
 50%|█████     | 5/10 [05:36<05:36, 67.38s/it][A
 50%|█████     | 5/10 [05:36<05:36, 67.38s/it][A
 50%|█████     | 5/10 [05:36<05:36, 67.31s/it][A
 50%|█████     | 5/10 [05:36<05:36, 67.38s/it][A
 50%|█████     | 5/10 [05:36<05:36, 67.34s/it][A
 60%|██████    | 6/10 [06:44<04:27, 67.00s/it][A[2024-05-29 16:11:31,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=386, skipped=0, lr=[1.9926319416312324e-05], mom=[(0.9, 0.999)]
steps: 386 loss: 0.3447 iter time (s): 65.164 samples/sec: 1.964

 60%|██████    | 6/10 [06:42<04:27, 66.86s/it][A
 60%|██████    | 6/10 [06:42<04:27, 66.86s/it][A
 60%|██████    | 6/10 [06:42<04:27, 66.85s/it][A
 60%|██████    | 6/10 [06:42<04:27, 66.87s/it][A
 60%|██████    | 6/10 [06:42<04:27, 66.82s/it][A
 60%|██████    | 6/10 [06:42<04:27, 66.85s/it][A
 60%|██████    | 6/10 [06:42<04:27, 66.83s/it][A
 70%|███████   | 7/10 [07:50<03:20, 66.71s/it][A[2024-05-29 16:12:37,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=387, skipped=0, lr=[1.9925631542748625e-05], mom=[(0.9, 0.999)]
steps: 387 loss: 0.3436 iter time (s): 65.555 samples/sec: 1.953

 70%|███████   | 7/10 [07:48<03:19, 66.64s/it][A
 70%|███████   | 7/10 [07:48<03:19, 66.65s/it][A
 70%|███████   | 7/10 [07:48<03:19, 66.63s/it][A
 70%|███████   | 7/10 [07:48<03:19, 66.65s/it][A
 70%|███████   | 7/10 [07:48<03:19, 66.61s/it][A
 70%|███████   | 7/10 [07:48<03:19, 66.64s/it][A
 70%|███████   | 7/10 [07:48<03:19, 66.63s/it][A
 80%|████████  | 8/10 [08:55<02:11, 65.96s/it][A[2024-05-29 16:13:42,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=388, skipped=0, lr=[1.9924940485116768e-05], mom=[(0.9, 0.999)]
steps: 388 loss: 0.3247 iter time (s): 63.590 samples/sec: 2.013

 80%|████████  | 8/10 [08:52<02:11, 65.87s/it][A
 80%|████████  | 8/10 [08:52<02:11, 65.91s/it][A
 80%|████████  | 8/10 [08:52<02:11, 65.88s/it][A
 80%|████████  | 8/10 [08:52<02:11, 65.87s/it][A
 80%|████████  | 8/10 [08:52<02:11, 65.85s/it][A
 80%|████████  | 8/10 [08:52<02:11, 65.88s/it][A
 80%|████████  | 8/10 [08:52<02:11, 65.87s/it][A
 90%|█████████ | 9/10 [10:01<01:05, 66.00s/it][A[2024-05-29 16:14:48,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=389, skipped=0, lr=[1.9924246243638464e-05], mom=[(0.9, 0.999)]
steps: 389 loss: 0.3401 iter time (s): 65.156 samples/sec: 1.965

 90%|█████████ | 9/10 [09:58<01:05, 65.85s/it][A
 90%|█████████ | 9/10 [09:58<01:05, 65.88s/it][A
 90%|█████████ | 9/10 [09:58<01:05, 65.87s/it][A
 90%|█████████ | 9/10 [09:58<01:05, 65.85s/it][A
 90%|█████████ | 9/10 [09:58<01:05, 65.85s/it][A
 90%|█████████ | 9/10 [09:58<01:05, 65.86s/it][A
 90%|█████████ | 9/10 [09:58<01:05, 65.85s/it][A
100%|██████████| 10/10 [11:06<00:00, 65.72s/it][A100%|██████████| 10/10 [11:06<00:00, 66.63s/it]
  8%|▊         | 39/520 [2:43:47<99:29:23, 744.62s/it] 
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 16:15:53,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[1.992354881853644e-05], mom=[(0.9, 0.999)]
steps: 390 loss: 0.3255 iter time (s): 64.755 samples/sec: 1.977

100%|██████████| 10/10 [11:03<00:00, 65.72s/it][A100%|██████████| 10/10 [11:03<00:00, 66.39s/it]
  8%|▊         | 39/520 [2:43:42<99:27:23, 744.37s/it] 
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [11:03<00:00, 65.71s/it][A100%|██████████| 10/10 [11:03<00:00, 66.39s/it]
  8%|▊         | 39/520 [2:43:52<99:28:17, 744.49s/it] 
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [11:03<00:00, 65.71s/it][A100%|██████████| 10/10 [11:03<00:00, 66.39s/it]
  8%|▊         | 39/520 [2:43:54<99:28:28, 744.51s/it] 
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [11:03<00:00, 65.73s/it][A100%|██████████| 10/10 [11:03<00:00, 66.40s/it]
  8%|▊         | 39/520 [2:43:53<99:28:24, 744.50s/it] 
100%|██████████| 10/10 [11:03<00:00, 65.71s/it][A100%|██████████| 10/10 [11:03<00:00, 66.39s/it]
  8%|▊         | 39/520 [2:43:50<99:28:03, 744.46s/it] 
100%|██████████| 10/10 [11:03<00:00, 65.71s/it][A100%|██████████| 10/10 [11:03<00:00, 66.37s/it]
  8%|▊         | 39/520 [2:43:53<99:28:22, 744.50s/it] 
100%|██████████| 10/10 [11:03<00:00, 65.71s/it][A100%|██████████| 10/10 [11:03<00:00, 66.38s/it]
  8%|▊         | 39/520 [2:43:57<99:28:45, 744.54s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_405

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][ATraining on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
 10%|█         | 1/10 [01:13<11:02, 73.65s/it][A[2024-05-29 16:17:07,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=391, skipped=0, lr=[1.992284821003445e-05], mom=[(0.9, 0.999)]
steps: 391 loss: 0.6634 iter time (s): 73.099 samples/sec: 1.751

 10%|█         | 1/10 [01:13<11:05, 73.99s/it][A
 10%|█         | 1/10 [01:14<11:07, 74.13s/it][A
 10%|█         | 1/10 [01:14<11:06, 74.08s/it][A
 10%|█         | 1/10 [01:13<11:05, 73.96s/it][A
 10%|█         | 1/10 [01:13<11:05, 73.99s/it][A
 10%|█         | 1/10 [01:13<11:05, 73.99s/it][A
 10%|█         | 1/10 [01:13<11:05, 73.93s/it][A
 20%|██        | 2/10 [02:49<11:35, 86.93s/it][A[2024-05-29 16:18:43,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=392, skipped=0, lr=[1.9922144418357264e-05], mom=[(0.9, 0.999)]
steps: 392 loss: 0.6327 iter time (s): 95.698 samples/sec: 1.338

 20%|██        | 2/10 [02:50<11:37, 87.20s/it][A
 20%|██        | 2/10 [02:50<11:38, 87.26s/it][A
 20%|██        | 2/10 [02:50<11:37, 87.22s/it][A
 20%|██        | 2/10 [02:50<11:37, 87.23s/it][A
 20%|██        | 2/10 [02:50<11:37, 87.20s/it][A
 20%|██        | 2/10 [02:50<11:37, 87.20s/it][A
 20%|██        | 2/10 [02:50<11:37, 87.17s/it][A
 30%|███       | 3/10 [04:17<10:12, 87.44s/it][A[2024-05-29 16:20:11,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=393, skipped=0, lr=[1.992143744373068e-05], mom=[(0.9, 0.999)]
steps: 393 loss: 0.6290 iter time (s): 87.156 samples/sec: 1.469

 30%|███       | 3/10 [04:18<10:12, 87.52s/it][A
 30%|███       | 3/10 [04:18<10:12, 87.53s/it][A
 30%|███       | 3/10 [04:18<10:12, 87.53s/it][A
 30%|███       | 3/10 [04:18<10:12, 87.51s/it][A
 30%|███       | 3/10 [04:18<10:12, 87.50s/it][A
 30%|███       | 3/10 [04:18<10:12, 87.52s/it][A
 30%|███       | 3/10 [04:18<10:12, 87.49s/it][A
 40%|████      | 4/10 [05:31<08:12, 82.04s/it][A[2024-05-29 16:21:25,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=394, skipped=0, lr=[1.9920727286381505e-05], mom=[(0.9, 0.999)]
steps: 394 loss: 0.6179 iter time (s): 72.950 samples/sec: 1.755

 40%|████      | 4/10 [05:31<08:12, 82.02s/it][A
 40%|████      | 4/10 [05:32<08:12, 82.07s/it][A
 40%|████      | 4/10 [05:32<08:12, 82.04s/it][A
 40%|████      | 4/10 [05:32<08:12, 82.05s/it][A
 40%|████      | 4/10 [05:31<08:12, 82.03s/it][A
 40%|████      | 4/10 [05:31<08:12, 82.04s/it][A
 40%|████      | 4/10 [05:31<08:12, 82.01s/it][A
 50%|█████     | 5/10 [06:45<06:35, 79.14s/it][A[2024-05-29 16:22:39,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=395, skipped=0, lr=[1.9920013946537585e-05], mom=[(0.9, 0.999)]
steps: 395 loss: 0.6048 iter time (s): 73.374 samples/sec: 1.744

 50%|█████     | 5/10 [06:46<06:35, 79.16s/it][A
 50%|█████     | 5/10 [06:46<06:35, 79.14s/it][A
 50%|█████     | 5/10 [06:46<06:35, 79.17s/it][A
 50%|█████     | 5/10 [06:46<06:35, 79.17s/it][A
 50%|█████     | 5/10 [06:46<06:35, 79.17s/it][A
 50%|█████     | 5/10 [06:46<06:35, 79.17s/it][A
 50%|█████     | 5/10 [06:45<06:35, 79.16s/it][A
 60%|██████    | 6/10 [07:59<05:09, 77.45s/it][A[2024-05-29 16:23:53,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=396, skipped=0, lr=[1.991929742442777e-05], mom=[(0.9, 0.999)]
steps: 396 loss: 0.6009 iter time (s): 73.465 samples/sec: 1.742

 60%|██████    | 6/10 [08:00<05:10, 77.52s/it][A
 60%|██████    | 6/10 [08:00<05:10, 77.58s/it][A
 60%|██████    | 6/10 [08:00<05:10, 77.55s/it][A
 60%|██████    | 6/10 [08:00<05:10, 77.54s/it][A
 60%|██████    | 6/10 [08:00<05:10, 77.54s/it][A
 60%|██████    | 6/10 [08:00<05:10, 77.56s/it][A
 60%|██████    | 6/10 [08:00<05:10, 77.54s/it][A
 70%|███████   | 7/10 [09:13<03:48, 76.24s/it][A[2024-05-29 16:25:07,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=397, skipped=0, lr=[1.991857772028194e-05], mom=[(0.9, 0.999)]
steps: 397 loss: 0.6122 iter time (s): 72.340 samples/sec: 1.769

 70%|███████   | 7/10 [09:13<03:48, 76.05s/it][A
 70%|███████   | 7/10 [09:13<03:48, 76.09s/it][A
 70%|███████   | 7/10 [09:13<03:48, 76.08s/it][A
 70%|███████   | 7/10 [09:13<03:48, 76.06s/it][A
 70%|███████   | 7/10 [09:13<03:48, 76.07s/it][A
 70%|███████   | 7/10 [09:13<03:48, 76.08s/it][A
 70%|███████   | 7/10 [09:13<03:48, 76.07s/it][A
 80%|████████  | 8/10 [10:27<02:30, 75.35s/it][A[2024-05-29 16:26:20,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=398, skipped=0, lr=[1.9917854834330996e-05], mom=[(0.9, 0.999)]
steps: 398 loss: 0.6108 iter time (s): 73.236 samples/sec: 1.748

 80%|████████  | 8/10 [10:27<02:30, 75.40s/it][A
 80%|████████  | 8/10 [10:27<02:30, 75.40s/it][A
 80%|████████  | 8/10 [10:27<02:30, 75.40s/it][A
 80%|████████  | 8/10 [10:27<02:30, 75.40s/it][A
 80%|████████  | 8/10 [10:27<02:30, 75.41s/it][A
 80%|████████  | 8/10 [10:27<02:30, 75.41s/it][A
 80%|████████  | 8/10 [10:27<02:30, 75.41s/it][A
 90%|█████████ | 9/10 [11:40<01:14, 74.62s/it][A[2024-05-29 16:27:33,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=399, skipped=0, lr=[1.991712876680685e-05], mom=[(0.9, 0.999)]
steps: 399 loss: 0.5678 iter time (s): 72.224 samples/sec: 1.772

 90%|█████████ | 9/10 [11:40<01:14, 74.64s/it][A
 90%|█████████ | 9/10 [11:40<01:14, 74.66s/it][A
 90%|█████████ | 9/10 [11:40<01:14, 74.64s/it][A
 90%|█████████ | 9/10 [11:40<01:14, 74.65s/it][A
 90%|█████████ | 9/10 [11:40<01:14, 74.65s/it][A
 90%|█████████ | 9/10 [11:40<01:14, 74.66s/it][A
 90%|█████████ | 9/10 [11:40<01:14, 74.65s/it][A
100%|██████████| 10/10 [12:53<00:00, 74.36s/it][A100%|██████████| 10/10 [12:53<00:00, 77.38s/it]
  8%|▊         | 40/520 [2:56:41<100:09:52, 751.23s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A[2024-05-29 16:28:47,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[1.9916399517942458e-05], mom=[(0.9, 0.999)]
steps: 400 loss: 0.5779 iter time (s): 73.029 samples/sec: 1.753

100%|██████████| 10/10 [12:54<00:00, 74.36s/it][A100%|██████████| 10/10 [12:54<00:00, 77.41s/it]
  8%|▊         | 40/520 [2:56:36<100:08:34, 751.07s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [12:54<00:00, 74.36s/it][A100%|██████████| 10/10 [12:54<00:00, 77.43s/it]
  8%|▊         | 40/520 [2:56:47<100:09:26, 751.18s/it]
  0%|          | 0/10 [00:00<?, ?it/s][A
100%|██████████| 10/10 [12:54<00:00, 74.39s/it][A100%|██████████| 10/10 [12:54<00:00, 77.43s/it]
  8%|▊         | 40/520 [2:56:49<100:09:46, 751.22s/it]
100%|██████████| 10/10 [12:54<00:00, 74.36s/it][A100%|██████████| 10/10 [12:54<00:00, 77.42s/it]
  8%|▊         | 40/520 [2:56:47<100:09:31, 751.19s/it]
100%|██████████| 10/10 [12:54<00:00, 74.37s/it][A100%|██████████| 10/10 [12:54<00:00, 77.42s/it]
  8%|▊         | 40/520 [2:56:48<100:09:33, 751.20s/it]
100%|██████████| 10/10 [12:54<00:00, 74.36s/it][A100%|██████████| 10/10 [12:54<00:00, 77.42s/it]
  8%|▊         | 40/520 [2:56:44<100:09:20, 751.17s/it]
100%|██████████| 10/10 [12:54<00:00, 74.36s/it][A100%|██████████| 10/10 [12:54<00:00, 77.41s/it]
  8%|▊         | 40/520 [2:56:51<100:09:51, 751.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_184

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][ATraining on 1280 of 1280 sentences.

  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A
  0%|          | 0/10 [00:00<?, ?it/s][A  0%|          | 0/10 [00:05<?, ?it/s]
  8%|▊         | 40/520 [2:56:57<35:23:33, 265.44s/it] 
Traceback (most recent call last):
  File "llava/train_parallel_deepspeed_mixtral_lora.py", line 395, in <module>
    main()
  File "llava/train_parallel_deepspeed_mixtral_lora.py", line 373, in main
    loss = engine.train_batch(data_iter=training_dataloader)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/deepspeed/runtime/pipe/engine.py", line 373, in train_batch
    self._exec_schedule(sched)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/deepspeed/runtime/pipe/engine.py", line 1373, in _exec_schedule
    self._exec_instr(**cmd.kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/deepspeed/runtime/pipe/engine.py", line 679, in _exec_forward_pass
    outputs = super().forward(inputs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1852, in forward
    loss = self.module(*inputs, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/deepspeed/runtime/pipe/module.py", line 351, in forward
    x = func(forward_input)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/deepspeed/runtime/pipe/module.py", line 344, in exec_func
    inputs = layer(inputs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/llava/deepspeed_pipeline_model.py", line 135, in forward
    layer_outputs = layer(
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/llava/mixtral_modification/modeling_mixtral.py", line 1110, in forward
    hidden_states, router_logits, shared_sparse_adapter_router_logits = self.block_sparse_moe(hidden_states)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/llava/mixtral_modification/modeling_mixtral.py", line 992, in forward
    current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, None]
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/llava/mixtral_modification/modeling_mixtral.py", line 882, in forward
    current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/activation.py", line 393, in forward
    return F.silu(input, inplace=self.inplace)
  File "/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py", line 2075, in silu
    return torch._C._nn.silu(input)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 178.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 44.75 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 74.26 GiB is allocated by PyTorch, and 214.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2024-05-29 16:29:17,237] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751098
[2024-05-29 16:29:17,239] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751099
[2024-05-29 16:29:18,166] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751100
[2024-05-29 16:29:19,049] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751101
[2024-05-29 16:29:19,928] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751102
[2024-05-29 16:29:20,845] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751103
[2024-05-29 16:29:21,722] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751104
[2024-05-29 16:29:23,059] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 1751105
[2024-05-29 16:29:24,020] [ERROR] [launch.py:322:sigkill_handler] ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=7', '--num_stages=8', '--lora_r=256', '--lora_alpha=512', '--save_model_shard=6', '--skip_shard=30', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_256_512_checkpoint'] exits with return code = 1
