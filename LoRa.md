LoRa
1 shared expert
2 sparse expert + Adapter
3 shared expert + sparse

1: (rank= [64, 128, 256] )
2: (rank= [32, 64, 128])
3: ()

Parallel Adapter




All Experiments to do:
for each of architecture design:
+   shared_adapter
+   shared_sparse_adapter
+   embedded_sparse_adapter

+   (shared_adapter + embedded_sparse_adapter) ?
+   (shared_sparse_adapter + embedded_sparse_adapter) ?

for each in (LoRA, parallel adapter)

for each of bottleneck size:
choose 3 for each (could vary according with #parameter size)







