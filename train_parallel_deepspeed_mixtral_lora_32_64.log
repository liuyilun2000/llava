[2024-06-23 11:52:40,289] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-23 11:52:44,558] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-06-23 11:52:44,558] [INFO] [runner.py:568:main] cmd = /home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train_parallel_deepspeed_mixtral_lora.py --num_stages=8 --lora_r=32 --lora_alpha=64 --save_model_shard=2 --skip_shard=5180 --checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint
[2024-06-23 11:52:48,105] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-23 11:52:49,842] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-06-23 11:52:49,842] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-06-23 11:52:49,842] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-06-23 11:52:49,842] [INFO] [launch.py:163:main] dist_world_size=8
[2024-06-23 11:52:49,842] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-06-23 11:52:49,855] [INFO] [launch.py:253:main] process 1585435 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=0', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:49,865] [INFO] [launch.py:253:main] process 1585436 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=1', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:49,876] [INFO] [launch.py:253:main] process 1585437 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=2', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:49,892] [INFO] [launch.py:253:main] process 1585438 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=3', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:49,908] [INFO] [launch.py:253:main] process 1585439 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=4', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:49,924] [INFO] [launch.py:253:main] process 1585440 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=5', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:49,940] [INFO] [launch.py:253:main] process 1585441 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=6', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:49,956] [INFO] [launch.py:253:main] process 1585443 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=7', '--num_stages=8', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=2', '--skip_shard=5180', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint']
[2024-06-23 11:52:56,186] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-23 11:52:56,215] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-23 11:52:56,910] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-23 11:52:56,925] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-23 11:53:00,757] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-23 11:53:00,904] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-23 11:53:01,502] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-23 11:53:01,637] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-23 11:53:01,637] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:26,  1.40s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:26,  1.41s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:26,  1.42s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:39,  2.20s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:44,  2.47s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:44,  2.47s/it][2024-06-23 11:53:10,221] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:33,  1.96s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:32,  1.94s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:33,  1.95s/it][2024-06-23 11:53:11,706] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-23 11:53:12,108] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-23 11:53:12,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:30,  1.90s/it][2024-06-23 11:53:13,019] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:30,  1.94s/it][2024-06-23 11:53:13,450] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  20%|██        | 4/20 [00:08<00:33,  2.09s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:27,  1.83s/it][2024-06-23 11:53:14,838] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:28,  1.87s/it][2024-06-23 11:53:14,959] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:28,  1.92s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  30%|███       | 6/20 [00:11<00:28,  2.00s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:30,  2.15s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:30,  2.17s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:34,  1.81s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  35%|███▌      | 7/20 [00:14<00:26,  2.06s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  35%|███▌      | 7/20 [00:14<00:27,  2.09s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:22,  1.17s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  35%|███▌      | 7/20 [00:15<00:31,  2.40s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:32,  1.71s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:28,  1.50s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:43,  2.41s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:25,  2.09s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:27,  1.54s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:27,  2.29s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:26,  2.24s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:36,  1.94s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:38,  2.14s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:37,  2.11s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:06<00:40,  2.39s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:23,  2.16s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:30,  1.82s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:27,  1.64s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:38,  2.13s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:19<00:25,  2.28s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:19<00:26,  2.40s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:33,  2.00s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:31,  1.94s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:27,  1.75s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:09<00:37,  2.34s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:21<00:23,  2.31s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:21<00:22,  2.26s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:06<00:37,  2.23s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:21<00:24,  2.43s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:28,  1.92s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:08<00:36,  2.30s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:28,  1.87s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:08<00:33,  2.10s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:11<00:35,  2.37s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:24<00:22,  2.47s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:10<00:25,  1.80s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:24<00:22,  2.48s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:25<00:23,  2.67s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:10<00:34,  2.30s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:10<00:33,  2.23s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:30,  2.19s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:14<00:34,  2.43s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:12<00:23,  1.77s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:26<00:19,  2.44s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:20,  2.54s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:20,  2.61s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:13<00:32,  2.35s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:13<00:26,  2.06s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:14<00:21,  1.82s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:13<00:31,  2.23s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:16<00:32,  2.50s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:29<00:17,  2.55s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:15<00:29,  2.23s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:30<00:18,  2.63s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:16<00:21,  1.93s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:15<00:28,  2.21s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:27,  2.28s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:30<00:19,  2.76s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:19<00:31,  2.63s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:17<00:26,  2.20s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:32<00:15,  2.53s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:18<00:19,  1.94s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:17<00:27,  2.30s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:33<00:16,  2.75s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:19<00:25,  2.35s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:33<00:15,  2.66s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:21<00:26,  2.42s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:19<00:24,  2.23s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:20<00:17,  1.95s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:34<00:13,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:35<00:12,  2.57s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:21<00:22,  2.29s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:20<00:26,  2.41s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:35<00:13,  2.62s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:22<00:16,  2.01s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:24<00:25,  2.58s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:22<00:24,  2.44s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:37<00:10,  2.53s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:22<00:21,  2.19s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:37<00:09,  2.43s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:37<00:09,  2.42s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:23<00:20,  2.32s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:26<00:22,  2.45s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:25<00:15,  2.16s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:24<00:20,  2.32s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:39<00:07,  2.53s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:24<00:21,  2.34s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:40<00:07,  2.52s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:40<00:07,  2.43s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:26<00:18,  2.37s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:26<00:12,  2.02s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:29<00:19,  2.38s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:19,  2.38s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:41<00:04,  2.42s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:42<00:04,  2.42s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:28<00:16,  2.31s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:19,  2.42s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:30<00:15,  2.20s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:29<00:10,  2.12s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:43<00:05,  2.76s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:44<00:02,  2.37s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:44<00:02,  2.29s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:30<00:17,  2.50s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:29<00:15,  2.28s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:44<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:44<00:00,  2.23s/it]
Loading checkpoint shards:  80%|████████  | 16/20 [00:30<00:08,  2.02s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:31<00:14,  2.45s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:45<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:45<00:00,  2.26s/it]
Loading checkpoint shards:  70%|███████   | 14/20 [00:33<00:14,  2.36s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:46<00:02,  2.72s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:32<00:14,  2.35s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:32<00:05,  1.90s/it]Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12
Loading checkpoint shards:  70%|███████   | 14/20 [00:31<00:14,  2.37s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:47<00:00,  2.28s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:47<00:00,  2.37s/it]
Loading checkpoint shards:  75%|███████▌  | 15/20 [00:33<00:12,  2.49s/it]Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12
Loading checkpoint shards:  90%|█████████ | 18/20 [00:34<00:03,  1.83s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:36<00:12,  2.45s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:34<00:11,  2.25s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:34<00:11,  2.28s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:35<00:01,  1.77s/it]Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12
Loading checkpoint shards: 100%|██████████| 20/20 [00:36<00:00,  1.36s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:36<00:00,  1.81s/it]
Loading checkpoint shards:  80%|████████  | 16/20 [00:36<00:10,  2.50s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:38<00:09,  2.36s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:36<00:08,  2.22s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:35<00:08,  2.15s/it]Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12
Loading checkpoint shards:  85%|████████▌ | 17/20 [00:38<00:07,  2.40s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:40<00:06,  2.28s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:38<00:06,  2.18s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:37<00:06,  2.07s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:42<00:04,  2.23s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:40<00:04,  2.34s/it]Rank 1 initialized with CUDA_MEM (60891594752, 85100068864)
Deepspeed engine initializing at --- RANK 1 --- ...
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:43<00:01,  1.86s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:41<00:01,  1.94s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  1.41s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.20s/it]
Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  1.46s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  2.09s/it]
Rank 7 initialized with CUDA_MEM (60365209600, 85100068864)
Deepspeed engine initializing at --- RANK 7 --- ...
Rank 3 initialized with CUDA_MEM (60891594752, 85100068864)
Deepspeed engine initializing at --- RANK 3 --- ...
Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12

SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1, ProcessCoord(pipe=2, data=0): 2, ProcessCoord(pipe=3, data=0): 3, ProcessCoord(pipe=4, data=0): 4, ProcessCoord(pipe=5, data=0): 5, ProcessCoord(pipe=6, data=0): 6, ProcessCoord(pipe=7, data=0): 7}
[2024-06-23 11:54:02,022] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:LanguageModelLayerWrapper
stage=0 layers=2Rank 4 initialized with CUDA_MEM (60891594752, 85100068864)

     0: LlavaMultiModalModuleWrapperDeepspeed engine initializing at --- RANK 4 --- ...

     1: LanguageModelLayerWrapper
stage=1 layers=1
     2: LanguageModelLayerWrapper
stage=2 layers=1
     3: LanguageModelLayerWrapper
stage=3 layers=1
     4: LanguageModelLayerWrapper
stage=4 layers=1
     5: LanguageModelLayerWrapper
stage=5 layers=1
     6: LanguageModelLayerWrapper
stage=6 layers=1
     7: LanguageModelLayerWrapper
stage=7 layers=2
     8: LanguageModelLayerWrapperLoading checkpoint shards:  90%|█████████ | 18/20 [00:42<00:05,  2.80s/it]
     9: LanguageModelFinalWrapper
  loss: loss_fn
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  90%|█████████ | 18/20 [00:42<00:05,  2.72s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:43<00:02,  2.22s/it]Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:42<00:02,  2.16s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  1.63s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.18s/it]
Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.15s/it]
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Print trainable params: 55590912 || all params: 47065053184 || trainable%: 0.12
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 3.1137325763702393 seconds
[2024-06-23 11:54:05,323] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 1.407339096069336 seconds
Time to load fused_adam op: 1.4080870151519775 seconds
[2024-06-23 11:54:05,360] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
[2024-06-23 11:54:05,362] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Time to load fused_adam op: 2.3154044151306152 seconds
[2024-06-23 11:54:05,368] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Rank 5 initialized with CUDA_MEM (60891594752, 85100068864)
Deepspeed engine initializing at --- RANK 5 --- ...
Rank 2 initialized with CUDA_MEM (60891594752, 85100068864)
Deepspeed engine initializing at --- RANK 2 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 6 initialized with CUDA_MEM (60891594752, 85100068864)
Deepspeed engine initializing at --- RANK 6 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 0 initialized with CUDA_MEM (58953826304, 85100068864)
Deepspeed engine initializing at --- RANK 0 --- ...
[2024-06-23 11:54:08,656] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-06-23 11:54:09,009] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 2.425757884979248 seconds
[2024-06-23 11:54:09,110] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 0.7056083679199219 seconds
[2024-06-23 11:54:09,131] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 1.4072742462158203 seconds
[2024-06-23 11:54:09,143] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 2.8596603870391846 seconds
[2024-06-23 11:54:12,851] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-06-23 11:54:12,852] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-23 11:54:12,863] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-06-23 11:54:12,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-06-23 11:54:12,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupCosineLR
[2024-06-23 11:54:12,864] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupCosineLR object at 0x150ab0a74a90>
[2024-06-23 11:54:12,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-06-23 11:54:12,865] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-06-23 11:54:12,865] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-23 11:54:12,865] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-23 11:54:12,865] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-06-23 11:54:12,865] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x150ab2268ac0>
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-06-23 11:54:12,866] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 128
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=True, output_path='/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint', job_name='deepspeed_monitor_logs') enabled=True
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   optimizer_name ............... adam
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 2e-05}
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True, 'use_reentrant': False}
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupCosineLR
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 5718, 'warmup_min_ratio': 0.1, 'warmup_num_steps': 171.54}
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   steps_per_print .............. 1
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-06-23 11:54:12,867] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-06-23 11:54:12,868] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-23 11:54:12,868] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-06-23 11:54:12,868] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-23 11:54:12,868] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-06-23 11:54:12,868] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 128, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupCosineLR", 
        "params": {
            "total_num_steps": 5.718000e+03, 
            "warmup_min_ratio": 0.1, 
            "warmup_num_steps": 171.54
        }
    }, 
    "pipeline": {
        "use_reentrant": false
    }, 
    "csv_monitor": {
        "enabled": true, 
        "output_path": "/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint", 
        "job_name": "deepspeed_monitor_logs"
    }
}
[2024-06-23 11:54:12,868] [INFO] [engine.py:101:__init__] CONFIG: micro_batches=128 micro_batch_size=1
[2024-06-23 11:54:12,868] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-06-23 11:54:17,295] [INFO] [engine.py:160:__init__] RANK=5 STAGE=5 LAYERS=1 [6, 7) STAGE_PARAMS=1703936 (1.704M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
[2024-06-23 11:54:17,295] [INFO] [engine.py:160:__init__] RANK=7 STAGE=7 LAYERS=2 [8, 10) STAGE_PARAMS=1703936 (1.704M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
[2024-06-23 11:54:17,295] [INFO] [engine.py:160:__init__] RANK=2 STAGE=2 LAYERS=1 [3, 4) STAGE_PARAMS=1703936 (1.704M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
[2024-06-23 11:54:17,295] [INFO] [engine.py:160:__init__] RANK=1 STAGE=1 LAYERS=1 [2, 3) STAGE_PARAMS=1703936 (1.704M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
[2024-06-23 11:54:17,295] [INFO] [engine.py:160:__init__] RANK=0 STAGE=0 LAYERS=2 [0, 2) STAGE_PARAMS=43663360 (43.663M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
[2024-06-23 11:54:17,295] [INFO] [engine.py:160:__init__] RANK=3 STAGE=3 LAYERS=1 [4, 5) STAGE_PARAMS=1703936 (1.704M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
[2024-06-23 11:54:17,296] [INFO] [engine.py:160:__init__] RANK=6 STAGE=6 LAYERS=1 [7, 8) STAGE_PARAMS=1703936 (1.704M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
[2024-06-23 11:54:17,296] [INFO] [engine.py:160:__init__] RANK=4 STAGE=4 LAYERS=1 [5, 6) STAGE_PARAMS=1703936 (1.704M) TOTAL_PARAMS=55590912 (55.591M) UNIQUE_PARAMS=55590912 (55.591M)
Deepspeed engine successfully initialized at --- RANK 0 --- hosting 24 of 136 trainable parameters
Loading latest model checkpoint at shard 5180
[2024-06-23 11:54:19,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:20,011] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:20,011] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:20,123] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:20,124] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_00-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 7 --- hosting 16 of 136 trainable parameters
[2024-06-23 11:54:20,305] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:20,421] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:20,421] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_07_model_states.pt...
[2024-06-23 11:54:20,426] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_07_model_states.pt.
[2024-06-23 11:54:20,427] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_08-model_states.pt...
[2024-06-23 11:54:20,628] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_00-model_states.pt.
[2024-06-23 11:54:20,630] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_00-model_states.pt...
[2024-06-23 11:54:21,112] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_00-model_states.pt.
[2024-06-23 11:54:21,276] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_01-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 1 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 2 --- hosting 16 of 136 trainable parameters
[2024-06-23 11:54:21,848] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:21,848] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:21,940] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:21,940] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_01_model_states.pt...
[2024-06-23 11:54:21,947] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_01_model_states.pt.
[2024-06-23 11:54:21,947] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_02-model_states.pt...
[2024-06-23 11:54:21,963] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:21,964] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_02_model_states.pt...
[2024-06-23 11:54:21,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_02_model_states.pt.
[2024-06-23 11:54:21,969] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_03-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 4 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 3 --- hosting 16 of 136 trainable parameters
[2024-06-23 11:54:21,973] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:21,974] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:22,061] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:22,061] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_03_model_states.pt...
[2024-06-23 11:54:22,066] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_03_model_states.pt.
[2024-06-23 11:54:22,067] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_04-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 6 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 5 --- hosting 16 of 136 trainable parameters
[2024-06-23 11:54:22,083] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:22,084] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt...
[2024-06-23 11:54:22,109] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:22,109] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_04_model_states.pt...
[2024-06-23 11:54:22,115] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_04_model_states.pt.
[2024-06-23 11:54:22,115] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_05-model_states.pt...
[2024-06-23 11:54:22,211] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:22,211] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_06_model_states.pt...
[2024-06-23 11:54:22,217] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_06_model_states.pt.
[2024-06-23 11:54:22,218] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_07-model_states.pt...
[2024-06-23 11:54:22,221] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_00_model_states.pt.
[2024-06-23 11:54:22,221] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_05_model_states.pt...
[2024-06-23 11:54:22,228] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/mp_rank_05_model_states.pt.
[2024-06-23 11:54:22,229] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_06-model_states.pt...
[2024-06-23 11:54:28,584] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_04-model_states.pt.
[2024-06-23 11:54:29,757] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_04-model_states.pt...
[2024-06-23 11:54:32,050] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_01-model_states.pt.
[2024-06-23 11:54:32,608] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_01-model_states.pt...
[2024-06-23 11:54:32,752] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_06-model_states.pt.
[2024-06-23 11:54:33,332] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_06-model_states.pt...
[2024-06-23 11:54:38,217] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_04-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 11:54:41,977] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_01-model_states.pt.
[2024-06-23 11:54:42,361] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_06-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]Shard 0 / 5180 skipped
Shard 1 / 5180 skipped
Shard 2 / 5180 skipped
Shard 3 / 5180 skipped
Shard 4 / 5180 skipped
Shard 5 / 5180 skipped
Shard 6 / 5180 skipped
Shard 7 / 5180 skipped
Shard 8 / 5180 skipped
Shard 9 / 5180 skipped
Shard 10 / 5180 skipped
Shard 11 / 5180 skipped
Shard 12 / 5180 skipped
Shard 13 / 5180 skipped
Shard 14 / 5180 skipped
Shard 15 / 5180 skipped
Shard 16 / 5180 skipped
Shard 17 / 5180 skipped
Shard 18 / 5180 skipped
Shard 19 / 5180 skipped
Shard 20 / 5180 skipped
Shard 21 / 5180 skipped
Shard 22 / 5180 skipped
Shard 23 / 5180 skipped
Shard 24 / 5180 skipped
Shard 25 / 5180 skipped
Shard 26 / 5180 skipped
Shard 27 / 5180 skipped
Shard 28 / 5180 skipped
Shard 29 / 5180 skipped
Shard 30 / 5180 skipped
Shard 31 / 5180 skipped
Shard 32 / 5180 skipped
Shard 33 / 5180 skipped
Shard 34 / 5180 skipped
Shard 35 / 5180 skipped
Shard 36 / 5180 skipped
Shard 37 / 5180 skipped
Shard 38 / 5180 skipped
Shard 39 / 5180 skipped
Shard 40 / 5180 skipped
Shard 41 / 5180 skipped
Shard 42 / 5180 skipped
Shard 43 / 5180 skipped
Shard 44 / 5180 skipped
Shard 45 / 5180 skipped
Shard 46 / 5180 skipped
Shard 47 / 5180 skipped
Shard 48 / 5180 skipped
Shard 49 / 5180 skipped
Shard 50 / 5180 skipped
Shard 51 / 5180 skipped
Shard 52 / 5180 skipped
Shard 53 / 5180 skipped
Shard 54 / 5180 skipped
Shard 55 / 5180 skipped
Shard 56 / 5180 skipped
Shard 57 / 5180 skipped
Shard 58 / 5180 skipped
Shard 59 / 5180 skipped
Shard 60 / 5180 skipped
Shard 61 / 5180 skipped
Shard 62 / 5180 skipped
Shard 63 / 5180 skipped
Shard 64 / 5180 skipped
Shard 65 / 5180 skipped
Shard 66 / 5180 skipped
Shard 67 / 5180 skipped
Shard 68 / 5180 skipped
Shard 69 / 5180 skipped
Shard 70 / 5180 skipped
Shard 71 / 5180 skipped
Shard 72 / 5180 skipped
Shard 73 / 5180 skipped
Shard 74 / 5180 skipped
Shard 75 / 5180 skipped
Shard 76 / 5180 skipped
Shard 77 / 5180 skipped
Shard 78 / 5180 skipped
Shard 79 / 5180 skipped
Shard 80 / 5180 skipped
Shard 81 / 5180 skipped
Shard 82 / 5180 skipped
Shard 83 / 5180 skipped
Shard 84 / 5180 skipped
Shard 85 / 5180 skipped
Shard 86 / 5180 skipped
Shard 87 / 5180 skipped
Shard 88 / 5180 skipped
Shard 89 / 5180 skipped
Shard 90 / 5180 skipped
Shard 91 / 5180 skipped
Shard 92 / 5180 skipped
Shard 93 / 5180 skipped
Shard 94 / 5180 skipped
Shard 95 / 5180 skipped
Shard 96 / 5180 skipped
Shard 97 / 5180 skipped
Shard 98 / 5180 skipped
Shard 99 / 5180 skipped
Shard 100 / 5180 skipped
Shard 101 / 5180 skipped
Shard 102 / 5180 skipped
Shard 103 / 5180 skipped
Shard 104 / 5180 skipped
Shard 105 / 5180 skipped
Shard 106 / 5180 skipped
Shard 107 / 5180 skipped
Shard 108 / 5180 skipped
Shard 109 / 5180 skipped
Shard 110 / 5180 skipped
Shard 111 / 5180 skipped
Shard 112 / 5180 skipped
Shard 113 / 5180 skipped
Shard 114 / 5180 skipped
Shard 115 / 5180 skipped
Shard 116 / 5180 skipped
Shard 117 / 5180 skipped
Shard 118 / 5180 skipped
Shard 119 / 5180 skipped
Shard 120 / 5180 skipped
Shard 121 / 5180 skipped
Shard 122 / 5180 skipped
Shard 123 / 5180 skipped
Shard 124 / 5180 skipped
Shard 125 / 5180 skipped
Shard 126 / 5180 skipped
Shard 127 / 5180 skipped
Shard 128 / 5180 skipped
Shard 129 / 5180 skipped
Shard 130 / 5180 skipped
Shard 131 / 5180 skipped
Shard 132 / 5180 skipped
Shard 133 / 5180 skipped
Shard 134 / 5180 skipped
Shard 135 / 5180 skipped
Shard 136 / 5180 skipped
Shard 137 / 5180 skipped
Shard 138 / 5180 skipped
Shard 139 / 5180 skipped
Shard 140 / 5180 skipped
Shard 141 / 5180 skipped
Shard 142 / 5180 skipped
Shard 143 / 5180 skipped
Shard 144 / 5180 skipped
Shard 145 / 5180 skipped
Shard 146 / 5180 skipped
Shard 147 / 5180 skipped
Shard 148 / 5180 skipped
Shard 149 / 5180 skipped
Shard 150 / 5180 skipped
Shard 151 / 5180 skipped
Shard 152 / 5180 skipped
Shard 153 / 5180 skipped
Shard 154 / 5180 skipped
Shard 155 / 5180 skipped
Shard 156 / 5180 skipped
Shard 157 / 5180 skipped
Shard 158 / 5180 skipped
Shard 159 / 5180 skipped
Shard 160 / 5180 skipped
Shard 161 / 5180 skipped
Shard 162 / 5180 skipped
Shard 163 / 5180 skipped
Shard 164 / 5180 skipped
Shard 165 / 5180 skipped
Shard 166 / 5180 skipped
Shard 167 / 5180 skipped
Shard 168 / 5180 skipped
Shard 169 / 5180 skipped
Shard 170 / 5180 skipped
Shard 171 / 5180 skipped
Shard 172 / 5180 skipped
Shard 173 / 5180 skipped
Shard 174 / 5180 skipped
Shard 175 / 5180 skipped
Shard 176 / 5180 skipped
Shard 177 / 5180 skipped
Shard 178 / 5180 skipped
Shard 179 / 5180 skipped
Shard 180 / 5180 skipped
Shard 181 / 5180 skipped
Shard 182 / 5180 skipped
Shard 183 / 5180 skipped
Shard 184 / 5180 skipped
Shard 185 / 5180 skipped
Shard 186 / 5180 skipped
Shard 187 / 5180 skipped
Shard 188 / 5180 skipped
Shard 189 / 5180 skipped
Shard 190 / 5180 skipped
Shard 191 / 5180 skipped
Shard 192 / 5180 skipped
Shard 193 / 5180 skipped
Shard 194 / 5180 skipped
Shard 195 / 5180 skipped
Shard 196 / 5180 skipped
Shard 197 / 5180 skipped
Shard 198 / 5180 skipped
Shard 199 / 5180 skipped
Shard 200 / 5180 skipped
Shard 201 / 5180 skipped
Shard 202 / 5180 skipped
Shard 203 / 5180 skipped
Shard 204 / 5180 skipped
Shard 205 / 5180 skipped
Shard 206 / 5180 skipped
Shard 207 / 5180 skipped
Shard 208 / 5180 skipped
Shard 209 / 5180 skipped
Shard 210 / 5180 skipped
Shard 211 / 5180 skipped
Shard 212 / 5180 skipped
Shard 213 / 5180 skipped
Shard 214 / 5180 skipped
Shard 215 / 5180 skipped
Shard 216 / 5180 skipped
Shard 217 / 5180 skipped
Shard 218 / 5180 skipped
Shard 219 / 5180 skipped
Shard 220 / 5180 skipped
Shard 221 / 5180 skipped
Shard 222 / 5180 skipped
Shard 223 / 5180 skipped
Shard 224 / 5180 skipped
Shard 225 / 5180 skipped
Shard 226 / 5180 skipped
Shard 227 / 5180 skipped
Shard 228 / 5180 skipped
Shard 229 / 5180 skipped
Shard 230 / 5180 skipped
Shard 231 / 5180 skipped
Shard 232 / 5180 skipped
Shard 233 / 5180 skipped
Shard 234 / 5180 skipped
Shard 235 / 5180 skipped
Shard 236 / 5180 skipped
Shard 237 / 5180 skipped
Shard 238 / 5180 skipped
Shard 239 / 5180 skipped
Shard 240 / 5180 skipped
Shard 241 / 5180 skipped
Shard 242 / 5180 skipped
Shard 243 / 5180 skipped
Shard 244 / 5180 skipped
Shard 245 / 5180 skipped
Shard 246 / 5180 skipped
Shard 247 / 5180 skipped
Shard 248 / 5180 skipped
Shard 249 / 5180 skipped
Shard 250 / 5180 skipped
Shard 251 / 5180 skipped
Shard 252 / 5180 skipped
Shard 253 / 5180 skipped
Shard 254 / 5180 skipped
Shard 255 / 5180 skipped
Shard 256 / 5180 skipped
Shard 257 / 5180 skipped
Shard 258 / 5180 skipped
Shard 259 / 5180 skipped
Shard 260 / 5180 skipped
Shard 261 / 5180 skipped
Shard 262 / 5180 skipped
Shard 263 / 5180 skipped
Shard 264 / 5180 skipped
Shard 265 / 5180 skipped
Shard 266 / 5180 skipped
Shard 267 / 5180 skipped
Shard 268 / 5180 skipped
Shard 269 / 5180 skipped
Shard 270 / 5180 skipped
Shard 271 / 5180 skipped
Shard 272 / 5180 skipped
Shard 273 / 5180 skipped
Shard 274 / 5180 skipped
Shard 275 / 5180 skipped
Shard 276 / 5180 skipped
Shard 277 / 5180 skipped
Shard 278 / 5180 skipped
Shard 279 / 5180 skipped
Shard 280 / 5180 skipped
Shard 281 / 5180 skipped
Shard 282 / 5180 skipped
Shard 283 / 5180 skipped
Shard 284 / 5180 skipped
Shard 285 / 5180 skipped
Shard 286 / 5180 skipped
Shard 287 / 5180 skipped
Shard 288 / 5180 skipped
Shard 289 / 5180 skipped
Shard 290 / 5180 skipped
Shard 291 / 5180 skipped
Shard 292 / 5180 skipped
Shard 293 / 5180 skipped
Shard 294 / 5180 skipped
Shard 295 / 5180 skipped
Shard 296 / 5180 skipped
Shard 297 / 5180 skipped
Shard 298 / 5180 skipped
Shard 299 / 5180 skipped
Shard 300 / 5180 skipped
Shard 301 / 5180 skipped
Shard 302 / 5180 skipped
Shard 303 / 5180 skipped
Shard 304 / 5180 skipped
Shard 305 / 5180 skipped
Shard 306 / 5180 skipped
Shard 307 / 5180 skipped
Shard 308 / 5180 skipped
Shard 309 / 5180 skipped
Shard 310 / 5180 skipped
Shard 311 / 5180 skipped
Shard 312 / 5180 skipped
Shard 313 / 5180 skipped
Shard 314 / 5180 skipped
Shard 315 / 5180 skipped
Shard 316 / 5180 skipped
Shard 317 / 5180 skipped
Shard 318 / 5180 skipped
Shard 319 / 5180 skipped
Shard 320 / 5180 skipped
Shard 321 / 5180 skipped
Shard 322 / 5180 skipped
Shard 323 / 5180 skipped
Shard 324 / 5180 skipped
Shard 325 / 5180 skipped
Shard 326 / 5180 skipped
Shard 327 / 5180 skipped
Shard 328 / 5180 skipped
Shard 329 / 5180 skipped
Shard 330 / 5180 skipped
Shard 331 / 5180 skipped
Shard 332 / 5180 skipped
Shard 333 / 5180 skipped
Shard 334 / 5180 skipped
Shard 335 / 5180 skipped
Shard 336 / 5180 skipped
Shard 337 / 5180 skipped
Shard 338 / 5180 skipped
Shard 339 / 5180 skipped
Shard 340 / 5180 skipped
Shard 341 / 5180 skipped
Shard 342 / 5180 skipped
Shard 343 / 5180 skipped
Shard 344 / 5180 skipped
Shard 345 / 5180 skipped
Shard 346 / 5180 skipped
Shard 347 / 5180 skipped
Shard 348 / 5180 skipped
Shard 349 / 5180 skipped
Shard 350 / 5180 skipped
Shard 351 / 5180 skipped
Shard 352 / 5180 skipped
Shard 353 / 5180 skipped
Shard 354 / 5180 skipped
Shard 355 / 5180 skipped
Shard 356 / 5180 skipped
Shard 357 / 5180 skipped
Shard 358 / 5180 skipped
Shard 359 / 5180 skipped
Shard 360 / 5180 skipped
Shard 361 / 5180 skipped
Shard 362 / 5180 skipped
Shard 363 / 5180 skipped
Shard 364 / 5180 skipped
Shard 365 / 5180 skipped
Shard 366 / 5180 skipped
Shard 367 / 5180 skipped
Shard 368 / 5180 skipped
Shard 369 / 5180 skipped
Shard 370 / 5180 skipped
Shard 371 / 5180 skipped
Shard 372 / 5180 skipped
Shard 373 / 5180 skipped
Shard 374 / 5180 skipped
Shard 375 / 5180 skipped
Shard 376 / 5180 skipped
Shard 377 / 5180 skipped
Shard 378 / 5180 skipped
Shard 379 / 5180 skipped
Shard 380 / 5180 skipped
Shard 381 / 5180 skipped
Shard 382 / 5180 skipped
Shard 383 / 5180 skipped
Shard 384 / 5180 skipped
Shard 385 / 5180 skipped
Shard 386 / 5180 skipped
Shard 387 / 5180 skipped
Shard 388 / 5180 skipped
Shard 389 / 5180 skipped
Shard 390 / 5180 skipped
Shard 391 / 5180 skipped
Shard 392 / 5180 skipped
Shard 393 / 5180 skipped
Shard 394 / 5180 skipped
Shard 395 / 5180 skipped
Shard 396 / 5180 skipped
Shard 397 / 5180 skipped
Shard 398 / 5180 skipped
Shard 399 / 5180 skipped
Shard 400 / 5180 skipped
Shard 401 / 5180 skipped
Shard 402 / 5180 skipped
Shard 403 / 5180 skipped
Shard 404 / 5180 skipped
Shard 405 / 5180 skipped
Shard 406 / 5180 skipped
Shard 407 / 5180 skipped
Shard 408 / 5180 skipped
Shard 409 / 5180 skipped
Shard 410 / 5180 skipped
Shard 411 / 5180 skipped
Shard 412 / 5180 skipped
Shard 413 / 5180 skipped
Shard 414 / 5180 skipped
Shard 415 / 5180 skipped
Shard 416 / 5180 skipped
Shard 417 / 5180 skipped
Shard 418 / 5180 skipped
Shard 419 / 5180 skipped
Shard 420 / 5180 skipped
Shard 421 / 5180 skipped
Shard 422 / 5180 skipped
Shard 423 / 5180 skipped
Shard 424 / 5180 skipped
Shard 425 / 5180 skipped
Shard 426 / 5180 skipped
Shard 427 / 5180 skipped
Shard 428 / 5180 skipped
Shard 429 / 5180 skipped
Shard 430 / 5180 skipped
Shard 431 / 5180 skipped
Shard 432 / 5180 skipped
Shard 433 / 5180 skipped
Shard 434 / 5180 skipped
Shard 435 / 5180 skipped
Shard 436 / 5180 skipped
Shard 437 / 5180 skipped
Shard 438 / 5180 skipped
Shard 439 / 5180 skipped
Shard 440 / 5180 skipped
Shard 441 / 5180 skipped
Shard 442 / 5180 skipped
Shard 443 / 5180 skipped
Shard 444 / 5180 skipped
Shard 445 / 5180 skipped
Shard 446 / 5180 skipped
Shard 447 / 5180 skipped
Shard 448 / 5180 skipped
Shard 449 / 5180 skipped
Shard 450 / 5180 skipped
Shard 451 / 5180 skipped
Shard 452 / 5180 skipped
Shard 453 / 5180 skipped
Shard 454 / 5180 skipped
Shard 455 / 5180 skipped
Shard 456 / 5180 skipped
Shard 457 / 5180 skipped
Shard 458 / 5180 skipped
Shard 459 / 5180 skipped
Shard 460 / 5180 skipped
Shard 461 / 5180 skipped
Shard 462 / 5180 skipped
Shard 463 / 5180 skipped
Shard 464 / 5180 skipped
Shard 465 / 5180 skipped
Shard 466 / 5180 skipped
Shard 467 / 5180 skipped
Shard 468 / 5180 skipped
Shard 469 / 5180 skipped
Shard 470 / 5180 skipped
Shard 471 / 5180 skipped
Shard 472 / 5180 skipped
Shard 473 / 5180 skipped
Shard 474 / 5180 skipped
Shard 475 / 5180 skipped
Shard 476 / 5180 skipped
Shard 477 / 5180 skipped
Shard 478 / 5180 skipped
Shard 479 / 5180 skipped
Shard 480 / 5180 skipped
Shard 481 / 5180 skipped
Shard 482 / 5180 skipped
Shard 483 / 5180 skipped
Shard 484 / 5180 skipped
Shard 485 / 5180 skipped
Shard 486 / 5180 skipped
Shard 487 / 5180 skipped
Shard 488 / 5180 skipped
Shard 489 / 5180 skipped
Shard 490 / 5180 skipped
Shard 491 / 5180 skipped
Shard 492 / 5180 skipped
Shard 493 / 5180 skipped
Shard 494 / 5180 skipped
Shard 495 / 5180 skipped
Shard 496 / 5180 skipped
Shard 497 / 5180 skipped
Shard 498 / 5180 skipped
Shard 499 / 5180 skipped
Shard 500 / 5180 skipped
Shard 501 / 5180 skipped
Shard 502 / 5180 skipped
Shard 503 / 5180 skipped
Shard 504 / 5180 skipped
Shard 505 / 5180 skipped
Shard 506 / 5180 skipped
Shard 507 / 5180 skipped
Shard 508 / 5180 skipped
Shard 509 / 5180 skipped
Shard 510 / 5180 skipped
Shard 511 / 5180 skipped
Shard 512 / 5180 skipped
Shard 513 / 5180 skipped
Shard 514 / 5180 skipped
Shard 515 / 5180 skipped
Shard 516 / 5180 skipped
Shard 517 / 5180 skipped
Shard 518 / 5180 skipped
Shard 519 / 5180 skipped
Shard 520 / 5180 skipped
Shard 521 / 5180 skipped
Shard 522 / 5180 skipped
Shard 523 / 5180 skipped
Shard 524 / 5180 skipped
Shard 525 / 5180 skipped
Shard 526 / 5180 skipped
Shard 527 / 5180 skipped
Shard 528 / 5180 skipped
Shard 529 / 5180 skipped
Shard 530 / 5180 skipped
Shard 531 / 5180 skipped
Shard 532 / 5180 skipped
Shard 533 / 5180 skipped
Shard 534 / 5180 skipped
Shard 535 / 5180 skipped
Shard 536 / 5180 skipped
Shard 537 / 5180 skipped
Shard 538 / 5180 skipped
Shard 539 / 5180 skipped
Shard 540 / 5180 skipped
Shard 541 / 5180 skipped
Shard 542 / 5180 skipped
Shard 543 / 5180 skipped
Shard 544 / 5180 skipped
Shard 545 / 5180 skipped
Shard 546 / 5180 skipped
Shard 547 / 5180 skipped
Shard 548 / 5180 skipped
Shard 549 / 5180 skipped
Shard 550 / 5180 skipped
Shard 551 / 5180 skipped
Shard 552 / 5180 skipped
Shard 553 / 5180 skipped
Shard 554 / 5180 skipped
Shard 555 / 5180 skipped
Shard 556 / 5180 skipped
Shard 557 / 5180 skipped
Shard 558 / 5180 skipped
Shard 559 / 5180 skipped
Shard 560 / 5180 skipped
Shard 561 / 5180 skipped
Shard 562 / 5180 skipped
Shard 563 / 5180 skipped
Shard 564 / 5180 skipped
Shard 565 / 5180 skipped
Shard 566 / 5180 skipped
Shard 567 / 5180 skipped
Shard 568 / 5180 skipped
Shard 569 / 5180 skipped
Shard 570 / 5180 skipped
Shard 571 / 5180 skipped
Shard 572 / 5180 skipped
Shard 573 / 5180 skipped
Shard 574 / 5180 skipped
Shard 575 / 5180 skipped
Shard 576 / 5180 skipped
Shard 577 / 5180 skipped
Shard 578 / 5180 skipped
Shard 579 / 5180 skipped
Shard 580 / 5180 skipped
Shard 581 / 5180 skipped
Shard 582 / 5180 skipped
Shard 583 / 5180 skipped
Shard 584 / 5180 skipped
Shard 585 / 5180 skipped
Shard 586 / 5180 skipped
Shard 587 / 5180 skipped
Shard 588 / 5180 skipped
Shard 589 / 5180 skipped
Shard 590 / 5180 skipped
Shard 591 / 5180 skipped
Shard 592 / 5180 skipped
Shard 593 / 5180 skipped
Shard 594 / 5180 skipped
Shard 595 / 5180 skipped
Shard 596 / 5180 skipped
Shard 597 / 5180 skipped
Shard 598 / 5180 skipped
Shard 599 / 5180 skipped
Shard 600 / 5180 skipped
Shard 601 / 5180 skipped
Shard 602 / 5180 skipped
Shard 603 / 5180 skipped
Shard 604 / 5180 skipped
Shard 605 / 5180 skipped
Shard 606 / 5180 skipped
Shard 607 / 5180 skipped
Shard 608 / 5180 skipped
Shard 609 / 5180 skipped
Shard 610 / 5180 skipped
Shard 611 / 5180 skipped
Shard 612 / 5180 skipped
Shard 613 / 5180 skipped
Shard 614 / 5180 skipped
Shard 615 / 5180 skipped
Shard 616 / 5180 skipped
Shard 617 / 5180 skipped
Shard 618 / 5180 skipped
Shard 619 / 5180 skipped
Shard 620 / 5180 skipped
Shard 621 / 5180 skipped
Shard 622 / 5180 skipped
Shard 623 / 5180 skipped
Shard 624 / 5180 skipped
Shard 625 / 5180 skipped
Shard 626 / 5180 skipped
Shard 627 / 5180 skipped
Shard 628 / 5180 skipped
Shard 629 / 5180 skipped
Shard 630 / 5180 skipped
Shard 631 / 5180 skipped
Shard 632 / 5180 skipped
Shard 633 / 5180 skipped
Shard 634 / 5180 skipped
Shard 635 / 5180 skipped
Shard 636 / 5180 skipped
Shard 637 / 5180 skipped
Shard 638 / 5180 skipped
Shard 639 / 5180 skipped
Shard 640 / 5180 skipped
Shard 641 / 5180 skipped
Shard 642 / 5180 skipped
Shard 643 / 5180 skipped
Shard 644 / 5180 skipped
Shard 645 / 5180 skipped
Shard 646 / 5180 skipped
Shard 647 / 5180 skipped
Shard 648 / 5180 skipped
Shard 649 / 5180 skipped
Shard 650 / 5180 skipped
Shard 651 / 5180 skipped
Shard 652 / 5180 skipped
Shard 653 / 5180 skipped
Shard 654 / 5180 skipped
Shard 655 / 5180 skipped
Shard 656 / 5180 skipped
Shard 657 / 5180 skipped
Shard 658 / 5180 skipped
Shard 659 / 5180 skipped
Shard 660 / 5180 skipped
Shard 661 / 5180 skipped
Shard 662 / 5180 skipped
Shard 663 / 5180 skipped
Shard 664 / 5180 skipped
Shard 665 / 5180 skipped
Shard 666 / 5180 skipped
Shard 667 / 5180 skipped
Shard 668 / 5180 skipped
Shard 669 / 5180 skipped
Shard 670 / 5180 skipped
Shard 671 / 5180 skipped
Shard 672 / 5180 skipped
Shard 673 / 5180 skipped
Shard 674 / 5180 skipped
Shard 675 / 5180 skipped
Shard 676 / 5180 skipped
Shard 677 / 5180 skipped
Shard 678 / 5180 skipped
Shard 679 / 5180 skipped
Shard 680 / 5180 skipped
Shard 681 / 5180 skipped
Shard 682 / 5180 skipped
Shard 683 / 5180 skipped
Shard 684 / 5180 skipped
Shard 685 / 5180 skipped
Shard 686 / 5180 skipped
Shard 687 / 5180 skipped
Shard 688 / 5180 skipped
Shard 689 / 5180 skipped
Shard 690 / 5180 skipped
Shard 691 / 5180 skipped
Shard 692 / 5180 skipped
Shard 693 / 5180 skipped
Shard 694 / 5180 skipped
Shard 695 / 5180 skipped
Shard 696 / 5180 skipped
Shard 697 / 5180 skipped
Shard 698 / 5180 skipped
Shard 699 / 5180 skipped
Shard 700 / 5180 skipped
Shard 701 / 5180 skipped
Shard 702 / 5180 skipped
Shard 703 / 5180 skipped
Shard 704 / 5180 skipped
Shard 705 / 5180 skipped
Shard 706 / 5180 skipped
Shard 707 / 5180 skipped
Shard 708 / 5180 skipped
Shard 709 / 5180 skipped
Shard 710 / 5180 skipped
Shard 711 / 5180 skipped
Shard 712 / 5180 skipped
Shard 713 / 5180 skipped
Shard 714 / 5180 skipped
Shard 715 / 5180 skipped
Shard 716 / 5180 skipped
Shard 717 / 5180 skipped
Shard 718 / 5180 skipped
Shard 719 / 5180 skipped
Shard 720 / 5180 skipped
Shard 721 / 5180 skipped
Shard 722 / 5180 skipped
Shard 723 / 5180 skipped
Shard 724 / 5180 skipped
Shard 725 / 5180 skipped
Shard 726 / 5180 skipped
Shard 727 / 5180 skipped
Shard 728 / 5180 skipped
Shard 729 / 5180 skipped
Shard 730 / 5180 skipped
Shard 731 / 5180 skipped
Shard 732 / 5180 skipped
Shard 733 / 5180 skipped
Shard 734 / 5180 skipped
Shard 735 / 5180 skipped
Shard 736 / 5180 skipped
Shard 737 / 5180 skipped
Shard 738 / 5180 skipped
Shard 739 / 5180 skipped
Shard 740 / 5180 skipped
Shard 741 / 5180 skipped
Shard 742 / 5180 skipped
Shard 743 / 5180 skipped
Shard 744 / 5180 skipped
Shard 745 / 5180 skipped
Shard 746 / 5180 skipped
Shard 747 / 5180 skipped
Shard 748 / 5180 skipped
Shard 749 / 5180 skipped
Shard 750 / 5180 skipped
Shard 751 / 5180 skipped
Shard 752 / 5180 skipped
Shard 753 / 5180 skipped
Shard 754 / 5180 skipped
Shard 755 / 5180 skipped
Shard 756 / 5180 skipped
Shard 757 / 5180 skipped
Shard 758 / 5180 skipped
Shard 759 / 5180 skipped
Shard 760 / 5180 skipped
Shard 761 / 5180 skipped
Shard 762 / 5180 skipped
Shard 763 / 5180 skipped
Shard 764 / 5180 skipped
Shard 765 / 5180 skipped
Shard 766 / 5180 skipped
Shard 767 / 5180 skipped
Shard 768 / 5180 skipped
Shard 769 / 5180 skipped
Shard 770 / 5180 skipped
Shard 771 / 5180 skipped
Shard 772 / 5180 skipped
Shard 773 / 5180 skipped
Shard 774 / 5180 skipped
Shard 775 / 5180 skipped
Shard 776 / 5180 skipped
Shard 777 / 5180 skipped
Shard 778 / 5180 skipped
Shard 779 / 5180 skipped
Shard 780 / 5180 skipped
Shard 781 / 5180 skipped
Shard 782 / 5180 skipped
Shard 783 / 5180 skipped
Shard 784 / 5180 skipped
Shard 785 / 5180 skipped
Shard 786 / 5180 skipped
Shard 787 / 5180 skipped
Shard 788 / 5180 skipped
Shard 789 / 5180 skipped
Shard 790 / 5180 skipped
Shard 791 / 5180 skipped
Shard 792 / 5180 skipped
Shard 793 / 5180 skipped
Shard 794 / 5180 skipped
Shard 795 / 5180 skipped
Shard 796 / 5180 skipped
Shard 797 / 5180 skipped
Shard 798 / 5180 skipped
Shard 799 / 5180 skipped
Shard 800 / 5180 skipped
Shard 801 / 5180 skipped
Shard 802 / 5180 skipped
Shard 803 / 5180 skipped
Shard 804 / 5180 skipped
Shard 805 / 5180 skipped
Shard 806 / 5180 skipped
Shard 807 / 5180 skipped
Shard 808 / 5180 skipped
Shard 809 / 5180 skipped
Shard 810 / 5180 skipped
Shard 811 / 5180 skipped
Shard 812 / 5180 skipped
Shard 813 / 5180 skipped
Shard 814 / 5180 skipped
Shard 815 / 5180 skipped
Shard 816 / 5180 skipped
Shard 817 / 5180 skipped
Shard 818 / 5180 skipped
Shard 819 / 5180 skipped
Shard 820 / 5180 skipped
Shard 821 / 5180 skipped
Shard 822 / 5180 skipped
Shard 823 / 5180 skipped
Shard 824 / 5180 skipped
Shard 825 / 5180 skipped
Shard 826 / 5180 skipped
Shard 827 / 5180 skipped
Shard 828 / 5180 skipped
Shard 829 / 5180 skipped
Shard 830 / 5180 skipped
Shard 831 / 5180 skipped
Shard 832 / 5180 skipped
Shard 833 / 5180 skipped
Shard 834 / 5180 skipped
Shard 835 / 5180 skipped
Shard 836 / 5180 skipped
Shard 837 / 5180 skipped
Shard 838 / 5180 skipped
Shard 839 / 5180 skipped
Shard 840 / 5180 skipped
Shard 841 / 5180 skipped
Shard 842 / 5180 skipped
Shard 843 / 5180 skipped
Shard 844 / 5180 skipped
Shard 845 / 5180 skipped
Shard 846 / 5180 skipped
Shard 847 / 5180 skipped
Shard 848 / 5180 skipped
Shard 849 / 5180 skipped
Shard 850 / 5180 skipped
Shard 851 / 5180 skipped
Shard 852 / 5180 skipped
Shard 853 / 5180 skipped
Shard 854 / 5180 skipped
Shard 855 / 5180 skipped
Shard 856 / 5180 skipped
Shard 857 / 5180 skipped
Shard 858 / 5180 skipped
Shard 859 / 5180 skipped
Shard 860 / 5180 skipped
Shard 861 / 5180 skipped
Shard 862 / 5180 skipped
Shard 863 / 5180 skipped
Shard 864 / 5180 skipped
Shard 865 / 5180 skipped
Shard 866 / 5180 skipped
Shard 867 / 5180 skipped
Shard 868 / 5180 skipped
Shard 869 / 5180 skipped
Shard 870 / 5180 skipped
Shard 871 / 5180 skipped
Shard 872 / 5180 skipped
Shard 873 / 5180 skipped
Shard 874 / 5180 skipped
Shard 875 / 5180 skipped
Shard 876 / 5180 skipped
Shard 877 / 5180 skipped
Shard 878 / 5180 skipped
Shard 879 / 5180 skipped
Shard 880 / 5180 skipped
Shard 881 / 5180 skipped
Shard 882 / 5180 skipped
Shard 883 / 5180 skipped
Shard 884 / 5180 skipped
Shard 885 / 5180 skipped
Shard 886 / 5180 skipped
Shard 887 / 5180 skipped
Shard 888 / 5180 skipped
Shard 889 / 5180 skipped
Shard 890 / 5180 skipped
Shard 891 / 5180 skipped
Shard 892 / 5180 skipped
Shard 893 / 5180 skipped
Shard 894 / 5180 skipped
Shard 895 / 5180 skipped
Shard 896 / 5180 skipped
Shard 897 / 5180 skipped
Shard 898 / 5180 skipped
Shard 899 / 5180 skipped
Shard 900 / 5180 skipped
Shard 901 / 5180 skipped
Shard 902 / 5180 skipped
Shard 903 / 5180 skipped
Shard 904 / 5180 skipped
Shard 905 / 5180 skipped
Shard 906 / 5180 skipped
Shard 907 / 5180 skipped
Shard 908 / 5180 skipped
Shard 909 / 5180 skipped
Shard 910 / 5180 skipped
Shard 911 / 5180 skipped
Shard 912 / 5180 skipped
Shard 913 / 5180 skipped
Shard 914 / 5180 skipped
Shard 915 / 5180 skipped
Shard 916 / 5180 skipped
Shard 917 / 5180 skipped
Shard 918 / 5180 skipped
Shard 919 / 5180 skipped
Shard 920 / 5180 skipped
Shard 921 / 5180 skipped
Shard 922 / 5180 skipped
Shard 923 / 5180 skipped
Shard 924 / 5180 skipped
Shard 925 / 5180 skipped
Shard 926 / 5180 skipped
Shard 927 / 5180 skipped
Shard 928 / 5180 skipped
Shard 929 / 5180 skipped
Shard 930 / 5180 skipped
Shard 931 / 5180 skipped
Shard 932 / 5180 skipped
Shard 933 / 5180 skipped
Shard 934 / 5180 skipped
Shard 935 / 5180 skipped
Shard 936 / 5180 skipped
Shard 937 / 5180 skipped
Shard 938 / 5180 skipped
Shard 939 / 5180 skipped
Shard 940 / 5180 skipped
Shard 941 / 5180 skipped
Shard 942 / 5180 skipped
Shard 943 / 5180 skipped
Shard 944 / 5180 skipped
Shard 945 / 5180 skipped
Shard 946 / 5180 skipped
Shard 947 / 5180 skipped
Shard 948 / 5180 skipped
Shard 949 / 5180 skipped
Shard 950 / 5180 skipped
Shard 951 / 5180 skipped
Shard 952 / 5180 skipped
Shard 953 / 5180 skipped
Shard 954 / 5180 skipped
Shard 955 / 5180 skipped
Shard 956 / 5180 skipped
Shard 957 / 5180 skipped
Shard 958 / 5180 skipped
Shard 959 / 5180 skipped
Shard 960 / 5180 skipped
Shard 961 / 5180 skipped
Shard 962 / 5180 skipped
Shard 963 / 5180 skipped
Shard 964 / 5180 skipped
Shard 965 / 5180 skipped
Shard 966 / 5180 skipped
Shard 967 / 5180 skipped
Shard 968 / 5180 skipped
Shard 969 / 5180 skipped
Shard 970 / 5180 skipped
Shard 971 / 5180 skipped
Shard 972 / 5180 skipped
Shard 973 / 5180 skipped
Shard 974 / 5180 skipped
Shard 975 / 5180 skipped
Shard 976 / 5180 skipped
Shard 977 / 5180 skipped
Shard 978 / 5180 skipped
Shard 979 / 5180 skipped
Shard 980 / 5180 skipped
Shard 981 / 5180 skipped
Shard 982 / 5180 skipped
Shard 983 / 5180 skipped
Shard 984 / 5180 skipped
Shard 985 / 5180 skipped
Shard 986 / 5180 skipped
Shard 987 / 5180 skipped
Shard 988 / 5180 skipped
Shard 989 / 5180 skipped
Shard 990 / 5180 skipped
Shard 991 / 5180 skipped
Shard 992 / 5180 skipped
Shard 993 / 5180 skipped
Shard 994 / 5180 skipped
Shard 995 / 5180 skipped
Shard 996 / 5180 skipped
Shard 997 / 5180 skipped
Shard 998 / 5180 skipped
Shard 999 / 5180 skipped
Shard 1000 / 5180 skipped
Shard 1001 / 5180 skipped
Shard 1002 / 5180 skipped
Shard 1003 / 5180 skipped
Shard 1004 / 5180 skipped
Shard 1005 / 5180 skipped
Shard 1006 / 5180 skipped
Shard 1007 / 5180 skipped
Shard 1008 / 5180 skipped
Shard 1009 / 5180 skipped
Shard 1010 / 5180 skipped
Shard 1011 / 5180 skipped
Shard 1012 / 5180 skipped
Shard 1013 / 5180 skipped
Shard 1014 / 5180 skipped
Shard 1015 / 5180 skipped
Shard 1016 / 5180 skipped
Shard 1017 / 5180 skipped
Shard 1018 / 5180 skipped
Shard 1019 / 5180 skipped
Shard 1020 / 5180 skipped
Shard 1021 / 5180 skipped
Shard 1022 / 5180 skipped
Shard 1023 / 5180 skipped
Shard 1024 / 5180 skipped
Shard 1025 / 5180 skipped
Shard 1026 / 5180 skipped
Shard 1027 / 5180 skipped
Shard 1028 / 5180 skipped
Shard 1029 / 5180 skipped
Shard 1030 / 5180 skipped
Shard 1031 / 5180 skipped
Shard 1032 / 5180 skipped
Shard 1033 / 5180 skipped
Shard 1034 / 5180 skipped
Shard 1035 / 5180 skipped
Shard 1036 / 5180 skipped
Shard 1037 / 5180 skipped
Shard 1038 / 5180 skipped
Shard 1039 / 5180 skipped
Shard 1040 / 5180 skipped
Shard 1041 / 5180 skipped
Shard 1042 / 5180 skipped
Shard 1043 / 5180 skipped
Shard 1044 / 5180 skipped
Shard 1045 / 5180 skipped
Shard 1046 / 5180 skipped
Shard 1047 / 5180 skipped
Shard 1048 / 5180 skipped
Shard 1049 / 5180 skipped
Shard 1050 / 5180 skipped
Shard 1051 / 5180 skipped
Shard 1052 / 5180 skipped
Shard 1053 / 5180 skipped
Shard 1054 / 5180 skipped
Shard 1055 / 5180 skipped
Shard 1056 / 5180 skipped
Shard 1057 / 5180 skipped
Shard 1058 / 5180 skipped
Shard 1059 / 5180 skipped
Shard 1060 / 5180 skipped
Shard 1061 / 5180 skipped
Shard 1062 / 5180 skipped
Shard 1063 / 5180 skipped
Shard 1064 / 5180 skipped
Shard 1065 / 5180 skipped
Shard 1066 / 5180 skipped
Shard 1067 / 5180 skipped
Shard 1068 / 5180 skipped
Shard 1069 / 5180 skipped
Shard 1070 / 5180 skipped
Shard 1071 / 5180 skipped
Shard 1072 / 5180 skipped
Shard 1073 / 5180 skipped
Shard 1074 / 5180 skipped
Shard 1075 / 5180 skipped
Shard 1076 / 5180 skipped
Shard 1077 / 5180 skipped
Shard 1078 / 5180 skipped
Shard 1079 / 5180 skipped
Shard 1080 / 5180 skipped
Shard 1081 / 5180 skipped
Shard 1082 / 5180 skipped
Shard 1083 / 5180 skipped
Shard 1084 / 5180 skipped
Shard 1085 / 5180 skipped
Shard 1086 / 5180 skipped
Shard 1087 / 5180 skipped
Shard 1088 / 5180 skipped
Shard 1089 / 5180 skipped
Shard 1090 / 5180 skipped
Shard 1091 / 5180 skipped
Shard 1092 / 5180 skipped
Shard 1093 / 5180 skipped
Shard 1094 / 5180 skipped
Shard 1095 / 5180 skipped
Shard 1096 / 5180 skipped
Shard 1097 / 5180 skipped
Shard 1098 / 5180 skipped
Shard 1099 / 5180 skipped
Shard 1100 / 5180 skipped
Shard 1101 / 5180 skipped
Shard 1102 / 5180 skipped
Shard 1103 / 5180 skipped
Shard 1104 / 5180 skipped
Shard 1105 / 5180 skipped
Shard 1106 / 5180 skipped
Shard 1107 / 5180 skipped
Shard 1108 / 5180 skipped
Shard 1109 / 5180 skipped
Shard 1110 / 5180 skipped
Shard 1111 / 5180 skipped
Shard 1112 / 5180 skipped
Shard 1113 / 5180 skipped
Shard 1114 / 5180 skipped
Shard 1115 / 5180 skipped
Shard 1116 / 5180 skipped
Shard 1117 / 5180 skipped
Shard 1118 / 5180 skipped
Shard 1119 / 5180 skipped
Shard 1120 / 5180 skipped
Shard 1121 / 5180 skipped
Shard 1122 / 5180 skipped
Shard 1123 / 5180 skipped
Shard 1124 / 5180 skipped
Shard 1125 / 5180 skipped
Shard 1126 / 5180 skipped
Shard 1127 / 5180 skipped
Shard 1128 / 5180 skipped
Shard 1129 / 5180 skipped
Shard 1130 / 5180 skipped
Shard 1131 / 5180 skipped
Shard 1132 / 5180 skipped
Shard 1133 / 5180 skipped
Shard 1134 / 5180 skipped
Shard 1135 / 5180 skipped
Shard 1136 / 5180 skipped
Shard 1137 / 5180 skipped
Shard 1138 / 5180 skipped
Shard 1139 / 5180 skipped
Shard 1140 / 5180 skipped
Shard 1141 / 5180 skipped
Shard 1142 / 5180 skipped
Shard 1143 / 5180 skipped
Shard 1144 / 5180 skipped
Shard 1145 / 5180 skipped
Shard 1146 / 5180 skipped
Shard 1147 / 5180 skipped
Shard 1148 / 5180 skipped
Shard 1149 / 5180 skipped
Shard 1150 / 5180 skipped
Shard 1151 / 5180 skipped
Shard 1152 / 5180 skipped
Shard 1153 / 5180 skipped
Shard 1154 / 5180 skipped
Shard 1155 / 5180 skipped
Shard 1156 / 5180 skipped
Shard 1157 / 5180 skipped
Shard 1158 / 5180 skipped
Shard 1159 / 5180 skipped
Shard 1160 / 5180 skipped
Shard 1161 / 5180 skipped
Shard 1162 / 5180 skipped
Shard 1163 / 5180 skipped
Shard 1164 / 5180 skipped
Shard 1165 / 5180 skipped
Shard 1166 / 5180 skipped
Shard 1167 / 5180 skipped
Shard 1168 / 5180 skipped
Shard 1169 / 5180 skipped
Shard 1170 / 5180 skipped
Shard 1171 / 5180 skipped
Shard 1172 / 5180 skipped
Shard 1173 / 5180 skipped
Shard 1174 / 5180 skipped
Shard 1175 / 5180 skipped
Shard 1176 / 5180 skipped
Shard 1177 / 5180 skipped
Shard 1178 / 5180 skipped
Shard 1179 / 5180 skipped
Shard 1180 / 5180 skipped
Shard 1181 / 5180 skipped
Shard 1182 / 5180 skipped
Shard 1183 / 5180 skipped
Shard 1184 / 5180 skipped
Shard 1185 / 5180 skipped
Shard 1186 / 5180 skipped
Shard 1187 / 5180 skipped
Shard 1188 / 5180 skipped
Shard 1189 / 5180 skipped
Shard 1190 / 5180 skipped
Shard 1191 / 5180 skipped
Shard 1192 / 5180 skipped
Shard 1193 / 5180 skipped
Shard 1194 / 5180 skipped
Shard 1195 / 5180 skipped
Shard 1196 / 5180 skipped
Shard 1197 / 5180 skipped
Shard 1198 / 5180 skipped
Shard 1199 / 5180 skipped
Shard 1200 / 5180 skipped
Shard 1201 / 5180 skipped
Shard 1202 / 5180 skipped
Shard 1203 / 5180 skipped
Shard 1204 / 5180 skipped
Shard 1205 / 5180 skipped
Shard 1206 / 5180 skipped
Shard 1207 / 5180 skipped
Shard 1208 / 5180 skipped
Shard 1209 / 5180 skipped
Shard 1210 / 5180 skipped
Shard 1211 / 5180 skipped
Shard 1212 / 5180 skipped
Shard 1213 / 5180 skipped
Shard 1214 / 5180 skipped
Shard 1215 / 5180 skipped
Shard 1216 / 5180 skipped
Shard 1217 / 5180 skipped
Shard 1218 / 5180 skipped
Shard 1219 / 5180 skipped
Shard 1220 / 5180 skipped
Shard 1221 / 5180 skipped
Shard 1222 / 5180 skipped
Shard 1223 / 5180 skipped
Shard 1224 / 5180 skipped
Shard 1225 / 5180 skipped
Shard 1226 / 5180 skipped
Shard 1227 / 5180 skipped
Shard 1228 / 5180 skipped
Shard 1229 / 5180 skipped
Shard 1230 / 5180 skipped
Shard 1231 / 5180 skipped
Shard 1232 / 5180 skipped
Shard 1233 / 5180 skipped
Shard 1234 / 5180 skipped
Shard 1235 / 5180 skipped
Shard 1236 / 5180 skipped
Shard 1237 / 5180 skipped
Shard 1238 / 5180 skipped
Shard 1239 / 5180 skipped
Shard 1240 / 5180 skipped
Shard 1241 / 5180 skipped
Shard 1242 / 5180 skipped
Shard 1243 / 5180 skipped
Shard 1244 / 5180 skipped
Shard 1245 / 5180 skipped
Shard 1246 / 5180 skipped
Shard 1247 / 5180 skipped
Shard 1248 / 5180 skipped
Shard 1249 / 5180 skipped
Shard 1250 / 5180 skipped
Shard 1251 / 5180 skipped
Shard 1252 / 5180 skipped
Shard 1253 / 5180 skipped
Shard 1254 / 5180 skipped
Shard 1255 / 5180 skipped
Shard 1256 / 5180 skipped
Shard 1257 / 5180 skipped
Shard 1258 / 5180 skipped
Shard 1259 / 5180 skipped
Shard 1260 / 5180 skipped
Shard 1261 / 5180 skipped
Shard 1262 / 5180 skipped
Shard 1263 / 5180 skipped
Shard 1264 / 5180 skipped
Shard 1265 / 5180 skipped
Shard 1266 / 5180 skipped
Shard 1267 / 5180 skipped
Shard 1268 / 5180 skipped
Shard 1269 / 5180 skipped
Shard 1270 / 5180 skipped
Shard 1271 / 5180 skipped
Shard 1272 / 5180 skipped
Shard 1273 / 5180 skipped
Shard 1274 / 5180 skipped
Shard 1275 / 5180 skipped
Shard 1276 / 5180 skipped
Shard 1277 / 5180 skipped
Shard 1278 / 5180 skipped
Shard 1279 / 5180 skipped
Shard 1280 / 5180 skipped
Shard 1281 / 5180 skipped
Shard 1282 / 5180 skipped
Shard 1283 / 5180 skipped
Shard 1284 / 5180 skipped
Shard 1285 / 5180 skipped
Shard 1286 / 5180 skipped
Shard 1287 / 5180 skipped
Shard 1288 / 5180 skipped
Shard 1289 / 5180 skipped
Shard 1290 / 5180 skipped
Shard 1291 / 5180 skipped
Shard 1292 / 5180 skipped
Shard 1293 / 5180 skipped
Shard 1294 / 5180 skipped
Shard 1295 / 5180 skipped
Shard 1296 / 5180 skipped
Shard 1297 / 5180 skipped
Shard 1298 / 5180 skipped
Shard 1299 / 5180 skipped
Shard 1300 / 5180 skipped
Shard 1301 / 5180 skipped
Shard 1302 / 5180 skipped
Shard 1303 / 5180 skipped
Shard 1304 / 5180 skipped
Shard 1305 / 5180 skipped
Shard 1306 / 5180 skipped
Shard 1307 / 5180 skipped
Shard 1308 / 5180 skipped
Shard 1309 / 5180 skipped
Shard 1310 / 5180 skipped
Shard 1311 / 5180 skipped
Shard 1312 / 5180 skipped
Shard 1313 / 5180 skipped
Shard 1314 / 5180 skipped
Shard 1315 / 5180 skipped
Shard 1316 / 5180 skipped
Shard 1317 / 5180 skipped
Shard 1318 / 5180 skipped
Shard 1319 / 5180 skipped
Shard 1320 / 5180 skipped
Shard 1321 / 5180 skipped
Shard 1322 / 5180 skipped
Shard 1323 / 5180 skipped
Shard 1324 / 5180 skipped
Shard 1325 / 5180 skipped
Shard 1326 / 5180 skipped
Shard 1327 / 5180 skipped
Shard 1328 / 5180 skipped
Shard 1329 / 5180 skipped
Shard 1330 / 5180 skipped
Shard 1331 / 5180 skipped
Shard 1332 / 5180 skipped
Shard 1333 / 5180 skipped
Shard 1334 / 5180 skipped
Shard 1335 / 5180 skipped
Shard 1336 / 5180 skipped
Shard 1337 / 5180 skipped
Shard 1338 / 5180 skipped
Shard 1339 / 5180 skipped
Shard 1340 / 5180 skipped
Shard 1341 / 5180 skipped
Shard 1342 / 5180 skipped
Shard 1343 / 5180 skipped
Shard 1344 / 5180 skipped
Shard 1345 / 5180 skipped
Shard 1346 / 5180 skipped
Shard 1347 / 5180 skipped
Shard 1348 / 5180 skipped
Shard 1349 / 5180 skipped
Shard 1350 / 5180 skipped
Shard 1351 / 5180 skipped
Shard 1352 / 5180 skipped
Shard 1353 / 5180 skipped
Shard 1354 / 5180 skipped
Shard 1355 / 5180 skipped
Shard 1356 / 5180 skipped
Shard 1357 / 5180 skipped
Shard 1358 / 5180 skipped
Shard 1359 / 5180 skipped
Shard 1360 / 5180 skipped
Shard 1361 / 5180 skipped
Shard 1362 / 5180 skipped
Shard 1363 / 5180 skipped
Shard 1364 / 5180 skipped
Shard 1365 / 5180 skipped
Shard 1366 / 5180 skipped
Shard 1367 / 5180 skipped
Shard 1368 / 5180 skipped
Shard 1369 / 5180 skipped
Shard 1370 / 5180 skipped
Shard 1371 / 5180 skipped
Shard 1372 / 5180 skipped
Shard 1373 / 5180 skipped
Shard 1374 / 5180 skipped
Shard 1375 / 5180 skipped
Shard 1376 / 5180 skipped
Shard 1377 / 5180 skipped
Shard 1378 / 5180 skipped
Shard 1379 / 5180 skipped
Shard 1380 / 5180 skipped
Shard 1381 / 5180 skipped
Shard 1382 / 5180 skipped
Shard 1383 / 5180 skipped
Shard 1384 / 5180 skipped
Shard 1385 / 5180 skipped
Shard 1386 / 5180 skipped
Shard 1387 / 5180 skipped
Shard 1388 / 5180 skipped
Shard 1389 / 5180 skipped
Shard 1390 / 5180 skipped
Shard 1391 / 5180 skipped
Shard 1392 / 5180 skipped
Shard 1393 / 5180 skipped
Shard 1394 / 5180 skipped
Shard 1395 / 5180 skipped
Shard 1396 / 5180 skipped
Shard 1397 / 5180 skipped
Shard 1398 / 5180 skipped
Shard 1399 / 5180 skipped
Shard 1400 / 5180 skipped
Shard 1401 / 5180 skipped
Shard 1402 / 5180 skipped
Shard 1403 / 5180 skipped
Shard 1404 / 5180 skipped
Shard 1405 / 5180 skipped
Shard 1406 / 5180 skipped
Shard 1407 / 5180 skipped
Shard 1408 / 5180 skipped
Shard 1409 / 5180 skipped
Shard 1410 / 5180 skipped
Shard 1411 / 5180 skipped
Shard 1412 / 5180 skipped
Shard 1413 / 5180 skipped
Shard 1414 / 5180 skipped
Shard 1415 / 5180 skipped
Shard 1416 / 5180 skipped
Shard 1417 / 5180 skipped
Shard 1418 / 5180 skipped
Shard 1419 / 5180 skipped
Shard 1420 / 5180 skipped
Shard 1421 / 5180 skipped
Shard 1422 / 5180 skipped
Shard 1423 / 5180 skipped
Shard 1424 / 5180 skipped
Shard 1425 / 5180 skipped
Shard 1426 / 5180 skipped
Shard 1427 / 5180 skipped
Shard 1428 / 5180 skipped
Shard 1429 / 5180 skipped
Shard 1430 / 5180 skipped
Shard 1431 / 5180 skipped
Shard 1432 / 5180 skipped
Shard 1433 / 5180 skipped
Shard 1434 / 5180 skipped
Shard 1435 / 5180 skipped
Shard 1436 / 5180 skipped
Shard 1437 / 5180 skipped
Shard 1438 / 5180 skipped
Shard 1439 / 5180 skipped
Shard 1440 / 5180 skipped
Shard 1441 / 5180 skipped
Shard 1442 / 5180 skipped
Shard 1443 / 5180 skipped
Shard 1444 / 5180 skipped
Shard 1445 / 5180 skipped
Shard 1446 / 5180 skipped
Shard 1447 / 5180 skipped
Shard 1448 / 5180 skipped
Shard 1449 / 5180 skipped
Shard 1450 / 5180 skipped
Shard 1451 / 5180 skipped
Shard 1452 / 5180 skipped
Shard 1453 / 5180 skipped
Shard 1454 / 5180 skipped
Shard 1455 / 5180 skipped
Shard 1456 / 5180 skipped
Shard 1457 / 5180 skipped
Shard 1458 / 5180 skipped
Shard 1459 / 5180 skipped
Shard 1460 / 5180 skipped
Shard 1461 / 5180 skipped
Shard 1462 / 5180 skipped
Shard 1463 / 5180 skipped
Shard 1464 / 5180 skipped
Shard 1465 / 5180 skipped
Shard 1466 / 5180 skipped
Shard 1467 / 5180 skipped
Shard 1468 / 5180 skipped
Shard 1469 / 5180 skipped
Shard 1470 / 5180 skipped
Shard 1471 / 5180 skipped
Shard 1472 / 5180 skipped
Shard 1473 / 5180 skipped
Shard 1474 / 5180 skipped
Shard 1475 / 5180 skipped
Shard 1476 / 5180 skipped
Shard 1477 / 5180 skipped
Shard 1478 / 5180 skipped
Shard 1479 / 5180 skipped
Shard 1480 / 5180 skipped
Shard 1481 / 5180 skipped
Shard 1482 / 5180 skipped
Shard 1483 / 5180 skipped
Shard 1484 / 5180 skipped
Shard 1485 / 5180 skipped
Shard 1486 / 5180 skipped
Shard 1487 / 5180 skipped
Shard 1488 / 5180 skipped
Shard 1489 / 5180 skipped
Shard 1490 / 5180 skipped
Shard 1491 / 5180 skipped
Shard 1492 / 5180 skipped
Shard 1493 / 5180 skipped
Shard 1494 / 5180 skipped
Shard 1495 / 5180 skipped
Shard 1496 / 5180 skipped
Shard 1497 / 5180 skipped
Shard 1498 / 5180 skipped
Shard 1499 / 5180 skipped
Shard 1500 / 5180 skipped
Shard 1501 / 5180 skipped
Shard 1502 / 5180 skipped
Shard 1503 / 5180 skipped
Shard 1504 / 5180 skipped
Shard 1505 / 5180 skipped
Shard 1506 / 5180 skipped
Shard 1507 / 5180 skipped
Shard 1508 / 5180 skipped
Shard 1509 / 5180 skipped
Shard 1510 / 5180 skipped
Shard 1511 / 5180 skipped
Shard 1512 / 5180 skipped
Shard 1513 / 5180 skipped
Shard 1514 / 5180 skipped
Shard 1515 / 5180 skipped
Shard 1516 / 5180 skipped
Shard 1517 / 5180 skipped
Shard 1518 / 5180 skipped
Shard 1519 / 5180 skipped
Shard 1520 / 5180 skipped
Shard 1521 / 5180 skipped
Shard 1522 / 5180 skipped
Shard 1523 / 5180 skipped
Shard 1524 / 5180 skipped
Shard 1525 / 5180 skipped
Shard 1526 / 5180 skipped
Shard 1527 / 5180 skipped
Shard 1528 / 5180 skipped
Shard 1529 / 5180 skipped
Shard 1530 / 5180 skipped
Shard 1531 / 5180 skipped
Shard 1532 / 5180 skipped
Shard 1533 / 5180 skipped
Shard 1534 / 5180 skipped
Shard 1535 / 5180 skipped
Shard 1536 / 5180 skipped
Shard 1537 / 5180 skipped
Shard 1538 / 5180 skipped
Shard 1539 / 5180 skipped
Shard 1540 / 5180 skipped
Shard 1541 / 5180 skipped
Shard 1542 / 5180 skipped
Shard 1543 / 5180 skipped
Shard 1544 / 5180 skipped
Shard 1545 / 5180 skipped
Shard 1546 / 5180 skipped
Shard 1547 / 5180 skipped
Shard 1548 / 5180 skipped
Shard 1549 / 5180 skipped
Shard 1550 / 5180 skipped
Shard 1551 / 5180 skipped
Shard 1552 / 5180 skipped
Shard 1553 / 5180 skipped
Shard 1554 / 5180 skipped
Shard 1555 / 5180 skipped
Shard 1556 / 5180 skipped
Shard 1557 / 5180 skipped
Shard 1558 / 5180 skipped
Shard 1559 / 5180 skipped
Shard 1560 / 5180 skipped
Shard 1561 / 5180 skipped
Shard 1562 / 5180 skipped
Shard 1563 / 5180 skipped
Shard 1564 / 5180 skipped
Shard 1565 / 5180 skipped
Shard 1566 / 5180 skipped
Shard 1567 / 5180 skipped
Shard 1568 / 5180 skipped
Shard 1569 / 5180 skipped
Shard 1570 / 5180 skipped
Shard 1571 / 5180 skipped
Shard 1572 / 5180 skipped
Shard 1573 / 5180 skipped
Shard 1574 / 5180 skipped
Shard 1575 / 5180 skipped
Shard 1576 / 5180 skipped
Shard 1577 / 5180 skipped
Shard 1578 / 5180 skipped
Shard 1579 / 5180 skipped
Shard 1580 / 5180 skipped
Shard 1581 / 5180 skipped
Shard 1582 / 5180 skipped
Shard 1583 / 5180 skipped
Shard 1584 / 5180 skipped
Shard 1585 / 5180 skipped
Shard 1586 / 5180 skipped
Shard 1587 / 5180 skipped
Shard 1588 / 5180 skipped
Shard 1589 / 5180 skipped
Shard 1590 / 5180 skipped
Shard 1591 / 5180 skipped
Shard 1592 / 5180 skipped
Shard 1593 / 5180 skipped
Shard 1594 / 5180 skipped
Shard 1595 / 5180 skipped
Shard 1596 / 5180 skipped
Shard 1597 / 5180 skipped
Shard 1598 / 5180 skipped
Shard 1599 / 5180 skipped
Shard 1600 / 5180 skipped
Shard 1601 / 5180 skipped
Shard 1602 / 5180 skipped
Shard 1603 / 5180 skipped
Shard 1604 / 5180 skipped
Shard 1605 / 5180 skipped
Shard 1606 / 5180 skipped
Shard 1607 / 5180 skipped
Shard 1608 / 5180 skipped
Shard 1609 / 5180 skipped
Shard 1610 / 5180 skipped
Shard 1611 / 5180 skipped
Shard 1612 / 5180 skipped
Shard 1613 / 5180 skipped
Shard 1614 / 5180 skipped
Shard 1615 / 5180 skipped
Shard 1616 / 5180 skipped
Shard 1617 / 5180 skipped
Shard 1618 / 5180 skipped
Shard 1619 / 5180 skipped
Shard 1620 / 5180 skipped
Shard 1621 / 5180 skipped
Shard 1622 / 5180 skipped
Shard 1623 / 5180 skipped
Shard 1624 / 5180 skipped
Shard 1625 / 5180 skipped
Shard 1626 / 5180 skipped
Shard 1627 / 5180 skipped
Shard 1628 / 5180 skipped
Shard 1629 / 5180 skipped
Shard 1630 / 5180 skipped
Shard 1631 / 5180 skipped
Shard 1632 / 5180 skipped
Shard 1633 / 5180 skipped
Shard 1634 / 5180 skipped
Shard 1635 / 5180 skipped
Shard 1636 / 5180 skipped
Shard 1637 / 5180 skipped
Shard 1638 / 5180 skipped
Shard 1639 / 5180 skipped
Shard 1640 / 5180 skipped
Shard 1641 / 5180 skipped
Shard 1642 / 5180 skipped
Shard 1643 / 5180 skipped
Shard 1644 / 5180 skipped
Shard 1645 / 5180 skipped
Shard 1646 / 5180 skipped
Shard 1647 / 5180 skipped
Shard 1648 / 5180 skipped
Shard 1649 / 5180 skipped
Shard 1650 / 5180 skipped
Shard 1651 / 5180 skipped
Shard 1652 / 5180 skipped
Shard 1653 / 5180 skipped
Shard 1654 / 5180 skipped
Shard 1655 / 5180 skipped
Shard 1656 / 5180 skipped
Shard 1657 / 5180 skipped
Shard 1658 / 5180 skipped
Shard 1659 / 5180 skipped
Shard 1660 / 5180 skipped
Shard 1661 / 5180 skipped
Shard 1662 / 5180 skipped
Shard 1663 / 5180 skipped
Shard 1664 / 5180 skipped
Shard 1665 / 5180 skipped
Shard 1666 / 5180 skipped
Shard 1667 / 5180 skipped
Shard 1668 / 5180 skipped
Shard 1669 / 5180 skipped
Shard 1670 / 5180 skipped
Shard 1671 / 5180 skipped
Shard 1672 / 5180 skipped
Shard 1673 / 5180 skipped
Shard 1674 / 5180 skipped
Shard 1675 / 5180 skipped
Shard 1676 / 5180 skipped
Shard 1677 / 5180 skipped
Shard 1678 / 5180 skipped
Shard 1679 / 5180 skipped
Shard 1680 / 5180 skipped
Shard 1681 / 5180 skipped
Shard 1682 / 5180 skipped
Shard 1683 / 5180 skipped
Shard 1684 / 5180 skipped
Shard 1685 / 5180 skipped
Shard 1686 / 5180 skipped
Shard 1687 / 5180 skipped
Shard 1688 / 5180 skipped
Shard 1689 / 5180 skipped
Shard 1690 / 5180 skipped
Shard 1691 / 5180 skipped
Shard 1692 / 5180 skipped
Shard 1693 / 5180 skipped
Shard 1694 / 5180 skipped
Shard 1695 / 5180 skipped
Shard 1696 / 5180 skipped
Shard 1697 / 5180 skipped
Shard 1698 / 5180 skipped
Shard 1699 / 5180 skipped
Shard 1700 / 5180 skipped
Shard 1701 / 5180 skipped
Shard 1702 / 5180 skipped
Shard 1703 / 5180 skipped
Shard 1704 / 5180 skipped
Shard 1705 / 5180 skipped
Shard 1706 / 5180 skipped
Shard 1707 / 5180 skipped
Shard 1708 / 5180 skipped
Shard 1709 / 5180 skipped
Shard 1710 / 5180 skipped
Shard 1711 / 5180 skipped
Shard 1712 / 5180 skipped
Shard 1713 / 5180 skipped
Shard 1714 / 5180 skipped
Shard 1715 / 5180 skipped
Shard 1716 / 5180 skipped
Shard 1717 / 5180 skipped
Shard 1718 / 5180 skipped
Shard 1719 / 5180 skipped
Shard 1720 / 5180 skipped
Shard 1721 / 5180 skipped
Shard 1722 / 5180 skipped
Shard 1723 / 5180 skipped
Shard 1724 / 5180 skipped
Shard 1725 / 5180 skipped
Shard 1726 / 5180 skipped
Shard 1727 / 5180 skipped
Shard 1728 / 5180 skipped
Shard 1729 / 5180 skipped
Shard 1730 / 5180 skipped
Shard 1731 / 5180 skipped
Shard 1732 / 5180 skipped
Shard 1733 / 5180 skipped
Shard 1734 / 5180 skipped
Shard 1735 / 5180 skipped
Shard 1736 / 5180 skipped
Shard 1737 / 5180 skipped
Shard 1738 / 5180 skipped
Shard 1739 / 5180 skipped
Shard 1740 / 5180 skipped
Shard 1741 / 5180 skipped
Shard 1742 / 5180 skipped
Shard 1743 / 5180 skipped
Shard 1744 / 5180 skipped
Shard 1745 / 5180 skipped
Shard 1746 / 5180 skipped
Shard 1747 / 5180 skipped
Shard 1748 / 5180 skipped
Shard 1749 / 5180 skipped
Shard 1750 / 5180 skipped
Shard 1751 / 5180 skipped
Shard 1752 / 5180 skipped
Shard 1753 / 5180 skipped
Shard 1754 / 5180 skipped
Shard 1755 / 5180 skipped
Shard 1756 / 5180 skipped
Shard 1757 / 5180 skipped
Shard 1758 / 5180 skipped
Shard 1759 / 5180 skipped
Shard 1760 / 5180 skipped
Shard 1761 / 5180 skipped
Shard 1762 / 5180 skipped
Shard 1763 / 5180 skipped
Shard 1764 / 5180 skipped
Shard 1765 / 5180 skipped
Shard 1766 / 5180 skipped
Shard 1767 / 5180 skipped
Shard 1768 / 5180 skipped
Shard 1769 / 5180 skipped
Shard 1770 / 5180 skipped
Shard 1771 / 5180 skipped
Shard 1772 / 5180 skipped
Shard 1773 / 5180 skipped
Shard 1774 / 5180 skipped
Shard 1775 / 5180 skipped
Shard 1776 / 5180 skipped
Shard 1777 / 5180 skipped
Shard 1778 / 5180 skipped
Shard 1779 / 5180 skipped
Shard 1780 / 5180 skipped
Shard 1781 / 5180 skipped
Shard 1782 / 5180 skipped
Shard 1783 / 5180 skipped
Shard 1784 / 5180 skipped
Shard 1785 / 5180 skipped
Shard 1786 / 5180 skipped
Shard 1787 / 5180 skipped
Shard 1788 / 5180 skipped
Shard 1789 / 5180 skipped
Shard 1790 / 5180 skipped
Shard 1791 / 5180 skipped
Shard 1792 / 5180 skipped
Shard 1793 / 5180 skipped
Shard 1794 / 5180 skipped
Shard 1795 / 5180 skipped
Shard 1796 / 5180 skipped
Shard 1797 / 5180 skipped
Shard 1798 / 5180 skipped
Shard 1799 / 5180 skipped
Shard 1800 / 5180 skipped
Shard 1801 / 5180 skipped
Shard 1802 / 5180 skipped
Shard 1803 / 5180 skipped
Shard 1804 / 5180 skipped
Shard 1805 / 5180 skipped
Shard 1806 / 5180 skipped
Shard 1807 / 5180 skipped
Shard 1808 / 5180 skipped
Shard 1809 / 5180 skipped
Shard 1810 / 5180 skipped
Shard 1811 / 5180 skipped
Shard 1812 / 5180 skipped
Shard 1813 / 5180 skipped
Shard 1814 / 5180 skipped
Shard 1815 / 5180 skipped
Shard 1816 / 5180 skipped
Shard 1817 / 5180 skipped
Shard 1818 / 5180 skipped
Shard 1819 / 5180 skipped
Shard 1820 / 5180 skipped
Shard 1821 / 5180 skipped
Shard 1822 / 5180 skipped
Shard 1823 / 5180 skipped
Shard 1824 / 5180 skipped
Shard 1825 / 5180 skipped
Shard 1826 / 5180 skipped
Shard 1827 / 5180 skipped
Shard 1828 / 5180 skipped
Shard 1829 / 5180 skipped
Shard 1830 / 5180 skipped
Shard 1831 / 5180 skipped
Shard 1832 / 5180 skipped
Shard 1833 / 5180 skipped
Shard 1834 / 5180 skipped
Shard 1835 / 5180 skipped
Shard 1836 / 5180 skipped
Shard 1837 / 5180 skipped
Shard 1838 / 5180 skipped
Shard 1839 / 5180 skipped
Shard 1840 / 5180 skipped
Shard 1841 / 5180 skipped
Shard 1842 / 5180 skipped
Shard 1843 / 5180 skipped
Shard 1844 / 5180 skipped
Shard 1845 / 5180 skipped
Shard 1846 / 5180 skipped
Shard 1847 / 5180 skipped
Shard 1848 / 5180 skipped
Shard 1849 / 5180 skipped
Shard 1850 / 5180 skipped
Shard 1851 / 5180 skipped
Shard 1852 / 5180 skipped
Shard 1853 / 5180 skipped
Shard 1854 / 5180 skipped
Shard 1855 / 5180 skipped
Shard 1856 / 5180 skipped
Shard 1857 / 5180 skipped
Shard 1858 / 5180 skipped
Shard 1859 / 5180 skipped
Shard 1860 / 5180 skipped
Shard 1861 / 5180 skipped
Shard 1862 / 5180 skipped
Shard 1863 / 5180 skipped
Shard 1864 / 5180 skipped
Shard 1865 / 5180 skipped
Shard 1866 / 5180 skipped
Shard 1867 / 5180 skipped
Shard 1868 / 5180 skipped
Shard 1869 / 5180 skipped
Shard 1870 / 5180 skipped
Shard 1871 / 5180 skipped
Shard 1872 / 5180 skipped
Shard 1873 / 5180 skipped
Shard 1874 / 5180 skipped
Shard 1875 / 5180 skipped
Shard 1876 / 5180 skipped
Shard 1877 / 5180 skipped
Shard 1878 / 5180 skipped
Shard 1879 / 5180 skipped
Shard 1880 / 5180 skipped
Shard 1881 / 5180 skipped
Shard 1882 / 5180 skipped
Shard 1883 / 5180 skipped
Shard 1884 / 5180 skipped
Shard 1885 / 5180 skipped
Shard 1886 / 5180 skipped
Shard 1887 / 5180 skipped
Shard 1888 / 5180 skipped
Shard 1889 / 5180 skipped
Shard 1890 / 5180 skipped
Shard 1891 / 5180 skipped
Shard 1892 / 5180 skipped
Shard 1893 / 5180 skipped
Shard 1894 / 5180 skipped
Shard 1895 / 5180 skipped
Shard 1896 / 5180 skipped
Shard 1897 / 5180 skipped
Shard 1898 / 5180 skipped
Shard 1899 / 5180 skipped
Shard 1900 / 5180 skipped
Shard 1901 / 5180 skipped
Shard 1902 / 5180 skipped
Shard 1903 / 5180 skipped
Shard 1904 / 5180 skipped
Shard 1905 / 5180 skipped
Shard 1906 / 5180 skipped
Shard 1907 / 5180 skipped
Shard 1908 / 5180 skipped
Shard 1909 / 5180 skipped
Shard 1910 / 5180 skipped
Shard 1911 / 5180 skipped
Shard 1912 / 5180 skipped
Shard 1913 / 5180 skipped
Shard 1914 / 5180 skipped
Shard 1915 / 5180 skipped
Shard 1916 / 5180 skipped
Shard 1917 / 5180 skipped
Shard 1918 / 5180 skipped
Shard 1919 / 5180 skipped
Shard 1920 / 5180 skipped
Shard 1921 / 5180 skipped
Shard 1922 / 5180 skipped
Shard 1923 / 5180 skipped
Shard 1924 / 5180 skipped
Shard 1925 / 5180 skipped
Shard 1926 / 5180 skipped
Shard 1927 / 5180 skipped
Shard 1928 / 5180 skipped
Shard 1929 / 5180 skipped
Shard 1930 / 5180 skipped
Shard 1931 / 5180 skipped
Shard 1932 / 5180 skipped
Shard 1933 / 5180 skipped
Shard 1934 / 5180 skipped
Shard 1935 / 5180 skipped
Shard 1936 / 5180 skipped
Shard 1937 / 5180 skipped
Shard 1938 / 5180 skipped
Shard 1939 / 5180 skipped
Shard 1940 / 5180 skipped
Shard 1941 / 5180 skipped
Shard 1942 / 5180 skipped
Shard 1943 / 5180 skipped
Shard 1944 / 5180 skipped
Shard 1945 / 5180 skipped
Shard 1946 / 5180 skipped
Shard 1947 / 5180 skipped
Shard 1948 / 5180 skipped
Shard 1949 / 5180 skipped
Shard 1950 / 5180 skipped
Shard 1951 / 5180 skipped
Shard 1952 / 5180 skipped
Shard 1953 / 5180 skipped
Shard 1954 / 5180 skipped
Shard 1955 / 5180 skipped
Shard 1956 / 5180 skipped
Shard 1957 / 5180 skipped
Shard 1958 / 5180 skipped
Shard 1959 / 5180 skipped
Shard 1960 / 5180 skipped
Shard 1961 / 5180 skipped
Shard 1962 / 5180 skipped
Shard 1963 / 5180 skipped
Shard 1964 / 5180 skipped
Shard 1965 / 5180 skipped
Shard 1966 / 5180 skipped
Shard 1967 / 5180 skipped
Shard 1968 / 5180 skipped
Shard 1969 / 5180 skipped
Shard 1970 / 5180 skipped
Shard 1971 / 5180 skipped
Shard 1972 / 5180 skipped
Shard 1973 / 5180 skipped
Shard 1974 / 5180 skipped
Shard 1975 / 5180 skipped
Shard 1976 / 5180 skipped
Shard 1977 / 5180 skipped
Shard 1978 / 5180 skipped
Shard 1979 / 5180 skipped
Shard 1980 / 5180 skipped
Shard 1981 / 5180 skipped
Shard 1982 / 5180 skipped
Shard 1983 / 5180 skipped
Shard 1984 / 5180 skipped
Shard 1985 / 5180 skipped
Shard 1986 / 5180 skipped
Shard 1987 / 5180 skipped
Shard 1988 / 5180 skipped
Shard 1989 / 5180 skipped
Shard 1990 / 5180 skipped
Shard 1991 / 5180 skipped
Shard 1992 / 5180 skipped
Shard 1993 / 5180 skipped
Shard 1994 / 5180 skipped
Shard 1995 / 5180 skipped
Shard 1996 / 5180 skipped
Shard 1997 / 5180 skipped
Shard 1998 / 5180 skipped
Shard 1999 / 5180 skipped
Shard 2000 / 5180 skipped
Shard 2001 / 5180 skipped
Shard 2002 / 5180 skipped
Shard 2003 / 5180 skipped
Shard 2004 / 5180 skipped
Shard 2005 / 5180 skipped
Shard 2006 / 5180 skipped
Shard 2007 / 5180 skipped
Shard 2008 / 5180 skipped
Shard 2009 / 5180 skipped
Shard 2010 / 5180 skipped
Shard 2011 / 5180 skipped
Shard 2012 / 5180 skipped
Shard 2013 / 5180 skipped
Shard 2014 / 5180 skipped
Shard 2015 / 5180 skipped
Shard 2016 / 5180 skipped
Shard 2017 / 5180 skipped
Shard 2018 / 5180 skipped
Shard 2019 / 5180 skipped
Shard 2020 / 5180 skipped
Shard 2021 / 5180 skipped
Shard 2022 / 5180 skipped
Shard 2023 / 5180 skipped
Shard 2024 / 5180 skipped
Shard 2025 / 5180 skipped
Shard 2026 / 5180 skipped
Shard 2027 / 5180 skipped
Shard 2028 / 5180 skipped
Shard 2029 / 5180 skipped
Shard 2030 / 5180 skipped
Shard 2031 / 5180 skipped
Shard 2032 / 5180 skipped
Shard 2033 / 5180 skipped
Shard 2034 / 5180 skipped
Shard 2035 / 5180 skipped
Shard 2036 / 5180 skipped
Shard 2037 / 5180 skipped
Shard 2038 / 5180 skipped
Shard 2039 / 5180 skipped
Shard 2040 / 5180 skipped
Shard 2041 / 5180 skipped
Shard 2042 / 5180 skipped
Shard 2043 / 5180 skipped
Shard 2044 / 5180 skipped
Shard 2045 / 5180 skipped
Shard 2046 / 5180 skipped
Shard 2047 / 5180 skipped
Shard 2048 / 5180 skipped
Shard 2049 / 5180 skipped
Shard 2050 / 5180 skipped
Shard 2051 / 5180 skipped
Shard 2052 / 5180 skipped
Shard 2053 / 5180 skipped
Shard 2054 / 5180 skipped
Shard 2055 / 5180 skipped
Shard 2056 / 5180 skipped
Shard 2057 / 5180 skipped
Shard 2058 / 5180 skipped
Shard 2059 / 5180 skipped
Shard 2060 / 5180 skipped
Shard 2061 / 5180 skipped
Shard 2062 / 5180 skipped
Shard 2063 / 5180 skipped
Shard 2064 / 5180 skipped
Shard 2065 / 5180 skipped
Shard 2066 / 5180 skipped
Shard 2067 / 5180 skipped
Shard 2068 / 5180 skipped
Shard 2069 / 5180 skipped
Shard 2070 / 5180 skipped
Shard 2071 / 5180 skipped
Shard 2072 / 5180 skipped
Shard 2073 / 5180 skipped
Shard 2074 / 5180 skipped
Shard 2075 / 5180 skipped
Shard 2076 / 5180 skipped
Shard 2077 / 5180 skipped
Shard 2078 / 5180 skipped
Shard 2079 / 5180 skipped
Shard 2080 / 5180 skipped
Shard 2081 / 5180 skipped
Shard 2082 / 5180 skipped
Shard 2083 / 5180 skipped
Shard 2084 / 5180 skipped
Shard 2085 / 5180 skipped
Shard 2086 / 5180 skipped
Shard 2087 / 5180 skipped
Shard 2088 / 5180 skipped
Shard 2089 / 5180 skipped
Shard 2090 / 5180 skipped
Shard 2091 / 5180 skipped
Shard 2092 / 5180 skipped
Shard 2093 / 5180 skipped
Shard 2094 / 5180 skipped
Shard 2095 / 5180 skipped
Shard 2096 / 5180 skipped
Shard 2097 / 5180 skipped
Shard 2098 / 5180 skipped
Shard 2099 / 5180 skipped
Shard 2100 / 5180 skipped
Shard 2101 / 5180 skipped
Shard 2102 / 5180 skipped
Shard 2103 / 5180 skipped
Shard 2104 / 5180 skipped
Shard 2105 / 5180 skipped
Shard 2106 / 5180 skipped
Shard 2107 / 5180 skipped
Shard 2108 / 5180 skipped
Shard 2109 / 5180 skipped
Shard 2110 / 5180 skipped
Shard 2111 / 5180 skipped
Shard 2112 / 5180 skipped
Shard 2113 / 5180 skipped
Shard 2114 / 5180 skipped
Shard 2115 / 5180 skipped
Shard 2116 / 5180 skipped
Shard 2117 / 5180 skipped
Shard 2118 / 5180 skipped
Shard 2119 / 5180 skipped
Shard 2120 / 5180 skipped
Shard 2121 / 5180 skipped
Shard 2122 / 5180 skipped
Shard 2123 / 5180 skipped
Shard 2124 / 5180 skipped
Shard 2125 / 5180 skipped
Shard 2126 / 5180 skipped
Shard 2127 / 5180 skipped
Shard 2128 / 5180 skipped
Shard 2129 / 5180 skipped
Shard 2130 / 5180 skipped
Shard 2131 / 5180 skipped
Shard 2132 / 5180 skipped
Shard 2133 / 5180 skipped
Shard 2134 / 5180 skipped
Shard 2135 / 5180 skipped
Shard 2136 / 5180 skipped
Shard 2137 / 5180 skipped
Shard 2138 / 5180 skipped
Shard 2139 / 5180 skipped
Shard 2140 / 5180 skipped
Shard 2141 / 5180 skipped
Shard 2142 / 5180 skipped
Shard 2143 / 5180 skipped
Shard 2144 / 5180 skipped
Shard 2145 / 5180 skipped
Shard 2146 / 5180 skipped
Shard 2147 / 5180 skipped
Shard 2148 / 5180 skipped
Shard 2149 / 5180 skipped
Shard 2150 / 5180 skipped
Shard 2151 / 5180 skipped
Shard 2152 / 5180 skipped
Shard 2153 / 5180 skipped
Shard 2154 / 5180 skipped
Shard 2155 / 5180 skipped
Shard 2156 / 5180 skipped
Shard 2157 / 5180 skipped
Shard 2158 / 5180 skipped
Shard 2159 / 5180 skipped
Shard 2160 / 5180 skipped
Shard 2161 / 5180 skipped
Shard 2162 / 5180 skipped
Shard 2163 / 5180 skipped
Shard 2164 / 5180 skipped
Shard 2165 / 5180 skipped
Shard 2166 / 5180 skipped
Shard 2167 / 5180 skipped
Shard 2168 / 5180 skipped
Shard 2169 / 5180 skipped
Shard 2170 / 5180 skipped
Shard 2171 / 5180 skipped
Shard 2172 / 5180 skipped
Shard 2173 / 5180 skipped
Shard 2174 / 5180 skipped
Shard 2175 / 5180 skipped
Shard 2176 / 5180 skipped
Shard 2177 / 5180 skipped
Shard 2178 / 5180 skipped
Shard 2179 / 5180 skipped
Shard 2180 / 5180 skipped
Shard 2181 / 5180 skipped
Shard 2182 / 5180 skipped
Shard 2183 / 5180 skipped
Shard 2184 / 5180 skipped
Shard 2185 / 5180 skipped
Shard 2186 / 5180 skipped
Shard 2187 / 5180 skipped
Shard 2188 / 5180 skipped
Shard 2189 / 5180 skipped
Shard 2190 / 5180 skipped
Shard 2191 / 5180 skipped
Shard 2192 / 5180 skipped
Shard 2193 / 5180 skipped
Shard 2194 / 5180 skipped
Shard 2195 / 5180 skipped
Shard 2196 / 5180 skipped
Shard 2197 / 5180 skipped
Shard 2198 / 5180 skipped
Shard 2199 / 5180 skipped
Shard 2200 / 5180 skipped
Shard 2201 / 5180 skipped
Shard 2202 / 5180 skipped
Shard 2203 / 5180 skipped
Shard 2204 / 5180 skipped
Shard 2205 / 5180 skipped
Shard 2206 / 5180 skipped
Shard 2207 / 5180 skipped
Shard 2208 / 5180 skipped
Shard 2209 / 5180 skipped
Shard 2210 / 5180 skipped
Shard 2211 / 5180 skipped
Shard 2212 / 5180 skipped
Shard 2213 / 5180 skipped
Shard 2214 / 5180 skipped
Shard 2215 / 5180 skipped
Shard 2216 / 5180 skipped
Shard 2217 / 5180 skipped
Shard 2218 / 5180 skipped
Shard 2219 / 5180 skipped
Shard 2220 / 5180 skipped
Shard 2221 / 5180 skipped
Shard 2222 / 5180 skipped
Shard 2223 / 5180 skipped
Shard 2224 / 5180 skipped
Shard 2225 / 5180 skipped
Shard 2226 / 5180 skipped
Shard 2227 / 5180 skipped
Shard 2228 / 5180 skipped
Shard 2229 / 5180 skipped
Shard 2230 / 5180 skipped
Shard 2231 / 5180 skipped
Shard 2232 / 5180 skipped
Shard 2233 / 5180 skipped
Shard 2234 / 5180 skipped
Shard 2235 / 5180 skipped
Shard 2236 / 5180 skipped
Shard 2237 / 5180 skipped
Shard 2238 / 5180 skipped
Shard 2239 / 5180 skipped
Shard 2240 / 5180 skipped
Shard 2241 / 5180 skipped
Shard 2242 / 5180 skipped
Shard 2243 / 5180 skipped
Shard 2244 / 5180 skipped
Shard 2245 / 5180 skipped
Shard 2246 / 5180 skipped
Shard 2247 / 5180 skipped
Shard 2248 / 5180 skipped
Shard 2249 / 5180 skipped
Shard 2250 / 5180 skipped
Shard 2251 / 5180 skipped
Shard 2252 / 5180 skipped
Shard 2253 / 5180 skipped
Shard 2254 / 5180 skipped
Shard 2255 / 5180 skipped
Shard 2256 / 5180 skipped
Shard 2257 / 5180 skipped
Shard 2258 / 5180 skipped
Shard 2259 / 5180 skipped
Shard 2260 / 5180 skipped
Shard 2261 / 5180 skipped
Shard 2262 / 5180 skipped
Shard 2263 / 5180 skipped
Shard 2264 / 5180 skipped
Shard 2265 / 5180 skipped
Shard 2266 / 5180 skipped
Shard 2267 / 5180 skipped
Shard 2268 / 5180 skipped
Shard 2269 / 5180 skipped
Shard 2270 / 5180 skipped
Shard 2271 / 5180 skipped
Shard 2272 / 5180 skipped
Shard 2273 / 5180 skipped
Shard 2274 / 5180 skipped
Shard 2275 / 5180 skipped
Shard 2276 / 5180 skipped
Shard 2277 / 5180 skipped
Shard 2278 / 5180 skipped
Shard 2279 / 5180 skipped
Shard 2280 / 5180 skipped
Shard 2281 / 5180 skipped
Shard 2282 / 5180 skipped
Shard 2283 / 5180 skipped
Shard 2284 / 5180 skipped
Shard 2285 / 5180 skipped
Shard 2286 / 5180 skipped
Shard 2287 / 5180 skipped
Shard 2288 / 5180 skipped
Shard 2289 / 5180 skipped
Shard 2290 / 5180 skipped
Shard 2291 / 5180 skipped
Shard 2292 / 5180 skipped
Shard 2293 / 5180 skipped
Shard 2294 / 5180 skipped
Shard 2295 / 5180 skipped
Shard 2296 / 5180 skipped
Shard 2297 / 5180 skipped
Shard 2298 / 5180 skipped
Shard 2299 / 5180 skipped
Shard 2300 / 5180 skipped
Shard 2301 / 5180 skipped
Shard 2302 / 5180 skipped
Shard 2303 / 5180 skipped
Shard 2304 / 5180 skipped
Shard 2305 / 5180 skipped
Shard 2306 / 5180 skipped
Shard 2307 / 5180 skipped
Shard 2308 / 5180 skipped
Shard 2309 / 5180 skipped
Shard 2310 / 5180 skipped
Shard 2311 / 5180 skipped
Shard 2312 / 5180 skipped
Shard 2313 / 5180 skipped
Shard 2314 / 5180 skipped
Shard 2315 / 5180 skipped
Shard 2316 / 5180 skipped
Shard 2317 / 5180 skipped
Shard 2318 / 5180 skipped
Shard 2319 / 5180 skipped
Shard 2320 / 5180 skipped
Shard 2321 / 5180 skipped
Shard 2322 / 5180 skipped
Shard 2323 / 5180 skipped
Shard 2324 / 5180 skipped
Shard 2325 / 5180 skipped
Shard 2326 / 5180 skipped
Shard 2327 / 5180 skipped
Shard 2328 / 5180 skipped
Shard 2329 / 5180 skipped
Shard 2330 / 5180 skipped
Shard 2331 / 5180 skipped
Shard 2332 / 5180 skipped
Shard 2333 / 5180 skipped
Shard 2334 / 5180 skipped
Shard 2335 / 5180 skipped
Shard 2336 / 5180 skipped
Shard 2337 / 5180 skipped
Shard 2338 / 5180 skipped
Shard 2339 / 5180 skipped
Shard 2340 / 5180 skipped
Shard 2341 / 5180 skipped
Shard 2342 / 5180 skipped
Shard 2343 / 5180 skipped
Shard 2344 / 5180 skipped
Shard 2345 / 5180 skipped
Shard 2346 / 5180 skipped
Shard 2347 / 5180 skipped
Shard 2348 / 5180 skipped
Shard 2349 / 5180 skipped
Shard 2350 / 5180 skipped
Shard 2351 / 5180 skipped
Shard 2352 / 5180 skipped
Shard 2353 / 5180 skipped
Shard 2354 / 5180 skipped
Shard 2355 / 5180 skipped
Shard 2356 / 5180 skipped
Shard 2357 / 5180 skipped
Shard 2358 / 5180 skipped
Shard 2359 / 5180 skipped
Shard 2360 / 5180 skipped
Shard 2361 / 5180 skipped
Shard 2362 / 5180 skipped
Shard 2363 / 5180 skipped
Shard 2364 / 5180 skipped
Shard 2365 / 5180 skipped
Shard 2366 / 5180 skipped
Shard 2367 / 5180 skipped
Shard 2368 / 5180 skipped
Shard 2369 / 5180 skipped
Shard 2370 / 5180 skipped
Shard 2371 / 5180 skipped
Shard 2372 / 5180 skipped
Shard 2373 / 5180 skipped
Shard 2374 / 5180 skipped
Shard 2375 / 5180 skipped
Shard 2376 / 5180 skipped
Shard 2377 / 5180 skipped
Shard 2378 / 5180 skipped
Shard 2379 / 5180 skipped
Shard 2380 / 5180 skipped
Shard 2381 / 5180 skipped
Shard 2382 / 5180 skipped
Shard 2383 / 5180 skipped
Shard 2384 / 5180 skipped
Shard 2385 / 5180 skipped
Shard 2386 / 5180 skipped
Shard 2387 / 5180 skipped
Shard 2388 / 5180 skipped
Shard 2389 / 5180 skipped
Shard 2390 / 5180 skipped
Shard 2391 / 5180 skipped
Shard 2392 / 5180 skipped
Shard 2393 / 5180 skipped
Shard 2394 / 5180 skipped
Shard 2395 / 5180 skipped
Shard 2396 / 5180 skipped
Shard 2397 / 5180 skipped
Shard 2398 / 5180 skipped
Shard 2399 / 5180 skipped
Shard 2400 / 5180 skipped
Shard 2401 / 5180 skipped
Shard 2402 / 5180 skipped
Shard 2403 / 5180 skipped
Shard 2404 / 5180 skipped
Shard 2405 / 5180 skipped
Shard 2406 / 5180 skipped
Shard 2407 / 5180 skipped
Shard 2408 / 5180 skipped
Shard 2409 / 5180 skipped
Shard 2410 / 5180 skipped
Shard 2411 / 5180 skipped
Shard 2412 / 5180 skipped
Shard 2413 / 5180 skipped
Shard 2414 / 5180 skipped
Shard 2415 / 5180 skipped
Shard 2416 / 5180 skipped
Shard 2417 / 5180 skipped
Shard 2418 / 5180 skipped
Shard 2419 / 5180 skipped
Shard 2420 / 5180 skipped
Shard 2421 / 5180 skipped
Shard 2422 / 5180 skipped
Shard 2423 / 5180 skipped
Shard 2424 / 5180 skipped
Shard 2425 / 5180 skipped
Shard 2426 / 5180 skipped
Shard 2427 / 5180 skipped
Shard 2428 / 5180 skipped
Shard 2429 / 5180 skipped
Shard 2430 / 5180 skipped
Shard 2431 / 5180 skipped
Shard 2432 / 5180 skipped
Shard 2433 / 5180 skipped
Shard 2434 / 5180 skipped
Shard 2435 / 5180 skipped
Shard 2436 / 5180 skipped
Shard 2437 / 5180 skipped
Shard 2438 / 5180 skipped
Shard 2439 / 5180 skipped
Shard 2440 / 5180 skipped
Shard 2441 / 5180 skipped
Shard 2442 / 5180 skipped
Shard 2443 / 5180 skipped
Shard 2444 / 5180 skipped
Shard 2445 / 5180 skipped
Shard 2446 / 5180 skipped
Shard 2447 / 5180 skipped
Shard 2448 / 5180 skipped
Shard 2449 / 5180 skipped
Shard 2450 / 5180 skipped
Shard 2451 / 5180 skipped
Shard 2452 / 5180 skipped
Shard 2453 / 5180 skipped
Shard 2454 / 5180 skipped
Shard 2455 / 5180 skipped
Shard 2456 / 5180 skipped
Shard 2457 / 5180 skipped
Shard 2458 / 5180 skipped
Shard 2459 / 5180 skipped
Shard 2460 / 5180 skipped
Shard 2461 / 5180 skipped
Shard 2462 / 5180 skipped
Shard 2463 / 5180 skipped
Shard 2464 / 5180 skipped
Shard 2465 / 5180 skipped
Shard 2466 / 5180 skipped
Shard 2467 / 5180 skipped
Shard 2468 / 5180 skipped
Shard 2469 / 5180 skipped
Shard 2470 / 5180 skipped
Shard 2471 / 5180 skipped
Shard 2472 / 5180 skipped
Shard 2473 / 5180 skipped
Shard 2474 / 5180 skipped
Shard 2475 / 5180 skipped
Shard 2476 / 5180 skipped
Shard 2477 / 5180 skipped
Shard 2478 / 5180 skipped
Shard 2479 / 5180 skipped
Shard 2480 / 5180 skipped
Shard 2481 / 5180 skipped
Shard 2482 / 5180 skipped
Shard 2483 / 5180 skipped
Shard 2484 / 5180 skipped
Shard 2485 / 5180 skipped
Shard 2486 / 5180 skipped
Shard 2487 / 5180 skipped
Shard 2488 / 5180 skipped
Shard 2489 / 5180 skipped
Shard 2490 / 5180 skipped
Shard 2491 / 5180 skipped
Shard 2492 / 5180 skipped
Shard 2493 / 5180 skipped
Shard 2494 / 5180 skipped
Shard 2495 / 5180 skipped
Shard 2496 / 5180 skipped
Shard 2497 / 5180 skipped
Shard 2498 / 5180 skipped
Shard 2499 / 5180 skipped
Shard 2500 / 5180 skipped
Shard 2501 / 5180 skipped
Shard 2502 / 5180 skipped
Shard 2503 / 5180 skipped
Shard 2504 / 5180 skipped
Shard 2505 / 5180 skipped
Shard 2506 / 5180 skipped
Shard 2507 / 5180 skipped
Shard 2508 / 5180 skipped
Shard 2509 / 5180 skipped
Shard 2510 / 5180 skipped
Shard 2511 / 5180 skipped
Shard 2512 / 5180 skipped
Shard 2513 / 5180 skipped
Shard 2514 / 5180 skipped
Shard 2515 / 5180 skipped
Shard 2516 / 5180 skipped
Shard 2517 / 5180 skipped
Shard 2518 / 5180 skipped
Shard 2519 / 5180 skipped
Shard 2520 / 5180 skipped
Shard 2521 / 5180 skipped
Shard 2522 / 5180 skipped
Shard 2523 / 5180 skipped
Shard 2524 / 5180 skipped
Shard 2525 / 5180 skipped
Shard 2526 / 5180 skipped
Shard 2527 / 5180 skipped
Shard 2528 / 5180 skipped
Shard 2529 / 5180 skipped
Shard 2530 / 5180 skipped
Shard 2531 / 5180 skipped
Shard 2532 / 5180 skipped
Shard 2533 / 5180 skipped
Shard 2534 / 5180 skipped
Shard 2535 / 5180 skipped
Shard 2536 / 5180 skipped
Shard 2537 / 5180 skipped
Shard 2538 / 5180 skipped
Shard 2539 / 5180 skipped
Shard 2540 / 5180 skipped
Shard 2541 / 5180 skipped
Shard 2542 / 5180 skipped
Shard 2543 / 5180 skipped
Shard 2544 / 5180 skipped
Shard 2545 / 5180 skipped
Shard 2546 / 5180 skipped
Shard 2547 / 5180 skipped
Shard 2548 / 5180 skipped
Shard 2549 / 5180 skipped
Shard 2550 / 5180 skipped
Shard 2551 / 5180 skipped
Shard 2552 / 5180 skipped
Shard 2553 / 5180 skipped
Shard 2554 / 5180 skipped
Shard 2555 / 5180 skipped
Shard 2556 / 5180 skipped
Shard 2557 / 5180 skipped
Shard 2558 / 5180 skipped
Shard 2559 / 5180 skipped
Shard 2560 / 5180 skipped
Shard 2561 / 5180 skipped
Shard 2562 / 5180 skipped
Shard 2563 / 5180 skipped
Shard 2564 / 5180 skipped
Shard 2565 / 5180 skipped
Shard 2566 / 5180 skipped
Shard 2567 / 5180 skipped
Shard 2568 / 5180 skipped
Shard 2569 / 5180 skipped
Shard 2570 / 5180 skipped
Shard 2571 / 5180 skipped
Shard 2572 / 5180 skipped
Shard 2573 / 5180 skipped
Shard 2574 / 5180 skipped
Shard 2575 / 5180 skipped
Shard 2576 / 5180 skipped
Shard 2577 / 5180 skipped
Shard 2578 / 5180 skipped
Shard 2579 / 5180 skipped
Shard 2580 / 5180 skipped
Shard 2581 / 5180 skipped
Shard 2582 / 5180 skipped
Shard 2583 / 5180 skipped
Shard 2584 / 5180 skipped
Shard 2585 / 5180 skipped
Shard 2586 / 5180 skipped
Shard 2587 / 5180 skipped
Shard 2588 / 5180 skipped
Shard 2589 / 5180 skipped
Shard 2590 / 5180 skipped
Shard 2591 / 5180 skipped
Shard 2592 / 5180 skipped
Shard 2593 / 5180 skipped
Shard 2594 / 5180 skipped
Shard 2595 / 5180 skipped
Shard 2596 / 5180 skipped
Shard 2597 / 5180 skipped
Shard 2598 / 5180 skipped
Shard 2599 / 5180 skipped
Shard 2600 / 5180 skipped
Shard 2601 / 5180 skipped
Shard 2602 / 5180 skipped
Shard 2603 / 5180 skipped
Shard 2604 / 5180 skipped
Shard 2605 / 5180 skipped
Shard 2606 / 5180 skipped
Shard 2607 / 5180 skipped
Shard 2608 / 5180 skipped
Shard 2609 / 5180 skipped
Shard 2610 / 5180 skipped
Shard 2611 / 5180 skipped
Shard 2612 / 5180 skipped
Shard 2613 / 5180 skipped
Shard 2614 / 5180 skipped
Shard 2615 / 5180 skipped
Shard 2616 / 5180 skipped
Shard 2617 / 5180 skipped
Shard 2618 / 5180 skipped
Shard 2619 / 5180 skipped
Shard 2620 / 5180 skipped
Shard 2621 / 5180 skipped
Shard 2622 / 5180 skipped
Shard 2623 / 5180 skipped
Shard 2624 / 5180 skipped
Shard 2625 / 5180 skipped
Shard 2626 / 5180 skipped
Shard 2627 / 5180 skipped
Shard 2628 / 5180 skipped
Shard 2629 / 5180 skipped
Shard 2630 / 5180 skipped
Shard 2631 / 5180 skipped
Shard 2632 / 5180 skipped
Shard 2633 / 5180 skipped
Shard 2634 / 5180 skipped
Shard 2635 / 5180 skipped
Shard 2636 / 5180 skipped
Shard 2637 / 5180 skipped
Shard 2638 / 5180 skipped
Shard 2639 / 5180 skipped
Shard 2640 / 5180 skipped
Shard 2641 / 5180 skipped
Shard 2642 / 5180 skipped
Shard 2643 / 5180 skipped
Shard 2644 / 5180 skipped
Shard 2645 / 5180 skipped
Shard 2646 / 5180 skipped
Shard 2647 / 5180 skipped
Shard 2648 / 5180 skipped
Shard 2649 / 5180 skipped
Shard 2650 / 5180 skipped
Shard 2651 / 5180 skipped
Shard 2652 / 5180 skipped
Shard 2653 / 5180 skipped
Shard 2654 / 5180 skipped
Shard 2655 / 5180 skipped
Shard 2656 / 5180 skipped
Shard 2657 / 5180 skipped
Shard 2658 / 5180 skipped
Shard 2659 / 5180 skipped
Shard 2660 / 5180 skipped
Shard 2661 / 5180 skipped
Shard 2662 / 5180 skipped
Shard 2663 / 5180 skipped
Shard 2664 / 5180 skipped
Shard 2665 / 5180 skipped
Shard 2666 / 5180 skipped
Shard 2667 / 5180 skipped
Shard 2668 / 5180 skipped
Shard 2669 / 5180 skipped
Shard 2670 / 5180 skipped
Shard 2671 / 5180 skipped
Shard 2672 / 5180 skipped
Shard 2673 / 5180 skipped
Shard 2674 / 5180 skipped
Shard 2675 / 5180 skipped
Shard 2676 / 5180 skipped
Shard 2677 / 5180 skipped
Shard 2678 / 5180 skipped
Shard 2679 / 5180 skipped
Shard 2680 / 5180 skipped
Shard 2681 / 5180 skipped
Shard 2682 / 5180 skipped
Shard 2683 / 5180 skipped
Shard 2684 / 5180 skipped
Shard 2685 / 5180 skipped
Shard 2686 / 5180 skipped
Shard 2687 / 5180 skipped
Shard 2688 / 5180 skipped
Shard 2689 / 5180 skipped
Shard 2690 / 5180 skipped
Shard 2691 / 5180 skipped
Shard 2692 / 5180 skipped
Shard 2693 / 5180 skipped
Shard 2694 / 5180 skipped
Shard 2695 / 5180 skipped
Shard 2696 / 5180 skipped
Shard 2697 / 5180 skipped
Shard 2698 / 5180 skipped
Shard 2699 / 5180 skipped
Shard 2700 / 5180 skipped
Shard 2701 / 5180 skipped
Shard 2702 / 5180 skipped
Shard 2703 / 5180 skipped
Shard 2704 / 5180 skipped
Shard 2705 / 5180 skipped
Shard 2706 / 5180 skipped
Shard 2707 / 5180 skipped
Shard 2708 / 5180 skipped
Shard 2709 / 5180 skipped
Shard 2710 / 5180 skipped
Shard 2711 / 5180 skipped
Shard 2712 / 5180 skipped
Shard 2713 / 5180 skipped
Shard 2714 / 5180 skipped
Shard 2715 / 5180 skipped
Shard 2716 / 5180 skipped
Shard 2717 / 5180 skipped
Shard 2718 / 5180 skipped
Shard 2719 / 5180 skipped
Shard 2720 / 5180 skipped
Shard 2721 / 5180 skipped
Shard 2722 / 5180 skipped
Shard 2723 / 5180 skipped
Shard 2724 / 5180 skipped
Shard 2725 / 5180 skipped
Shard 2726 / 5180 skipped
Shard 2727 / 5180 skipped
Shard 2728 / 5180 skipped
Shard 2729 / 5180 skipped
Shard 2730 / 5180 skipped
Shard 2731 / 5180 skipped
Shard 2732 / 5180 skipped
Shard 2733 / 5180 skipped
Shard 2734 / 5180 skipped
Shard 2735 / 5180 skipped
Shard 2736 / 5180 skipped
Shard 2737 / 5180 skipped
Shard 2738 / 5180 skipped
Shard 2739 / 5180 skipped
Shard 2740 / 5180 skipped
Shard 2741 / 5180 skipped
Shard 2742 / 5180 skipped
Shard 2743 / 5180 skipped
Shard 2744 / 5180 skipped
Shard 2745 / 5180 skipped
Shard 2746 / 5180 skipped
Shard 2747 / 5180 skipped
Shard 2748 / 5180 skipped
Shard 2749 / 5180 skipped
Shard 2750 / 5180 skipped
Shard 2751 / 5180 skipped
Shard 2752 / 5180 skipped
Shard 2753 / 5180 skipped
Shard 2754 / 5180 skipped
Shard 2755 / 5180 skipped
Shard 2756 / 5180 skipped
Shard 2757 / 5180 skipped
Shard 2758 / 5180 skipped
Shard 2759 / 5180 skipped
Shard 2760 / 5180 skipped
Shard 2761 / 5180 skipped
Shard 2762 / 5180 skipped
Shard 2763 / 5180 skipped
Shard 2764 / 5180 skipped
Shard 2765 / 5180 skipped
Shard 2766 / 5180 skipped
Shard 2767 / 5180 skipped
Shard 2768 / 5180 skipped
Shard 2769 / 5180 skipped
Shard 2770 / 5180 skipped
Shard 2771 / 5180 skipped
Shard 2772 / 5180 skipped
Shard 2773 / 5180 skipped
Shard 2774 / 5180 skipped
Shard 2775 / 5180 skipped
Shard 2776 / 5180 skipped
Shard 2777 / 5180 skipped
Shard 2778 / 5180 skipped
Shard 2779 / 5180 skipped
Shard 2780 / 5180 skipped
Shard 2781 / 5180 skipped
Shard 2782 / 5180 skipped
Shard 2783 / 5180 skipped
Shard 2784 / 5180 skipped
Shard 2785 / 5180 skipped
Shard 2786 / 5180 skipped
Shard 2787 / 5180 skipped
Shard 2788 / 5180 skipped
Shard 2789 / 5180 skipped
Shard 2790 / 5180 skipped
Shard 2791 / 5180 skipped
Shard 2792 / 5180 skipped
Shard 2793 / 5180 skipped
Shard 2794 / 5180 skipped
Shard 2795 / 5180 skipped
Shard 2796 / 5180 skipped
Shard 2797 / 5180 skipped
Shard 2798 / 5180 skipped
Shard 2799 / 5180 skipped
Shard 2800 / 5180 skipped
Shard 2801 / 5180 skipped
Shard 2802 / 5180 skipped
Shard 2803 / 5180 skipped
Shard 2804 / 5180 skipped
Shard 2805 / 5180 skipped
Shard 2806 / 5180 skipped
Shard 2807 / 5180 skipped
Shard 2808 / 5180 skipped
Shard 2809 / 5180 skipped
Shard 2810 / 5180 skipped
Shard 2811 / 5180 skipped
Shard 2812 / 5180 skipped
Shard 2813 / 5180 skipped
Shard 2814 / 5180 skipped
Shard 2815 / 5180 skipped
Shard 2816 / 5180 skipped
Shard 2817 / 5180 skipped
Shard 2818 / 5180 skipped
Shard 2819 / 5180 skipped
Shard 2820 / 5180 skipped
Shard 2821 / 5180 skipped
Shard 2822 / 5180 skipped
Shard 2823 / 5180 skipped
Shard 2824 / 5180 skipped
Shard 2825 / 5180 skipped
Shard 2826 / 5180 skipped
Shard 2827 / 5180 skipped
Shard 2828 / 5180 skipped
Shard 2829 / 5180 skipped
Shard 2830 / 5180 skipped
Shard 2831 / 5180 skipped
Shard 2832 / 5180 skipped
Shard 2833 / 5180 skipped
Shard 2834 / 5180 skipped
Shard 2835 / 5180 skipped
Shard 2836 / 5180 skipped
Shard 2837 / 5180 skipped
Shard 2838 / 5180 skipped
Shard 2839 / 5180 skipped
Shard 2840 / 5180 skipped
Shard 2841 / 5180 skipped
Shard 2842 / 5180 skipped
Shard 2843 / 5180 skipped
Shard 2844 / 5180 skipped
Shard 2845 / 5180 skipped
Shard 2846 / 5180 skipped
Shard 2847 / 5180 skipped
Shard 2848 / 5180 skipped
Shard 2849 / 5180 skipped
Shard 2850 / 5180 skipped
Shard 2851 / 5180 skipped
Shard 2852 / 5180 skipped
Shard 2853 / 5180 skipped
Shard 2854 / 5180 skipped
Shard 2855 / 5180 skipped
Shard 2856 / 5180 skipped
Shard 2857 / 5180 skipped
Shard 2858 / 5180 skipped
Shard 2859 / 5180 skipped
Shard 2860 / 5180 skipped
Shard 2861 / 5180 skipped
Shard 2862 / 5180 skipped
Shard 2863 / 5180 skipped
Shard 2864 / 5180 skipped
Shard 2865 / 5180 skipped
Shard 2866 / 5180 skipped
Shard 2867 / 5180 skipped
Shard 2868 / 5180 skipped
Shard 2869 / 5180 skipped
Shard 2870 / 5180 skipped
Shard 2871 / 5180 skipped
Shard 2872 / 5180 skipped
Shard 2873 / 5180 skipped
Shard 2874 / 5180 skipped
Shard 2875 / 5180 skipped
Shard 2876 / 5180 skipped
Shard 2877 / 5180 skipped
Shard 2878 / 5180 skipped
Shard 2879 / 5180 skipped
Shard 2880 / 5180 skipped
Shard 2881 / 5180 skipped
Shard 2882 / 5180 skipped
Shard 2883 / 5180 skipped
Shard 2884 / 5180 skipped
Shard 2885 / 5180 skipped
Shard 2886 / 5180 skipped
Shard 2887 / 5180 skipped
Shard 2888 / 5180 skipped
Shard 2889 / 5180 skipped
Shard 2890 / 5180 skipped
Shard 2891 / 5180 skipped
Shard 2892 / 5180 skipped
Shard 2893 / 5180 skipped
Shard 2894 / 5180 skipped
Shard 2895 / 5180 skipped
Shard 2896 / 5180 skipped
Shard 2897 / 5180 skipped
Shard 2898 / 5180 skipped
Shard 2899 / 5180 skipped
Shard 2900 / 5180 skipped
Shard 2901 / 5180 skipped
Shard 2902 / 5180 skipped
Shard 2903 / 5180 skipped
Shard 2904 / 5180 skipped
Shard 2905 / 5180 skipped
Shard 2906 / 5180 skipped
Shard 2907 / 5180 skipped
Shard 2908 / 5180 skipped
Shard 2909 / 5180 skipped
Shard 2910 / 5180 skipped
Shard 2911 / 5180 skipped
Shard 2912 / 5180 skipped
Shard 2913 / 5180 skipped
Shard 2914 / 5180 skipped
Shard 2915 / 5180 skipped
Shard 2916 / 5180 skipped
Shard 2917 / 5180 skipped
Shard 2918 / 5180 skipped
Shard 2919 / 5180 skipped
Shard 2920 / 5180 skipped
Shard 2921 / 5180 skipped
Shard 2922 / 5180 skipped
Shard 2923 / 5180 skipped
Shard 2924 / 5180 skipped
Shard 2925 / 5180 skipped
Shard 2926 / 5180 skipped
Shard 2927 / 5180 skipped
Shard 2928 / 5180 skipped
Shard 2929 / 5180 skipped
Shard 2930 / 5180 skipped
Shard 2931 / 5180 skipped
Shard 2932 / 5180 skipped
Shard 2933 / 5180 skipped
Shard 2934 / 5180 skipped
Shard 2935 / 5180 skipped
Shard 2936 / 5180 skipped
Shard 2937 / 5180 skipped
Shard 2938 / 5180 skipped
Shard 2939 / 5180 skipped
Shard 2940 / 5180 skipped
Shard 2941 / 5180 skipped
Shard 2942 / 5180 skipped
Shard 2943 / 5180 skipped
Shard 2944 / 5180 skipped
Shard 2945 / 5180 skipped
Shard 2946 / 5180 skipped
Shard 2947 / 5180 skipped
Shard 2948 / 5180 skipped
Shard 2949 / 5180 skipped
Shard 2950 / 5180 skipped
Shard 2951 / 5180 skipped
Shard 2952 / 5180 skipped
Shard 2953 / 5180 skipped
Shard 2954 / 5180 skipped
Shard 2955 / 5180 skipped
Shard 2956 / 5180 skipped
Shard 2957 / 5180 skipped
Shard 2958 / 5180 skipped
Shard 2959 / 5180 skipped
Shard 2960 / 5180 skipped
Shard 2961 / 5180 skipped
Shard 2962 / 5180 skipped
Shard 2963 / 5180 skipped
Shard 2964 / 5180 skipped
Shard 2965 / 5180 skipped
Shard 2966 / 5180 skipped
Shard 2967 / 5180 skipped
Shard 2968 / 5180 skipped
Shard 2969 / 5180 skipped
Shard 2970 / 5180 skipped
Shard 2971 / 5180 skipped
Shard 2972 / 5180 skipped
Shard 2973 / 5180 skipped
Shard 2974 / 5180 skipped
Shard 2975 / 5180 skipped
Shard 2976 / 5180 skipped
Shard 2977 / 5180 skipped
Shard 2978 / 5180 skipped
Shard 2979 / 5180 skipped
Shard 2980 / 5180 skipped
Shard 2981 / 5180 skipped
Shard 2982 / 5180 skipped
Shard 2983 / 5180 skipped
Shard 2984 / 5180 skipped
Shard 2985 / 5180 skipped
Shard 2986 / 5180 skipped
Shard 2987 / 5180 skipped
Shard 2988 / 5180 skipped
Shard 2989 / 5180 skipped
Shard 2990 / 5180 skipped
Shard 2991 / 5180 skipped
Shard 2992 / 5180 skipped
Shard 2993 / 5180 skipped
Shard 2994 / 5180 skipped
Shard 2995 / 5180 skipped
Shard 2996 / 5180 skipped
Shard 2997 / 5180 skipped
Shard 2998 / 5180 skipped
Shard 2999 / 5180 skipped
Shard 3000 / 5180 skipped
Shard 3001 / 5180 skipped
Shard 3002 / 5180 skipped
Shard 3003 / 5180 skipped
Shard 3004 / 5180 skipped
Shard 3005 / 5180 skipped
Shard 3006 / 5180 skipped
Shard 3007 / 5180 skipped
Shard 3008 / 5180 skipped
Shard 3009 / 5180 skipped
Shard 3010 / 5180 skipped
Shard 3011 / 5180 skipped
Shard 3012 / 5180 skipped
Shard 3013 / 5180 skipped
Shard 3014 / 5180 skipped
Shard 3015 / 5180 skipped
Shard 3016 / 5180 skipped
Shard 3017 / 5180 skipped
Shard 3018 / 5180 skipped
Shard 3019 / 5180 skipped
Shard 3020 / 5180 skipped
Shard 3021 / 5180 skipped
Shard 3022 / 5180 skipped
Shard 3023 / 5180 skipped
Shard 3024 / 5180 skipped
Shard 3025 / 5180 skipped
Shard 3026 / 5180 skipped
Shard 3027 / 5180 skipped
Shard 3028 / 5180 skipped
Shard 3029 / 5180 skipped
Shard 3030 / 5180 skipped
Shard 3031 / 5180 skipped
Shard 3032 / 5180 skipped
Shard 3033 / 5180 skipped
Shard 3034 / 5180 skipped
Shard 3035 / 5180 skipped
Shard 3036 / 5180 skipped
Shard 3037 / 5180 skipped
Shard 3038 / 5180 skipped
Shard 3039 / 5180 skipped
Shard 3040 / 5180 skipped
Shard 3041 / 5180 skipped
Shard 3042 / 5180 skipped
Shard 3043 / 5180 skipped
Shard 3044 / 5180 skipped
Shard 3045 / 5180 skipped
Shard 3046 / 5180 skipped
Shard 3047 / 5180 skipped
Shard 3048 / 5180 skipped
Shard 3049 / 5180 skipped
Shard 3050 / 5180 skipped
Shard 3051 / 5180 skipped
Shard 3052 / 5180 skipped
Shard 3053 / 5180 skipped
Shard 3054 / 5180 skipped
Shard 3055 / 5180 skipped
Shard 3056 / 5180 skipped
Shard 3057 / 5180 skipped
Shard 3058 / 5180 skipped
Shard 3059 / 5180 skipped
Shard 3060 / 5180 skipped
Shard 3061 / 5180 skipped
Shard 3062 / 5180 skipped
Shard 3063 / 5180 skipped
Shard 3064 / 5180 skipped
Shard 3065 / 5180 skipped
Shard 3066 / 5180 skipped
Shard 3067 / 5180 skipped
Shard 3068 / 5180 skipped
Shard 3069 / 5180 skipped
Shard 3070 / 5180 skipped
Shard 3071 / 5180 skipped
Shard 3072 / 5180 skipped
Shard 3073 / 5180 skipped
Shard 3074 / 5180 skipped
Shard 3075 / 5180 skipped
Shard 3076 / 5180 skipped
Shard 3077 / 5180 skipped
Shard 3078 / 5180 skipped
Shard 3079 / 5180 skipped
Shard 3080 / 5180 skipped
Shard 3081 / 5180 skipped
Shard 3082 / 5180 skipped
Shard 3083 / 5180 skipped
Shard 3084 / 5180 skipped
Shard 3085 / 5180 skipped
Shard 3086 / 5180 skipped
Shard 3087 / 5180 skipped
Shard 3088 / 5180 skipped
Shard 3089 / 5180 skipped
Shard 3090 / 5180 skipped
Shard 3091 / 5180 skipped
Shard 3092 / 5180 skipped
Shard 3093 / 5180 skipped
Shard 3094 / 5180 skipped
Shard 3095 / 5180 skipped
Shard 3096 / 5180 skipped
Shard 3097 / 5180 skipped
Shard 3098 / 5180 skipped
Shard 3099 / 5180 skipped
Shard 3100 / 5180 skipped
Shard 3101 / 5180 skipped
Shard 3102 / 5180 skipped
Shard 3103 / 5180 skipped
Shard 3104 / 5180 skipped
Shard 3105 / 5180 skipped
Shard 3106 / 5180 skipped
Shard 3107 / 5180 skipped
Shard 3108 / 5180 skipped
Shard 3109 / 5180 skipped
Shard 3110 / 5180 skipped
Shard 3111 / 5180 skipped
Shard 3112 / 5180 skipped
Shard 3113 / 5180 skipped
Shard 3114 / 5180 skipped
Shard 3115 / 5180 skipped
Shard 3116 / 5180 skipped
Shard 3117 / 5180 skipped
Shard 3118 / 5180 skipped
Shard 3119 / 5180 skipped
Shard 3120 / 5180 skipped
Shard 3121 / 5180 skipped
Shard 3122 / 5180 skipped
Shard 3123 / 5180 skipped
Shard 3124 / 5180 skipped
Shard 3125 / 5180 skipped
Shard 3126 / 5180 skipped
Shard 3127 / 5180 skipped
Shard 3128 / 5180 skipped
Shard 3129 / 5180 skipped
Shard 3130 / 5180 skipped
Shard 3131 / 5180 skipped
Shard 3132 / 5180 skipped
Shard 3133 / 5180 skipped
Shard 3134 / 5180 skipped
Shard 3135 / 5180 skipped
Shard 3136 / 5180 skipped
Shard 3137 / 5180 skipped
Shard 3138 / 5180 skipped
Shard 3139 / 5180 skipped
Shard 3140 / 5180 skipped
Shard 3141 / 5180 skipped
Shard 3142 / 5180 skipped
Shard 3143 / 5180 skipped
Shard 3144 / 5180 skipped
Shard 3145 / 5180 skipped
Shard 3146 / 5180 skipped
Shard 3147 / 5180 skipped
Shard 3148 / 5180 skipped
Shard 3149 / 5180 skipped
Shard 3150 / 5180 skipped
Shard 3151 / 5180 skipped
Shard 3152 / 5180 skipped
Shard 3153 / 5180 skipped
Shard 3154 / 5180 skipped
Shard 3155 / 5180 skipped
Shard 3156 / 5180 skipped
Shard 3157 / 5180 skipped
Shard 3158 / 5180 skipped
Shard 3159 / 5180 skipped
Shard 3160 / 5180 skipped
Shard 3161 / 5180 skipped
Shard 3162 / 5180 skipped
Shard 3163 / 5180 skipped
Shard 3164 / 5180 skipped
Shard 3165 / 5180 skipped
Shard 3166 / 5180 skipped
Shard 3167 / 5180 skipped
Shard 3168 / 5180 skipped
Shard 3169 / 5180 skipped
Shard 3170 / 5180 skipped
Shard 3171 / 5180 skipped
Shard 3172 / 5180 skipped
Shard 3173 / 5180 skipped
Shard 3174 / 5180 skipped
Shard 3175 / 5180 skipped
Shard 3176 / 5180 skipped
Shard 3177 / 5180 skipped
Shard 3178 / 5180 skipped
Shard 3179 / 5180 skipped
Shard 3180 / 5180 skipped
Shard 3181 / 5180 skipped
Shard 3182 / 5180 skipped
Shard 3183 / 5180 skipped
Shard 3184 / 5180 skipped
Shard 3185 / 5180 skipped
Shard 3186 / 5180 skipped
Shard 3187 / 5180 skipped
Shard 3188 / 5180 skipped
Shard 3189 / 5180 skipped
Shard 3190 / 5180 skipped
Shard 3191 / 5180 skipped
Shard 3192 / 5180 skipped
Shard 3193 / 5180 skipped
Shard 3194 / 5180 skipped
Shard 3195 / 5180 skipped
Shard 3196 / 5180 skipped
Shard 3197 / 5180 skipped
Shard 3198 / 5180 skipped
Shard 3199 / 5180 skipped
Shard 3200 / 5180 skipped
Shard 3201 / 5180 skipped
Shard 3202 / 5180 skipped
Shard 3203 / 5180 skipped
Shard 3204 / 5180 skipped
Shard 3205 / 5180 skipped
Shard 3206 / 5180 skipped
Shard 3207 / 5180 skipped
Shard 3208 / 5180 skipped
Shard 3209 / 5180 skipped
Shard 3210 / 5180 skipped
Shard 3211 / 5180 skipped
Shard 3212 / 5180 skipped
Shard 3213 / 5180 skipped
Shard 3214 / 5180 skipped
Shard 3215 / 5180 skipped
Shard 3216 / 5180 skipped
Shard 3217 / 5180 skipped
Shard 3218 / 5180 skipped
Shard 3219 / 5180 skipped
Shard 3220 / 5180 skipped
Shard 3221 / 5180 skipped
Shard 3222 / 5180 skipped
Shard 3223 / 5180 skipped
Shard 3224 / 5180 skipped
Shard 3225 / 5180 skipped
Shard 3226 / 5180 skipped
Shard 3227 / 5180 skipped
Shard 3228 / 5180 skipped
Shard 3229 / 5180 skipped
Shard 3230 / 5180 skipped
Shard 3231 / 5180 skipped
Shard 3232 / 5180 skipped
Shard 3233 / 5180 skipped
Shard 3234 / 5180 skipped
Shard 3235 / 5180 skipped
Shard 3236 / 5180 skipped
Shard 3237 / 5180 skipped
Shard 3238 / 5180 skipped
Shard 3239 / 5180 skipped
Shard 3240 / 5180 skipped
Shard 3241 / 5180 skipped
Shard 3242 / 5180 skipped
Shard 3243 / 5180 skipped
Shard 3244 / 5180 skipped
Shard 3245 / 5180 skipped
Shard 3246 / 5180 skipped
Shard 3247 / 5180 skipped
Shard 3248 / 5180 skipped
Shard 3249 / 5180 skipped
Shard 3250 / 5180 skipped
Shard 3251 / 5180 skipped
Shard 3252 / 5180 skipped
Shard 3253 / 5180 skipped
Shard 3254 / 5180 skipped
Shard 3255 / 5180 skipped
Shard 3256 / 5180 skipped
Shard 3257 / 5180 skipped
Shard 3258 / 5180 skipped
Shard 3259 / 5180 skipped
Shard 3260 / 5180 skipped
Shard 3261 / 5180 skipped
Shard 3262 / 5180 skipped
Shard 3263 / 5180 skipped
Shard 3264 / 5180 skipped
Shard 3265 / 5180 skipped
Shard 3266 / 5180 skipped
Shard 3267 / 5180 skipped
Shard 3268 / 5180 skipped
Shard 3269 / 5180 skipped
Shard 3270 / 5180 skipped
Shard 3271 / 5180 skipped
Shard 3272 / 5180 skipped
Shard 3273 / 5180 skipped
Shard 3274 / 5180 skipped
Shard 3275 / 5180 skipped
Shard 3276 / 5180 skipped
Shard 3277 / 5180 skipped
Shard 3278 / 5180 skipped
Shard 3279 / 5180 skipped
Shard 3280 / 5180 skipped
Shard 3281 / 5180 skipped
Shard 3282 / 5180 skipped
Shard 3283 / 5180 skipped
Shard 3284 / 5180 skipped
Shard 3285 / 5180 skipped
Shard 3286 / 5180 skipped
Shard 3287 / 5180 skipped
Shard 3288 / 5180 skipped
Shard 3289 / 5180 skipped
Shard 3290 / 5180 skipped
Shard 3291 / 5180 skipped
Shard 3292 / 5180 skipped
Shard 3293 / 5180 skipped
Shard 3294 / 5180 skipped
Shard 3295 / 5180 skipped
Shard 3296 / 5180 skipped
Shard 3297 / 5180 skipped
Shard 3298 / 5180 skipped
Shard 3299 / 5180 skipped
Shard 3300 / 5180 skipped
Shard 3301 / 5180 skipped
Shard 3302 / 5180 skipped
Shard 3303 / 5180 skipped
Shard 3304 / 5180 skipped
Shard 3305 / 5180 skipped
Shard 3306 / 5180 skipped
Shard 3307 / 5180 skipped
Shard 3308 / 5180 skipped
Shard 3309 / 5180 skipped
Shard 3310 / 5180 skipped
Shard 3311 / 5180 skipped
Shard 3312 / 5180 skipped
Shard 3313 / 5180 skipped
Shard 3314 / 5180 skipped
Shard 3315 / 5180 skipped
Shard 3316 / 5180 skipped
Shard 3317 / 5180 skipped
Shard 3318 / 5180 skipped
Shard 3319 / 5180 skipped
Shard 3320 / 5180 skipped
Shard 3321 / 5180 skipped
Shard 3322 / 5180 skipped
Shard 3323 / 5180 skipped
Shard 3324 / 5180 skipped
Shard 3325 / 5180 skipped
Shard 3326 / 5180 skipped
Shard 3327 / 5180 skipped
Shard 3328 / 5180 skipped
Shard 3329 / 5180 skipped
Shard 3330 / 5180 skipped
Shard 3331 / 5180 skipped
Shard 3332 / 5180 skipped
Shard 3333 / 5180 skipped
Shard 3334 / 5180 skipped
Shard 3335 / 5180 skipped
Shard 3336 / 5180 skipped
Shard 3337 / 5180 skipped
Shard 3338 / 5180 skipped
Shard 3339 / 5180 skipped
Shard 3340 / 5180 skipped
Shard 3341 / 5180 skipped
Shard 3342 / 5180 skipped
Shard 3343 / 5180 skipped
Shard 3344 / 5180 skipped
Shard 3345 / 5180 skipped
Shard 3346 / 5180 skipped
Shard 3347 / 5180 skipped
Shard 3348 / 5180 skipped
Shard 3349 / 5180 skipped
Shard 3350 / 5180 skipped
Shard 3351 / 5180 skipped
Shard 3352 / 5180 skipped
Shard 3353 / 5180 skipped
Shard 3354 / 5180 skipped
Shard 3355 / 5180 skipped
Shard 3356 / 5180 skipped
Shard 3357 / 5180 skipped
Shard 3358 / 5180 skipped
Shard 3359 / 5180 skipped
Shard 3360 / 5180 skipped
Shard 3361 / 5180 skipped
Shard 3362 / 5180 skipped
Shard 3363 / 5180 skipped
Shard 3364 / 5180 skipped
Shard 3365 / 5180 skipped
Shard 3366 / 5180 skipped
Shard 3367 / 5180 skipped
Shard 3368 / 5180 skipped
Shard 3369 / 5180 skipped
Shard 3370 / 5180 skipped
Shard 3371 / 5180 skipped
Shard 3372 / 5180 skipped
Shard 3373 / 5180 skipped
Shard 3374 / 5180 skipped
Shard 3375 / 5180 skipped
Shard 3376 / 5180 skipped
Shard 3377 / 5180 skipped
Shard 3378 / 5180 skipped
Shard 3379 / 5180 skipped
Shard 3380 / 5180 skipped
Shard 3381 / 5180 skipped
Shard 3382 / 5180 skipped
Shard 3383 / 5180 skipped
Shard 3384 / 5180 skipped
Shard 3385 / 5180 skipped
Shard 3386 / 5180 skipped
Shard 3387 / 5180 skipped
Shard 3388 / 5180 skipped
Shard 3389 / 5180 skipped
Shard 3390 / 5180 skipped
Shard 3391 / 5180 skipped
Shard 3392 / 5180 skipped
Shard 3393 / 5180 skipped
Shard 3394 / 5180 skipped
Shard 3395 / 5180 skipped
Shard 3396 / 5180 skipped
Shard 3397 / 5180 skipped
Shard 3398 / 5180 skipped
Shard 3399 / 5180 skipped
Shard 3400 / 5180 skipped
Shard 3401 / 5180 skipped
Shard 3402 / 5180 skipped
Shard 3403 / 5180 skipped
Shard 3404 / 5180 skipped
Shard 3405 / 5180 skipped
Shard 3406 / 5180 skipped
Shard 3407 / 5180 skipped
Shard 3408 / 5180 skipped
Shard 3409 / 5180 skipped
Shard 3410 / 5180 skipped
Shard 3411 / 5180 skipped
Shard 3412 / 5180 skipped
Shard 3413 / 5180 skipped
Shard 3414 / 5180 skipped
Shard 3415 / 5180 skipped
Shard 3416 / 5180 skipped
Shard 3417 / 5180 skipped
Shard 3418 / 5180 skipped
Shard 3419 / 5180 skipped
Shard 3420 / 5180 skipped
Shard 3421 / 5180 skipped
Shard 3422 / 5180 skipped
Shard 3423 / 5180 skipped
Shard 3424 / 5180 skipped
Shard 3425 / 5180 skipped
Shard 3426 / 5180 skipped
Shard 3427 / 5180 skipped
Shard 3428 / 5180 skipped
Shard 3429 / 5180 skipped
Shard 3430 / 5180 skipped
Shard 3431 / 5180 skipped
Shard 3432 / 5180 skipped
Shard 3433 / 5180 skipped
Shard 3434 / 5180 skipped
Shard 3435 / 5180 skipped
Shard 3436 / 5180 skipped
Shard 3437 / 5180 skipped
Shard 3438 / 5180 skipped
Shard 3439 / 5180 skipped
Shard 3440 / 5180 skipped
Shard 3441 / 5180 skipped
Shard 3442 / 5180 skipped
Shard 3443 / 5180 skipped
Shard 3444 / 5180 skipped
Shard 3445 / 5180 skipped
Shard 3446 / 5180 skipped
Shard 3447 / 5180 skipped
Shard 3448 / 5180 skipped
Shard 3449 / 5180 skipped
Shard 3450 / 5180 skipped
Shard 3451 / 5180 skipped
Shard 3452 / 5180 skipped
Shard 3453 / 5180 skipped
Shard 3454 / 5180 skipped
Shard 3455 / 5180 skipped
Shard 3456 / 5180 skipped
Shard 3457 / 5180 skipped
Shard 3458 / 5180 skipped
Shard 3459 / 5180 skipped
Shard 3460 / 5180 skipped
Shard 3461 / 5180 skipped
Shard 3462 / 5180 skipped
Shard 3463 / 5180 skipped
Shard 3464 / 5180 skipped
Shard 3465 / 5180 skipped
Shard 3466 / 5180 skipped
Shard 3467 / 5180 skipped
Shard 3468 / 5180 skipped
Shard 3469 / 5180 skipped
Shard 3470 / 5180 skipped
Shard 3471 / 5180 skipped
Shard 3472 / 5180 skipped
Shard 3473 / 5180 skipped
Shard 3474 / 5180 skipped
Shard 3475 / 5180 skipped
Shard 3476 / 5180 skipped
Shard 3477 / 5180 skipped
Shard 3478 / 5180 skipped
Shard 3479 / 5180 skipped
Shard 3480 / 5180 skipped
Shard 3481 / 5180 skipped
Shard 3482 / 5180 skipped
Shard 3483 / 5180 skipped
Shard 3484 / 5180 skipped
Shard 3485 / 5180 skipped
Shard 3486 / 5180 skipped
Shard 3487 / 5180 skipped
Shard 3488 / 5180 skipped
Shard 3489 / 5180 skipped
Shard 3490 / 5180 skipped
Shard 3491 / 5180 skipped
Shard 3492 / 5180 skipped
Shard 3493 / 5180 skipped
Shard 3494 / 5180 skipped
Shard 3495 / 5180 skipped
Shard 3496 / 5180 skipped
Shard 3497 / 5180 skipped
Shard 3498 / 5180 skipped
Shard 3499 / 5180 skipped
Shard 3500 / 5180 skipped
Shard 3501 / 5180 skipped
Shard 3502 / 5180 skipped
Shard 3503 / 5180 skipped
Shard 3504 / 5180 skipped
Shard 3505 / 5180 skipped
Shard 3506 / 5180 skipped
Shard 3507 / 5180 skipped
Shard 3508 / 5180 skipped
Shard 3509 / 5180 skipped
Shard 3510 / 5180 skipped
Shard 3511 / 5180 skipped
Shard 3512 / 5180 skipped
Shard 3513 / 5180 skipped
Shard 3514 / 5180 skipped
Shard 3515 / 5180 skipped
Shard 3516 / 5180 skipped
Shard 3517 / 5180 skipped
Shard 3518 / 5180 skipped
Shard 3519 / 5180 skipped
Shard 3520 / 5180 skipped
Shard 3521 / 5180 skipped
Shard 3522 / 5180 skipped
Shard 3523 / 5180 skipped
Shard 3524 / 5180 skipped
Shard 3525 / 5180 skipped
Shard 3526 / 5180 skipped
Shard 3527 / 5180 skipped
Shard 3528 / 5180 skipped
Shard 3529 / 5180 skipped
Shard 3530 / 5180 skipped
Shard 3531 / 5180 skipped
Shard 3532 / 5180 skipped
Shard 3533 / 5180 skipped
Shard 3534 / 5180 skipped
Shard 3535 / 5180 skipped
Shard 3536 / 5180 skipped
Shard 3537 / 5180 skipped
Shard 3538 / 5180 skipped
Shard 3539 / 5180 skipped
Shard 3540 / 5180 skipped
Shard 3541 / 5180 skipped
Shard 3542 / 5180 skipped
Shard 3543 / 5180 skipped
Shard 3544 / 5180 skipped
Shard 3545 / 5180 skipped
Shard 3546 / 5180 skipped
Shard 3547 / 5180 skipped
Shard 3548 / 5180 skipped
Shard 3549 / 5180 skipped
Shard 3550 / 5180 skipped
Shard 3551 / 5180 skipped
Shard 3552 / 5180 skipped
Shard 3553 / 5180 skipped
Shard 3554 / 5180 skipped
Shard 3555 / 5180 skipped
Shard 3556 / 5180 skipped
Shard 3557 / 5180 skipped
Shard 3558 / 5180 skipped
Shard 3559 / 5180 skipped
Shard 3560 / 5180 skipped
Shard 3561 / 5180 skipped
Shard 3562 / 5180 skipped
Shard 3563 / 5180 skipped
Shard 3564 / 5180 skipped
Shard 3565 / 5180 skipped
Shard 3566 / 5180 skipped
Shard 3567 / 5180 skipped
Shard 3568 / 5180 skipped
Shard 3569 / 5180 skipped
Shard 3570 / 5180 skipped
Shard 3571 / 5180 skipped
Shard 3572 / 5180 skipped
Shard 3573 / 5180 skipped
Shard 3574 / 5180 skipped
Shard 3575 / 5180 skipped
Shard 3576 / 5180 skipped
Shard 3577 / 5180 skipped
Shard 3578 / 5180 skipped
Shard 3579 / 5180 skipped
Shard 3580 / 5180 skipped
Shard 3581 / 5180 skipped
Shard 3582 / 5180 skipped
Shard 3583 / 5180 skipped
Shard 3584 / 5180 skipped
Shard 3585 / 5180 skipped
Shard 3586 / 5180 skipped
Shard 3587 / 5180 skipped
Shard 3588 / 5180 skipped
Shard 3589 / 5180 skipped
Shard 3590 / 5180 skipped
Shard 3591 / 5180 skipped
Shard 3592 / 5180 skipped
Shard 3593 / 5180 skipped
Shard 3594 / 5180 skipped
Shard 3595 / 5180 skipped
Shard 3596 / 5180 skipped
Shard 3597 / 5180 skipped
Shard 3598 / 5180 skipped
Shard 3599 / 5180 skipped
Shard 3600 / 5180 skipped
Shard 3601 / 5180 skipped
Shard 3602 / 5180 skipped
Shard 3603 / 5180 skipped
Shard 3604 / 5180 skipped
Shard 3605 / 5180 skipped
Shard 3606 / 5180 skipped
Shard 3607 / 5180 skipped
Shard 3608 / 5180 skipped
Shard 3609 / 5180 skipped
Shard 3610 / 5180 skipped
Shard 3611 / 5180 skipped
Shard 3612 / 5180 skipped
Shard 3613 / 5180 skipped
Shard 3614 / 5180 skipped
Shard 3615 / 5180 skipped
Shard 3616 / 5180 skipped
Shard 3617 / 5180 skipped
Shard 3618 / 5180 skipped
Shard 3619 / 5180 skipped
Shard 3620 / 5180 skipped
Shard 3621 / 5180 skipped
Shard 3622 / 5180 skipped
Shard 3623 / 5180 skipped
Shard 3624 / 5180 skipped
Shard 3625 / 5180 skipped
Shard 3626 / 5180 skipped
Shard 3627 / 5180 skipped
Shard 3628 / 5180 skipped
Shard 3629 / 5180 skipped
Shard 3630 / 5180 skipped
Shard 3631 / 5180 skipped
Shard 3632 / 5180 skipped
Shard 3633 / 5180 skipped
Shard 3634 / 5180 skipped
Shard 3635 / 5180 skipped
Shard 3636 / 5180 skipped
Shard 3637 / 5180 skipped
Shard 3638 / 5180 skipped
Shard 3639 / 5180 skipped
Shard 3640 / 5180 skipped
Shard 3641 / 5180 skipped
Shard 3642 / 5180 skipped
Shard 3643 / 5180 skipped
Shard 3644 / 5180 skipped
Shard 3645 / 5180 skipped
Shard 3646 / 5180 skipped
Shard 3647 / 5180 skipped
Shard 3648 / 5180 skipped
Shard 3649 / 5180 skipped
Shard 3650 / 5180 skipped
Shard 3651 / 5180 skipped
Shard 3652 / 5180 skipped
Shard 3653 / 5180 skipped
Shard 3654 / 5180 skipped
Shard 3655 / 5180 skipped
Shard 3656 / 5180 skipped
Shard 3657 / 5180 skipped
Shard 3658 / 5180 skipped
Shard 3659 / 5180 skipped
Shard 3660 / 5180 skipped
Shard 3661 / 5180 skipped
Shard 3662 / 5180 skipped
Shard 3663 / 5180 skipped
Shard 3664 / 5180 skipped
Shard 3665 / 5180 skipped
Shard 3666 / 5180 skipped
Shard 3667 / 5180 skipped
Shard 3668 / 5180 skipped
Shard 3669 / 5180 skipped
Shard 3670 / 5180 skipped
Shard 3671 / 5180 skipped
Shard 3672 / 5180 skipped
Shard 3673 / 5180 skipped
Shard 3674 / 5180 skipped
Shard 3675 / 5180 skipped
Shard 3676 / 5180 skipped
Shard 3677 / 5180 skipped
Shard 3678 / 5180 skipped
Shard 3679 / 5180 skipped
Shard 3680 / 5180 skipped
Shard 3681 / 5180 skipped
Shard 3682 / 5180 skipped
Shard 3683 / 5180 skipped
Shard 3684 / 5180 skipped
Shard 3685 / 5180 skipped
Shard 3686 / 5180 skipped
Shard 3687 / 5180 skipped
Shard 3688 / 5180 skipped
Shard 3689 / 5180 skipped
Shard 3690 / 5180 skipped
Shard 3691 / 5180 skipped
Shard 3692 / 5180 skipped
Shard 3693 / 5180 skipped
Shard 3694 / 5180 skipped
Shard 3695 / 5180 skipped
Shard 3696 / 5180 skipped
Shard 3697 / 5180 skipped
Shard 3698 / 5180 skipped
Shard 3699 / 5180 skipped
Shard 3700 / 5180 skipped
Shard 3701 / 5180 skipped
Shard 3702 / 5180 skipped
Shard 3703 / 5180 skipped
Shard 3704 / 5180 skipped
Shard 3705 / 5180 skipped
Shard 3706 / 5180 skipped
Shard 3707 / 5180 skipped
Shard 3708 / 5180 skipped
Shard 3709 / 5180 skipped
Shard 3710 / 5180 skipped
Shard 3711 / 5180 skipped
Shard 3712 / 5180 skipped
Shard 3713 / 5180 skipped
Shard 3714 / 5180 skipped
Shard 3715 / 5180 skipped
Shard 3716 / 5180 skipped
Shard 3717 / 5180 skipped
Shard 3718 / 5180 skipped
Shard 3719 / 5180 skipped
Shard 3720 / 5180 skipped
Shard 3721 / 5180 skipped
Shard 3722 / 5180 skipped
Shard 3723 / 5180 skipped
Shard 3724 / 5180 skipped
Shard 3725 / 5180 skipped
Shard 3726 / 5180 skipped
Shard 3727 / 5180 skipped
Shard 3728 / 5180 skipped
Shard 3729 / 5180 skipped
Shard 3730 / 5180 skipped
Shard 3731 / 5180 skipped
Shard 3732 / 5180 skipped
Shard 3733 / 5180 skipped
Shard 3734 / 5180 skipped
Shard 3735 / 5180 skipped
Shard 3736 / 5180 skipped
Shard 3737 / 5180 skipped
Shard 3738 / 5180 skipped
Shard 3739 / 5180 skipped
Shard 3740 / 5180 skipped
Shard 3741 / 5180 skipped
Shard 3742 / 5180 skipped
Shard 3743 / 5180 skipped
Shard 3744 / 5180 skipped
Shard 3745 / 5180 skipped
Shard 3746 / 5180 skipped
Shard 3747 / 5180 skipped
Shard 3748 / 5180 skipped
Shard 3749 / 5180 skipped
Shard 3750 / 5180 skipped
Shard 3751 / 5180 skipped
Shard 3752 / 5180 skipped
Shard 3753 / 5180 skipped
Shard 3754 / 5180 skipped
Shard 3755 / 5180 skipped
Shard 3756 / 5180 skipped
Shard 3757 / 5180 skipped
Shard 3758 / 5180 skipped
Shard 3759 / 5180 skipped
Shard 3760 / 5180 skipped
Shard 3761 / 5180 skipped
Shard 3762 / 5180 skipped
Shard 3763 / 5180 skipped
Shard 3764 / 5180 skipped
Shard 3765 / 5180 skipped
Shard 3766 / 5180 skipped
Shard 3767 / 5180 skipped
Shard 3768 / 5180 skipped
Shard 3769 / 5180 skipped
Shard 3770 / 5180 skipped
Shard 3771 / 5180 skipped
Shard 3772 / 5180 skipped
Shard 3773 / 5180 skipped
Shard 3774 / 5180 skipped
Shard 3775 / 5180 skipped
Shard 3776 / 5180 skipped
Shard 3777 / 5180 skipped
Shard 3778 / 5180 skipped
Shard 3779 / 5180 skipped
Shard 3780 / 5180 skipped
Shard 3781 / 5180 skipped
Shard 3782 / 5180 skipped
Shard 3783 / 5180 skipped
Shard 3784 / 5180 skipped
Shard 3785 / 5180 skipped
Shard 3786 / 5180 skipped
Shard 3787 / 5180 skipped
Shard 3788 / 5180 skipped
Shard 3789 / 5180 skipped
Shard 3790 / 5180 skipped
Shard 3791 / 5180 skipped
Shard 3792 / 5180 skipped
Shard 3793 / 5180 skipped
Shard 3794 / 5180 skipped
Shard 3795 / 5180 skipped
Shard 3796 / 5180 skipped
Shard 3797 / 5180 skipped
Shard 3798 / 5180 skipped
Shard 3799 / 5180 skipped
Shard 3800 / 5180 skipped
Shard 3801 / 5180 skipped
Shard 3802 / 5180 skipped
Shard 3803 / 5180 skipped
Shard 3804 / 5180 skipped
Shard 3805 / 5180 skipped
Shard 3806 / 5180 skipped
Shard 3807 / 5180 skipped
Shard 3808 / 5180 skipped
Shard 3809 / 5180 skipped
Shard 3810 / 5180 skipped
Shard 3811 / 5180 skipped
Shard 3812 / 5180 skipped
Shard 3813 / 5180 skipped
Shard 3814 / 5180 skipped
Shard 3815 / 5180 skipped
Shard 3816 / 5180 skipped
Shard 3817 / 5180 skipped
Shard 3818 / 5180 skipped
Shard 3819 / 5180 skipped
Shard 3820 / 5180 skipped
Shard 3821 / 5180 skipped
Shard 3822 / 5180 skipped
Shard 3823 / 5180 skipped
Shard 3824 / 5180 skipped
Shard 3825 / 5180 skipped
Shard 3826 / 5180 skipped
Shard 3827 / 5180 skipped
Shard 3828 / 5180 skipped
Shard 3829 / 5180 skipped
Shard 3830 / 5180 skipped
Shard 3831 / 5180 skipped
Shard 3832 / 5180 skipped
Shard 3833 / 5180 skipped
Shard 3834 / 5180 skipped
Shard 3835 / 5180 skipped
Shard 3836 / 5180 skipped
Shard 3837 / 5180 skipped
Shard 3838 / 5180 skipped
Shard 3839 / 5180 skipped
Shard 3840 / 5180 skipped
Shard 3841 / 5180 skipped
Shard 3842 / 5180 skipped
Shard 3843 / 5180 skipped
Shard 3844 / 5180 skipped
Shard 3845 / 5180 skipped
Shard 3846 / 5180 skipped
Shard 3847 / 5180 skipped
Shard 3848 / 5180 skipped
Shard 3849 / 5180 skipped
Shard 3850 / 5180 skipped
Shard 3851 / 5180 skipped
Shard 3852 / 5180 skipped
Shard 3853 / 5180 skipped
Shard 3854 / 5180 skipped
Shard 3855 / 5180 skipped
Shard 3856 / 5180 skipped
Shard 3857 / 5180 skipped
Shard 3858 / 5180 skipped
Shard 3859 / 5180 skipped
Shard 3860 / 5180 skipped
Shard 3861 / 5180 skipped
Shard 3862 / 5180 skipped
Shard 3863 / 5180 skipped
Shard 3864 / 5180 skipped
Shard 3865 / 5180 skipped
Shard 3866 / 5180 skipped
Shard 3867 / 5180 skipped
Shard 3868 / 5180 skipped
Shard 3869 / 5180 skipped
Shard 3870 / 5180 skipped
Shard 3871 / 5180 skipped
Shard 3872 / 5180 skipped
Shard 3873 / 5180 skipped
Shard 3874 / 5180 skipped
Shard 3875 / 5180 skipped
Shard 3876 / 5180 skipped
Shard 3877 / 5180 skipped
Shard 3878 / 5180 skipped
Shard 3879 / 5180 skipped
Shard 3880 / 5180 skipped
Shard 3881 / 5180 skipped
Shard 3882 / 5180 skipped
Shard 3883 / 5180 skipped
Shard 3884 / 5180 skipped
Shard 3885 / 5180 skipped
Shard 3886 / 5180 skipped
Shard 3887 / 5180 skipped
Shard 3888 / 5180 skipped
Shard 3889 / 5180 skipped
Shard 3890 / 5180 skipped
Shard 3891 / 5180 skipped
Shard 3892 / 5180 skipped
Shard 3893 / 5180 skipped
Shard 3894 / 5180 skipped
Shard 3895 / 5180 skipped
Shard 3896 / 5180 skipped
Shard 3897 / 5180 skipped
Shard 3898 / 5180 skipped
Shard 3899 / 5180 skipped
Shard 3900 / 5180 skipped
Shard 3901 / 5180 skipped
Shard 3902 / 5180 skipped
Shard 3903 / 5180 skipped
Shard 3904 / 5180 skipped
Shard 3905 / 5180 skipped
Shard 3906 / 5180 skipped
Shard 3907 / 5180 skipped
Shard 3908 / 5180 skipped
Shard 3909 / 5180 skipped
Shard 3910 / 5180 skipped
Shard 3911 / 5180 skipped
Shard 3912 / 5180 skipped
Shard 3913 / 5180 skipped
Shard 3914 / 5180 skipped
Shard 3915 / 5180 skipped
Shard 3916 / 5180 skipped
Shard 3917 / 5180 skipped
Shard 3918 / 5180 skipped
Shard 3919 / 5180 skipped
Shard 3920 / 5180 skipped
Shard 3921 / 5180 skipped
Shard 3922 / 5180 skipped
Shard 3923 / 5180 skipped
Shard 3924 / 5180 skipped
Shard 3925 / 5180 skipped
Shard 3926 / 5180 skipped
Shard 3927 / 5180 skipped
Shard 3928 / 5180 skipped
Shard 3929 / 5180 skipped
Shard 3930 / 5180 skipped
Shard 3931 / 5180 skipped
Shard 3932 / 5180 skipped
Shard 3933 / 5180 skipped
Shard 3934 / 5180 skipped
Shard 3935 / 5180 skipped
Shard 3936 / 5180 skipped
Shard 3937 / 5180 skipped
Shard 3938 / 5180 skipped
Shard 3939 / 5180 skipped
Shard 3940 / 5180 skipped
Shard 3941 / 5180 skipped
Shard 3942 / 5180 skipped
Shard 3943 / 5180 skipped
Shard 3944 / 5180 skipped
Shard 3945 / 5180 skipped
Shard 3946 / 5180 skipped
Shard 3947 / 5180 skipped
Shard 3948 / 5180 skipped
Shard 3949 / 5180 skipped
Shard 3950 / 5180 skipped
Shard 3951 / 5180 skipped
Shard 3952 / 5180 skipped
Shard 3953 / 5180 skipped
Shard 3954 / 5180 skipped
Shard 3955 / 5180 skipped
Shard 3956 / 5180 skipped
Shard 3957 / 5180 skipped
Shard 3958 / 5180 skipped
Shard 3959 / 5180 skipped
Shard 3960 / 5180 skipped
Shard 3961 / 5180 skipped
Shard 3962 / 5180 skipped
Shard 3963 / 5180 skipped
Shard 3964 / 5180 skipped
Shard 3965 / 5180 skipped
Shard 3966 / 5180 skipped
Shard 3967 / 5180 skipped
Shard 3968 / 5180 skipped
Shard 3969 / 5180 skipped
Shard 3970 / 5180 skipped
Shard 3971 / 5180 skipped
Shard 3972 / 5180 skipped
Shard 3973 / 5180 skipped
Shard 3974 / 5180 skipped
Shard 3975 / 5180 skipped
Shard 3976 / 5180 skipped
Shard 3977 / 5180 skipped
Shard 3978 / 5180 skipped
Shard 3979 / 5180 skipped
Shard 3980 / 5180 skipped
Shard 3981 / 5180 skipped
Shard 3982 / 5180 skipped
Shard 3983 / 5180 skipped
Shard 3984 / 5180 skipped
Shard 3985 / 5180 skipped
Shard 3986 / 5180 skipped
Shard 3987 / 5180 skipped
Shard 3988 / 5180 skipped
Shard 3989 / 5180 skipped
Shard 3990 / 5180 skipped
Shard 3991 / 5180 skipped
Shard 3992 / 5180 skipped
Shard 3993 / 5180 skipped
Shard 3994 / 5180 skipped
Shard 3995 / 5180 skipped
Shard 3996 / 5180 skipped
Shard 3997 / 5180 skipped
Shard 3998 / 5180 skipped
Shard 3999 / 5180 skipped
Shard 4000 / 5180 skipped
Shard 4001 / 5180 skipped
Shard 4002 / 5180 skipped
Shard 4003 / 5180 skipped
Shard 4004 / 5180 skipped
Shard 4005 / 5180 skipped
Shard 4006 / 5180 skipped
Shard 4007 / 5180 skipped
Shard 4008 / 5180 skipped
Shard 4009 / 5180 skipped
Shard 4010 / 5180 skipped
Shard 4011 / 5180 skipped
Shard 4012 / 5180 skipped
Shard 4013 / 5180 skipped
Shard 4014 / 5180 skipped
Shard 4015 / 5180 skipped
Shard 4016 / 5180 skipped
Shard 4017 / 5180 skipped
Shard 4018 / 5180 skipped
Shard 4019 / 5180 skipped
Shard 4020 / 5180 skipped
Shard 4021 / 5180 skipped
Shard 4022 / 5180 skipped
Shard 4023 / 5180 skipped
Shard 4024 / 5180 skipped
Shard 4025 / 5180 skipped
Shard 4026 / 5180 skipped
Shard 4027 / 5180 skipped
Shard 4028 / 5180 skipped
Shard 4029 / 5180 skipped
Shard 4030 / 5180 skipped
Shard 4031 / 5180 skipped
Shard 4032 / 5180 skipped
Shard 4033 / 5180 skipped
Shard 4034 / 5180 skipped
Shard 4035 / 5180 skipped
Shard 4036 / 5180 skipped
Shard 4037 / 5180 skipped
Shard 4038 / 5180 skipped
Shard 4039 / 5180 skipped
Shard 4040 / 5180 skipped
Shard 4041 / 5180 skipped
Shard 4042 / 5180 skipped
Shard 4043 / 5180 skipped
Shard 4044 / 5180 skipped
Shard 4045 / 5180 skipped
Shard 4046 / 5180 skipped
Shard 4047 / 5180 skipped
Shard 4048 / 5180 skipped
Shard 4049 / 5180 skipped
Shard 4050 / 5180 skipped
Shard 4051 / 5180 skipped
Shard 4052 / 5180 skipped
Shard 4053 / 5180 skipped
Shard 4054 / 5180 skipped
Shard 4055 / 5180 skipped
Shard 4056 / 5180 skipped
Shard 4057 / 5180 skipped
Shard 4058 / 5180 skipped
Shard 4059 / 5180 skipped
Shard 4060 / 5180 skipped
Shard 4061 / 5180 skipped
Shard 4062 / 5180 skipped
Shard 4063 / 5180 skipped
Shard 4064 / 5180 skipped
Shard 4065 / 5180 skipped
Shard 4066 / 5180 skipped
Shard 4067 / 5180 skipped
Shard 4068 / 5180 skipped
Shard 4069 / 5180 skipped
Shard 4070 / 5180 skipped
Shard 4071 / 5180 skipped
Shard 4072 / 5180 skipped
Shard 4073 / 5180 skipped
Shard 4074 / 5180 skipped
Shard 4075 / 5180 skipped
Shard 4076 / 5180 skipped
Shard 4077 / 5180 skipped
Shard 4078 / 5180 skipped
Shard 4079 / 5180 skipped
Shard 4080 / 5180 skipped
Shard 4081 / 5180 skipped
Shard 4082 / 5180 skipped
Shard 4083 / 5180 skipped
Shard 4084 / 5180 skipped
Shard 4085 / 5180 skipped
Shard 4086 / 5180 skipped
Shard 4087 / 5180 skipped
Shard 4088 / 5180 skipped
Shard 4089 / 5180 skipped
Shard 4090 / 5180 skipped
Shard 4091 / 5180 skipped
Shard 4092 / 5180 skipped
Shard 4093 / 5180 skipped
Shard 4094 / 5180 skipped
Shard 4095 / 5180 skipped
Shard 4096 / 5180 skipped
Shard 4097 / 5180 skipped
Shard 4098 / 5180 skipped
Shard 4099 / 5180 skipped
Shard 4100 / 5180 skipped
Shard 4101 / 5180 skipped
Shard 4102 / 5180 skipped
Shard 4103 / 5180 skipped
Shard 4104 / 5180 skipped
Shard 4105 / 5180 skipped
Shard 4106 / 5180 skipped
Shard 4107 / 5180 skipped
Shard 4108 / 5180 skipped
Shard 4109 / 5180 skipped
Shard 4110 / 5180 skipped
Shard 4111 / 5180 skipped
Shard 4112 / 5180 skipped
Shard 4113 / 5180 skipped
Shard 4114 / 5180 skipped
Shard 4115 / 5180 skipped
Shard 4116 / 5180 skipped
Shard 4117 / 5180 skipped
Shard 4118 / 5180 skipped
Shard 4119 / 5180 skipped
Shard 4120 / 5180 skipped
Shard 4121 / 5180 skipped
Shard 4122 / 5180 skipped
Shard 4123 / 5180 skipped
Shard 4124 / 5180 skipped
Shard 4125 / 5180 skipped
Shard 4126 / 5180 skipped
Shard 4127 / 5180 skipped
Shard 4128 / 5180 skipped
Shard 4129 / 5180 skipped
Shard 4130 / 5180 skipped
Shard 4131 / 5180 skipped
Shard 4132 / 5180 skipped
Shard 4133 / 5180 skipped
Shard 4134 / 5180 skipped
Shard 4135 / 5180 skipped
Shard 4136 / 5180 skipped
Shard 4137 / 5180 skipped
Shard 4138 / 5180 skipped
Shard 4139 / 5180 skipped
Shard 4140 / 5180 skipped
Shard 4141 / 5180 skipped
Shard 4142 / 5180 skipped
Shard 4143 / 5180 skipped
Shard 4144 / 5180 skipped
Shard 4145 / 5180 skipped
Shard 4146 / 5180 skipped
Shard 4147 / 5180 skipped
Shard 4148 / 5180 skipped
Shard 4149 / 5180 skipped
Shard 4150 / 5180 skipped
Shard 4151 / 5180 skipped
Shard 4152 / 5180 skipped
Shard 4153 / 5180 skipped
Shard 4154 / 5180 skipped
Shard 4155 / 5180 skipped
Shard 4156 / 5180 skipped
Shard 4157 / 5180 skipped
Shard 4158 / 5180 skipped
Shard 4159 / 5180 skipped
Shard 4160 / 5180 skipped
Shard 4161 / 5180 skipped
Shard 4162 / 5180 skipped
Shard 4163 / 5180 skipped
Shard 4164 / 5180 skipped
Shard 4165 / 5180 skipped
Shard 4166 / 5180 skipped
Shard 4167 / 5180 skipped
Shard 4168 / 5180 skipped
Shard 4169 / 5180 skipped
Shard 4170 / 5180 skipped
Shard 4171 / 5180 skipped
Shard 4172 / 5180 skipped
Shard 4173 / 5180 skipped
Shard 4174 / 5180 skipped
Shard 4175 / 5180 skipped
Shard 4176 / 5180 skipped
Shard 4177 / 5180 skipped
Shard 4178 / 5180 skipped
Shard 4179 / 5180 skipped
Shard 4180 / 5180 skipped
Shard 4181 / 5180 skipped
Shard 4182 / 5180 skipped
Shard 4183 / 5180 skipped
Shard 4184 / 5180 skipped
Shard 4185 / 5180 skipped
Shard 4186 / 5180 skipped
Shard 4187 / 5180 skipped
Shard 4188 / 5180 skipped
Shard 4189 / 5180 skipped
Shard 4190 / 5180 skipped
Shard 4191 / 5180 skipped
Shard 4192 / 5180 skipped
Shard 4193 / 5180 skipped
Shard 4194 / 5180 skipped
Shard 4195 / 5180 skipped
Shard 4196 / 5180 skipped
Shard 4197 / 5180 skipped
Shard 4198 / 5180 skipped
Shard 4199 / 5180 skipped
Shard 4200 / 5180 skipped
Shard 4201 / 5180 skipped
Shard 4202 / 5180 skipped
Shard 4203 / 5180 skipped
Shard 4204 / 5180 skipped
Shard 4205 / 5180 skipped
Shard 4206 / 5180 skipped
Shard 4207 / 5180 skipped
Shard 4208 / 5180 skipped
Shard 4209 / 5180 skipped
Shard 4210 / 5180 skipped
Shard 4211 / 5180 skipped
Shard 4212 / 5180 skipped
Shard 4213 / 5180 skipped
Shard 4214 / 5180 skipped
Shard 4215 / 5180 skipped
Shard 4216 / 5180 skipped
Shard 4217 / 5180 skipped
Shard 4218 / 5180 skipped
Shard 4219 / 5180 skipped
Shard 4220 / 5180 skipped
Shard 4221 / 5180 skipped
Shard 4222 / 5180 skipped
Shard 4223 / 5180 skipped
Shard 4224 / 5180 skipped
Shard 4225 / 5180 skipped
Shard 4226 / 5180 skipped
Shard 4227 / 5180 skipped
Shard 4228 / 5180 skipped
Shard 4229 / 5180 skipped
Shard 4230 / 5180 skipped
Shard 4231 / 5180 skipped
Shard 4232 / 5180 skipped
Shard 4233 / 5180 skipped
Shard 4234 / 5180 skipped
Shard 4235 / 5180 skipped
Shard 4236 / 5180 skipped
Shard 4237 / 5180 skipped
Shard 4238 / 5180 skipped
Shard 4239 / 5180 skipped
Shard 4240 / 5180 skipped
Shard 4241 / 5180 skipped
Shard 4242 / 5180 skipped
Shard 4243 / 5180 skipped
Shard 4244 / 5180 skipped
Shard 4245 / 5180 skipped
Shard 4246 / 5180 skipped
Shard 4247 / 5180 skipped
Shard 4248 / 5180 skipped
Shard 4249 / 5180 skipped
Shard 4250 / 5180 skipped
Shard 4251 / 5180 skipped
Shard 4252 / 5180 skipped
Shard 4253 / 5180 skipped
Shard 4254 / 5180 skipped
Shard 4255 / 5180 skipped
Shard 4256 / 5180 skipped
Shard 4257 / 5180 skipped
Shard 4258 / 5180 skipped
Shard 4259 / 5180 skipped
Shard 4260 / 5180 skipped
Shard 4261 / 5180 skipped
Shard 4262 / 5180 skipped
Shard 4263 / 5180 skipped
Shard 4264 / 5180 skipped
Shard 4265 / 5180 skipped
Shard 4266 / 5180 skipped
Shard 4267 / 5180 skipped
Shard 4268 / 5180 skipped
Shard 4269 / 5180 skipped
Shard 4270 / 5180 skipped
Shard 4271 / 5180 skipped
Shard 4272 / 5180 skipped
Shard 4273 / 5180 skipped
Shard 4274 / 5180 skipped
Shard 4275 / 5180 skipped
Shard 4276 / 5180 skipped
Shard 4277 / 5180 skipped
Shard 4278 / 5180 skipped
Shard 4279 / 5180 skipped
Shard 4280 / 5180 skipped
Shard 4281 / 5180 skipped
Shard 4282 / 5180 skipped
Shard 4283 / 5180 skipped
Shard 4284 / 5180 skipped
Shard 4285 / 5180 skipped
Shard 4286 / 5180 skipped
Shard 4287 / 5180 skipped
Shard 4288 / 5180 skipped
Shard 4289 / 5180 skipped
Shard 4290 / 5180 skipped
Shard 4291 / 5180 skipped
Shard 4292 / 5180 skipped
Shard 4293 / 5180 skipped
Shard 4294 / 5180 skipped
Shard 4295 / 5180 skipped
Shard 4296 / 5180 skipped
Shard 4297 / 5180 skipped
Shard 4298 / 5180 skipped
Shard 4299 / 5180 skipped
Shard 4300 / 5180 skipped
Shard 4301 / 5180 skipped
Shard 4302 / 5180 skipped
Shard 4303 / 5180 skipped
Shard 4304 / 5180 skipped
Shard 4305 / 5180 skipped
Shard 4306 / 5180 skipped
Shard 4307 / 5180 skipped
Shard 4308 / 5180 skipped
Shard 4309 / 5180 skipped
Shard 4310 / 5180 skipped
Shard 4311 / 5180 skipped
Shard 4312 / 5180 skipped
Shard 4313 / 5180 skipped
Shard 4314 / 5180 skipped
Shard 4315 / 5180 skipped
Shard 4316 / 5180 skipped
Shard 4317 / 5180 skipped
Shard 4318 / 5180 skipped
Shard 4319 / 5180 skipped
Shard 4320 / 5180 skipped
Shard 4321 / 5180 skipped
Shard 4322 / 5180 skipped
Shard 4323 / 5180 skipped
Shard 4324 / 5180 skipped
Shard 4325 / 5180 skipped
Shard 4326 / 5180 skipped
Shard 4327 / 5180 skipped
Shard 4328 / 5180 skipped
Shard 4329 / 5180 skipped
Shard 4330 / 5180 skipped
Shard 4331 / 5180 skipped
Shard 4332 / 5180 skipped
Shard 4333 / 5180 skipped
Shard 4334 / 5180 skipped
Shard 4335 / 5180 skipped
Shard 4336 / 5180 skipped
Shard 4337 / 5180 skipped
Shard 4338 / 5180 skipped
Shard 4339 / 5180 skipped
Shard 4340 / 5180 skipped
Shard 4341 / 5180 skipped
Shard 4342 / 5180 skipped
Shard 4343 / 5180 skipped
Shard 4344 / 5180 skipped
Shard 4345 / 5180 skipped
Shard 4346 / 5180 skipped
Shard 4347 / 5180 skipped
Shard 4348 / 5180 skipped
Shard 4349 / 5180 skipped
Shard 4350 / 5180 skipped
Shard 4351 / 5180 skipped
Shard 4352 / 5180 skipped
Shard 4353 / 5180 skipped
Shard 4354 / 5180 skipped
Shard 4355 / 5180 skipped
Shard 4356 / 5180 skipped
Shard 4357 / 5180 skipped
Shard 4358 / 5180 skipped
Shard 4359 / 5180 skipped
Shard 4360 / 5180 skipped
Shard 4361 / 5180 skipped
Shard 4362 / 5180 skipped
Shard 4363 / 5180 skipped
Shard 4364 / 5180 skipped
Shard 4365 / 5180 skipped
Shard 4366 / 5180 skipped
Shard 4367 / 5180 skipped
Shard 4368 / 5180 skipped
Shard 4369 / 5180 skipped
Shard 4370 / 5180 skipped
Shard 4371 / 5180 skipped
Shard 4372 / 5180 skipped
Shard 4373 / 5180 skipped
Shard 4374 / 5180 skipped
Shard 4375 / 5180 skipped
Shard 4376 / 5180 skipped
Shard 4377 / 5180 skipped
Shard 4378 / 5180 skipped
Shard 4379 / 5180 skipped
Shard 4380 / 5180 skipped
Shard 4381 / 5180 skipped
Shard 4382 / 5180 skipped
Shard 4383 / 5180 skipped
Shard 4384 / 5180 skipped
Shard 4385 / 5180 skipped
Shard 4386 / 5180 skipped
Shard 4387 / 5180 skipped
Shard 4388 / 5180 skipped
Shard 4389 / 5180 skipped
Shard 4390 / 5180 skipped
Shard 4391 / 5180 skipped
Shard 4392 / 5180 skipped
Shard 4393 / 5180 skipped
Shard 4394 / 5180 skipped
Shard 4395 / 5180 skipped
Shard 4396 / 5180 skipped
Shard 4397 / 5180 skipped
Shard 4398 / 5180 skipped
Shard 4399 / 5180 skipped
Shard 4400 / 5180 skipped
Shard 4401 / 5180 skipped
Shard 4402 / 5180 skipped
Shard 4403 / 5180 skipped
Shard 4404 / 5180 skipped
Shard 4405 / 5180 skipped
Shard 4406 / 5180 skipped
Shard 4407 / 5180 skipped
Shard 4408 / 5180 skipped
Shard 4409 / 5180 skipped
Shard 4410 / 5180 skipped
Shard 4411 / 5180 skipped
Shard 4412 / 5180 skipped
Shard 4413 / 5180 skipped
Shard 4414 / 5180 skipped
Shard 4415 / 5180 skipped
Shard 4416 / 5180 skipped
Shard 4417 / 5180 skipped
Shard 4418 / 5180 skipped
Shard 4419 / 5180 skipped
Shard 4420 / 5180 skipped
Shard 4421 / 5180 skipped
Shard 4422 / 5180 skipped
Shard 4423 / 5180 skipped
Shard 4424 / 5180 skipped
Shard 4425 / 5180 skipped
Shard 4426 / 5180 skipped
Shard 4427 / 5180 skipped
Shard 4428 / 5180 skipped
Shard 4429 / 5180 skipped
Shard 4430 / 5180 skipped
Shard 4431 / 5180 skipped
Shard 4432 / 5180 skipped
Shard 4433 / 5180 skipped
Shard 4434 / 5180 skipped
Shard 4435 / 5180 skipped
Shard 4436 / 5180 skipped
Shard 4437 / 5180 skipped
Shard 4438 / 5180 skipped
Shard 4439 / 5180 skipped
Shard 4440 / 5180 skipped
Shard 4441 / 5180 skipped
Shard 4442 / 5180 skipped
Shard 4443 / 5180 skipped
Shard 4444 / 5180 skipped
Shard 4445 / 5180 skipped
Shard 4446 / 5180 skipped
Shard 4447 / 5180 skipped
Shard 4448 / 5180 skipped
Shard 4449 / 5180 skipped
Shard 4450 / 5180 skipped
Shard 4451 / 5180 skipped
Shard 4452 / 5180 skipped
Shard 4453 / 5180 skipped
Shard 4454 / 5180 skipped
Shard 4455 / 5180 skipped
Shard 4456 / 5180 skipped
Shard 4457 / 5180 skipped
Shard 4458 / 5180 skipped
Shard 4459 / 5180 skipped
Shard 4460 / 5180 skipped
Shard 4461 / 5180 skipped
Shard 4462 / 5180 skipped
Shard 4463 / 5180 skipped
Shard 4464 / 5180 skipped
Shard 4465 / 5180 skipped
Shard 4466 / 5180 skipped
Shard 4467 / 5180 skipped
Shard 4468 / 5180 skipped
Shard 4469 / 5180 skipped
Shard 4470 / 5180 skipped
Shard 4471 / 5180 skipped
Shard 4472 / 5180 skipped
Shard 4473 / 5180 skipped
Shard 4474 / 5180 skipped
Shard 4475 / 5180 skipped
Shard 4476 / 5180 skipped
Shard 4477 / 5180 skipped
Shard 4478 / 5180 skipped
Shard 4479 / 5180 skipped
Shard 4480 / 5180 skipped
Shard 4481 / 5180 skipped
Shard 4482 / 5180 skipped
Shard 4483 / 5180 skipped
Shard 4484 / 5180 skipped
Shard 4485 / 5180 skipped
Shard 4486 / 5180 skipped
Shard 4487 / 5180 skipped
Shard 4488 / 5180 skipped
Shard 4489 / 5180 skipped
Shard 4490 / 5180 skipped
Shard 4491 / 5180 skipped
Shard 4492 / 5180 skipped
Shard 4493 / 5180 skipped
Shard 4494 / 5180 skipped
Shard 4495 / 5180 skipped
Shard 4496 / 5180 skipped
Shard 4497 / 5180 skipped
Shard 4498 / 5180 skipped
Shard 4499 / 5180 skipped
Shard 4500 / 5180 skipped
Shard 4501 / 5180 skipped
Shard 4502 / 5180 skipped
Shard 4503 / 5180 skipped
Shard 4504 / 5180 skipped
Shard 4505 / 5180 skipped
Shard 4506 / 5180 skipped
Shard 4507 / 5180 skipped
Shard 4508 / 5180 skipped
Shard 4509 / 5180 skipped
Shard 4510 / 5180 skipped
Shard 4511 / 5180 skipped
Shard 4512 / 5180 skipped
Shard 4513 / 5180 skipped
Shard 4514 / 5180 skipped
Shard 4515 / 5180 skipped
Shard 4516 / 5180 skipped
Shard 4517 / 5180 skipped
Shard 4518 / 5180 skipped
Shard 4519 / 5180 skipped
Shard 4520 / 5180 skipped
Shard 4521 / 5180 skipped
Shard 4522 / 5180 skipped
Shard 4523 / 5180 skipped
Shard 4524 / 5180 skipped
Shard 4525 / 5180 skipped
Shard 4526 / 5180 skipped
Shard 4527 / 5180 skipped
Shard 4528 / 5180 skipped
Shard 4529 / 5180 skipped
Shard 4530 / 5180 skipped
Shard 4531 / 5180 skipped
Shard 4532 / 5180 skipped
Shard 4533 / 5180 skipped
Shard 4534 / 5180 skipped
Shard 4535 / 5180 skipped
Shard 4536 / 5180 skipped
Shard 4537 / 5180 skipped
Shard 4538 / 5180 skipped
Shard 4539 / 5180 skipped
Shard 4540 / 5180 skipped
Shard 4541 / 5180 skipped
Shard 4542 / 5180 skipped
Shard 4543 / 5180 skipped
Shard 4544 / 5180 skipped
Shard 4545 / 5180 skipped
Shard 4546 / 5180 skipped
Shard 4547 / 5180 skipped
Shard 4548 / 5180 skipped
Shard 4549 / 5180 skipped
Shard 4550 / 5180 skipped
Shard 4551 / 5180 skipped
Shard 4552 / 5180 skipped
Shard 4553 / 5180 skipped
Shard 4554 / 5180 skipped
Shard 4555 / 5180 skipped
Shard 4556 / 5180 skipped
Shard 4557 / 5180 skipped
Shard 4558 / 5180 skipped
Shard 4559 / 5180 skipped
Shard 4560 / 5180 skipped
Shard 4561 / 5180 skipped
Shard 4562 / 5180 skipped
Shard 4563 / 5180 skipped
Shard 4564 / 5180 skipped
Shard 4565 / 5180 skipped
Shard 4566 / 5180 skipped
Shard 4567 / 5180 skipped
Shard 4568 / 5180 skipped
Shard 4569 / 5180 skipped
Shard 4570 / 5180 skipped
Shard 4571 / 5180 skipped
Shard 4572 / 5180 skipped
Shard 4573 / 5180 skipped
Shard 4574 / 5180 skipped
Shard 4575 / 5180 skipped
Shard 4576 / 5180 skipped
Shard 4577 / 5180 skipped
Shard 4578 / 5180 skipped
Shard 4579 / 5180 skipped
Shard 4580 / 5180 skipped
Shard 4581 / 5180 skipped
Shard 4582 / 5180 skipped
Shard 4583 / 5180 skipped
Shard 4584 / 5180 skipped
Shard 4585 / 5180 skipped
Shard 4586 / 5180 skipped
Shard 4587 / 5180 skipped
Shard 4588 / 5180 skipped
Shard 4589 / 5180 skipped
Shard 4590 / 5180 skipped
Shard 4591 / 5180 skipped
Shard 4592 / 5180 skipped
Shard 4593 / 5180 skipped
Shard 4594 / 5180 skipped
Shard 4595 / 5180 skipped
Shard 4596 / 5180 skipped
Shard 4597 / 5180 skipped
Shard 4598 / 5180 skipped
Shard 4599 / 5180 skipped
Shard 4600 / 5180 skipped
Shard 4601 / 5180 skipped
Shard 4602 / 5180 skipped
Shard 4603 / 5180 skipped
Shard 4604 / 5180 skipped
Shard 4605 / 5180 skipped
Shard 4606 / 5180 skipped
Shard 4607 / 5180 skipped
Shard 4608 / 5180 skipped
Shard 4609 / 5180 skipped
Shard 4610 / 5180 skipped
Shard 4611 / 5180 skipped
Shard 4612 / 5180 skipped
Shard 4613 / 5180 skipped
Shard 4614 / 5180 skipped
Shard 4615 / 5180 skipped
Shard 4616 / 5180 skipped
Shard 4617 / 5180 skipped
Shard 4618 / 5180 skipped
Shard 4619 / 5180 skipped
Shard 4620 / 5180 skipped
Shard 4621 / 5180 skipped
Shard 4622 / 5180 skipped
Shard 4623 / 5180 skipped
Shard 4624 / 5180 skipped
Shard 4625 / 5180 skipped
Shard 4626 / 5180 skipped
Shard 4627 / 5180 skipped
Shard 4628 / 5180 skipped
Shard 4629 / 5180 skipped
Shard 4630 / 5180 skipped
Shard 4631 / 5180 skipped
Shard 4632 / 5180 skipped
Shard 4633 / 5180 skipped
Shard 4634 / 5180 skipped
Shard 4635 / 5180 skipped
Shard 4636 / 5180 skipped
Shard 4637 / 5180 skipped
Shard 4638 / 5180 skipped
Shard 4639 / 5180 skipped
Shard 4640 / 5180 skipped
Shard 4641 / 5180 skipped
Shard 4642 / 5180 skipped
Shard 4643 / 5180 skipped
Shard 4644 / 5180 skipped
Shard 4645 / 5180 skipped
Shard 4646 / 5180 skipped
Shard 4647 / 5180 skipped
Shard 4648 / 5180 skipped
Shard 4649 / 5180 skipped
Shard 4650 / 5180 skipped
Shard 4651 / 5180 skipped
Shard 4652 / 5180 skipped
Shard 4653 / 5180 skipped
Shard 4654 / 5180 skipped
Shard 4655 / 5180 skipped
Shard 4656 / 5180 skipped
Shard 4657 / 5180 skipped
Shard 4658 / 5180 skipped
Shard 4659 / 5180 skipped
Shard 4660 / 5180 skipped
Shard 4661 / 5180 skipped
Shard 4662 / 5180 skipped
Shard 4663 / 5180 skipped
Shard 4664 / 5180 skipped
Shard 4665 / 5180 skipped
Shard 4666 / 5180 skipped
Shard 4667 / 5180 skipped
Shard 4668 / 5180 skipped
Shard 4669 / 5180 skipped
Shard 4670 / 5180 skipped
Shard 4671 / 5180 skipped
Shard 4672 / 5180 skipped
Shard 4673 / 5180 skipped
Shard 4674 / 5180 skipped
Shard 4675 / 5180 skipped
Shard 4676 / 5180 skipped
Shard 4677 / 5180 skipped
Shard 4678 / 5180 skipped
Shard 4679 / 5180 skipped
Shard 4680 / 5180 skipped
Shard 4681 / 5180 skipped
Shard 4682 / 5180 skipped
Shard 4683 / 5180 skipped
Shard 4684 / 5180 skipped
Shard 4685 / 5180 skipped
Shard 4686 / 5180 skipped
Shard 4687 / 5180 skipped
Shard 4688 / 5180 skipped
Shard 4689 / 5180 skipped
Shard 4690 / 5180 skipped
Shard 4691 / 5180 skipped
Shard 4692 / 5180 skipped
Shard 4693 / 5180 skipped
Shard 4694 / 5180 skipped
Shard 4695 / 5180 skipped
Shard 4696 / 5180 skipped
Shard 4697 / 5180 skipped
Shard 4698 / 5180 skipped
Shard 4699 / 5180 skipped
Shard 4700 / 5180 skipped
Shard 4701 / 5180 skipped
Shard 4702 / 5180 skipped
Shard 4703 / 5180 skipped
Shard 4704 / 5180 skipped
Shard 4705 / 5180 skipped
Shard 4706 / 5180 skipped
Shard 4707 / 5180 skipped
Shard 4708 / 5180 skipped
Shard 4709 / 5180 skipped
Shard 4710 / 5180 skipped
Shard 4711 / 5180 skipped
Shard 4712 / 5180 skipped
Shard 4713 / 5180 skipped
Shard 4714 / 5180 skipped
Shard 4715 / 5180 skipped
Shard 4716 / 5180 skipped
Shard 4717 / 5180 skipped
Shard 4718 / 5180 skipped
Shard 4719 / 5180 skipped
Shard 4720 / 5180 skipped
Shard 4721 / 5180 skipped
Shard 4722 / 5180 skipped
Shard 4723 / 5180 skipped
Shard 4724 / 5180 skipped
Shard 4725 / 5180 skipped
Shard 4726 / 5180 skipped
Shard 4727 / 5180 skipped
Shard 4728 / 5180 skipped
Shard 4729 / 5180 skipped
Shard 4730 / 5180 skipped
Shard 4731 / 5180 skipped
Shard 4732 / 5180 skipped
Shard 4733 / 5180 skipped
Shard 4734 / 5180 skipped
Shard 4735 / 5180 skipped
Shard 4736 / 5180 skipped
Shard 4737 / 5180 skipped
Shard 4738 / 5180 skipped
Shard 4739 / 5180 skipped
Shard 4740 / 5180 skipped
Shard 4741 / 5180 skipped
Shard 4742 / 5180 skipped
Shard 4743 / 5180 skipped
Shard 4744 / 5180 skipped
Shard 4745 / 5180 skipped
Shard 4746 / 5180 skipped
Shard 4747 / 5180 skipped
Shard 4748 / 5180 skipped
Shard 4749 / 5180 skipped
Shard 4750 / 5180 skipped
Shard 4751 / 5180 skipped
Shard 4752 / 5180 skipped
Shard 4753 / 5180 skipped
Shard 4754 / 5180 skipped
Shard 4755 / 5180 skipped
Shard 4756 / 5180 skipped
Shard 4757 / 5180 skipped
Shard 4758 / 5180 skipped
Shard 4759 / 5180 skipped
Shard 4760 / 5180 skipped
Shard 4761 / 5180 skipped
Shard 4762 / 5180 skipped
Shard 4763 / 5180 skipped
Shard 4764 / 5180 skipped
Shard 4765 / 5180 skipped
Shard 4766 / 5180 skipped
Shard 4767 / 5180 skipped
Shard 4768 / 5180 skipped
Shard 4769 / 5180 skipped
Shard 4770 / 5180 skipped
Shard 4771 / 5180 skipped
Shard 4772 / 5180 skipped
Shard 4773 / 5180 skipped
Shard 4774 / 5180 skipped
Shard 4775 / 5180 skipped
Shard 4776 / 5180 skipped
Shard 4777 / 5180 skipped
Shard 4778 / 5180 skipped
Shard 4779 / 5180 skipped
Shard 4780 / 5180 skipped
Shard 4781 / 5180 skipped
Shard 4782 / 5180 skipped
Shard 4783 / 5180 skipped
Shard 4784 / 5180 skipped
Shard 4785 / 5180 skipped
Shard 4786 / 5180 skipped
Shard 4787 / 5180 skipped
Shard 4788 / 5180 skipped
Shard 4789 / 5180 skipped
Shard 4790 / 5180 skipped
Shard 4791 / 5180 skipped
Shard 4792 / 5180 skipped
Shard 4793 / 5180 skipped
Shard 4794 / 5180 skipped
Shard 4795 / 5180 skipped
Shard 4796 / 5180 skipped
Shard 4797 / 5180 skipped
Shard 4798 / 5180 skipped
Shard 4799 / 5180 skipped
Shard 4800 / 5180 skipped
Shard 4801 / 5180 skipped
Shard 4802 / 5180 skipped
Shard 4803 / 5180 skipped
Shard 4804 / 5180 skipped
Shard 4805 / 5180 skipped
Shard 4806 / 5180 skipped
Shard 4807 / 5180 skipped
Shard 4808 / 5180 skipped
Shard 4809 / 5180 skipped
Shard 4810 / 5180 skipped
Shard 4811 / 5180 skipped
Shard 4812 / 5180 skipped
Shard 4813 / 5180 skipped
Shard 4814 / 5180 skipped
Shard 4815 / 5180 skipped
Shard 4816 / 5180 skipped
Shard 4817 / 5180 skipped
Shard 4818 / 5180 skipped
Shard 4819 / 5180 skipped
Shard 4820 / 5180 skipped
Shard 4821 / 5180 skipped
Shard 4822 / 5180 skipped
Shard 4823 / 5180 skipped
Shard 4824 / 5180 skipped
Shard 4825 / 5180 skipped
Shard 4826 / 5180 skipped
Shard 4827 / 5180 skipped
Shard 4828 / 5180 skipped
Shard 4829 / 5180 skipped
Shard 4830 / 5180 skipped
Shard 4831 / 5180 skipped
Shard 4832 / 5180 skipped
Shard 4833 / 5180 skipped
Shard 4834 / 5180 skipped
Shard 4835 / 5180 skipped
Shard 4836 / 5180 skipped
Shard 4837 / 5180 skipped
Shard 4838 / 5180 skipped
Shard 4839 / 5180 skipped
Shard 4840 / 5180 skipped
Shard 4841 / 5180 skipped
Shard 4842 / 5180 skipped
Shard 4843 / 5180 skipped
Shard 4844 / 5180 skipped
Shard 4845 / 5180 skipped
Shard 4846 / 5180 skipped
Shard 4847 / 5180 skipped
Shard 4848 / 5180 skipped
Shard 4849 / 5180 skipped
Shard 4850 / 5180 skipped
Shard 4851 / 5180 skipped
Shard 4852 / 5180 skipped
Shard 4853 / 5180 skipped
Shard 4854 / 5180 skipped
Shard 4855 / 5180 skipped
Shard 4856 / 5180 skipped
Shard 4857 / 5180 skipped
Shard 4858 / 5180 skipped
Shard 4859 / 5180 skipped
Shard 4860 / 5180 skipped
Shard 4861 / 5180 skipped
Shard 4862 / 5180 skipped
Shard 4863 / 5180 skipped
Shard 4864 / 5180 skipped
Shard 4865 / 5180 skipped
Shard 4866 / 5180 skipped
Shard 4867 / 5180 skipped
Shard 4868 / 5180 skipped
Shard 4869 / 5180 skipped
Shard 4870 / 5180 skipped
Shard 4871 / 5180 skipped
Shard 4872 / 5180 skipped
Shard 4873 / 5180 skipped
Shard 4874 / 5180 skipped
Shard 4875 / 5180 skipped
Shard 4876 / 5180 skipped
Shard 4877 / 5180 skipped
Shard 4878 / 5180 skipped
Shard 4879 / 5180 skipped
Shard 4880 / 5180 skipped
Shard 4881 / 5180 skipped
Shard 4882 / 5180 skipped
Shard 4883 / 5180 skipped
Shard 4884 / 5180 skipped
Shard 4885 / 5180 skipped
Shard 4886 / 5180 skipped
Shard 4887 / 5180 skipped
Shard 4888 / 5180 skipped
Shard 4889 / 5180 skipped
Shard 4890 / 5180 skipped
Shard 4891 / 5180 skipped
Shard 4892 / 5180 skipped
Shard 4893 / 5180 skipped
Shard 4894 / 5180 skipped
Shard 4895 / 5180 skipped
Shard 4896 / 5180 skipped
Shard 4897 / 5180 skipped
Shard 4898 / 5180 skipped
Shard 4899 / 5180 skipped
Shard 4900 / 5180 skipped
Shard 4901 / 5180 skipped
Shard 4902 / 5180 skipped
Shard 4903 / 5180 skipped
Shard 4904 / 5180 skipped
Shard 4905 / 5180 skipped
Shard 4906 / 5180 skipped
Shard 4907 / 5180 skipped
Shard 4908 / 5180 skipped
Shard 4909 / 5180 skipped
Shard 4910 / 5180 skipped
Shard 4911 / 5180 skipped
Shard 4912 / 5180 skipped
Shard 4913 / 5180 skipped
Shard 4914 / 5180 skipped
Shard 4915 / 5180 skipped
Shard 4916 / 5180 skipped
Shard 4917 / 5180 skipped
Shard 4918 / 5180 skipped
Shard 4919 / 5180 skipped
Shard 4920 / 5180 skipped
Shard 4921 / 5180 skipped
Shard 4922 / 5180 skipped
Shard 4923 / 5180 skipped
Shard 4924 / 5180 skipped
Shard 4925 / 5180 skipped
Shard 4926 / 5180 skipped
Shard 4927 / 5180 skipped
Shard 4928 / 5180 skipped
Shard 4929 / 5180 skipped
Shard 4930 / 5180 skipped
Shard 4931 / 5180 skipped
Shard 4932 / 5180 skipped
Shard 4933 / 5180 skipped
Shard 4934 / 5180 skipped
Shard 4935 / 5180 skipped
Shard 4936 / 5180 skipped
Shard 4937 / 5180 skipped
Shard 4938 / 5180 skipped
Shard 4939 / 5180 skipped
Shard 4940 / 5180 skipped
Shard 4941 / 5180 skipped
Shard 4942 / 5180 skipped
Shard 4943 / 5180 skipped
Shard 4944 / 5180 skipped
Shard 4945 / 5180 skipped
Shard 4946 / 5180 skipped
Shard 4947 / 5180 skipped
Shard 4948 / 5180 skipped
Shard 4949 / 5180 skipped
Shard 4950 / 5180 skipped
Shard 4951 / 5180 skipped
Shard 4952 / 5180 skipped
Shard 4953 / 5180 skipped
Shard 4954 / 5180 skipped
Shard 4955 / 5180 skipped
Shard 4956 / 5180 skipped
Shard 4957 / 5180 skipped
Shard 4958 / 5180 skipped
Shard 4959 / 5180 skipped
Shard 4960 / 5180 skipped
Shard 4961 / 5180 skipped
Shard 4962 / 5180 skipped
Shard 4963 / 5180 skipped
Shard 4964 / 5180 skipped
Shard 4965 / 5180 skipped
Shard 4966 / 5180 skipped
Shard 4967 / 5180 skipped
Shard 4968 / 5180 skipped
Shard 4969 / 5180 skipped
Shard 4970 / 5180 skipped
Shard 4971 / 5180 skipped
Shard 4972 / 5180 skipped
Shard 4973 / 5180 skipped
Shard 4974 / 5180 skipped
Shard 4975 / 5180 skipped
Shard 4976 / 5180 skipped
Shard 4977 / 5180 skipped
Shard 4978 / 5180 skipped
Shard 4979 / 5180 skipped
Shard 4980 / 5180 skipped
Shard 4981 / 5180 skipped
Shard 4982 / 5180 skipped
Shard 4983 / 5180 skipped
Shard 4984 / 5180 skipped
Shard 4985 / 5180 skipped
Shard 4986 / 5180 skipped
Shard 4987 / 5180 skipped
Shard 4988 / 5180 skipped
Shard 4989 / 5180 skipped
Shard 4990 / 5180 skipped
Shard 4991 / 5180 skipped
Shard 4992 / 5180 skipped
Shard 4993 / 5180 skipped
Shard 4994 / 5180 skipped
Shard 4995 / 5180 skipped
Shard 4996 / 5180 skipped
Shard 4997 / 5180 skipped
Shard 4998 / 5180 skipped
Shard 4999 / 5180 skipped
Shard 5000 / 5180 skipped
Shard 5001 / 5180 skipped
Shard 5002 / 5180 skipped
Shard 5003 / 5180 skipped
Shard 5004 / 5180 skipped
Shard 5005 / 5180 skipped
Shard 5006 / 5180 skipped
Shard 5007 / 5180 skipped
Shard 5008 / 5180 skipped
Shard 5009 / 5180 skipped
Shard 5010 / 5180 skipped
Shard 5011 / 5180 skipped
Shard 5012 / 5180 skipped
Shard 5013 / 5180 skipped
Shard 5014 / 5180 skipped
Shard 5015 / 5180 skipped
Shard 5016 / 5180 skipped
Shard 5017 / 5180 skipped
Shard 5018 / 5180 skipped
Shard 5019 / 5180 skipped
Shard 5020 / 5180 skipped
Shard 5021 / 5180 skipped
Shard 5022 / 5180 skipped
Shard 5023 / 5180 skipped
Shard 5024 / 5180 skipped
Shard 5025 / 5180 skipped
Shard 5026 / 5180 skipped
Shard 5027 / 5180 skipped
Shard 5028 / 5180 skipped
Shard 5029 / 5180 skipped
Shard 5030 / 5180 skipped
Shard 5031 / 5180 skipped
Shard 5032 / 5180 skipped
Shard 5033 / 5180 skipped
Shard 5034 / 5180 skipped
Shard 5035 / 5180 skipped
Shard 5036 / 5180 skipped
Shard 5037 / 5180 skipped
Shard 5038 / 5180 skipped
Shard 5039 / 5180 skipped
Shard 5040 / 5180 skipped
Shard 5041 / 5180 skipped
Shard 5042 / 5180 skipped
Shard 5043 / 5180 skipped
Shard 5044 / 5180 skipped
Shard 5045 / 5180 skipped
Shard 5046 / 5180 skipped
Shard 5047 / 5180 skipped
Shard 5048 / 5180 skipped
Shard 5049 / 5180 skipped
Shard 5050 / 5180 skipped
Shard 5051 / 5180 skipped
Shard 5052 / 5180 skipped
Shard 5053 / 5180 skipped
Shard 5054 / 5180 skipped
Shard 5055 / 5180 skipped
Shard 5056 / 5180 skipped
Shard 5057 / 5180 skipped
Shard 5058 / 5180 skipped
Shard 5059 / 5180 skipped
Shard 5060 / 5180 skipped
Shard 5061 / 5180 skipped
Shard 5062 / 5180 skipped
Shard 5063 / 5180 skipped
Shard 5064 / 5180 skipped
Shard 5065 / 5180 skipped
Shard 5066 / 5180 skipped
Shard 5067 / 5180 skipped
Shard 5068 / 5180 skipped
Shard 5069 / 5180 skipped
Shard 5070 / 5180 skipped
Shard 5071 / 5180 skipped
Shard 5072 / 5180 skipped
Shard 5073 / 5180 skipped
Shard 5074 / 5180 skipped
Shard 5075 / 5180 skipped
Shard 5076 / 5180 skipped
Shard 5077 / 5180 skipped
Shard 5078 / 5180 skipped
Shard 5079 / 5180 skipped
Shard 5080 / 5180 skipped
Shard 5081 / 5180 skipped
Shard 5082 / 5180 skipped
Shard 5083 / 5180 skipped
Shard 5084 / 5180 skipped
Shard 5085 / 5180 skipped
Shard 5086 / 5180 skipped
Shard 5087 / 5180 skipped
Shard 5088 / 5180 skipped
Shard 5089 / 5180 skipped
Shard 5090 / 5180 skipped
Shard 5091 / 5180 skipped
Shard 5092 / 5180 skipped
Shard 5093 / 5180 skipped
Shard 5094 / 5180 skipped
Shard 5095 / 5180 skipped
Shard 5096 / 5180 skipped
Shard 5097 / 5180 skipped
Shard 5098 / 5180 skipped
Shard 5099 / 5180 skipped
Shard 5100 / 5180 skipped
Shard 5101 / 5180 skipped
Shard 5102 / 5180 skipped
Shard 5103 / 5180 skipped
Shard 5104 / 5180 skipped
Shard 5105 / 5180 skipped
Shard 5106 / 5180 skipped
Shard 5107 / 5180 skipped
Shard 5108 / 5180 skipped
Shard 5109 / 5180 skipped
Shard 5110 / 5180 skipped
Shard 5111 / 5180 skipped
Shard 5112 / 5180 skipped
Shard 5113 / 5180 skipped
Shard 5114 / 5180 skipped
Shard 5115 / 5180 skipped
Shard 5116 / 5180 skipped
Shard 5117 / 5180 skipped
Shard 5118 / 5180 skipped
Shard 5119 / 5180 skipped
Shard 5120 / 5180 skipped
Shard 5121 / 5180 skipped
Shard 5122 / 5180 skipped
Shard 5123 / 5180 skipped
Shard 5124 / 5180 skipped
Shard 5125 / 5180 skipped
Shard 5126 / 5180 skipped
Shard 5127 / 5180 skipped
Shard 5128 / 5180 skipped
Shard 5129 / 5180 skipped
Shard 5130 / 5180 skipped
Shard 5131 / 5180 skipped
Shard 5132 / 5180 skipped
Shard 5133 / 5180 skipped
Shard 5134 / 5180 skipped
Shard 5135 / 5180 skipped
Shard 5136 / 5180 skipped
Shard 5137 / 5180 skipped
Shard 5138 / 5180 skipped
Shard 5139 / 5180 skipped
Shard 5140 / 5180 skipped
Shard 5141 / 5180 skipped
Shard 5142 / 5180 skipped
Shard 5143 / 5180 skipped
Shard 5144 / 5180 skipped
Shard 5145 / 5180 skipped
Shard 5146 / 5180 skipped
Shard 5147 / 5180 skipped
Shard 5148 / 5180 skipped
Shard 5149 / 5180 skipped
Shard 5150 / 5180 skipped
Shard 5151 / 5180 skipped
Shard 5152 / 5180 skipped
Shard 5153 / 5180 skipped
Shard 5154 / 5180 skipped
Shard 5155 / 5180 skipped
Shard 5156 / 5180 skipped
Shard 5157 / 5180 skipped
Shard 5158 / 5180 skipped
Shard 5159 / 5180 skipped
Shard 5160 / 5180 skipped
Shard 5161 / 5180 skipped
Shard 5162 / 5180 skipped
Shard 5163 / 5180 skipped
Shard 5164 / 5180 skipped
Shard 5165 / 5180 skipped
Shard 5166 / 5180 skipped
Shard 5167 / 5180 skipped
Shard 5168 / 5180 skipped
Shard 5169 / 5180 skipped
Shard 5170 / 5180 skipped
Shard 5171 / 5180 skipped
Shard 5172 / 5180 skipped
Shard 5173 / 5180 skipped
Shard 5174 / 5180 skipped
Shard 5175 / 5180 skipped
Shard 5176 / 5180 skipped
Shard 5177 / 5180 skipped
Shard 5178 / 5180 skipped
Shard 5179 / 5180 skipped
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4862
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 11:54:46,819] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_08-model_states.pt.
[2024-06-23 11:54:47,583] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_08-model_states.pt...
[2024-06-23 11:54:48,285] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_03-model_states.pt.
[2024-06-23 11:54:49,106] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_03-model_states.pt...
[2024-06-23 11:55:01,195] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_07-model_states.pt.
[2024-06-23 11:55:01,303] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_05-model_states.pt.
[2024-06-23 11:55:01,889] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_07-model_states.pt...
[2024-06-23 11:55:02,038] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_05-model_states.pt...
[2024-06-23 11:55:36,960] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_02-model_states.pt.
[2024-06-23 11:55:37,464] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_02-model_states.pt...
[2024-06-23 11:55:56,479] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_03-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 11:56:20,843] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_05-model_states.pt.
[2024-06-23 11:56:21,865] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_02-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 11:56:33,307] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_08-model_states.pt.
[2024-06-23 11:56:34,747] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_09-model_states.pt...
[2024-06-23 11:56:34,846] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_09-model_states.pt.
[2024-06-23 11:56:34,847] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_09-model_states.pt...
[2024-06-23 11:56:34,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_09-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 11:56:35,730] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5168/layer_07-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 11:58:21,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=5169, skipped=0, lr=[4.815532445665026e-07], mom=[(0.9, 0.999)]
steps: 5169 loss: 0.6072 iter time (s): 222.246 samples/sec: 0.576

100%|██████████| 1/1 [01:51<00:00, 111.16s/it][A100%|██████████| 1/1 [01:51<00:00, 111.16s/it]
100%|█████████▉| 5181/5198 [01:51<00:00, 46.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.07s/it][A100%|██████████| 1/1 [01:49<00:00, 109.07s/it]
100%|█████████▉| 5181/5198 [01:49<00:00, 47.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [03:42<00:00, 222.02s/it][A100%|██████████| 1/1 [03:42<00:00, 222.02s/it]
100%|█████████▉| 5181/5198 [03:42<00:00, 23.33it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:04<00:00, 124.55s/it][A100%|██████████| 1/1 [02:04<00:00, 124.56s/it]
100%|█████████▉| 5181/5198 [02:04<00:00, 41.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [03:46<00:00, 226.68s/it][A100%|██████████| 1/1 [03:46<00:00, 226.68s/it]
100%|█████████▉| 5181/5198 [03:46<00:00, 22.84it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:28<00:00, 148.01s/it][A100%|██████████| 1/1 [02:28<00:00, 148.01s/it]
100%|█████████▉| 5181/5198 [02:28<00:00, 35.00it/s]
100%|██████████| 1/1 [02:03<00:00, 123.34s/it][A100%|██████████| 1/1 [02:03<00:00, 123.34s/it]
100%|█████████▉| 5181/5198 [02:03<00:00, 42.00it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [03:43<00:00, 223.05s/it][A100%|██████████| 1/1 [03:43<00:00, 223.05s/it]
100%|█████████▉| 5181/5198 [03:43<00:00, 23.23it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4863
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A100%|█████████▉| 5181/5198 [03:52<00:00, 23.33it/s]100%|█████████▉| 5181/5198 [02:00<00:00, 47.50it/s]100%|█████████▉| 5181/5198 [03:58<00:00, 22.84it/s]100%|█████████▉| 5181/5198 [02:40<00:00, 35.00it/s]100%|█████████▉| 5181/5198 [03:55<00:00, 23.23it/s]100%|█████████▉| 5181/5198 [02:20<00:00, 41.59it/s]100%|█████████▉| 5181/5198 [02:20<00:00, 42.00it/s]100%|█████████▉| 5181/5198 [02:09<00:00, 46.61it/s]
100%|██████████| 1/1 [01:52<00:00, 112.38s/it][A100%|██████████| 1/1 [01:52<00:00, 112.38s/it]
[2024-06-23 12:00:21,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=5170, skipped=0, lr=[4.798218655929169e-07], mom=[(0.9, 0.999)]
steps: 5170 loss: 0.5854 iter time (s): 114.450 samples/sec: 1.118

100%|██████████| 1/1 [01:55<00:00, 115.33s/it][A100%|██████████| 1/1 [01:55<00:00, 115.33s/it]

100%|██████████| 1/1 [01:55<00:00, 115.22s/it][A100%|██████████| 1/1 [01:55<00:00, 115.22s/it]

100%|██████████| 1/1 [01:55<00:00, 115.31s/it][A100%|██████████| 1/1 [01:55<00:00, 115.31s/it]

100%|██████████| 1/1 [01:55<00:00, 115.19s/it][A100%|██████████| 1/1 [01:55<00:00, 115.19s/it]

100%|██████████| 1/1 [01:55<00:00, 115.23s/it][A100%|██████████| 1/1 [01:55<00:00, 115.23s/it]

100%|██████████| 1/1 [01:55<00:00, 115.27s/it][A100%|██████████| 1/1 [01:55<00:00, 115.27s/it]

100%|██████████| 1/1 [01:55<00:00, 115.27s/it][A100%|██████████| 1/1 [01:55<00:00, 115.27s/it]
Checkpointing at shard 5181
[2024-06-23 12:00:26,969] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5170 is about to be saved!
[2024-06-23 12:00:27,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_00-model_states.pt...
[2024-06-23 12:00:32,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_00-model_states.pt.
[2024-06-23 12:00:32,800] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_02-model_states.pt...
[2024-06-23 12:00:33,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_07-model_states.pt...
[2024-06-23 12:00:33,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_08-model_states.pt...
[2024-06-23 12:00:39,056] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_04-model_states.pt...
[2024-06-23 12:00:39,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_03-model_states.pt...
[2024-06-23 12:00:41,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_05-model_states.pt...
[2024-06-23 12:00:41,915] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_06-model_states.pt...
[2024-06-23 12:00:43,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_01-model_states.pt...
[2024-06-23 12:04:35,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_04-model_states.pt.
[2024-06-23 12:04:35,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_02-model_states.pt.
[2024-06-23 12:04:35,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_06-model_states.pt.
[2024-06-23 12:04:35,605] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_05-model_states.pt.
[2024-06-23 12:04:35,620] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_08-model_states.pt.
[2024-06-23 12:04:35,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_03-model_states.pt.
[2024-06-23 12:04:35,687] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_07-model_states.pt.
[2024-06-23 12:04:36,023] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_01_model_states.pt
[2024-06-23 12:04:36,024] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_01_model_states.pt...
[2024-06-23 12:04:36,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_03_model_states.pt...
[2024-06-23 12:04:36,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_01_model_states.pt.
[2024-06-23 12:04:36,108] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
[2024-06-23 12:04:36,125] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_06_model_states.pt...
[2024-06-23 12:04:36,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_03_model_states.pt.
[2024-06-23 12:04:36,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
[2024-06-23 12:04:36,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_06_model_states.pt.
[2024-06-23 12:04:36,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
[2024-06-23 12:04:36,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_09-model_states.pt...
[2024-06-23 12:04:36,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_05_model_states.pt...
[2024-06-23 12:04:36,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_05_model_states.pt.
[2024-06-23 12:04:36,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
[2024-06-23 12:04:36,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_04_model_states.pt...
[2024-06-23 12:04:36,811] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_02_model_states.pt...
[2024-06-23 12:04:36,821] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_04_model_states.pt.
[2024-06-23 12:04:36,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
[2024-06-23 12:04:36,926] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_02_model_states.pt.
[2024-06-23 12:04:36,926] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
[2024-06-23 12:04:37,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_09-model_states.pt.
[2024-06-23 12:04:37,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_07_model_states.pt...
[2024-06-23 12:04:37,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_07_model_states.pt.
[2024-06-23 12:04:37,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
[2024-06-23 12:04:42,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/layer_01-model_states.pt.
[2024-06-23 12:04:43,489] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_00_model_states.pt
[2024-06-23 12:04:43,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_00_model_states.pt...
[2024-06-23 12:04:44,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5170/mp_rank_00_model_states.pt.
[2024-06-23 12:04:44,002] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5170 is ready now!
Checkpoint saved using --- 261.6042182445526 seconds ---
100%|█████████▉| 5182/5198 [09:59<00:02,  6.81it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4864
100%|█████████▉| 5182/5198 [08:21<00:02,  7.81it/s]100%|█████████▉| 5182/5198 [08:06<00:02,  8.00it/s]100%|█████████▉| 5182/5198 [08:20<00:02,  7.83it/s]100%|█████████▉| 5182/5198 [09:59<00:02,  6.81it/s]100%|█████████▉| 5182/5198 [08:44<00:02,  7.55it/s]100%|█████████▉| 5182/5198 [10:03<00:02,  6.77it/s]100%|█████████▉| 5182/5198 [08:08<00:02,  7.97it/s]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.46s/it][A100%|██████████| 1/1 [01:27<00:00, 87.46s/it]
100%|█████████▉| 5183/5198 [09:36<00:02,  6.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 12:06:13,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=5171, skipped=0, lr=[4.780935412496186e-07], mom=[(0.9, 0.999)]
steps: 5171 loss: 0.5803 iter time (s): 89.526 samples/sec: 1.430

100%|██████████| 1/1 [01:29<00:00, 89.91s/it][A100%|██████████| 1/1 [01:29<00:00, 89.91s/it]
100%|█████████▉| 5183/5198 [09:36<00:02,  6.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.04s/it][A100%|██████████| 1/1 [01:30<00:00, 90.04s/it]
100%|█████████▉| 5183/5198 [11:29<00:02,  5.49it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.18s/it][A100%|██████████| 1/1 [01:30<00:00, 90.18s/it]
100%|█████████▉| 5183/5198 [09:51<00:02,  6.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.27s/it][A100%|██████████| 1/1 [01:30<00:00, 90.27s/it]
100%|█████████▉| 5183/5198 [11:34<00:02,  5.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.31s/it][A100%|██████████| 1/1 [01:30<00:00, 90.31s/it]
100%|█████████▉| 5183/5198 [10:15<00:02,  5.95it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.33s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
100%|█████████▉| 5183/5198 [09:50<00:02,  6.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.36s/it][A100%|██████████| 1/1 [01:30<00:00, 90.36s/it]
100%|█████████▉| 5183/5198 [11:30<00:02,  5.48it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4865
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.75s/it][A100%|██████████| 1/1 [01:22<00:00, 82.76s/it]
[2024-06-23 12:07:36,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=5172, skipped=0, lr=[4.763682720910987e-07], mom=[(0.9, 0.999)]
steps: 5172 loss: 0.6469 iter time (s): 81.678 samples/sec: 1.567

100%|██████████| 1/1 [01:22<00:00, 82.41s/it][A100%|██████████| 1/1 [01:22<00:00, 82.41s/it]

100%|██████████| 1/1 [01:22<00:00, 82.50s/it][A100%|██████████| 1/1 [01:22<00:00, 82.50s/it]

100%|██████████| 1/1 [01:22<00:00, 82.47s/it][A100%|██████████| 1/1 [01:22<00:00, 82.47s/it]

100%|██████████| 1/1 [01:22<00:00, 82.49s/it][A100%|██████████| 1/1 [01:22<00:00, 82.49s/it]

100%|██████████| 1/1 [01:22<00:00, 82.49s/it][A100%|██████████| 1/1 [01:22<00:00, 82.50s/it]

100%|██████████| 1/1 [01:22<00:00, 82.50s/it][A100%|██████████| 1/1 [01:22<00:00, 82.51s/it]

100%|██████████| 1/1 [01:22<00:00, 82.49s/it][A100%|██████████| 1/1 [01:22<00:00, 82.49s/it]
Checkpointing at shard 5183
[2024-06-23 12:07:36,931] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5172 is about to be saved!
[2024-06-23 12:07:37,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_00-model_states.pt...
[2024-06-23 12:07:41,693] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_08-model_states.pt...
[2024-06-23 12:07:42,394] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_00-model_states.pt.
[2024-06-23 12:07:42,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_02-model_states.pt...
[2024-06-23 12:07:42,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_07-model_states.pt...
[2024-06-23 12:07:48,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_04-model_states.pt...
[2024-06-23 12:07:49,024] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_03-model_states.pt...
[2024-06-23 12:07:52,026] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_01-model_states.pt...
[2024-06-23 12:07:52,249] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_05-model_states.pt...
[2024-06-23 12:07:52,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_06-model_states.pt...
[2024-06-23 12:09:53,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_01-model_states.pt.
[2024-06-23 12:09:53,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_06-model_states.pt.
[2024-06-23 12:09:54,092] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_05-model_states.pt.
[2024-06-23 12:09:54,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_04_model_states.pt...
[2024-06-23 12:09:54,807] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_04_model_states.pt.
[2024-06-23 12:09:54,808] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
[2024-06-23 12:09:55,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_05_model_states.pt...
[2024-06-23 12:09:55,648] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_00_model_states.pt
[2024-06-23 12:09:55,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_00_model_states.pt...
[2024-06-23 12:09:55,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_05_model_states.pt.
[2024-06-23 12:09:55,753] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
[2024-06-23 12:09:56,288] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_00_model_states.pt.
[2024-06-23 12:09:56,288] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
[2024-06-23 12:09:57,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_08-model_states.pt.
[2024-06-23 12:09:58,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_09-model_states.pt...
[2024-06-23 12:09:58,897] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_09-model_states.pt.
[2024-06-23 12:09:58,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_07_model_states.pt...
[2024-06-23 12:09:58,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_07_model_states.pt.
[2024-06-23 12:09:58,982] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
[2024-06-23 12:10:05,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_07-model_states.pt.
[2024-06-23 12:10:05,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_06_model_states.pt...
[2024-06-23 12:10:05,908] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_06_model_states.pt.
[2024-06-23 12:10:05,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
[2024-06-23 12:10:06,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_02-model_states.pt.
[2024-06-23 12:10:06,461] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_01_model_states.pt
[2024-06-23 12:10:06,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_01_model_states.pt...
[2024-06-23 12:10:06,635] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_01_model_states.pt.
[2024-06-23 12:10:06,635] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
[2024-06-23 12:10:27,093] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_04-model_states.pt.
[2024-06-23 12:10:27,825] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/layer_03-model_states.pt.
[2024-06-23 12:10:27,876] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_03_model_states.pt...
[2024-06-23 12:10:27,902] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_03_model_states.pt.
[2024-06-23 12:10:27,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
[2024-06-23 12:10:28,424] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_02_model_states.pt...
[2024-06-23 12:10:28,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5172/mp_rank_02_model_states.pt.
[2024-06-23 12:10:28,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5172 is ready now!
100%|█████████▉| 5184/5198 [15:48<00:04,  3.07it/s]100%|█████████▉| 5184/5198 [13:53<00:04,  3.29it/s]100%|█████████▉| 5184/5198 [14:04<00:04,  3.27it/s]100%|█████████▉| 5184/5198 [14:29<00:04,  3.22it/s]100%|█████████▉| 5184/5198 [14:06<00:04,  3.27it/s]100%|█████████▉| 5184/5198 [13:50<00:04,  3.30it/s]100%|█████████▉| 5184/5198 [15:43<00:04,  3.08it/s]Checkpoint saved using --- 171.552725315094 seconds ---
100%|█████████▉| 5184/5198 [15:44<00:04,  3.08it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4866

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.36s/it][A100%|██████████| 1/1 [01:40<00:00, 100.36s/it]
100%|█████████▉| 5185/5198 [15:33<00:04,  2.60it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 12:12:11,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=5173, skipped=0, lr=[4.7464605867086574e-07], mom=[(0.9, 0.999)]
steps: 5173 loss: 0.6174 iter time (s): 102.762 samples/sec: 1.246

100%|██████████| 1/1 [01:43<00:00, 103.15s/it][A100%|██████████| 1/1 [01:43<00:00, 103.15s/it]
100%|█████████▉| 5185/5198 [15:34<00:05,  2.59it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.23s/it][A100%|██████████| 1/1 [01:43<00:00, 103.23s/it]
100%|█████████▉| 5185/5198 [17:27<00:05,  2.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.50s/it][A100%|██████████| 1/1 [01:43<00:00, 103.50s/it]
100%|█████████▉| 5185/5198 [17:31<00:05,  2.45it/s]
100%|██████████| 1/1 [01:43<00:00, 103.50s/it][A100%|██████████| 1/1 [01:43<00:00, 103.50s/it]
100%|█████████▉| 5185/5198 [15:49<00:05,  2.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.58s/it][A100%|██████████| 1/1 [01:43<00:00, 103.58s/it]
100%|█████████▉| 5185/5198 [16:13<00:05,  2.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.61s/it][A100%|██████████| 1/1 [01:43<00:00, 103.61s/it]
100%|█████████▉| 5185/5198 [15:48<00:05,  2.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.64s/it][A100%|██████████| 1/1 [01:43<00:00, 103.64s/it]
100%|█████████▉| 5185/5198 [17:28<00:05,  2.45it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4867
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.98s/it][A100%|██████████| 1/1 [01:32<00:00, 92.98s/it]
[2024-06-23 12:13:43,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=5174, skipped=0, lr=[4.7292690154144895e-07], mom=[(0.9, 0.999)]
steps: 5174 loss: 0.6605 iter time (s): 91.769 samples/sec: 1.395

100%|██████████| 1/1 [01:32<00:00, 92.62s/it][A100%|██████████| 1/1 [01:32<00:00, 92.62s/it]

100%|██████████| 1/1 [01:32<00:00, 92.73s/it][A100%|██████████| 1/1 [01:32<00:00, 92.73s/it]

100%|██████████| 1/1 [01:32<00:00, 92.56s/it][A100%|██████████| 1/1 [01:32<00:00, 92.56s/it]

100%|██████████| 1/1 [01:32<00:00, 92.63s/it][A100%|██████████| 1/1 [01:32<00:00, 92.63s/it]

100%|██████████| 1/1 [01:32<00:00, 92.62s/it][A100%|██████████| 1/1 [01:32<00:00, 92.62s/it]

100%|██████████| 1/1 [01:32<00:00, 92.65s/it][A100%|██████████| 1/1 [01:32<00:00, 92.65s/it]

100%|██████████| 1/1 [01:32<00:00, 92.63s/it][A100%|██████████| 1/1 [01:32<00:00, 92.63s/it]
Checkpointing at shard 5185
[2024-06-23 12:13:44,838] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5174 is about to be saved!
[2024-06-23 12:13:45,332] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_00-model_states.pt...
[2024-06-23 12:13:49,283] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_08-model_states.pt...
[2024-06-23 12:13:50,488] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_02-model_states.pt...
[2024-06-23 12:13:50,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_07-model_states.pt...
[2024-06-23 12:13:51,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_00-model_states.pt.
[2024-06-23 12:13:57,441] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_03-model_states.pt...
[2024-06-23 12:13:57,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_04-model_states.pt...
[2024-06-23 12:14:00,142] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_01-model_states.pt...
[2024-06-23 12:14:00,276] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_05-model_states.pt...
[2024-06-23 12:14:00,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_06-model_states.pt...
[2024-06-23 12:18:32,353] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_08-model_states.pt.
[2024-06-23 12:18:32,625] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_09-model_states.pt...
[2024-06-23 12:18:33,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_09-model_states.pt.
[2024-06-23 12:18:33,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_07_model_states.pt...
[2024-06-23 12:18:33,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_07_model_states.pt.
[2024-06-23 12:18:33,665] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
[2024-06-23 12:18:37,131] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_02-model_states.pt.
[2024-06-23 12:18:37,613] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_01_model_states.pt
[2024-06-23 12:18:37,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_01_model_states.pt...
[2024-06-23 12:18:37,660] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_01_model_states.pt.
[2024-06-23 12:18:37,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
[2024-06-23 12:18:40,440] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_06-model_states.pt.
[2024-06-23 12:18:41,309] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_05_model_states.pt...
[2024-06-23 12:18:41,317] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_07-model_states.pt.
[2024-06-23 12:18:41,389] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_05_model_states.pt.
[2024-06-23 12:18:41,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
[2024-06-23 12:18:41,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_06_model_states.pt...
[2024-06-23 12:18:41,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_06_model_states.pt.
[2024-06-23 12:18:41,783] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
[2024-06-23 12:18:45,437] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_01-model_states.pt.
[2024-06-23 12:18:46,245] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_00_model_states.pt
[2024-06-23 12:18:46,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_00_model_states.pt...
[2024-06-23 12:18:50,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_00_model_states.pt.
[2024-06-23 12:18:50,761] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
[2024-06-23 12:19:00,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_04-model_states.pt.
[2024-06-23 12:19:01,005] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_03-model_states.pt.
[2024-06-23 12:19:01,261] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_03_model_states.pt...
[2024-06-23 12:19:01,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_03_model_states.pt.
[2024-06-23 12:19:01,300] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
[2024-06-23 12:19:01,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_02_model_states.pt...
[2024-06-23 12:19:01,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_02_model_states.pt.
[2024-06-23 12:19:01,749] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
[2024-06-23 12:19:10,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/layer_05-model_states.pt.
[2024-06-23 12:19:11,295] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_04_model_states.pt...
[2024-06-23 12:19:11,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5174/mp_rank_04_model_states.pt.
[2024-06-23 12:19:11,336] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5174 is ready now!
100%|█████████▉| 5186/5198 [22:48<00:10,  1.15it/s]100%|█████████▉| 5186/5198 [23:12<00:10,  1.15it/s]Checkpoint saved using --- 326.4999420642853 seconds ---
100%|█████████▉| 5186/5198 [24:26<00:10,  1.13it/s]100%|█████████▉| 5186/5198 [22:33<00:10,  1.15it/s]100%|█████████▉| 5186/5198 [22:36<00:10,  1.15it/s]100%|█████████▉| 5186/5198 [24:31<00:10,  1.13it/s]100%|█████████▉| 5186/5198 [22:47<00:10,  1.15it/s]100%|█████████▉| 5186/5198 [24:27<00:10,  1.13it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4868
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.92s/it][A100%|██████████| 1/1 [01:41<00:00, 101.92s/it]
100%|█████████▉| 5187/5198 [24:17<00:11,  1.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 12:20:55,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=5175, skipped=0, lr=[4.712108012543976e-07], mom=[(0.9, 0.999)]
steps: 5175 loss: 0.6312 iter time (s): 104.412 samples/sec: 1.226

100%|██████████| 1/1 [01:44<00:00, 104.88s/it][A100%|██████████| 1/1 [01:44<00:00, 104.88s/it]
100%|█████████▉| 5187/5198 [24:18<00:11,  1.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.03s/it][A100%|██████████| 1/1 [01:45<00:00, 105.03s/it]
100%|█████████▉| 5187/5198 [26:11<00:11,  1.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.15s/it][A100%|██████████| 1/1 [01:45<00:00, 105.15s/it]
100%|█████████▉| 5187/5198 [26:16<00:11,  1.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.18s/it][A100%|██████████| 1/1 [01:45<00:00, 105.18s/it]
100%|█████████▉| 5187/5198 [24:34<00:11,  1.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.23s/it][A100%|██████████| 1/1 [01:45<00:00, 105.23s/it]
100%|█████████▉| 5187/5198 [24:57<00:11,  1.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.27s/it][A100%|██████████| 1/1 [01:45<00:00, 105.27s/it]
100%|█████████▉| 5187/5198 [24:32<00:11,  1.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.29s/it][A100%|██████████| 1/1 [01:45<00:00, 105.29s/it]
100%|█████████▉| 5187/5198 [26:12<00:11,  1.06s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4869
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.19s/it][A100%|██████████| 1/1 [01:39<00:00, 99.19s/it]
[2024-06-23 12:22:34,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=5176, skipped=0, lr=[4.6949775836027846e-07], mom=[(0.9, 0.999)]
steps: 5176 loss: 0.6307 iter time (s): 98.066 samples/sec: 1.305

100%|██████████| 1/1 [01:38<00:00, 98.88s/it][A100%|██████████| 1/1 [01:38<00:00, 98.88s/it]

100%|██████████| 1/1 [01:38<00:00, 98.83s/it][A100%|██████████| 1/1 [01:38<00:00, 98.83s/it]

100%|██████████| 1/1 [01:38<00:00, 98.91s/it][A100%|██████████| 1/1 [01:38<00:00, 98.91s/it]

100%|██████████| 1/1 [01:38<00:00, 98.96s/it][A100%|██████████| 1/1 [01:38<00:00, 98.97s/it]

100%|██████████| 1/1 [01:38<00:00, 99.00s/it][A100%|██████████| 1/1 [01:39<00:00, 99.00s/it]

100%|██████████| 1/1 [01:39<00:00, 99.00s/it][A100%|██████████| 1/1 [01:39<00:00, 99.00s/it]

100%|██████████| 1/1 [01:38<00:00, 98.98s/it][A100%|██████████| 1/1 [01:38<00:00, 98.98s/it]
Checkpointing at shard 5187
[2024-06-23 12:22:35,653] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5176 is about to be saved!
[2024-06-23 12:22:36,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_00-model_states.pt...
[2024-06-23 12:22:40,581] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_08-model_states.pt...
[2024-06-23 12:22:41,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_00-model_states.pt.
[2024-06-23 12:22:41,524] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_07-model_states.pt...
[2024-06-23 12:22:41,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_02-model_states.pt...
[2024-06-23 12:22:47,356] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_04-model_states.pt...
[2024-06-23 12:22:47,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_03-model_states.pt...
[2024-06-23 12:22:50,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_01-model_states.pt...
[2024-06-23 12:22:50,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_05-model_states.pt...
[2024-06-23 12:22:51,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_06-model_states.pt...
[2024-06-23 12:25:04,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_04-model_states.pt.
[2024-06-23 12:25:05,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_03_model_states.pt...
[2024-06-23 12:25:05,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_03_model_states.pt.
[2024-06-23 12:25:05,413] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
[2024-06-23 12:25:05,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_08-model_states.pt.
[2024-06-23 12:25:05,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_01-model_states.pt.
[2024-06-23 12:25:05,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_07-model_states.pt.
[2024-06-23 12:25:05,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_03-model_states.pt.
[2024-06-23 12:25:05,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_02-model_states.pt.
[2024-06-23 12:25:05,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_09-model_states.pt...
[2024-06-23 12:25:05,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_06_model_states.pt...
[2024-06-23 12:25:06,081] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_01_model_states.pt
[2024-06-23 12:25:06,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_01_model_states.pt...
[2024-06-23 12:25:06,084] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_02_model_states.pt...
[2024-06-23 12:25:06,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_06_model_states.pt.
[2024-06-23 12:25:06,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
[2024-06-23 12:25:06,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_01_model_states.pt.
[2024-06-23 12:25:06,238] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
[2024-06-23 12:25:06,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_02_model_states.pt.
[2024-06-23 12:25:06,241] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
[2024-06-23 12:25:06,543] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_00_model_states.pt
[2024-06-23 12:25:06,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_00_model_states.pt...
[2024-06-23 12:25:06,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_09-model_states.pt.
[2024-06-23 12:25:06,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_07_model_states.pt...
[2024-06-23 12:25:06,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_07_model_states.pt.
[2024-06-23 12:25:06,814] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
[2024-06-23 12:25:07,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_00_model_states.pt.
[2024-06-23 12:25:07,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
[2024-06-23 12:25:08,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_06-model_states.pt.
[2024-06-23 12:25:08,265] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/layer_05-model_states.pt.
[2024-06-23 12:25:08,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_04_model_states.pt...
[2024-06-23 12:25:08,935] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_04_model_states.pt.
[2024-06-23 12:25:08,935] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
[2024-06-23 12:25:09,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_05_model_states.pt...
[2024-06-23 12:25:09,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5176/mp_rank_05_model_states.pt.
[2024-06-23 12:25:09,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5176 is ready now!
Checkpoint saved using --- 153.47258257865906 seconds ---
100%|█████████▉| 5188/5198 [28:46<00:16,  1.62s/it]100%|█████████▉| 5188/5198 [28:33<00:16,  1.63s/it]100%|█████████▉| 5188/5198 [28:31<00:16,  1.62s/it]100%|█████████▉| 5188/5198 [30:24<00:16,  1.64s/it]100%|█████████▉| 5188/5198 [30:25<00:16,  1.64s/it]100%|█████████▉| 5188/5198 [29:10<00:16,  1.63s/it]100%|█████████▉| 5188/5198 [28:45<00:16,  1.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4870
100%|█████████▉| 5188/5198 [30:28<00:16,  1.64s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.19s/it][A100%|██████████| 1/1 [02:12<00:00, 132.19s/it]
100%|█████████▉| 5189/5198 [30:46<00:18,  2.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 12:27:24,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=5177, skipped=0, lr=[4.6778777340867683e-07], mom=[(0.9, 0.999)]
steps: 5177 loss: 0.6296 iter time (s): 135.606 samples/sec: 0.944

100%|██████████| 1/1 [02:16<00:00, 136.01s/it][A100%|██████████| 1/1 [02:16<00:00, 136.01s/it]
100%|█████████▉| 5189/5198 [30:47<00:18,  2.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.06s/it][A100%|██████████| 1/1 [02:16<00:00, 136.06s/it]
100%|█████████▉| 5189/5198 [32:40<00:18,  2.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.31s/it][A100%|██████████| 1/1 [02:16<00:00, 136.31s/it]
100%|█████████▉| 5189/5198 [31:03<00:18,  2.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.36s/it][A100%|██████████| 1/1 [02:16<00:00, 136.36s/it]
100%|█████████▉| 5189/5198 [32:45<00:18,  2.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.43s/it][A100%|██████████| 1/1 [02:16<00:00, 136.43s/it]
100%|█████████▉| 5189/5198 [31:26<00:18,  2.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.46s/it][A100%|██████████| 1/1 [02:16<00:00, 136.46s/it]
100%|█████████▉| 5189/5198 [31:01<00:18,  2.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.47s/it][A100%|██████████| 1/1 [02:16<00:00, 136.47s/it]
100%|█████████▉| 5189/5198 [32:41<00:18,  2.09s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4871
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.35s/it][A100%|██████████| 1/1 [01:44<00:00, 104.36s/it]
[2024-06-23 12:29:08,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=5178, skipped=0, lr=[4.6608084694819896e-07], mom=[(0.9, 0.999)]
steps: 5178 loss: 0.6255 iter time (s): 102.483 samples/sec: 1.249

100%|██████████| 1/1 [01:43<00:00, 103.27s/it][A100%|██████████| 1/1 [01:43<00:00, 103.27s/it]

100%|██████████| 1/1 [01:43<00:00, 103.36s/it][A100%|██████████| 1/1 [01:43<00:00, 103.36s/it]

100%|██████████| 1/1 [01:43<00:00, 103.31s/it][A100%|██████████| 1/1 [01:43<00:00, 103.31s/it]

100%|██████████| 1/1 [01:43<00:00, 103.34s/it][A100%|██████████| 1/1 [01:43<00:00, 103.34s/it]

100%|██████████| 1/1 [01:43<00:00, 103.30s/it][A100%|██████████| 1/1 [01:43<00:00, 103.30s/it]

100%|██████████| 1/1 [01:43<00:00, 103.32s/it][A100%|██████████| 1/1 [01:43<00:00, 103.32s/it]

100%|██████████| 1/1 [01:43<00:00, 103.32s/it][A100%|██████████| 1/1 [01:43<00:00, 103.32s/it]
Checkpointing at shard 5189
[2024-06-23 12:29:09,018] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5178 is about to be saved!
[2024-06-23 12:29:09,505] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_00-model_states.pt...
[2024-06-23 12:29:13,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_08-model_states.pt...
[2024-06-23 12:29:14,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_02-model_states.pt...
[2024-06-23 12:29:14,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_07-model_states.pt...
[2024-06-23 12:29:15,200] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_00-model_states.pt.
[2024-06-23 12:29:21,828] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_03-model_states.pt...
[2024-06-23 12:29:21,831] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_04-model_states.pt...
[2024-06-23 12:29:24,241] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_01-model_states.pt...
[2024-06-23 12:29:24,392] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_05-model_states.pt...
[2024-06-23 12:29:25,093] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_06-model_states.pt...
[2024-06-23 12:32:00,039] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_08-model_states.pt.
[2024-06-23 12:32:00,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_09-model_states.pt...
[2024-06-23 12:32:00,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_02-model_states.pt.
[2024-06-23 12:32:00,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_07-model_states.pt.
[2024-06-23 12:32:00,769] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_01_model_states.pt
[2024-06-23 12:32:00,770] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_01_model_states.pt...
[2024-06-23 12:32:00,850] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_06_model_states.pt...
[2024-06-23 12:32:00,863] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_01_model_states.pt.
[2024-06-23 12:32:00,863] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
[2024-06-23 12:32:00,905] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_06_model_states.pt.
[2024-06-23 12:32:00,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
[2024-06-23 12:32:01,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_09-model_states.pt.
[2024-06-23 12:32:01,231] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_07_model_states.pt...
[2024-06-23 12:32:01,359] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_07_model_states.pt.
[2024-06-23 12:32:01,360] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
[2024-06-23 12:32:04,552] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_04-model_states.pt.
[2024-06-23 12:32:05,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_03_model_states.pt...
[2024-06-23 12:32:05,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_03_model_states.pt.
[2024-06-23 12:32:05,446] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
[2024-06-23 12:32:09,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_03-model_states.pt.
[2024-06-23 12:32:10,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_02_model_states.pt...
[2024-06-23 12:32:10,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_02_model_states.pt.
[2024-06-23 12:32:10,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
[2024-06-23 12:32:43,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_06-model_states.pt.
[2024-06-23 12:32:43,549] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_01-model_states.pt.
[2024-06-23 12:32:44,250] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_05_model_states.pt...
[2024-06-23 12:32:44,324] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_05_model_states.pt.
[2024-06-23 12:32:44,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
[2024-06-23 12:32:44,382] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_00_model_states.pt
[2024-06-23 12:32:44,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_00_model_states.pt...
[2024-06-23 12:32:44,971] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_00_model_states.pt.
[2024-06-23 12:32:44,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
[2024-06-23 12:32:48,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/layer_05-model_states.pt.
[2024-06-23 12:32:49,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_04_model_states.pt...
[2024-06-23 12:32:49,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5178/mp_rank_04_model_states.pt.
[2024-06-23 12:32:49,474] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5178 is ready now!
Checkpoint saved using --- 220.45724844932556 seconds ---
100%|█████████▉| 5190/5198 [38:05<00:28,  3.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4872
100%|█████████▉| 5190/5198 [36:27<00:28,  3.59s/it]100%|█████████▉| 5190/5198 [38:04<00:28,  3.61s/it]100%|█████████▉| 5190/5198 [36:14<00:28,  3.59s/it]100%|█████████▉| 5190/5198 [38:09<00:28,  3.61s/it]100%|█████████▉| 5190/5198 [36:50<00:28,  3.59s/it]100%|█████████▉| 5190/5198 [36:25<00:28,  3.59s/it]100%|█████████▉| 5190/5198 [36:11<00:28,  3.59s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.14s/it][A100%|██████████| 1/1 [01:20<00:00, 80.14s/it]
100%|█████████▉| 5191/5198 [37:34<00:28,  4.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 12:34:11,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=5179, skipped=0, lr=[4.643769795264708e-07], mom=[(0.9, 0.999)]
steps: 5179 loss: 0.6270 iter time (s): 81.978 samples/sec: 1.561

100%|██████████| 1/1 [01:22<00:00, 82.37s/it][A100%|██████████| 1/1 [01:22<00:00, 82.37s/it]
100%|█████████▉| 5191/5198 [37:34<00:28,  4.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.52s/it][A100%|██████████| 1/1 [01:22<00:00, 82.52s/it]
100%|█████████▉| 5191/5198 [39:27<00:28,  4.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.63s/it][A100%|██████████| 1/1 [01:22<00:00, 82.63s/it]
100%|█████████▉| 5191/5198 [37:49<00:28,  4.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.69s/it][A100%|██████████| 1/1 [01:22<00:00, 82.69s/it]
100%|█████████▉| 5191/5198 [39:31<00:28,  4.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.79s/it][A100%|██████████| 1/1 [01:22<00:00, 82.79s/it]
100%|█████████▉| 5191/5198 [38:13<00:28,  4.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.82s/it][A100%|██████████| 1/1 [01:22<00:00, 82.82s/it]
100%|█████████▉| 5191/5198 [37:48<00:28,  4.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.85s/it][A100%|██████████| 1/1 [01:22<00:00, 82.85s/it]
100%|█████████▉| 5191/5198 [39:28<00:28,  4.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4873
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.55s/it][A100%|██████████| 1/1 [01:35<00:00, 95.55s/it]
[2024-06-23 12:35:47,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=5180, skipped=0, lr=[4.6267617169013385e-07], mom=[(0.9, 0.999)]
steps: 5180 loss: 0.6630 iter time (s): 95.022 samples/sec: 1.347

100%|██████████| 1/1 [01:35<00:00, 95.91s/it][A100%|██████████| 1/1 [01:35<00:00, 95.91s/it]

100%|██████████| 1/1 [01:35<00:00, 95.85s/it][A100%|██████████| 1/1 [01:35<00:00, 95.85s/it]

100%|██████████| 1/1 [01:35<00:00, 95.89s/it][A100%|██████████| 1/1 [01:35<00:00, 95.89s/it]

100%|██████████| 1/1 [01:35<00:00, 95.93s/it][A100%|██████████| 1/1 [01:35<00:00, 95.93s/it]

100%|██████████| 1/1 [01:35<00:00, 95.87s/it][A100%|██████████| 1/1 [01:35<00:00, 95.87s/it]

100%|██████████| 1/1 [01:35<00:00, 95.89s/it][A100%|██████████| 1/1 [01:35<00:00, 95.89s/it]

100%|██████████| 1/1 [01:35<00:00, 95.89s/it][A100%|██████████| 1/1 [01:35<00:00, 95.89s/it]
Checkpointing at shard 5191
[2024-06-23 12:35:48,291] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5180 is about to be saved!
[2024-06-23 12:35:48,825] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_00-model_states.pt...
[2024-06-23 12:35:52,289] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_07-model_states.pt...
[2024-06-23 12:35:53,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_08-model_states.pt...
[2024-06-23 12:35:54,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_02-model_states.pt...
[2024-06-23 12:35:54,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_00-model_states.pt.
[2024-06-23 12:36:00,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_03-model_states.pt...
[2024-06-23 12:36:00,960] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_04-model_states.pt...
[2024-06-23 12:36:03,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_05-model_states.pt...
[2024-06-23 12:36:03,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_01-model_states.pt...
[2024-06-23 12:36:03,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_06-model_states.pt...
[2024-06-23 12:38:45,518] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_06-model_states.pt.
[2024-06-23 12:38:46,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_05_model_states.pt...
[2024-06-23 12:38:47,169] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_05_model_states.pt.
[2024-06-23 12:38:47,169] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
[2024-06-23 12:38:48,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_03-model_states.pt.
[2024-06-23 12:38:48,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_04-model_states.pt.
[2024-06-23 12:38:48,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_07-model_states.pt.
[2024-06-23 12:38:48,211] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_08-model_states.pt.
[2024-06-23 12:38:48,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_01-model_states.pt.
[2024-06-23 12:38:48,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_06_model_states.pt...
[2024-06-23 12:38:48,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_06_model_states.pt.
[2024-06-23 12:38:48,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
[2024-06-23 12:38:48,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_02-model_states.pt.
[2024-06-23 12:38:48,696] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_09-model_states.pt...
[2024-06-23 12:38:48,714] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_01_model_states.pt
[2024-06-23 12:38:48,714] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_01_model_states.pt...
[2024-06-23 12:38:48,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_01_model_states.pt.
[2024-06-23 12:38:48,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
[2024-06-23 12:38:48,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_02_model_states.pt...
[2024-06-23 12:38:48,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_02_model_states.pt.
[2024-06-23 12:38:48,925] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
[2024-06-23 12:38:48,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_03_model_states.pt...
[2024-06-23 12:38:48,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_03_model_states.pt.
[2024-06-23 12:38:48,974] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
[2024-06-23 12:38:49,272] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_00_model_states.pt
[2024-06-23 12:38:49,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_00_model_states.pt...
[2024-06-23 12:38:49,273] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_09-model_states.pt.
[2024-06-23 12:38:49,276] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_07_model_states.pt...
[2024-06-23 12:38:49,311] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_07_model_states.pt.
[2024-06-23 12:38:49,311] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
[2024-06-23 12:38:49,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_00_model_states.pt.
[2024-06-23 12:38:49,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
[2024-06-23 12:38:55,001] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/layer_05-model_states.pt.
[2024-06-23 12:38:55,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_04_model_states.pt...
[2024-06-23 12:38:55,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5180/mp_rank_04_model_states.pt.
[2024-06-23 12:38:55,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5180 is ready now!
Checkpoint saved using --- 187.64692497253418 seconds ---
100%|█████████▉| 5192/5198 [44:11<00:40,  6.78s/it]100%|█████████▉| 5192/5198 [42:20<00:40,  6.78s/it]100%|█████████▉| 5192/5198 [42:33<00:40,  6.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4874
100%|█████████▉| 5192/5198 [44:11<00:40,  6.78s/it]100%|█████████▉| 5192/5198 [42:56<00:40,  6.77s/it]100%|█████████▉| 5192/5198 [44:15<00:40,  6.78s/it]100%|█████████▉| 5192/5198 [42:18<00:40,  6.76s/it]100%|█████████▉| 5192/5198 [42:32<00:40,  6.76s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.91s/it][A100%|██████████| 1/1 [01:26<00:00, 86.91s/it]
100%|█████████▉| 5193/5198 [43:47<00:39,  7.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 12:40:24,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=5181, skipped=0, lr=[4.6097842398485033e-07], mom=[(0.9, 0.999)]
steps: 5181 loss: 0.6010 iter time (s): 88.930 samples/sec: 1.439

100%|██████████| 1/1 [01:29<00:00, 89.31s/it][A100%|██████████| 1/1 [01:29<00:00, 89.31s/it]
100%|█████████▉| 5193/5198 [43:47<00:39,  7.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.43s/it][A100%|██████████| 1/1 [01:29<00:00, 89.43s/it]
100%|█████████▉| 5193/5198 [45:40<00:39,  7.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.53s/it][A100%|██████████| 1/1 [01:29<00:00, 89.53s/it]
100%|█████████▉| 5193/5198 [44:03<00:39,  7.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.58s/it][A100%|██████████| 1/1 [01:29<00:00, 89.58s/it]
100%|█████████▉| 5193/5198 [45:45<00:39,  7.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.69s/it][A100%|██████████| 1/1 [01:29<00:00, 89.69s/it]
100%|█████████▉| 5193/5198 [44:26<00:39,  7.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.76s/it][A100%|██████████| 1/1 [01:29<00:00, 89.76s/it]
100%|█████████▉| 5193/5198 [44:02<00:39,  7.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.77s/it][A100%|██████████| 1/1 [01:29<00:00, 89.77s/it]
100%|█████████▉| 5193/5198 [45:41<00:39,  7.89s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4875
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.41s/it][A100%|██████████| 1/1 [01:43<00:00, 103.41s/it]
[2024-06-23 12:42:08,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=5182, skipped=0, lr=[4.5928373695529987e-07], mom=[(0.9, 0.999)]
steps: 5182 loss: 0.6226 iter time (s): 102.998 samples/sec: 1.243

100%|██████████| 1/1 [01:43<00:00, 103.79s/it][A100%|██████████| 1/1 [01:43<00:00, 103.79s/it]

100%|██████████| 1/1 [01:43<00:00, 103.74s/it][A100%|██████████| 1/1 [01:43<00:00, 103.74s/it]

100%|██████████| 1/1 [01:43<00:00, 103.84s/it][A100%|██████████| 1/1 [01:43<00:00, 103.84s/it]

100%|██████████| 1/1 [01:43<00:00, 103.88s/it][A100%|██████████| 1/1 [01:43<00:00, 103.88s/it]

100%|██████████| 1/1 [01:43<00:00, 103.80s/it][A100%|██████████| 1/1 [01:43<00:00, 103.80s/it]

100%|██████████| 1/1 [01:43<00:00, 103.89s/it][A100%|██████████| 1/1 [01:43<00:00, 103.89s/it]

100%|██████████| 1/1 [01:43<00:00, 103.83s/it][A100%|██████████| 1/1 [01:43<00:00, 103.83s/it]
Checkpointing at shard 5193
[2024-06-23 12:42:09,634] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5182 is about to be saved!
[2024-06-23 12:42:10,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_00-model_states.pt...
[2024-06-23 12:42:15,195] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_07-model_states.pt...
[2024-06-23 12:42:15,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_02-model_states.pt...
[2024-06-23 12:42:15,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_00-model_states.pt.
[2024-06-23 12:42:16,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_08-model_states.pt...
[2024-06-23 12:42:19,559] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_01-model_states.pt...
[2024-06-23 12:42:22,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_04-model_states.pt...
[2024-06-23 12:42:23,003] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_03-model_states.pt...
[2024-06-23 12:42:24,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_05-model_states.pt...
[2024-06-23 12:42:24,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_06-model_states.pt...
[2024-06-23 12:44:20,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_04-model_states.pt.
[2024-06-23 12:44:20,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_03_model_states.pt...
[2024-06-23 12:44:21,071] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_03_model_states.pt.
[2024-06-23 12:44:21,071] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
[2024-06-23 12:44:28,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_08-model_states.pt.
[2024-06-23 12:44:29,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_09-model_states.pt...
[2024-06-23 12:44:31,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_09-model_states.pt.
[2024-06-23 12:44:31,068] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_07_model_states.pt...
[2024-06-23 12:44:31,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_07_model_states.pt.
[2024-06-23 12:44:31,245] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
[2024-06-23 12:44:32,812] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_01-model_states.pt.
[2024-06-23 12:44:32,889] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_00_model_states.pt
[2024-06-23 12:44:32,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_00_model_states.pt...
[2024-06-23 12:44:33,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_00_model_states.pt.
[2024-06-23 12:44:33,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
[2024-06-23 12:44:34,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_07-model_states.pt.
[2024-06-23 12:44:34,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_03-model_states.pt.
[2024-06-23 12:44:34,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_02-model_states.pt.
[2024-06-23 12:44:34,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_05-model_states.pt.
[2024-06-23 12:44:34,895] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/layer_06-model_states.pt.
[2024-06-23 12:44:35,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_06_model_states.pt...
[2024-06-23 12:44:35,210] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_01_model_states.pt
[2024-06-23 12:44:35,210] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_01_model_states.pt...
[2024-06-23 12:44:35,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_06_model_states.pt.
[2024-06-23 12:44:35,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
[2024-06-23 12:44:35,251] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_01_model_states.pt.
[2024-06-23 12:44:35,251] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
[2024-06-23 12:44:35,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_02_model_states.pt...
[2024-06-23 12:44:35,824] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_02_model_states.pt.
[2024-06-23 12:44:35,824] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
[2024-06-23 12:44:35,902] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_05_model_states.pt...
[2024-06-23 12:44:35,925] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_04_model_states.pt...
[2024-06-23 12:44:35,932] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_05_model_states.pt.
[2024-06-23 12:44:35,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
[2024-06-23 12:44:35,953] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5182/mp_rank_04_model_states.pt.
[2024-06-23 12:44:35,953] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5182 is ready now!
Checkpoint saved using --- 146.32047629356384 seconds ---
100%|█████████▉| 5194/5198 [49:51<00:49, 12.42s/it]100%|█████████▉| 5194/5198 [48:36<00:49, 12.40s/it]100%|█████████▉| 5194/5198 [49:51<00:49, 12.42s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4876
100%|█████████▉| 5194/5198 [47:58<00:49, 12.40s/it]100%|█████████▉| 5194/5198 [48:00<00:49, 12.43s/it]100%|█████████▉| 5194/5198 [48:12<00:49, 12.40s/it]100%|█████████▉| 5194/5198 [48:13<00:49, 12.40s/it]100%|█████████▉| 5194/5198 [49:55<00:49, 12.42s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.77s/it][A100%|██████████| 1/1 [01:27<00:00, 87.77s/it]
100%|█████████▉| 5195/5198 [49:28<00:43, 14.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-23 12:46:05,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=5183, skipped=0, lr=[4.575921111451777e-07], mom=[(0.9, 0.999)]
steps: 5183 loss: 0.5823 iter time (s): 89.795 samples/sec: 1.425

100%|██████████| 1/1 [01:30<00:00, 90.17s/it][A100%|██████████| 1/1 [01:30<00:00, 90.17s/it]
100%|█████████▉| 5195/5198 [49:28<00:43, 14.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.30s/it][A100%|██████████| 1/1 [01:30<00:00, 90.30s/it]
100%|█████████▉| 5195/5198 [51:21<00:43, 14.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.45s/it][A100%|██████████| 1/1 [01:30<00:00, 90.45s/it]
100%|█████████▉| 5195/5198 [49:44<00:43, 14.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.50s/it][A100%|██████████| 1/1 [01:30<00:00, 90.50s/it]
100%|█████████▉| 5195/5198 [51:26<00:43, 14.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.55s/it][A100%|██████████| 1/1 [01:30<00:00, 90.55s/it]
100%|█████████▉| 5195/5198 [50:07<00:43, 14.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.57s/it][A100%|██████████| 1/1 [01:30<00:00, 90.57s/it]
100%|█████████▉| 5195/5198 [49:42<00:43, 14.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.59s/it][A100%|██████████| 1/1 [01:30<00:00, 90.59s/it]
100%|█████████▉| 5195/5198 [51:22<00:43, 14.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4877
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.48s/it][A100%|██████████| 1/1 [01:24<00:00, 84.48s/it]
[2024-06-23 12:47:30,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=5184, skipped=0, lr=[4.559035470972031e-07], mom=[(0.9, 0.999)]
steps: 5184 loss: 0.5922 iter time (s): 83.534 samples/sec: 1.532

100%|██████████| 1/1 [01:24<00:00, 84.42s/it][A100%|██████████| 1/1 [01:24<00:00, 84.42s/it]

100%|██████████| 1/1 [01:24<00:00, 84.30s/it][A100%|██████████| 1/1 [01:24<00:00, 84.30s/it]

100%|██████████| 1/1 [01:24<00:00, 84.32s/it][A100%|██████████| 1/1 [01:24<00:00, 84.32s/it]

100%|██████████| 1/1 [01:24<00:00, 84.42s/it][A100%|██████████| 1/1 [01:24<00:00, 84.42s/it]

100%|██████████| 1/1 [01:24<00:00, 84.37s/it][A100%|██████████| 1/1 [01:24<00:00, 84.37s/it]

100%|██████████| 1/1 [01:24<00:00, 84.40s/it][A100%|██████████| 1/1 [01:24<00:00, 84.40s/it]

100%|██████████| 1/1 [01:24<00:00, 84.40s/it][A100%|██████████| 1/1 [01:24<00:00, 84.40s/it]
Checkpointing at shard 5195
[2024-06-23 12:47:31,121] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5184 is about to be saved!
[2024-06-23 12:47:31,620] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_00-model_states.pt...
[2024-06-23 12:47:34,727] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_07-model_states.pt...
[2024-06-23 12:47:36,759] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_02-model_states.pt...
[2024-06-23 12:47:37,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_00-model_states.pt.
[2024-06-23 12:47:38,142] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_08-model_states.pt...
[2024-06-23 12:47:43,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_01-model_states.pt...
[2024-06-23 12:47:44,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_04-model_states.pt...
[2024-06-23 12:47:44,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_03-model_states.pt...
[2024-06-23 12:47:46,244] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_05-model_states.pt...
[2024-06-23 12:47:46,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_06-model_states.pt...
[2024-06-23 12:49:23,557] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_08-model_states.pt.
[2024-06-23 12:49:23,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_07-model_states.pt.
[2024-06-23 12:49:24,068] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_06_model_states.pt...
[2024-06-23 12:49:24,326] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_06_model_states.pt.
[2024-06-23 12:49:24,327] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
[2024-06-23 12:49:24,575] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_09-model_states.pt...
[2024-06-23 12:49:26,093] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_09-model_states.pt.
[2024-06-23 12:49:26,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_07_model_states.pt...
[2024-06-23 12:49:26,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_07_model_states.pt.
[2024-06-23 12:49:26,176] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
[2024-06-23 12:49:42,512] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_02-model_states.pt.
[2024-06-23 12:49:42,944] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_01_model_states.pt
[2024-06-23 12:49:42,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_01_model_states.pt...
[2024-06-23 12:49:43,091] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_01_model_states.pt.
[2024-06-23 12:49:43,091] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
[2024-06-23 12:49:44,155] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_03-model_states.pt.
[2024-06-23 12:49:44,830] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_02_model_states.pt...
[2024-06-23 12:49:45,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_02_model_states.pt.
[2024-06-23 12:49:45,067] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
[2024-06-23 12:49:51,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_01-model_states.pt.
[2024-06-23 12:49:51,888] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_00_model_states.pt
[2024-06-23 12:49:51,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_00_model_states.pt...
[2024-06-23 12:49:52,850] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_00_model_states.pt.
[2024-06-23 12:49:52,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
[2024-06-23 12:49:56,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_04-model_states.pt.
[2024-06-23 12:49:57,143] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_03_model_states.pt...
[2024-06-23 12:49:57,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_03_model_states.pt.
[2024-06-23 12:49:57,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
[2024-06-23 12:50:06,072] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_05-model_states.pt.
[2024-06-23 12:50:06,288] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/layer_06-model_states.pt.
[2024-06-23 12:50:06,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_04_model_states.pt...
[2024-06-23 12:50:06,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_04_model_states.pt.
[2024-06-23 12:50:06,797] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
[2024-06-23 12:50:07,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_05_model_states.pt...
[2024-06-23 12:50:07,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5184/mp_rank_05_model_states.pt.
[2024-06-23 12:50:07,295] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5184 is ready now!
Checkpoint saved using --- 156.17452454566956 seconds ---
100%|█████████▉| 5196/5198 [53:44<00:45, 22.54s/it]100%|█████████▉| 5196/5198 [55:23<00:45, 22.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4878
100%|█████████▉| 5196/5198 [55:22<00:45, 22.56s/it]100%|█████████▉| 5196/5198 [53:31<00:45, 22.60s/it]100%|█████████▉| 5196/5198 [55:27<00:45, 22.56s/it]100%|█████████▉| 5196/5198 [54:08<00:45, 22.54s/it]100%|█████████▉| 5196/5198 [53:43<00:45, 22.54s/it]100%|█████████▉| 5196/5198 [53:29<00:45, 22.54s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.53s/it][A100%|██████████| 1/1 [01:41<00:00, 101.53s/it]
100%|█████████▉| 5197/5198 [55:13<00:26, 26.44s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
[2024-06-23 12:51:51,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=5185, skipped=0, lr=[4.542180453531064e-07], mom=[(0.9, 0.999)]
steps: 5185 loss: 0.6229 iter time (s): 104.046 samples/sec: 1.230

100%|██████████| 1/1 [01:44<00:00, 104.36s/it][A100%|██████████| 1/1 [01:44<00:00, 104.36s/it]
100%|█████████▉| 5197/5198 [55:14<00:26, 26.53s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

100%|██████████| 1/1 [01:44<00:00, 104.51s/it][A100%|██████████| 1/1 [01:44<00:00, 104.51s/it]
100%|█████████▉| 5197/5198 [57:07<00:26, 26.55s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

100%|██████████| 1/1 [01:44<00:00, 104.72s/it][A100%|██████████| 1/1 [01:44<00:00, 104.72s/it]
100%|█████████▉| 5197/5198 [55:29<00:26, 26.55s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

100%|██████████| 1/1 [01:44<00:00, 104.75s/it][A100%|██████████| 1/1 [01:44<00:00, 104.75s/it]
100%|█████████▉| 5197/5198 [57:11<00:26, 26.56s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

100%|██████████| 1/1 [01:44<00:00, 104.83s/it][A100%|██████████| 1/1 [01:44<00:00, 104.83s/it]
100%|█████████▉| 5197/5198 [55:53<00:26, 26.55s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

100%|██████████| 1/1 [01:44<00:00, 104.86s/it][A100%|██████████| 1/1 [01:44<00:00, 104.86s/it]
100%|█████████▉| 5197/5198 [55:28<00:26, 26.55s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

100%|██████████| 1/1 [01:44<00:00, 104.88s/it][A100%|██████████| 1/1 [01:44<00:00, 104.88s/it]
100%|█████████▉| 5197/5198 [57:08<00:26, 26.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4879
Training on 0 of 98 sentences.

0it [00:00, ?it/s][A0it [00:00, ?it/s]
Checkpointing at shard 5197
[2024-06-23 12:51:52,285] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5185 is about to be saved!
[2024-06-23 12:51:52,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_00-model_states.pt...
[2024-06-23 12:51:56,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_07-model_states.pt...
[2024-06-23 12:51:58,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_00-model_states.pt.
[2024-06-23 12:51:58,575] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_08-model_states.pt...
[2024-06-23 12:51:58,991] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_02-model_states.pt...
[2024-06-23 12:52:04,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_01-model_states.pt...
[2024-06-23 12:52:04,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_05-model_states.pt...
[2024-06-23 12:52:05,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_06-model_states.pt...
[2024-06-23 12:52:05,312] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_04-model_states.pt...
[2024-06-23 12:52:05,344] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_03-model_states.pt...
[2024-06-23 12:55:24,686] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_01-model_states.pt.
[2024-06-23 12:55:25,042] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_00_model_states.pt
[2024-06-23 12:55:25,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_00_model_states.pt...
[2024-06-23 12:55:25,802] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_08-model_states.pt.
[2024-06-23 12:55:25,804] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_07-model_states.pt.
[2024-06-23 12:55:25,836] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_04-model_states.pt.
[2024-06-23 12:55:25,857] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_00_model_states.pt.
[2024-06-23 12:55:25,858] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:55:25,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_03-model_states.pt.
[2024-06-23 12:55:25,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_06_model_states.pt...
[2024-06-23 12:55:26,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_06_model_states.pt.
[2024-06-23 12:55:26,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:55:26,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_02-model_states.pt.
[2024-06-23 12:55:26,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_03_model_states.pt...
[2024-06-23 12:55:26,616] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_03_model_states.pt.
[2024-06-23 12:55:26,617] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:55:26,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_02_model_states.pt...
[2024-06-23 12:55:26,658] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_02_model_states.pt.
[2024-06-23 12:55:26,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:55:26,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_09-model_states.pt...
[2024-06-23 12:55:26,895] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_01_model_states.pt
[2024-06-23 12:55:26,895] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_01_model_states.pt...
[2024-06-23 12:55:26,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_01_model_states.pt.
[2024-06-23 12:55:26,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:55:27,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_09-model_states.pt.
[2024-06-23 12:55:27,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_07_model_states.pt...
[2024-06-23 12:55:27,603] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_07_model_states.pt.
[2024-06-23 12:55:27,603] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:55:37,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_05-model_states.pt.
[2024-06-23 12:55:38,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_04_model_states.pt...
[2024-06-23 12:55:38,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_04_model_states.pt.
[2024-06-23 12:55:38,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:55:45,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_06-model_states.pt.
[2024-06-23 12:55:46,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_05_model_states.pt...
[2024-06-23 12:55:46,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_05_model_states.pt.
[2024-06-23 12:55:46,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
Checkpoint saved using --- 234.19645309448242 seconds ---
100%|██████████| 5198/5198 [1:01:02<00:00, 40.07s/it]100%|██████████| 5198/5198 [1:01:01<00:00, 40.08s/it]100%|██████████| 5198/5198 [59:24<00:00, 40.06s/it]100%|██████████| 5198/5198 [1:01:02<00:00,  1.42it/s]
--- FINISHED ---
Checkpointing at shard 5197
100%|██████████| 5198/5198 [1:01:01<00:00,  1.42it/s]
100%|██████████| 5198/5198 [59:24<00:00,  1.46it/s]
100%|██████████| 5198/5198 [59:47<00:00, 40.06s/it]100%|██████████| 5198/5198 [59:47<00:00,  1.45it/s]
100%|██████████| 5198/5198 [1:01:06<00:00, 40.07s/it]100%|██████████| 5198/5198 [59:22<00:00, 40.05s/it]100%|██████████| 5198/5198 [59:08<00:00, 40.07s/it]100%|██████████| 5198/5198 [59:22<00:00,  1.46it/s]
100%|██████████| 5198/5198 [1:01:06<00:00,  1.42it/s]
100%|██████████| 5198/5198 [59:08<00:00,  1.46it/s]
100%|██████████| 5198/5198 [59:11<00:00, 40.17s/it]100%|██████████| 5198/5198 [59:11<00:00,  1.46it/s]
[2024-06-23 12:55:46,483] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5185 is about to be saved!
[2024-06-23 12:55:46,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_00-model_states.pt...
[2024-06-23 12:55:49,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_07-model_states.pt...
[2024-06-23 12:55:52,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_02-model_states.pt...
[2024-06-23 12:55:52,442] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_00-model_states.pt.
[2024-06-23 12:55:53,446] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_08-model_states.pt...
[2024-06-23 12:55:58,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_01-model_states.pt...
[2024-06-23 12:55:59,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_03-model_states.pt...
[2024-06-23 12:55:59,433] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_04-model_states.pt...
[2024-06-23 12:56:01,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_05-model_states.pt...
[2024-06-23 12:56:01,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_06-model_states.pt...
[2024-06-23 12:57:31,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_07-model_states.pt.
[2024-06-23 12:57:31,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_06_model_states.pt...
[2024-06-23 12:57:31,745] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_06_model_states.pt.
[2024-06-23 12:57:31,745] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:58:01,031] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_06-model_states.pt.
[2024-06-23 12:58:02,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_03-model_states.pt.
[2024-06-23 12:58:02,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_05_model_states.pt...
[2024-06-23 12:58:02,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_05_model_states.pt.
[2024-06-23 12:58:02,289] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:58:02,778] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_02_model_states.pt...
[2024-06-23 12:58:02,845] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_02_model_states.pt.
[2024-06-23 12:58:02,845] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:58:02,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_02-model_states.pt.
[2024-06-23 12:58:02,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_04-model_states.pt.
[2024-06-23 12:58:03,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_05-model_states.pt.
[2024-06-23 12:58:03,443] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_01_model_states.pt
[2024-06-23 12:58:03,443] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_01_model_states.pt...
[2024-06-23 12:58:03,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_01_model_states.pt.
[2024-06-23 12:58:03,537] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:58:03,985] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_03_model_states.pt...
[2024-06-23 12:58:04,057] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_03_model_states.pt.
[2024-06-23 12:58:04,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:58:04,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_04_model_states.pt...
[2024-06-23 12:58:04,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_04_model_states.pt.
[2024-06-23 12:58:04,303] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:58:05,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_01-model_states.pt.
[2024-06-23 12:58:05,799] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_00_model_states.pt
[2024-06-23 12:58:05,799] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_00_model_states.pt...
[2024-06-23 12:58:06,549] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_00_model_states.pt.
[2024-06-23 12:58:06,549] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
[2024-06-23 12:58:11,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_08-model_states.pt.
[2024-06-23 12:58:12,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_09-model_states.pt...
[2024-06-23 12:58:13,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/layer_09-model_states.pt.
[2024-06-23 12:58:13,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_07_model_states.pt...
[2024-06-23 12:58:13,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_32_64_checkpoint/global_step5185/mp_rank_07_model_states.pt.
[2024-06-23 12:58:13,253] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5185 is ready now!
Checkpoint saved using --- 146.77130770683289 seconds ---
[2024-06-23 12:58:22,992] [INFO] [launch.py:348:main] Process 1585441 exits successfully.
[2024-06-23 12:58:22,995] [INFO] [launch.py:348:main] Process 1585443 exits successfully.
[2024-06-23 12:58:27,000] [INFO] [launch.py:348:main] Process 1585438 exits successfully.
[2024-06-23 12:58:27,000] [INFO] [launch.py:348:main] Process 1585437 exits successfully.
[2024-06-23 12:58:27,000] [INFO] [launch.py:348:main] Process 1585436 exits successfully.
[2024-06-23 12:58:27,000] [INFO] [launch.py:348:main] Process 1585435 exits successfully.
[2024-06-23 12:58:27,000] [INFO] [launch.py:348:main] Process 1585440 exits successfully.
[2024-06-23 12:58:28,002] [INFO] [launch.py:348:main] Process 1585439 exits successfully.
