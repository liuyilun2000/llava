[2024-06-26 17:23:33,702] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:23:42,776] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-06-26 17:23:42,776] [INFO] [runner.py:568:main] cmd = /home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train_parallel_deepspeed_mixtral_lora.py --num_stages=8 --lora_r=128 --lora_alpha=256 --save_model_shard=26 --skip_shard=5040 --checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint
[2024-06-26 17:23:45,941] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:23:47,397] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-06-26 17:23:47,398] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-06-26 17:23:47,398] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-06-26 17:23:47,398] [INFO] [launch.py:163:main] dist_world_size=8
[2024-06-26 17:23:47,398] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-06-26 17:23:47,404] [INFO] [launch.py:253:main] process 432668 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=0', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:23:47,411] [INFO] [launch.py:253:main] process 432669 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=1', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:23:47,422] [INFO] [launch.py:253:main] process 432670 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=2', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:23:47,432] [INFO] [launch.py:253:main] process 432671 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=3', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:23:47,443] [INFO] [launch.py:253:main] process 432672 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=4', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:23:47,451] [INFO] [launch.py:253:main] process 432673 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=5', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:23:47,462] [INFO] [launch.py:253:main] process 432674 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=6', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:23:47,472] [INFO] [launch.py:253:main] process 432675 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=7', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=26', '--skip_shard=5040', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-06-26 17:24:06,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:24:06,978] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:24:08,468] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:24:09,078] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:24:09,716] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:24:14,874] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:24:14,934] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:24:14,935] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-26 17:24:15,135] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:24:15,192] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:24:15,223] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-26 17:24:15,475] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:24:15,750] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:24:15,839] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-26 17:24:15,855] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:13,  1.44it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:00<00:15,  1.25it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:27,  1.46s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:28,  1.48s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:26,  1.40s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:24,  1.31s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:20,  1.14s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:22,  1.28s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:27,  1.55s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:26,  1.48s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:02<00:25,  1.41s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:03<00:23,  1.37s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:30,  1.67s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:04<00:27,  1.62s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:04<00:25,  1.50s/it][2024-06-26 17:24:23,819] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Loading checkpoint shards:  15%|█▌        | 3/20 [00:04<00:23,  1.39s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:04<00:27,  1.64s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:05<00:26,  1.64s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:32,  1.93s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:05<00:21,  1.33s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:06<00:26,  1.68s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:06<00:25,  1.60s/it][2024-06-26 17:24:25,984] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:  20%|██        | 4/20 [00:06<00:28,  1.75s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:07<00:24,  1.62s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:06<00:19,  1.32s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:07<00:24,  1.62s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:30,  1.88s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:08<00:27,  1.85s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:08<00:27,  1.83s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:08<00:20,  1.48s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:09<00:24,  1.74s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:09<00:25,  1.83s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:31,  2.12s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:10<00:27,  1.97s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:10<00:26,  1.89s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:11<00:25,  1.94s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:10<00:23,  1.81s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:11<00:24,  1.88s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:30,  2.17s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:12<00:24,  1.87s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  40%|████      | 8/20 [00:12<00:21,  1.80s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  40%|████      | 8/20 [00:13<00:22,  1.92s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:14<00:24,  2.05s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:14<00:27,  2.09s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:13<00:30,  2.35s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:34,  1.80s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:14<00:22,  1.91s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:14<00:21,  1.91s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:02<00:44,  2.33s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:15<00:22,  2.02s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:35,  1.99s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:16<00:24,  2.22s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:26,  2.18s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:28,  2.38s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:16<00:22,  2.02s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:16<00:19,  1.93s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:18<00:20,  2.05s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:05<00:47,  2.64s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:25,  2.33s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:19<00:23,  2.31s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:24,  2.25s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:06<00:38,  2.29s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:18<00:17,  1.95s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:24,  2.48s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:07<00:41,  2.42s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:21,  2.14s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:21<00:20,  2.28s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:08<00:35,  2.24s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:21<00:23,  2.35s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:21<00:22,  2.53s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:21<00:17,  2.16s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:22<00:20,  2.33s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:09<00:36,  2.29s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:23<00:20,  2.24s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:23<00:18,  2.32s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:24<00:19,  2.49s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:23<00:14,  2.06s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:11<00:37,  2.47s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:23<00:22,  2.51s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:24<00:19,  2.38s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:11<00:35,  2.35s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:25<00:12,  2.05s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:26<00:17,  2.44s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:25<00:18,  2.35s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:26<00:19,  2.41s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:26<00:17,  2.44s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:13<00:33,  2.41s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:14<00:31,  2.29s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:27<00:17,  2.46s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:27<00:10,  2.07s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:28<00:13,  2.31s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:28<00:16,  2.34s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:28<00:16,  2.42s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:16<00:31,  2.44s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:29<00:15,  2.50s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:16<00:30,  2.35s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:29<00:15,  2.51s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:30<00:11,  2.34s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:30<00:08,  2.24s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:30<00:13,  2.30s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:18<00:27,  2.28s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:31<00:12,  2.44s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:31<00:15,  2.60s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:19<00:28,  2.41s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:32<00:12,  2.49s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:32<00:10,  2.18s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:32<00:06,  2.19s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:20<00:25,  2.30s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:33<00:09,  2.30s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:33<00:09,  2.47s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:34<00:09,  2.32s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:34<00:13,  2.66s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:21<00:25,  2.34s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:34<00:04,  2.27s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:22<00:21,  2.19s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:35<00:09,  2.33s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:35<00:06,  2.28s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:36<00:07,  2.56s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:36<00:02,  2.10s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:36<00:07,  2.37s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:23<00:23,  2.34s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:36<00:10,  2.60s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:36<00:00,  1.63s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:36<00:00,  1.84s/it]
Loading checkpoint shards:  55%|█████▌    | 11/20 [00:24<00:19,  2.21s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:38<00:04,  2.27s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:37<00:07,  2.44s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:38<00:05,  2.57s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:25<00:19,  2.20s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:39<00:07,  2.51s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:39<00:04,  2.43s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:18,  2.33s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:40<00:02,  2.30s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:27<00:16,  2.07s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:40<00:05,  2.59s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:41<00:02,  2.51s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  2.07s/it]
Loading checkpoint shards:  90%|█████████ | 18/20 [00:41<00:04,  2.41s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:41<00:02,  2.38s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:28<00:12,  1.83s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:42<00:00,  2.12s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:42<00:00,  2.13s/it]
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:42<00:02,  2.37s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:42<00:02,  2.20s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.09s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.15s/it]
Loading checkpoint shards: 100%|██████████| 20/20 [00:42<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:42<00:00,  2.15s/it]
Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  1.67s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:43<00:00,  2.17s/it]
Loading checkpoint shards:  65%|██████▌   | 13/20 [00:31<00:19,  2.73s/it]Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Loading checkpoint shards:  70%|███████   | 14/20 [00:30<00:11,  1.92s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:32<00:10,  2.01s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:34<00:16,  2.83s/it]Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Loading checkpoint shards:  80%|████████  | 16/20 [00:34<00:07,  1.97s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:36<00:13,  2.61s/it]Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Loading checkpoint shards:  85%|████████▌ | 17/20 [00:36<00:05,  1.79s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:37<00:08,  2.15s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:37<00:03,  1.60s/it]Rank 5 initialized with CUDA_MEM (60870623232, 85100068864)
Deepspeed engine initializing at --- RANK 5 --- ...
Loading checkpoint shards:  85%|████████▌ | 17/20 [00:38<00:05,  1.84s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:38<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:38<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:38<00:00,  1.92s/it]
Loading checkpoint shards:  90%|█████████ | 18/20 [00:39<00:03,  1.56s/it]Rank 3 initialized with CUDA_MEM (60870623232, 85100068864)
Deepspeed engine initializing at --- RANK 3 --- ...
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:40<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:40<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:40<00:00,  2.04s/it]
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1, ProcessCoord(pipe=2, data=0): 2, ProcessCoord(pipe=3, data=0): 3, ProcessCoord(pipe=4, data=0): 4, ProcessCoord(pipe=5, data=0): 5, ProcessCoord(pipe=6, data=0): 6, ProcessCoord(pipe=7, data=0): 7}
[2024-06-26 17:25:12,739] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:LanguageModelLayerWrapper
stage=0 layers=2
     0: LlavaMultiModalModuleWrapper
     1: LanguageModelLayerWrapper
stage=1 layers=1
     2: LanguageModelLayerWrapper
stage=2 layers=1
     3: LanguageModelLayerWrapper
stage=3 layers=1
     4: LanguageModelLayerWrapper
stage=4 layers=1
     5: LanguageModelLayerWrapper
stage=5 layers=1
     6: LanguageModelLayerWrapper
stage=6 layers=1
     7: LanguageModelLayerWrapper
stage=7 layers=2
     8: LanguageModelLayerWrapper
     9: LanguageModelFinalWrapper
  loss: loss_fn
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 7 initialized with CUDA_MEM (60344238080, 85100068864)
Deepspeed engine initializing at --- RANK 7 --- ...
Rank 4 initialized with CUDA_MEM (60870623232, 85100068864)
Deepspeed engine initializing at --- RANK 4 --- ...
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 1 initialized with CUDA_MEM (60870623232, 85100068864)
Deepspeed engine initializing at --- RANK 1 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 1.2123746871948242 seconds
Time to load fused_adam op: 2.334660291671753 seconds
Time to load fused_adam op: 2.7385294437408447 seconds
[2024-06-26 17:25:15,550] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-06-26 17:25:15,551] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-06-26 17:25:15,551] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 0.5043315887451172 seconds
[2024-06-26 17:25:15,596] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 0 initialized with CUDA_MEM (58909786112, 85100068864)
Deepspeed engine initializing at --- RANK 0 --- ...
[2024-06-26 17:25:16,353] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-06-26 17:25:16,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Rank 6 initialized with CUDA_MEM (60870623232, 85100068864)
Deepspeed engine initializing at --- RANK 6 --- ...
Rank 2 initialized with CUDA_MEM (60870623232, 85100068864)
Deepspeed engine initializing at --- RANK 2 --- ...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.1141164302825928 seconds
[2024-06-26 17:25:17,006] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 1.471769094467163 seconds
[2024-06-26 17:25:18,563] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-06-26 17:25:18,563] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-26 17:25:18,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-06-26 17:25:18,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-06-26 17:25:18,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupCosineLR
[2024-06-26 17:25:18,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupCosineLR object at 0x14fe504c3a60>
[2024-06-26 17:25:18,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-06-26 17:25:18,573] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14fe50cffa90>
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-06-26 17:25:18,574] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 128
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=True, output_path='/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint', job_name='deepspeed_monitor_logs') enabled=True
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   optimizer_name ............... adam
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 2e-05}
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True, 'use_reentrant': False}
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupCosineLR
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 5718, 'warmup_min_ratio': 0.1, 'warmup_num_steps': 171.54}
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   steps_per_print .............. 1
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-06-26 17:25:18,575] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-06-26 17:25:18,576] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-26 17:25:18,576] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-06-26 17:25:18,576] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-06-26 17:25:18,576] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-06-26 17:25:18,576] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 128, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupCosineLR", 
        "params": {
            "total_num_steps": 5.718000e+03, 
            "warmup_min_ratio": 0.1, 
            "warmup_num_steps": 171.54
        }
    }, 
    "pipeline": {
        "use_reentrant": false
    }, 
    "csv_monitor": {
        "enabled": true, 
        "output_path": "/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint", 
        "job_name": "deepspeed_monitor_logs"
    }
}
[2024-06-26 17:25:18,576] [INFO] [engine.py:101:__init__] CONFIG: micro_batches=128 micro_batch_size=1
[2024-06-26 17:25:18,576] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 1.0054411888122559 seconds
[2024-06-26 17:25:18,640] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 0.9041862487792969 seconds
[2024-06-26 17:25:18,654] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-06-26 17:25:23,208] [INFO] [engine.py:160:__init__] RANK=6 STAGE=6 LAYERS=1 [7, 8) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-06-26 17:25:23,208] [INFO] [engine.py:160:__init__] RANK=2 STAGE=2 LAYERS=1 [3, 4) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-06-26 17:25:23,208] [INFO] [engine.py:160:__init__] RANK=7 STAGE=7 LAYERS=2 [8, 10) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-06-26 17:25:23,208] [INFO] [engine.py:160:__init__] RANK=1 STAGE=1 LAYERS=1 [2, 3) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-06-26 17:25:23,209] [INFO] [engine.py:160:__init__] RANK=3 STAGE=3 LAYERS=1 [4, 5) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-06-26 17:25:23,209] [INFO] [engine.py:160:__init__] RANK=5 STAGE=5 LAYERS=1 [6, 7) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-06-26 17:25:23,209] [INFO] [engine.py:160:__init__] RANK=0 STAGE=0 LAYERS=2 [0, 2) STAGE_PARAMS=48775168 (48.775M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-06-26 17:25:23,209] [INFO] [engine.py:160:__init__] RANK=4 STAGE=4 LAYERS=1 [5, 6) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
Deepspeed engine successfully initialized at --- RANK 0 --- hosting 24 of 136 trainable parameters
Loading latest model checkpoint at shard 5040
[2024-06-26 17:25:26,014] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 7 --- hosting 16 of 136 trainable parameters
[2024-06-26 17:25:26,331] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 2 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 1 --- hosting 16 of 136 trainable parameters
[2024-06-26 17:25:28,214] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
[2024-06-26 17:25:28,214] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 3 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 4 --- hosting 16 of 136 trainable parameters
[2024-06-26 17:25:28,353] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
[2024-06-26 17:25:28,354] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 6 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 5 --- hosting 16 of 136 trainable parameters
[2024-06-26 17:25:28,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
[2024-06-26 17:25:28,437] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
[2024-06-26 17:25:33,390] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,390] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,390] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_05_model_states.pt...
[2024-06-26 17:25:33,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_07_model_states.pt...
[2024-06-26 17:25:33,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt...
[2024-06-26 17:25:33,391] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,391] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,391] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,391] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,391] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,391] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_02_model_states.pt...
[2024-06-26 17:25:33,391] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_06_model_states.pt...
[2024-06-26 17:25:33,392] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_01_model_states.pt...
[2024-06-26 17:25:33,392] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_03_model_states.pt...
[2024-06-26 17:25:33,392] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_04_model_states.pt...
[2024-06-26 17:25:33,452] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_00_model_states.pt.
[2024-06-26 17:25:33,453] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_00-model_states.pt...
[2024-06-26 17:25:34,577] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_04_model_states.pt.
[2024-06-26 17:25:34,577] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_05-model_states.pt...
[2024-06-26 17:25:34,830] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_02_model_states.pt.
[2024-06-26 17:25:34,830] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_03-model_states.pt...
[2024-06-26 17:25:34,966] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_01_model_states.pt.
[2024-06-26 17:25:34,967] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_02-model_states.pt...
[2024-06-26 17:25:35,005] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_06_model_states.pt.
[2024-06-26 17:25:35,006] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_07-model_states.pt...
[2024-06-26 17:25:35,097] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_07_model_states.pt.
[2024-06-26 17:25:35,098] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_08-model_states.pt...
[2024-06-26 17:25:35,290] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_05_model_states.pt.
[2024-06-26 17:25:35,291] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_06-model_states.pt...
[2024-06-26 17:25:35,868] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/mp_rank_03_model_states.pt.
[2024-06-26 17:25:35,869] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_04-model_states.pt...
[2024-06-26 17:26:17,544] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_00-model_states.pt.
[2024-06-26 17:26:17,546] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_00-model_states.pt...
[2024-06-26 17:26:17,808] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_00-model_states.pt.
[2024-06-26 17:26:17,979] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_01-model_states.pt...
[2024-06-26 17:33:10,521] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_04-model_states.pt.
[2024-06-26 17:33:10,555] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_04-model_states.pt...
[2024-06-26 17:33:14,002] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_04-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:33:38,092] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_02-model_states.pt.
[2024-06-26 17:33:38,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_02-model_states.pt...
[2024-06-26 17:33:43,930] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_05-model_states.pt.
[2024-06-26 17:33:44,019] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_05-model_states.pt...
[2024-06-26 17:33:45,220] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_07-model_states.pt.
[2024-06-26 17:33:45,254] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_07-model_states.pt...
[2024-06-26 17:33:45,803] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_03-model_states.pt.
[2024-06-26 17:33:45,836] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_03-model_states.pt...
[2024-06-26 17:33:48,689] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_07-model_states.pt.
[2024-06-26 17:33:49,251] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_03-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:33:51,297] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_08-model_states.pt.
[2024-06-26 17:33:51,417] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_08-model_states.pt...
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:33:59,642] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_06-model_states.pt.
[2024-06-26 17:33:59,790] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_06-model_states.pt...
[2024-06-26 17:34:41,003] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_01-model_states.pt.
[2024-06-26 17:34:41,118] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_01-model_states.pt...
[2024-06-26 17:35:54,037] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_06-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:36:13,581] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_02-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:36:29,265] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_05-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:37:10,775] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_08-model_states.pt.
[2024-06-26 17:37:12,537] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_09-model_states.pt...
[2024-06-26 17:37:20,928] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_09-model_states.pt.
[2024-06-26 17:37:20,944] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_09-model_states.pt...
[2024-06-26 17:37:21,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_09-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:39:02,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5010/layer_01-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]Shard 0 / 5040 skipped
Shard 1 / 5040 skipped
Shard 2 / 5040 skipped
Shard 3 / 5040 skipped
Shard 4 / 5040 skipped
Shard 5 / 5040 skipped
Shard 6 / 5040 skipped
Shard 7 / 5040 skipped
Shard 8 / 5040 skipped
Shard 9 / 5040 skipped
Shard 10 / 5040 skipped
Shard 11 / 5040 skipped
Shard 12 / 5040 skipped
Shard 13 / 5040 skipped
Shard 14 / 5040 skipped
Shard 15 / 5040 skipped
Shard 16 / 5040 skipped
Shard 17 / 5040 skipped
Shard 18 / 5040 skipped
Shard 19 / 5040 skipped
Shard 20 / 5040 skipped
Shard 21 / 5040 skipped
Shard 22 / 5040 skipped
Shard 23 / 5040 skipped
Shard 24 / 5040 skipped
Shard 25 / 5040 skipped
Shard 26 / 5040 skipped
Shard 27 / 5040 skipped
Shard 28 / 5040 skipped
Shard 29 / 5040 skipped
Shard 30 / 5040 skipped
Shard 31 / 5040 skipped
Shard 32 / 5040 skipped
Shard 33 / 5040 skipped
Shard 34 / 5040 skipped
Shard 35 / 5040 skipped
Shard 36 / 5040 skipped
Shard 37 / 5040 skipped
Shard 38 / 5040 skipped
Shard 39 / 5040 skipped
Shard 40 / 5040 skipped
Shard 41 / 5040 skipped
Shard 42 / 5040 skipped
Shard 43 / 5040 skipped
Shard 44 / 5040 skipped
Shard 45 / 5040 skipped
Shard 46 / 5040 skipped
Shard 47 / 5040 skipped
Shard 48 / 5040 skipped
Shard 49 / 5040 skipped
Shard 50 / 5040 skipped
Shard 51 / 5040 skipped
Shard 52 / 5040 skipped
Shard 53 / 5040 skipped
Shard 54 / 5040 skipped
Shard 55 / 5040 skipped
Shard 56 / 5040 skipped
Shard 57 / 5040 skipped
Shard 58 / 5040 skipped
Shard 59 / 5040 skipped
Shard 60 / 5040 skipped
Shard 61 / 5040 skipped
Shard 62 / 5040 skipped
Shard 63 / 5040 skipped
Shard 64 / 5040 skipped
Shard 65 / 5040 skipped
Shard 66 / 5040 skipped
Shard 67 / 5040 skipped
Shard 68 / 5040 skipped
Shard 69 / 5040 skipped
Shard 70 / 5040 skipped
Shard 71 / 5040 skipped
Shard 72 / 5040 skipped
Shard 73 / 5040 skipped
Shard 74 / 5040 skipped
Shard 75 / 5040 skipped
Shard 76 / 5040 skipped
Shard 77 / 5040 skipped
Shard 78 / 5040 skipped
Shard 79 / 5040 skipped
Shard 80 / 5040 skipped
Shard 81 / 5040 skipped
Shard 82 / 5040 skipped
Shard 83 / 5040 skipped
Shard 84 / 5040 skipped
Shard 85 / 5040 skipped
Shard 86 / 5040 skipped
Shard 87 / 5040 skipped
Shard 88 / 5040 skipped
Shard 89 / 5040 skipped
Shard 90 / 5040 skipped
Shard 91 / 5040 skipped
Shard 92 / 5040 skipped
Shard 93 / 5040 skipped
Shard 94 / 5040 skipped
Shard 95 / 5040 skipped
Shard 96 / 5040 skipped
Shard 97 / 5040 skipped
Shard 98 / 5040 skipped
Shard 99 / 5040 skipped
Shard 100 / 5040 skipped
Shard 101 / 5040 skipped
Shard 102 / 5040 skipped
Shard 103 / 5040 skipped
Shard 104 / 5040 skipped
Shard 105 / 5040 skipped
Shard 106 / 5040 skipped
Shard 107 / 5040 skipped
Shard 108 / 5040 skipped
Shard 109 / 5040 skipped
Shard 110 / 5040 skipped
Shard 111 / 5040 skipped
Shard 112 / 5040 skipped
Shard 113 / 5040 skipped
Shard 114 / 5040 skipped
Shard 115 / 5040 skipped
Shard 116 / 5040 skipped
Shard 117 / 5040 skipped
Shard 118 / 5040 skipped
Shard 119 / 5040 skipped
Shard 120 / 5040 skipped
Shard 121 / 5040 skipped
Shard 122 / 5040 skipped
Shard 123 / 5040 skipped
Shard 124 / 5040 skipped
Shard 125 / 5040 skipped
Shard 126 / 5040 skipped
Shard 127 / 5040 skipped
Shard 128 / 5040 skipped
Shard 129 / 5040 skipped
Shard 130 / 5040 skipped
Shard 131 / 5040 skipped
Shard 132 / 5040 skipped
Shard 133 / 5040 skipped
Shard 134 / 5040 skipped
Shard 135 / 5040 skipped
Shard 136 / 5040 skipped
Shard 137 / 5040 skipped
Shard 138 / 5040 skipped
Shard 139 / 5040 skipped
Shard 140 / 5040 skipped
Shard 141 / 5040 skipped
Shard 142 / 5040 skipped
Shard 143 / 5040 skipped
Shard 144 / 5040 skipped
Shard 145 / 5040 skipped
Shard 146 / 5040 skipped
Shard 147 / 5040 skipped
Shard 148 / 5040 skipped
Shard 149 / 5040 skipped
Shard 150 / 5040 skipped
Shard 151 / 5040 skipped
Shard 152 / 5040 skipped
Shard 153 / 5040 skipped
Shard 154 / 5040 skipped
Shard 155 / 5040 skipped
Shard 156 / 5040 skipped
Shard 157 / 5040 skipped
Shard 158 / 5040 skipped
Shard 159 / 5040 skipped
Shard 160 / 5040 skipped
Shard 161 / 5040 skipped
Shard 162 / 5040 skipped
Shard 163 / 5040 skipped
Shard 164 / 5040 skipped
Shard 165 / 5040 skipped
Shard 166 / 5040 skipped
Shard 167 / 5040 skipped
Shard 168 / 5040 skipped
Shard 169 / 5040 skipped
Shard 170 / 5040 skipped
Shard 171 / 5040 skipped
Shard 172 / 5040 skipped
Shard 173 / 5040 skipped
Shard 174 / 5040 skipped
Shard 175 / 5040 skipped
Shard 176 / 5040 skipped
Shard 177 / 5040 skipped
Shard 178 / 5040 skipped
Shard 179 / 5040 skipped
Shard 180 / 5040 skipped
Shard 181 / 5040 skipped
Shard 182 / 5040 skipped
Shard 183 / 5040 skipped
Shard 184 / 5040 skipped
Shard 185 / 5040 skipped
Shard 186 / 5040 skipped
Shard 187 / 5040 skipped
Shard 188 / 5040 skipped
Shard 189 / 5040 skipped
Shard 190 / 5040 skipped
Shard 191 / 5040 skipped
Shard 192 / 5040 skipped
Shard 193 / 5040 skipped
Shard 194 / 5040 skipped
Shard 195 / 5040 skipped
Shard 196 / 5040 skipped
Shard 197 / 5040 skipped
Shard 198 / 5040 skipped
Shard 199 / 5040 skipped
Shard 200 / 5040 skipped
Shard 201 / 5040 skipped
Shard 202 / 5040 skipped
Shard 203 / 5040 skipped
Shard 204 / 5040 skipped
Shard 205 / 5040 skipped
Shard 206 / 5040 skipped
Shard 207 / 5040 skipped
Shard 208 / 5040 skipped
Shard 209 / 5040 skipped
Shard 210 / 5040 skipped
Shard 211 / 5040 skipped
Shard 212 / 5040 skipped
Shard 213 / 5040 skipped
Shard 214 / 5040 skipped
Shard 215 / 5040 skipped
Shard 216 / 5040 skipped
Shard 217 / 5040 skipped
Shard 218 / 5040 skipped
Shard 219 / 5040 skipped
Shard 220 / 5040 skipped
Shard 221 / 5040 skipped
Shard 222 / 5040 skipped
Shard 223 / 5040 skipped
Shard 224 / 5040 skipped
Shard 225 / 5040 skipped
Shard 226 / 5040 skipped
Shard 227 / 5040 skipped
Shard 228 / 5040 skipped
Shard 229 / 5040 skipped
Shard 230 / 5040 skipped
Shard 231 / 5040 skipped
Shard 232 / 5040 skipped
Shard 233 / 5040 skipped
Shard 234 / 5040 skipped
Shard 235 / 5040 skipped
Shard 236 / 5040 skipped
Shard 237 / 5040 skipped
Shard 238 / 5040 skipped
Shard 239 / 5040 skipped
Shard 240 / 5040 skipped
Shard 241 / 5040 skipped
Shard 242 / 5040 skipped
Shard 243 / 5040 skipped
Shard 244 / 5040 skipped
Shard 245 / 5040 skipped
Shard 246 / 5040 skipped
Shard 247 / 5040 skipped
Shard 248 / 5040 skipped
Shard 249 / 5040 skipped
Shard 250 / 5040 skipped
Shard 251 / 5040 skipped
Shard 252 / 5040 skipped
Shard 253 / 5040 skipped
Shard 254 / 5040 skipped
Shard 255 / 5040 skipped
Shard 256 / 5040 skipped
Shard 257 / 5040 skipped
Shard 258 / 5040 skipped
Shard 259 / 5040 skipped
Shard 260 / 5040 skipped
Shard 261 / 5040 skipped
Shard 262 / 5040 skipped
Shard 263 / 5040 skipped
Shard 264 / 5040 skipped
Shard 265 / 5040 skipped
Shard 266 / 5040 skipped
Shard 267 / 5040 skipped
Shard 268 / 5040 skipped
Shard 269 / 5040 skipped
Shard 270 / 5040 skipped
Shard 271 / 5040 skipped
Shard 272 / 5040 skipped
Shard 273 / 5040 skipped
Shard 274 / 5040 skipped
Shard 275 / 5040 skipped
Shard 276 / 5040 skipped
Shard 277 / 5040 skipped
Shard 278 / 5040 skipped
Shard 279 / 5040 skipped
Shard 280 / 5040 skipped
Shard 281 / 5040 skipped
Shard 282 / 5040 skipped
Shard 283 / 5040 skipped
Shard 284 / 5040 skipped
Shard 285 / 5040 skipped
Shard 286 / 5040 skipped
Shard 287 / 5040 skipped
Shard 288 / 5040 skipped
Shard 289 / 5040 skipped
Shard 290 / 5040 skipped
Shard 291 / 5040 skipped
Shard 292 / 5040 skipped
Shard 293 / 5040 skipped
Shard 294 / 5040 skipped
Shard 295 / 5040 skipped
Shard 296 / 5040 skipped
Shard 297 / 5040 skipped
Shard 298 / 5040 skipped
Shard 299 / 5040 skipped
Shard 300 / 5040 skipped
Shard 301 / 5040 skipped
Shard 302 / 5040 skipped
Shard 303 / 5040 skipped
Shard 304 / 5040 skipped
Shard 305 / 5040 skipped
Shard 306 / 5040 skipped
Shard 307 / 5040 skipped
Shard 308 / 5040 skipped
Shard 309 / 5040 skipped
Shard 310 / 5040 skipped
Shard 311 / 5040 skipped
Shard 312 / 5040 skipped
Shard 313 / 5040 skipped
Shard 314 / 5040 skipped
Shard 315 / 5040 skipped
Shard 316 / 5040 skipped
Shard 317 / 5040 skipped
Shard 318 / 5040 skipped
Shard 319 / 5040 skipped
Shard 320 / 5040 skipped
Shard 321 / 5040 skipped
Shard 322 / 5040 skipped
Shard 323 / 5040 skipped
Shard 324 / 5040 skipped
Shard 325 / 5040 skipped
Shard 326 / 5040 skipped
Shard 327 / 5040 skipped
Shard 328 / 5040 skipped
Shard 329 / 5040 skipped
Shard 330 / 5040 skipped
Shard 331 / 5040 skipped
Shard 332 / 5040 skipped
Shard 333 / 5040 skipped
Shard 334 / 5040 skipped
Shard 335 / 5040 skipped
Shard 336 / 5040 skipped
Shard 337 / 5040 skipped
Shard 338 / 5040 skipped
Shard 339 / 5040 skipped
Shard 340 / 5040 skipped
Shard 341 / 5040 skipped
Shard 342 / 5040 skipped
Shard 343 / 5040 skipped
Shard 344 / 5040 skipped
Shard 345 / 5040 skipped
Shard 346 / 5040 skipped
Shard 347 / 5040 skipped
Shard 348 / 5040 skipped
Shard 349 / 5040 skipped
Shard 350 / 5040 skipped
Shard 351 / 5040 skipped
Shard 352 / 5040 skipped
Shard 353 / 5040 skipped
Shard 354 / 5040 skipped
Shard 355 / 5040 skipped
Shard 356 / 5040 skipped
Shard 357 / 5040 skipped
Shard 358 / 5040 skipped
Shard 359 / 5040 skipped
Shard 360 / 5040 skipped
Shard 361 / 5040 skipped
Shard 362 / 5040 skipped
Shard 363 / 5040 skipped
Shard 364 / 5040 skipped
Shard 365 / 5040 skipped
Shard 366 / 5040 skipped
Shard 367 / 5040 skipped
Shard 368 / 5040 skipped
Shard 369 / 5040 skipped
Shard 370 / 5040 skipped
Shard 371 / 5040 skipped
Shard 372 / 5040 skipped
Shard 373 / 5040 skipped
Shard 374 / 5040 skipped
Shard 375 / 5040 skipped
Shard 376 / 5040 skipped
Shard 377 / 5040 skipped
Shard 378 / 5040 skipped
Shard 379 / 5040 skipped
Shard 380 / 5040 skipped
Shard 381 / 5040 skipped
Shard 382 / 5040 skipped
Shard 383 / 5040 skipped
Shard 384 / 5040 skipped
Shard 385 / 5040 skipped
Shard 386 / 5040 skipped
Shard 387 / 5040 skipped
Shard 388 / 5040 skipped
Shard 389 / 5040 skipped
Shard 390 / 5040 skipped
Shard 391 / 5040 skipped
Shard 392 / 5040 skipped
Shard 393 / 5040 skipped
Shard 394 / 5040 skipped
Shard 395 / 5040 skipped
Shard 396 / 5040 skipped
Shard 397 / 5040 skipped
Shard 398 / 5040 skipped
Shard 399 / 5040 skipped
Shard 400 / 5040 skipped
Shard 401 / 5040 skipped
Shard 402 / 5040 skipped
Shard 403 / 5040 skipped
Shard 404 / 5040 skipped
Shard 405 / 5040 skipped
Shard 406 / 5040 skipped
Shard 407 / 5040 skipped
Shard 408 / 5040 skipped
Shard 409 / 5040 skipped
Shard 410 / 5040 skipped
Shard 411 / 5040 skipped
Shard 412 / 5040 skipped
Shard 413 / 5040 skipped
Shard 414 / 5040 skipped
Shard 415 / 5040 skipped
Shard 416 / 5040 skipped
Shard 417 / 5040 skipped
Shard 418 / 5040 skipped
Shard 419 / 5040 skipped
Shard 420 / 5040 skipped
Shard 421 / 5040 skipped
Shard 422 / 5040 skipped
Shard 423 / 5040 skipped
Shard 424 / 5040 skipped
Shard 425 / 5040 skipped
Shard 426 / 5040 skipped
Shard 427 / 5040 skipped
Shard 428 / 5040 skipped
Shard 429 / 5040 skipped
Shard 430 / 5040 skipped
Shard 431 / 5040 skipped
Shard 432 / 5040 skipped
Shard 433 / 5040 skipped
Shard 434 / 5040 skipped
Shard 435 / 5040 skipped
Shard 436 / 5040 skipped
Shard 437 / 5040 skipped
Shard 438 / 5040 skipped
Shard 439 / 5040 skipped
Shard 440 / 5040 skipped
Shard 441 / 5040 skipped
Shard 442 / 5040 skipped
Shard 443 / 5040 skipped
Shard 444 / 5040 skipped
Shard 445 / 5040 skipped
Shard 446 / 5040 skipped
Shard 447 / 5040 skipped
Shard 448 / 5040 skipped
Shard 449 / 5040 skipped
Shard 450 / 5040 skipped
Shard 451 / 5040 skipped
Shard 452 / 5040 skipped
Shard 453 / 5040 skipped
Shard 454 / 5040 skipped
Shard 455 / 5040 skipped
Shard 456 / 5040 skipped
Shard 457 / 5040 skipped
Shard 458 / 5040 skipped
Shard 459 / 5040 skipped
Shard 460 / 5040 skipped
Shard 461 / 5040 skipped
Shard 462 / 5040 skipped
Shard 463 / 5040 skipped
Shard 464 / 5040 skipped
Shard 465 / 5040 skipped
Shard 466 / 5040 skipped
Shard 467 / 5040 skipped
Shard 468 / 5040 skipped
Shard 469 / 5040 skipped
Shard 470 / 5040 skipped
Shard 471 / 5040 skipped
Shard 472 / 5040 skipped
Shard 473 / 5040 skipped
Shard 474 / 5040 skipped
Shard 475 / 5040 skipped
Shard 476 / 5040 skipped
Shard 477 / 5040 skipped
Shard 478 / 5040 skipped
Shard 479 / 5040 skipped
Shard 480 / 5040 skipped
Shard 481 / 5040 skipped
Shard 482 / 5040 skipped
Shard 483 / 5040 skipped
Shard 484 / 5040 skipped
Shard 485 / 5040 skipped
Shard 486 / 5040 skipped
Shard 487 / 5040 skipped
Shard 488 / 5040 skipped
Shard 489 / 5040 skipped
Shard 490 / 5040 skipped
Shard 491 / 5040 skipped
Shard 492 / 5040 skipped
Shard 493 / 5040 skipped
Shard 494 / 5040 skipped
Shard 495 / 5040 skipped
Shard 496 / 5040 skipped
Shard 497 / 5040 skipped
Shard 498 / 5040 skipped
Shard 499 / 5040 skipped
Shard 500 / 5040 skipped
Shard 501 / 5040 skipped
Shard 502 / 5040 skipped
Shard 503 / 5040 skipped
Shard 504 / 5040 skipped
Shard 505 / 5040 skipped
Shard 506 / 5040 skipped
Shard 507 / 5040 skipped
Shard 508 / 5040 skipped
Shard 509 / 5040 skipped
Shard 510 / 5040 skipped
Shard 511 / 5040 skipped
Shard 512 / 5040 skipped
Shard 513 / 5040 skipped
Shard 514 / 5040 skipped
Shard 515 / 5040 skipped
Shard 516 / 5040 skipped
Shard 517 / 5040 skipped
Shard 518 / 5040 skipped
Shard 519 / 5040 skipped
Shard 520 / 5040 skipped
Shard 521 / 5040 skipped
Shard 522 / 5040 skipped
Shard 523 / 5040 skipped
Shard 524 / 5040 skipped
Shard 525 / 5040 skipped
Shard 526 / 5040 skipped
Shard 527 / 5040 skipped
Shard 528 / 5040 skipped
Shard 529 / 5040 skipped
Shard 530 / 5040 skipped
Shard 531 / 5040 skipped
Shard 532 / 5040 skipped
Shard 533 / 5040 skipped
Shard 534 / 5040 skipped
Shard 535 / 5040 skipped
Shard 536 / 5040 skipped
Shard 537 / 5040 skipped
Shard 538 / 5040 skipped
Shard 539 / 5040 skipped
Shard 540 / 5040 skipped
Shard 541 / 5040 skipped
Shard 542 / 5040 skipped
Shard 543 / 5040 skipped
Shard 544 / 5040 skipped
Shard 545 / 5040 skipped
Shard 546 / 5040 skipped
Shard 547 / 5040 skipped
Shard 548 / 5040 skipped
Shard 549 / 5040 skipped
Shard 550 / 5040 skipped
Shard 551 / 5040 skipped
Shard 552 / 5040 skipped
Shard 553 / 5040 skipped
Shard 554 / 5040 skipped
Shard 555 / 5040 skipped
Shard 556 / 5040 skipped
Shard 557 / 5040 skipped
Shard 558 / 5040 skipped
Shard 559 / 5040 skipped
Shard 560 / 5040 skipped
Shard 561 / 5040 skipped
Shard 562 / 5040 skipped
Shard 563 / 5040 skipped
Shard 564 / 5040 skipped
Shard 565 / 5040 skipped
Shard 566 / 5040 skipped
Shard 567 / 5040 skipped
Shard 568 / 5040 skipped
Shard 569 / 5040 skipped
Shard 570 / 5040 skipped
Shard 571 / 5040 skipped
Shard 572 / 5040 skipped
Shard 573 / 5040 skipped
Shard 574 / 5040 skipped
Shard 575 / 5040 skipped
Shard 576 / 5040 skipped
Shard 577 / 5040 skipped
Shard 578 / 5040 skipped
Shard 579 / 5040 skipped
Shard 580 / 5040 skipped
Shard 581 / 5040 skipped
Shard 582 / 5040 skipped
Shard 583 / 5040 skipped
Shard 584 / 5040 skipped
Shard 585 / 5040 skipped
Shard 586 / 5040 skipped
Shard 587 / 5040 skipped
Shard 588 / 5040 skipped
Shard 589 / 5040 skipped
Shard 590 / 5040 skipped
Shard 591 / 5040 skipped
Shard 592 / 5040 skipped
Shard 593 / 5040 skipped
Shard 594 / 5040 skipped
Shard 595 / 5040 skipped
Shard 596 / 5040 skipped
Shard 597 / 5040 skipped
Shard 598 / 5040 skipped
Shard 599 / 5040 skipped
Shard 600 / 5040 skipped
Shard 601 / 5040 skipped
Shard 602 / 5040 skipped
Shard 603 / 5040 skipped
Shard 604 / 5040 skipped
Shard 605 / 5040 skipped
Shard 606 / 5040 skipped
Shard 607 / 5040 skipped
Shard 608 / 5040 skipped
Shard 609 / 5040 skipped
Shard 610 / 5040 skipped
Shard 611 / 5040 skipped
Shard 612 / 5040 skipped
Shard 613 / 5040 skipped
Shard 614 / 5040 skipped
Shard 615 / 5040 skipped
Shard 616 / 5040 skipped
Shard 617 / 5040 skipped
Shard 618 / 5040 skipped
Shard 619 / 5040 skipped
Shard 620 / 5040 skipped
Shard 621 / 5040 skipped
Shard 622 / 5040 skipped
Shard 623 / 5040 skipped
Shard 624 / 5040 skipped
Shard 625 / 5040 skipped
Shard 626 / 5040 skipped
Shard 627 / 5040 skipped
Shard 628 / 5040 skipped
Shard 629 / 5040 skipped
Shard 630 / 5040 skipped
Shard 631 / 5040 skipped
Shard 632 / 5040 skipped
Shard 633 / 5040 skipped
Shard 634 / 5040 skipped
Shard 635 / 5040 skipped
Shard 636 / 5040 skipped
Shard 637 / 5040 skipped
Shard 638 / 5040 skipped
Shard 639 / 5040 skipped
Shard 640 / 5040 skipped
Shard 641 / 5040 skipped
Shard 642 / 5040 skipped
Shard 643 / 5040 skipped
Shard 644 / 5040 skipped
Shard 645 / 5040 skipped
Shard 646 / 5040 skipped
Shard 647 / 5040 skipped
Shard 648 / 5040 skipped
Shard 649 / 5040 skipped
Shard 650 / 5040 skipped
Shard 651 / 5040 skipped
Shard 652 / 5040 skipped
Shard 653 / 5040 skipped
Shard 654 / 5040 skipped
Shard 655 / 5040 skipped
Shard 656 / 5040 skipped
Shard 657 / 5040 skipped
Shard 658 / 5040 skipped
Shard 659 / 5040 skipped
Shard 660 / 5040 skipped
Shard 661 / 5040 skipped
Shard 662 / 5040 skipped
Shard 663 / 5040 skipped
Shard 664 / 5040 skipped
Shard 665 / 5040 skipped
Shard 666 / 5040 skipped
Shard 667 / 5040 skipped
Shard 668 / 5040 skipped
Shard 669 / 5040 skipped
Shard 670 / 5040 skipped
Shard 671 / 5040 skipped
Shard 672 / 5040 skipped
Shard 673 / 5040 skipped
Shard 674 / 5040 skipped
Shard 675 / 5040 skipped
Shard 676 / 5040 skipped
Shard 677 / 5040 skipped
Shard 678 / 5040 skipped
Shard 679 / 5040 skipped
Shard 680 / 5040 skipped
Shard 681 / 5040 skipped
Shard 682 / 5040 skipped
Shard 683 / 5040 skipped
Shard 684 / 5040 skipped
Shard 685 / 5040 skipped
Shard 686 / 5040 skipped
Shard 687 / 5040 skipped
Shard 688 / 5040 skipped
Shard 689 / 5040 skipped
Shard 690 / 5040 skipped
Shard 691 / 5040 skipped
Shard 692 / 5040 skipped
Shard 693 / 5040 skipped
Shard 694 / 5040 skipped
Shard 695 / 5040 skipped
Shard 696 / 5040 skipped
Shard 697 / 5040 skipped
Shard 698 / 5040 skipped
Shard 699 / 5040 skipped
Shard 700 / 5040 skipped
Shard 701 / 5040 skipped
Shard 702 / 5040 skipped
Shard 703 / 5040 skipped
Shard 704 / 5040 skipped
Shard 705 / 5040 skipped
Shard 706 / 5040 skipped
Shard 707 / 5040 skipped
Shard 708 / 5040 skipped
Shard 709 / 5040 skipped
Shard 710 / 5040 skipped
Shard 711 / 5040 skipped
Shard 712 / 5040 skipped
Shard 713 / 5040 skipped
Shard 714 / 5040 skipped
Shard 715 / 5040 skipped
Shard 716 / 5040 skipped
Shard 717 / 5040 skipped
Shard 718 / 5040 skipped
Shard 719 / 5040 skipped
Shard 720 / 5040 skipped
Shard 721 / 5040 skipped
Shard 722 / 5040 skipped
Shard 723 / 5040 skipped
Shard 724 / 5040 skipped
Shard 725 / 5040 skipped
Shard 726 / 5040 skipped
Shard 727 / 5040 skipped
Shard 728 / 5040 skipped
Shard 729 / 5040 skipped
Shard 730 / 5040 skipped
Shard 731 / 5040 skipped
Shard 732 / 5040 skipped
Shard 733 / 5040 skipped
Shard 734 / 5040 skipped
Shard 735 / 5040 skipped
Shard 736 / 5040 skipped
Shard 737 / 5040 skipped
Shard 738 / 5040 skipped
Shard 739 / 5040 skipped
Shard 740 / 5040 skipped
Shard 741 / 5040 skipped
Shard 742 / 5040 skipped
Shard 743 / 5040 skipped
Shard 744 / 5040 skipped
Shard 745 / 5040 skipped
Shard 746 / 5040 skipped
Shard 747 / 5040 skipped
Shard 748 / 5040 skipped
Shard 749 / 5040 skipped
Shard 750 / 5040 skipped
Shard 751 / 5040 skipped
Shard 752 / 5040 skipped
Shard 753 / 5040 skipped
Shard 754 / 5040 skipped
Shard 755 / 5040 skipped
Shard 756 / 5040 skipped
Shard 757 / 5040 skipped
Shard 758 / 5040 skipped
Shard 759 / 5040 skipped
Shard 760 / 5040 skipped
Shard 761 / 5040 skipped
Shard 762 / 5040 skipped
Shard 763 / 5040 skipped
Shard 764 / 5040 skipped
Shard 765 / 5040 skipped
Shard 766 / 5040 skipped
Shard 767 / 5040 skipped
Shard 768 / 5040 skipped
Shard 769 / 5040 skipped
Shard 770 / 5040 skipped
Shard 771 / 5040 skipped
Shard 772 / 5040 skipped
Shard 773 / 5040 skipped
Shard 774 / 5040 skipped
Shard 775 / 5040 skipped
Shard 776 / 5040 skipped
Shard 777 / 5040 skipped
Shard 778 / 5040 skipped
Shard 779 / 5040 skipped
Shard 780 / 5040 skipped
Shard 781 / 5040 skipped
Shard 782 / 5040 skipped
Shard 783 / 5040 skipped
Shard 784 / 5040 skipped
Shard 785 / 5040 skipped
Shard 786 / 5040 skipped
Shard 787 / 5040 skipped
Shard 788 / 5040 skipped
Shard 789 / 5040 skipped
Shard 790 / 5040 skipped
Shard 791 / 5040 skipped
Shard 792 / 5040 skipped
Shard 793 / 5040 skipped
Shard 794 / 5040 skipped
Shard 795 / 5040 skipped
Shard 796 / 5040 skipped
Shard 797 / 5040 skipped
Shard 798 / 5040 skipped
Shard 799 / 5040 skipped
Shard 800 / 5040 skipped
Shard 801 / 5040 skipped
Shard 802 / 5040 skipped
Shard 803 / 5040 skipped
Shard 804 / 5040 skipped
Shard 805 / 5040 skipped
Shard 806 / 5040 skipped
Shard 807 / 5040 skipped
Shard 808 / 5040 skipped
Shard 809 / 5040 skipped
Shard 810 / 5040 skipped
Shard 811 / 5040 skipped
Shard 812 / 5040 skipped
Shard 813 / 5040 skipped
Shard 814 / 5040 skipped
Shard 815 / 5040 skipped
Shard 816 / 5040 skipped
Shard 817 / 5040 skipped
Shard 818 / 5040 skipped
Shard 819 / 5040 skipped
Shard 820 / 5040 skipped
Shard 821 / 5040 skipped
Shard 822 / 5040 skipped
Shard 823 / 5040 skipped
Shard 824 / 5040 skipped
Shard 825 / 5040 skipped
Shard 826 / 5040 skipped
Shard 827 / 5040 skipped
Shard 828 / 5040 skipped
Shard 829 / 5040 skipped
Shard 830 / 5040 skipped
Shard 831 / 5040 skipped
Shard 832 / 5040 skipped
Shard 833 / 5040 skipped
Shard 834 / 5040 skipped
Shard 835 / 5040 skipped
Shard 836 / 5040 skipped
Shard 837 / 5040 skipped
Shard 838 / 5040 skipped
Shard 839 / 5040 skipped
Shard 840 / 5040 skipped
Shard 841 / 5040 skipped
Shard 842 / 5040 skipped
Shard 843 / 5040 skipped
Shard 844 / 5040 skipped
Shard 845 / 5040 skipped
Shard 846 / 5040 skipped
Shard 847 / 5040 skipped
Shard 848 / 5040 skipped
Shard 849 / 5040 skipped
Shard 850 / 5040 skipped
Shard 851 / 5040 skipped
Shard 852 / 5040 skipped
Shard 853 / 5040 skipped
Shard 854 / 5040 skipped
Shard 855 / 5040 skipped
Shard 856 / 5040 skipped
Shard 857 / 5040 skipped
Shard 858 / 5040 skipped
Shard 859 / 5040 skipped
Shard 860 / 5040 skipped
Shard 861 / 5040 skipped
Shard 862 / 5040 skipped
Shard 863 / 5040 skipped
Shard 864 / 5040 skipped
Shard 865 / 5040 skipped
Shard 866 / 5040 skipped
Shard 867 / 5040 skipped
Shard 868 / 5040 skipped
Shard 869 / 5040 skipped
Shard 870 / 5040 skipped
Shard 871 / 5040 skipped
Shard 872 / 5040 skipped
Shard 873 / 5040 skipped
Shard 874 / 5040 skipped
Shard 875 / 5040 skipped
Shard 876 / 5040 skipped
Shard 877 / 5040 skipped
Shard 878 / 5040 skipped
Shard 879 / 5040 skipped
Shard 880 / 5040 skipped
Shard 881 / 5040 skipped
Shard 882 / 5040 skipped
Shard 883 / 5040 skipped
Shard 884 / 5040 skipped
Shard 885 / 5040 skipped
Shard 886 / 5040 skipped
Shard 887 / 5040 skipped
Shard 888 / 5040 skipped
Shard 889 / 5040 skipped
Shard 890 / 5040 skipped
Shard 891 / 5040 skipped
Shard 892 / 5040 skipped
Shard 893 / 5040 skipped
Shard 894 / 5040 skipped
Shard 895 / 5040 skipped
Shard 896 / 5040 skipped
Shard 897 / 5040 skipped
Shard 898 / 5040 skipped
Shard 899 / 5040 skipped
Shard 900 / 5040 skipped
Shard 901 / 5040 skipped
Shard 902 / 5040 skipped
Shard 903 / 5040 skipped
Shard 904 / 5040 skipped
Shard 905 / 5040 skipped
Shard 906 / 5040 skipped
Shard 907 / 5040 skipped
Shard 908 / 5040 skipped
Shard 909 / 5040 skipped
Shard 910 / 5040 skipped
Shard 911 / 5040 skipped
Shard 912 / 5040 skipped
Shard 913 / 5040 skipped
Shard 914 / 5040 skipped
Shard 915 / 5040 skipped
Shard 916 / 5040 skipped
Shard 917 / 5040 skipped
Shard 918 / 5040 skipped
Shard 919 / 5040 skipped
Shard 920 / 5040 skipped
Shard 921 / 5040 skipped
Shard 922 / 5040 skipped
Shard 923 / 5040 skipped
Shard 924 / 5040 skipped
Shard 925 / 5040 skipped
Shard 926 / 5040 skipped
Shard 927 / 5040 skipped
Shard 928 / 5040 skipped
Shard 929 / 5040 skipped
Shard 930 / 5040 skipped
Shard 931 / 5040 skipped
Shard 932 / 5040 skipped
Shard 933 / 5040 skipped
Shard 934 / 5040 skipped
Shard 935 / 5040 skipped
Shard 936 / 5040 skipped
Shard 937 / 5040 skipped
Shard 938 / 5040 skipped
Shard 939 / 5040 skipped
Shard 940 / 5040 skipped
Shard 941 / 5040 skipped
Shard 942 / 5040 skipped
Shard 943 / 5040 skipped
Shard 944 / 5040 skipped
Shard 945 / 5040 skipped
Shard 946 / 5040 skipped
Shard 947 / 5040 skipped
Shard 948 / 5040 skipped
Shard 949 / 5040 skipped
Shard 950 / 5040 skipped
Shard 951 / 5040 skipped
Shard 952 / 5040 skipped
Shard 953 / 5040 skipped
Shard 954 / 5040 skipped
Shard 955 / 5040 skipped
Shard 956 / 5040 skipped
Shard 957 / 5040 skipped
Shard 958 / 5040 skipped
Shard 959 / 5040 skipped
Shard 960 / 5040 skipped
Shard 961 / 5040 skipped
Shard 962 / 5040 skipped
Shard 963 / 5040 skipped
Shard 964 / 5040 skipped
Shard 965 / 5040 skipped
Shard 966 / 5040 skipped
Shard 967 / 5040 skipped
Shard 968 / 5040 skipped
Shard 969 / 5040 skipped
Shard 970 / 5040 skipped
Shard 971 / 5040 skipped
Shard 972 / 5040 skipped
Shard 973 / 5040 skipped
Shard 974 / 5040 skipped
Shard 975 / 5040 skipped
Shard 976 / 5040 skipped
Shard 977 / 5040 skipped
Shard 978 / 5040 skipped
Shard 979 / 5040 skipped
Shard 980 / 5040 skipped
Shard 981 / 5040 skipped
Shard 982 / 5040 skipped
Shard 983 / 5040 skipped
Shard 984 / 5040 skipped
Shard 985 / 5040 skipped
Shard 986 / 5040 skipped
Shard 987 / 5040 skipped
Shard 988 / 5040 skipped
Shard 989 / 5040 skipped
Shard 990 / 5040 skipped
Shard 991 / 5040 skipped
Shard 992 / 5040 skipped
Shard 993 / 5040 skipped
Shard 994 / 5040 skipped
Shard 995 / 5040 skipped
Shard 996 / 5040 skipped
Shard 997 / 5040 skipped
Shard 998 / 5040 skipped
Shard 999 / 5040 skipped
Shard 1000 / 5040 skipped
Shard 1001 / 5040 skipped
Shard 1002 / 5040 skipped
Shard 1003 / 5040 skipped
Shard 1004 / 5040 skipped
Shard 1005 / 5040 skipped
Shard 1006 / 5040 skipped
Shard 1007 / 5040 skipped
Shard 1008 / 5040 skipped
Shard 1009 / 5040 skipped
Shard 1010 / 5040 skipped
Shard 1011 / 5040 skipped
Shard 1012 / 5040 skipped
Shard 1013 / 5040 skipped
Shard 1014 / 5040 skipped
Shard 1015 / 5040 skipped
Shard 1016 / 5040 skipped
Shard 1017 / 5040 skipped
Shard 1018 / 5040 skipped
Shard 1019 / 5040 skipped
Shard 1020 / 5040 skipped
Shard 1021 / 5040 skipped
Shard 1022 / 5040 skipped
Shard 1023 / 5040 skipped
Shard 1024 / 5040 skipped
Shard 1025 / 5040 skipped
Shard 1026 / 5040 skipped
Shard 1027 / 5040 skipped
Shard 1028 / 5040 skipped
Shard 1029 / 5040 skipped
Shard 1030 / 5040 skipped
Shard 1031 / 5040 skipped
Shard 1032 / 5040 skipped
Shard 1033 / 5040 skipped
Shard 1034 / 5040 skipped
Shard 1035 / 5040 skipped
Shard 1036 / 5040 skipped
Shard 1037 / 5040 skipped
Shard 1038 / 5040 skipped
Shard 1039 / 5040 skipped
Shard 1040 / 5040 skipped
Shard 1041 / 5040 skipped
Shard 1042 / 5040 skipped
Shard 1043 / 5040 skipped
Shard 1044 / 5040 skipped
Shard 1045 / 5040 skipped
Shard 1046 / 5040 skipped
Shard 1047 / 5040 skipped
Shard 1048 / 5040 skipped
Shard 1049 / 5040 skipped
Shard 1050 / 5040 skipped
Shard 1051 / 5040 skipped
Shard 1052 / 5040 skipped
Shard 1053 / 5040 skipped
Shard 1054 / 5040 skipped
Shard 1055 / 5040 skipped
Shard 1056 / 5040 skipped
Shard 1057 / 5040 skipped
Shard 1058 / 5040 skipped
Shard 1059 / 5040 skipped
Shard 1060 / 5040 skipped
Shard 1061 / 5040 skipped
Shard 1062 / 5040 skipped
Shard 1063 / 5040 skipped
Shard 1064 / 5040 skipped
Shard 1065 / 5040 skipped
Shard 1066 / 5040 skipped
Shard 1067 / 5040 skipped
Shard 1068 / 5040 skipped
Shard 1069 / 5040 skipped
Shard 1070 / 5040 skipped
Shard 1071 / 5040 skipped
Shard 1072 / 5040 skipped
Shard 1073 / 5040 skipped
Shard 1074 / 5040 skipped
Shard 1075 / 5040 skipped
Shard 1076 / 5040 skipped
Shard 1077 / 5040 skipped
Shard 1078 / 5040 skipped
Shard 1079 / 5040 skipped
Shard 1080 / 5040 skipped
Shard 1081 / 5040 skipped
Shard 1082 / 5040 skipped
Shard 1083 / 5040 skipped
Shard 1084 / 5040 skipped
Shard 1085 / 5040 skipped
Shard 1086 / 5040 skipped
Shard 1087 / 5040 skipped
Shard 1088 / 5040 skipped
Shard 1089 / 5040 skipped
Shard 1090 / 5040 skipped
Shard 1091 / 5040 skipped
Shard 1092 / 5040 skipped
Shard 1093 / 5040 skipped
Shard 1094 / 5040 skipped
Shard 1095 / 5040 skipped
Shard 1096 / 5040 skipped
Shard 1097 / 5040 skipped
Shard 1098 / 5040 skipped
Shard 1099 / 5040 skipped
Shard 1100 / 5040 skipped
Shard 1101 / 5040 skipped
Shard 1102 / 5040 skipped
Shard 1103 / 5040 skipped
Shard 1104 / 5040 skipped
Shard 1105 / 5040 skipped
Shard 1106 / 5040 skipped
Shard 1107 / 5040 skipped
Shard 1108 / 5040 skipped
Shard 1109 / 5040 skipped
Shard 1110 / 5040 skipped
Shard 1111 / 5040 skipped
Shard 1112 / 5040 skipped
Shard 1113 / 5040 skipped
Shard 1114 / 5040 skipped
Shard 1115 / 5040 skipped
Shard 1116 / 5040 skipped
Shard 1117 / 5040 skipped
Shard 1118 / 5040 skipped
Shard 1119 / 5040 skipped
Shard 1120 / 5040 skipped
Shard 1121 / 5040 skipped
Shard 1122 / 5040 skipped
Shard 1123 / 5040 skipped
Shard 1124 / 5040 skipped
Shard 1125 / 5040 skipped
Shard 1126 / 5040 skipped
Shard 1127 / 5040 skipped
Shard 1128 / 5040 skipped
Shard 1129 / 5040 skipped
Shard 1130 / 5040 skipped
Shard 1131 / 5040 skipped
Shard 1132 / 5040 skipped
Shard 1133 / 5040 skipped
Shard 1134 / 5040 skipped
Shard 1135 / 5040 skipped
Shard 1136 / 5040 skipped
Shard 1137 / 5040 skipped
Shard 1138 / 5040 skipped
Shard 1139 / 5040 skipped
Shard 1140 / 5040 skipped
Shard 1141 / 5040 skipped
Shard 1142 / 5040 skipped
Shard 1143 / 5040 skipped
Shard 1144 / 5040 skipped
Shard 1145 / 5040 skipped
Shard 1146 / 5040 skipped
Shard 1147 / 5040 skipped
Shard 1148 / 5040 skipped
Shard 1149 / 5040 skipped
Shard 1150 / 5040 skipped
Shard 1151 / 5040 skipped
Shard 1152 / 5040 skipped
Shard 1153 / 5040 skipped
Shard 1154 / 5040 skipped
Shard 1155 / 5040 skipped
Shard 1156 / 5040 skipped
Shard 1157 / 5040 skipped
Shard 1158 / 5040 skipped
Shard 1159 / 5040 skipped
Shard 1160 / 5040 skipped
Shard 1161 / 5040 skipped
Shard 1162 / 5040 skipped
Shard 1163 / 5040 skipped
Shard 1164 / 5040 skipped
Shard 1165 / 5040 skipped
Shard 1166 / 5040 skipped
Shard 1167 / 5040 skipped
Shard 1168 / 5040 skipped
Shard 1169 / 5040 skipped
Shard 1170 / 5040 skipped
Shard 1171 / 5040 skipped
Shard 1172 / 5040 skipped
Shard 1173 / 5040 skipped
Shard 1174 / 5040 skipped
Shard 1175 / 5040 skipped
Shard 1176 / 5040 skipped
Shard 1177 / 5040 skipped
Shard 1178 / 5040 skipped
Shard 1179 / 5040 skipped
Shard 1180 / 5040 skipped
Shard 1181 / 5040 skipped
Shard 1182 / 5040 skipped
Shard 1183 / 5040 skipped
Shard 1184 / 5040 skipped
Shard 1185 / 5040 skipped
Shard 1186 / 5040 skipped
Shard 1187 / 5040 skipped
Shard 1188 / 5040 skipped
Shard 1189 / 5040 skipped
Shard 1190 / 5040 skipped
Shard 1191 / 5040 skipped
Shard 1192 / 5040 skipped
Shard 1193 / 5040 skipped
Shard 1194 / 5040 skipped
Shard 1195 / 5040 skipped
Shard 1196 / 5040 skipped
Shard 1197 / 5040 skipped
Shard 1198 / 5040 skipped
Shard 1199 / 5040 skipped
Shard 1200 / 5040 skipped
Shard 1201 / 5040 skipped
Shard 1202 / 5040 skipped
Shard 1203 / 5040 skipped
Shard 1204 / 5040 skipped
Shard 1205 / 5040 skipped
Shard 1206 / 5040 skipped
Shard 1207 / 5040 skipped
Shard 1208 / 5040 skipped
Shard 1209 / 5040 skipped
Shard 1210 / 5040 skipped
Shard 1211 / 5040 skipped
Shard 1212 / 5040 skipped
Shard 1213 / 5040 skipped
Shard 1214 / 5040 skipped
Shard 1215 / 5040 skipped
Shard 1216 / 5040 skipped
Shard 1217 / 5040 skipped
Shard 1218 / 5040 skipped
Shard 1219 / 5040 skipped
Shard 1220 / 5040 skipped
Shard 1221 / 5040 skipped
Shard 1222 / 5040 skipped
Shard 1223 / 5040 skipped
Shard 1224 / 5040 skipped
Shard 1225 / 5040 skipped
Shard 1226 / 5040 skipped
Shard 1227 / 5040 skipped
Shard 1228 / 5040 skipped
Shard 1229 / 5040 skipped
Shard 1230 / 5040 skipped
Shard 1231 / 5040 skipped
Shard 1232 / 5040 skipped
Shard 1233 / 5040 skipped
Shard 1234 / 5040 skipped
Shard 1235 / 5040 skipped
Shard 1236 / 5040 skipped
Shard 1237 / 5040 skipped
Shard 1238 / 5040 skipped
Shard 1239 / 5040 skipped
Shard 1240 / 5040 skipped
Shard 1241 / 5040 skipped
Shard 1242 / 5040 skipped
Shard 1243 / 5040 skipped
Shard 1244 / 5040 skipped
Shard 1245 / 5040 skipped
Shard 1246 / 5040 skipped
Shard 1247 / 5040 skipped
Shard 1248 / 5040 skipped
Shard 1249 / 5040 skipped
Shard 1250 / 5040 skipped
Shard 1251 / 5040 skipped
Shard 1252 / 5040 skipped
Shard 1253 / 5040 skipped
Shard 1254 / 5040 skipped
Shard 1255 / 5040 skipped
Shard 1256 / 5040 skipped
Shard 1257 / 5040 skipped
Shard 1258 / 5040 skipped
Shard 1259 / 5040 skipped
Shard 1260 / 5040 skipped
Shard 1261 / 5040 skipped
Shard 1262 / 5040 skipped
Shard 1263 / 5040 skipped
Shard 1264 / 5040 skipped
Shard 1265 / 5040 skipped
Shard 1266 / 5040 skipped
Shard 1267 / 5040 skipped
Shard 1268 / 5040 skipped
Shard 1269 / 5040 skipped
Shard 1270 / 5040 skipped
Shard 1271 / 5040 skipped
Shard 1272 / 5040 skipped
Shard 1273 / 5040 skipped
Shard 1274 / 5040 skipped
Shard 1275 / 5040 skipped
Shard 1276 / 5040 skipped
Shard 1277 / 5040 skipped
Shard 1278 / 5040 skipped
Shard 1279 / 5040 skipped
Shard 1280 / 5040 skipped
Shard 1281 / 5040 skipped
Shard 1282 / 5040 skipped
Shard 1283 / 5040 skipped
Shard 1284 / 5040 skipped
Shard 1285 / 5040 skipped
Shard 1286 / 5040 skipped
Shard 1287 / 5040 skipped
Shard 1288 / 5040 skipped
Shard 1289 / 5040 skipped
Shard 1290 / 5040 skipped
Shard 1291 / 5040 skipped
Shard 1292 / 5040 skipped
Shard 1293 / 5040 skipped
Shard 1294 / 5040 skipped
Shard 1295 / 5040 skipped
Shard 1296 / 5040 skipped
Shard 1297 / 5040 skipped
Shard 1298 / 5040 skipped
Shard 1299 / 5040 skipped
Shard 1300 / 5040 skipped
Shard 1301 / 5040 skipped
Shard 1302 / 5040 skipped
Shard 1303 / 5040 skipped
Shard 1304 / 5040 skipped
Shard 1305 / 5040 skipped
Shard 1306 / 5040 skipped
Shard 1307 / 5040 skipped
Shard 1308 / 5040 skipped
Shard 1309 / 5040 skipped
Shard 1310 / 5040 skipped
Shard 1311 / 5040 skipped
Shard 1312 / 5040 skipped
Shard 1313 / 5040 skipped
Shard 1314 / 5040 skipped
Shard 1315 / 5040 skipped
Shard 1316 / 5040 skipped
Shard 1317 / 5040 skipped
Shard 1318 / 5040 skipped
Shard 1319 / 5040 skipped
Shard 1320 / 5040 skipped
Shard 1321 / 5040 skipped
Shard 1322 / 5040 skipped
Shard 1323 / 5040 skipped
Shard 1324 / 5040 skipped
Shard 1325 / 5040 skipped
Shard 1326 / 5040 skipped
Shard 1327 / 5040 skipped
Shard 1328 / 5040 skipped
Shard 1329 / 5040 skipped
Shard 1330 / 5040 skipped
Shard 1331 / 5040 skipped
Shard 1332 / 5040 skipped
Shard 1333 / 5040 skipped
Shard 1334 / 5040 skipped
Shard 1335 / 5040 skipped
Shard 1336 / 5040 skipped
Shard 1337 / 5040 skipped
Shard 1338 / 5040 skipped
Shard 1339 / 5040 skipped
Shard 1340 / 5040 skipped
Shard 1341 / 5040 skipped
Shard 1342 / 5040 skipped
Shard 1343 / 5040 skipped
Shard 1344 / 5040 skipped
Shard 1345 / 5040 skipped
Shard 1346 / 5040 skipped
Shard 1347 / 5040 skipped
Shard 1348 / 5040 skipped
Shard 1349 / 5040 skipped
Shard 1350 / 5040 skipped
Shard 1351 / 5040 skipped
Shard 1352 / 5040 skipped
Shard 1353 / 5040 skipped
Shard 1354 / 5040 skipped
Shard 1355 / 5040 skipped
Shard 1356 / 5040 skipped
Shard 1357 / 5040 skipped
Shard 1358 / 5040 skipped
Shard 1359 / 5040 skipped
Shard 1360 / 5040 skipped
Shard 1361 / 5040 skipped
Shard 1362 / 5040 skipped
Shard 1363 / 5040 skipped
Shard 1364 / 5040 skipped
Shard 1365 / 5040 skipped
Shard 1366 / 5040 skipped
Shard 1367 / 5040 skipped
Shard 1368 / 5040 skipped
Shard 1369 / 5040 skipped
Shard 1370 / 5040 skipped
Shard 1371 / 5040 skipped
Shard 1372 / 5040 skipped
Shard 1373 / 5040 skipped
Shard 1374 / 5040 skipped
Shard 1375 / 5040 skipped
Shard 1376 / 5040 skipped
Shard 1377 / 5040 skipped
Shard 1378 / 5040 skipped
Shard 1379 / 5040 skipped
Shard 1380 / 5040 skipped
Shard 1381 / 5040 skipped
Shard 1382 / 5040 skipped
Shard 1383 / 5040 skipped
Shard 1384 / 5040 skipped
Shard 1385 / 5040 skipped
Shard 1386 / 5040 skipped
Shard 1387 / 5040 skipped
Shard 1388 / 5040 skipped
Shard 1389 / 5040 skipped
Shard 1390 / 5040 skipped
Shard 1391 / 5040 skipped
Shard 1392 / 5040 skipped
Shard 1393 / 5040 skipped
Shard 1394 / 5040 skipped
Shard 1395 / 5040 skipped
Shard 1396 / 5040 skipped
Shard 1397 / 5040 skipped
Shard 1398 / 5040 skipped
Shard 1399 / 5040 skipped
Shard 1400 / 5040 skipped
Shard 1401 / 5040 skipped
Shard 1402 / 5040 skipped
Shard 1403 / 5040 skipped
Shard 1404 / 5040 skipped
Shard 1405 / 5040 skipped
Shard 1406 / 5040 skipped
Shard 1407 / 5040 skipped
Shard 1408 / 5040 skipped
Shard 1409 / 5040 skipped
Shard 1410 / 5040 skipped
Shard 1411 / 5040 skipped
Shard 1412 / 5040 skipped
Shard 1413 / 5040 skipped
Shard 1414 / 5040 skipped
Shard 1415 / 5040 skipped
Shard 1416 / 5040 skipped
Shard 1417 / 5040 skipped
Shard 1418 / 5040 skipped
Shard 1419 / 5040 skipped
Shard 1420 / 5040 skipped
Shard 1421 / 5040 skipped
Shard 1422 / 5040 skipped
Shard 1423 / 5040 skipped
Shard 1424 / 5040 skipped
Shard 1425 / 5040 skipped
Shard 1426 / 5040 skipped
Shard 1427 / 5040 skipped
Shard 1428 / 5040 skipped
Shard 1429 / 5040 skipped
Shard 1430 / 5040 skipped
Shard 1431 / 5040 skipped
Shard 1432 / 5040 skipped
Shard 1433 / 5040 skipped
Shard 1434 / 5040 skipped
Shard 1435 / 5040 skipped
Shard 1436 / 5040 skipped
Shard 1437 / 5040 skipped
Shard 1438 / 5040 skipped
Shard 1439 / 5040 skipped
Shard 1440 / 5040 skipped
Shard 1441 / 5040 skipped
Shard 1442 / 5040 skipped
Shard 1443 / 5040 skipped
Shard 1444 / 5040 skipped
Shard 1445 / 5040 skipped
Shard 1446 / 5040 skipped
Shard 1447 / 5040 skipped
Shard 1448 / 5040 skipped
Shard 1449 / 5040 skipped
Shard 1450 / 5040 skipped
Shard 1451 / 5040 skipped
Shard 1452 / 5040 skipped
Shard 1453 / 5040 skipped
Shard 1454 / 5040 skipped
Shard 1455 / 5040 skipped
Shard 1456 / 5040 skipped
Shard 1457 / 5040 skipped
Shard 1458 / 5040 skipped
Shard 1459 / 5040 skipped
Shard 1460 / 5040 skipped
Shard 1461 / 5040 skipped
Shard 1462 / 5040 skipped
Shard 1463 / 5040 skipped
Shard 1464 / 5040 skipped
Shard 1465 / 5040 skipped
Shard 1466 / 5040 skipped
Shard 1467 / 5040 skipped
Shard 1468 / 5040 skipped
Shard 1469 / 5040 skipped
Shard 1470 / 5040 skipped
Shard 1471 / 5040 skipped
Shard 1472 / 5040 skipped
Shard 1473 / 5040 skipped
Shard 1474 / 5040 skipped
Shard 1475 / 5040 skipped
Shard 1476 / 5040 skipped
Shard 1477 / 5040 skipped
Shard 1478 / 5040 skipped
Shard 1479 / 5040 skipped
Shard 1480 / 5040 skipped
Shard 1481 / 5040 skipped
Shard 1482 / 5040 skipped
Shard 1483 / 5040 skipped
Shard 1484 / 5040 skipped
Shard 1485 / 5040 skipped
Shard 1486 / 5040 skipped
Shard 1487 / 5040 skipped
Shard 1488 / 5040 skipped
Shard 1489 / 5040 skipped
Shard 1490 / 5040 skipped
Shard 1491 / 5040 skipped
Shard 1492 / 5040 skipped
Shard 1493 / 5040 skipped
Shard 1494 / 5040 skipped
Shard 1495 / 5040 skipped
Shard 1496 / 5040 skipped
Shard 1497 / 5040 skipped
Shard 1498 / 5040 skipped
Shard 1499 / 5040 skipped
Shard 1500 / 5040 skipped
Shard 1501 / 5040 skipped
Shard 1502 / 5040 skipped
Shard 1503 / 5040 skipped
Shard 1504 / 5040 skipped
Shard 1505 / 5040 skipped
Shard 1506 / 5040 skipped
Shard 1507 / 5040 skipped
Shard 1508 / 5040 skipped
Shard 1509 / 5040 skipped
Shard 1510 / 5040 skipped
Shard 1511 / 5040 skipped
Shard 1512 / 5040 skipped
Shard 1513 / 5040 skipped
Shard 1514 / 5040 skipped
Shard 1515 / 5040 skipped
Shard 1516 / 5040 skipped
Shard 1517 / 5040 skipped
Shard 1518 / 5040 skipped
Shard 1519 / 5040 skipped
Shard 1520 / 5040 skipped
Shard 1521 / 5040 skipped
Shard 1522 / 5040 skipped
Shard 1523 / 5040 skipped
Shard 1524 / 5040 skipped
Shard 1525 / 5040 skipped
Shard 1526 / 5040 skipped
Shard 1527 / 5040 skipped
Shard 1528 / 5040 skipped
Shard 1529 / 5040 skipped
Shard 1530 / 5040 skipped
Shard 1531 / 5040 skipped
Shard 1532 / 5040 skipped
Shard 1533 / 5040 skipped
Shard 1534 / 5040 skipped
Shard 1535 / 5040 skipped
Shard 1536 / 5040 skipped
Shard 1537 / 5040 skipped
Shard 1538 / 5040 skipped
Shard 1539 / 5040 skipped
Shard 1540 / 5040 skipped
Shard 1541 / 5040 skipped
Shard 1542 / 5040 skipped
Shard 1543 / 5040 skipped
Shard 1544 / 5040 skipped
Shard 1545 / 5040 skipped
Shard 1546 / 5040 skipped
Shard 1547 / 5040 skipped
Shard 1548 / 5040 skipped
Shard 1549 / 5040 skipped
Shard 1550 / 5040 skipped
Shard 1551 / 5040 skipped
Shard 1552 / 5040 skipped
Shard 1553 / 5040 skipped
Shard 1554 / 5040 skipped
Shard 1555 / 5040 skipped
Shard 1556 / 5040 skipped
Shard 1557 / 5040 skipped
Shard 1558 / 5040 skipped
Shard 1559 / 5040 skipped
Shard 1560 / 5040 skipped
Shard 1561 / 5040 skipped
Shard 1562 / 5040 skipped
Shard 1563 / 5040 skipped
Shard 1564 / 5040 skipped
Shard 1565 / 5040 skipped
Shard 1566 / 5040 skipped
Shard 1567 / 5040 skipped
Shard 1568 / 5040 skipped
Shard 1569 / 5040 skipped
Shard 1570 / 5040 skipped
Shard 1571 / 5040 skipped
Shard 1572 / 5040 skipped
Shard 1573 / 5040 skipped
Shard 1574 / 5040 skipped
Shard 1575 / 5040 skipped
Shard 1576 / 5040 skipped
Shard 1577 / 5040 skipped
Shard 1578 / 5040 skipped
Shard 1579 / 5040 skipped
Shard 1580 / 5040 skipped
Shard 1581 / 5040 skipped
Shard 1582 / 5040 skipped
Shard 1583 / 5040 skipped
Shard 1584 / 5040 skipped
Shard 1585 / 5040 skipped
Shard 1586 / 5040 skipped
Shard 1587 / 5040 skipped
Shard 1588 / 5040 skipped
Shard 1589 / 5040 skipped
Shard 1590 / 5040 skipped
Shard 1591 / 5040 skipped
Shard 1592 / 5040 skipped
Shard 1593 / 5040 skipped
Shard 1594 / 5040 skipped
Shard 1595 / 5040 skipped
Shard 1596 / 5040 skipped
Shard 1597 / 5040 skipped
Shard 1598 / 5040 skipped
Shard 1599 / 5040 skipped
Shard 1600 / 5040 skipped
Shard 1601 / 5040 skipped
Shard 1602 / 5040 skipped
Shard 1603 / 5040 skipped
Shard 1604 / 5040 skipped
Shard 1605 / 5040 skipped
Shard 1606 / 5040 skipped
Shard 1607 / 5040 skipped
Shard 1608 / 5040 skipped
Shard 1609 / 5040 skipped
Shard 1610 / 5040 skipped
Shard 1611 / 5040 skipped
Shard 1612 / 5040 skipped
Shard 1613 / 5040 skipped
Shard 1614 / 5040 skipped
Shard 1615 / 5040 skipped
Shard 1616 / 5040 skipped
Shard 1617 / 5040 skipped
Shard 1618 / 5040 skipped
Shard 1619 / 5040 skipped
Shard 1620 / 5040 skipped
Shard 1621 / 5040 skipped
Shard 1622 / 5040 skipped
Shard 1623 / 5040 skipped
Shard 1624 / 5040 skipped
Shard 1625 / 5040 skipped
Shard 1626 / 5040 skipped
Shard 1627 / 5040 skipped
Shard 1628 / 5040 skipped
Shard 1629 / 5040 skipped
Shard 1630 / 5040 skipped
Shard 1631 / 5040 skipped
Shard 1632 / 5040 skipped
Shard 1633 / 5040 skipped
Shard 1634 / 5040 skipped
Shard 1635 / 5040 skipped
Shard 1636 / 5040 skipped
Shard 1637 / 5040 skipped
Shard 1638 / 5040 skipped
Shard 1639 / 5040 skipped
Shard 1640 / 5040 skipped
Shard 1641 / 5040 skipped
Shard 1642 / 5040 skipped
Shard 1643 / 5040 skipped
Shard 1644 / 5040 skipped
Shard 1645 / 5040 skipped
Shard 1646 / 5040 skipped
Shard 1647 / 5040 skipped
Shard 1648 / 5040 skipped
Shard 1649 / 5040 skipped
Shard 1650 / 5040 skipped
Shard 1651 / 5040 skipped
Shard 1652 / 5040 skipped
Shard 1653 / 5040 skipped
Shard 1654 / 5040 skipped
Shard 1655 / 5040 skipped
Shard 1656 / 5040 skipped
Shard 1657 / 5040 skipped
Shard 1658 / 5040 skipped
Shard 1659 / 5040 skipped
Shard 1660 / 5040 skipped
Shard 1661 / 5040 skipped
Shard 1662 / 5040 skipped
Shard 1663 / 5040 skipped
Shard 1664 / 5040 skipped
Shard 1665 / 5040 skipped
Shard 1666 / 5040 skipped
Shard 1667 / 5040 skipped
Shard 1668 / 5040 skipped
Shard 1669 / 5040 skipped
Shard 1670 / 5040 skipped
Shard 1671 / 5040 skipped
Shard 1672 / 5040 skipped
Shard 1673 / 5040 skipped
Shard 1674 / 5040 skipped
Shard 1675 / 5040 skipped
Shard 1676 / 5040 skipped
Shard 1677 / 5040 skipped
Shard 1678 / 5040 skipped
Shard 1679 / 5040 skipped
Shard 1680 / 5040 skipped
Shard 1681 / 5040 skipped
Shard 1682 / 5040 skipped
Shard 1683 / 5040 skipped
Shard 1684 / 5040 skipped
Shard 1685 / 5040 skipped
Shard 1686 / 5040 skipped
Shard 1687 / 5040 skipped
Shard 1688 / 5040 skipped
Shard 1689 / 5040 skipped
Shard 1690 / 5040 skipped
Shard 1691 / 5040 skipped
Shard 1692 / 5040 skipped
Shard 1693 / 5040 skipped
Shard 1694 / 5040 skipped
Shard 1695 / 5040 skipped
Shard 1696 / 5040 skipped
Shard 1697 / 5040 skipped
Shard 1698 / 5040 skipped
Shard 1699 / 5040 skipped
Shard 1700 / 5040 skipped
Shard 1701 / 5040 skipped
Shard 1702 / 5040 skipped
Shard 1703 / 5040 skipped
Shard 1704 / 5040 skipped
Shard 1705 / 5040 skipped
Shard 1706 / 5040 skipped
Shard 1707 / 5040 skipped
Shard 1708 / 5040 skipped
Shard 1709 / 5040 skipped
Shard 1710 / 5040 skipped
Shard 1711 / 5040 skipped
Shard 1712 / 5040 skipped
Shard 1713 / 5040 skipped
Shard 1714 / 5040 skipped
Shard 1715 / 5040 skipped
Shard 1716 / 5040 skipped
Shard 1717 / 5040 skipped
Shard 1718 / 5040 skipped
Shard 1719 / 5040 skipped
Shard 1720 / 5040 skipped
Shard 1721 / 5040 skipped
Shard 1722 / 5040 skipped
Shard 1723 / 5040 skipped
Shard 1724 / 5040 skipped
Shard 1725 / 5040 skipped
Shard 1726 / 5040 skipped
Shard 1727 / 5040 skipped
Shard 1728 / 5040 skipped
Shard 1729 / 5040 skipped
Shard 1730 / 5040 skipped
Shard 1731 / 5040 skipped
Shard 1732 / 5040 skipped
Shard 1733 / 5040 skipped
Shard 1734 / 5040 skipped
Shard 1735 / 5040 skipped
Shard 1736 / 5040 skipped
Shard 1737 / 5040 skipped
Shard 1738 / 5040 skipped
Shard 1739 / 5040 skipped
Shard 1740 / 5040 skipped
Shard 1741 / 5040 skipped
Shard 1742 / 5040 skipped
Shard 1743 / 5040 skipped
Shard 1744 / 5040 skipped
Shard 1745 / 5040 skipped
Shard 1746 / 5040 skipped
Shard 1747 / 5040 skipped
Shard 1748 / 5040 skipped
Shard 1749 / 5040 skipped
Shard 1750 / 5040 skipped
Shard 1751 / 5040 skipped
Shard 1752 / 5040 skipped
Shard 1753 / 5040 skipped
Shard 1754 / 5040 skipped
Shard 1755 / 5040 skipped
Shard 1756 / 5040 skipped
Shard 1757 / 5040 skipped
Shard 1758 / 5040 skipped
Shard 1759 / 5040 skipped
Shard 1760 / 5040 skipped
Shard 1761 / 5040 skipped
Shard 1762 / 5040 skipped
Shard 1763 / 5040 skipped
Shard 1764 / 5040 skipped
Shard 1765 / 5040 skipped
Shard 1766 / 5040 skipped
Shard 1767 / 5040 skipped
Shard 1768 / 5040 skipped
Shard 1769 / 5040 skipped
Shard 1770 / 5040 skipped
Shard 1771 / 5040 skipped
Shard 1772 / 5040 skipped
Shard 1773 / 5040 skipped
Shard 1774 / 5040 skipped
Shard 1775 / 5040 skipped
Shard 1776 / 5040 skipped
Shard 1777 / 5040 skipped
Shard 1778 / 5040 skipped
Shard 1779 / 5040 skipped
Shard 1780 / 5040 skipped
Shard 1781 / 5040 skipped
Shard 1782 / 5040 skipped
Shard 1783 / 5040 skipped
Shard 1784 / 5040 skipped
Shard 1785 / 5040 skipped
Shard 1786 / 5040 skipped
Shard 1787 / 5040 skipped
Shard 1788 / 5040 skipped
Shard 1789 / 5040 skipped
Shard 1790 / 5040 skipped
Shard 1791 / 5040 skipped
Shard 1792 / 5040 skipped
Shard 1793 / 5040 skipped
Shard 1794 / 5040 skipped
Shard 1795 / 5040 skipped
Shard 1796 / 5040 skipped
Shard 1797 / 5040 skipped
Shard 1798 / 5040 skipped
Shard 1799 / 5040 skipped
Shard 1800 / 5040 skipped
Shard 1801 / 5040 skipped
Shard 1802 / 5040 skipped
Shard 1803 / 5040 skipped
Shard 1804 / 5040 skipped
Shard 1805 / 5040 skipped
Shard 1806 / 5040 skipped
Shard 1807 / 5040 skipped
Shard 1808 / 5040 skipped
Shard 1809 / 5040 skipped
Shard 1810 / 5040 skipped
Shard 1811 / 5040 skipped
Shard 1812 / 5040 skipped
Shard 1813 / 5040 skipped
Shard 1814 / 5040 skipped
Shard 1815 / 5040 skipped
Shard 1816 / 5040 skipped
Shard 1817 / 5040 skipped
Shard 1818 / 5040 skipped
Shard 1819 / 5040 skipped
Shard 1820 / 5040 skipped
Shard 1821 / 5040 skipped
Shard 1822 / 5040 skipped
Shard 1823 / 5040 skipped
Shard 1824 / 5040 skipped
Shard 1825 / 5040 skipped
Shard 1826 / 5040 skipped
Shard 1827 / 5040 skipped
Shard 1828 / 5040 skipped
Shard 1829 / 5040 skipped
Shard 1830 / 5040 skipped
Shard 1831 / 5040 skipped
Shard 1832 / 5040 skipped
Shard 1833 / 5040 skipped
Shard 1834 / 5040 skipped
Shard 1835 / 5040 skipped
Shard 1836 / 5040 skipped
Shard 1837 / 5040 skipped
Shard 1838 / 5040 skipped
Shard 1839 / 5040 skipped
Shard 1840 / 5040 skipped
Shard 1841 / 5040 skipped
Shard 1842 / 5040 skipped
Shard 1843 / 5040 skipped
Shard 1844 / 5040 skipped
Shard 1845 / 5040 skipped
Shard 1846 / 5040 skipped
Shard 1847 / 5040 skipped
Shard 1848 / 5040 skipped
Shard 1849 / 5040 skipped
Shard 1850 / 5040 skipped
Shard 1851 / 5040 skipped
Shard 1852 / 5040 skipped
Shard 1853 / 5040 skipped
Shard 1854 / 5040 skipped
Shard 1855 / 5040 skipped
Shard 1856 / 5040 skipped
Shard 1857 / 5040 skipped
Shard 1858 / 5040 skipped
Shard 1859 / 5040 skipped
Shard 1860 / 5040 skipped
Shard 1861 / 5040 skipped
Shard 1862 / 5040 skipped
Shard 1863 / 5040 skipped
Shard 1864 / 5040 skipped
Shard 1865 / 5040 skipped
Shard 1866 / 5040 skipped
Shard 1867 / 5040 skipped
Shard 1868 / 5040 skipped
Shard 1869 / 5040 skipped
Shard 1870 / 5040 skipped
Shard 1871 / 5040 skipped
Shard 1872 / 5040 skipped
Shard 1873 / 5040 skipped
Shard 1874 / 5040 skipped
Shard 1875 / 5040 skipped
Shard 1876 / 5040 skipped
Shard 1877 / 5040 skipped
Shard 1878 / 5040 skipped
Shard 1879 / 5040 skipped
Shard 1880 / 5040 skipped
Shard 1881 / 5040 skipped
Shard 1882 / 5040 skipped
Shard 1883 / 5040 skipped
Shard 1884 / 5040 skipped
Shard 1885 / 5040 skipped
Shard 1886 / 5040 skipped
Shard 1887 / 5040 skipped
Shard 1888 / 5040 skipped
Shard 1889 / 5040 skipped
Shard 1890 / 5040 skipped
Shard 1891 / 5040 skipped
Shard 1892 / 5040 skipped
Shard 1893 / 5040 skipped
Shard 1894 / 5040 skipped
Shard 1895 / 5040 skipped
Shard 1896 / 5040 skipped
Shard 1897 / 5040 skipped
Shard 1898 / 5040 skipped
Shard 1899 / 5040 skipped
Shard 1900 / 5040 skipped
Shard 1901 / 5040 skipped
Shard 1902 / 5040 skipped
Shard 1903 / 5040 skipped
Shard 1904 / 5040 skipped
Shard 1905 / 5040 skipped
Shard 1906 / 5040 skipped
Shard 1907 / 5040 skipped
Shard 1908 / 5040 skipped
Shard 1909 / 5040 skipped
Shard 1910 / 5040 skipped
Shard 1911 / 5040 skipped
Shard 1912 / 5040 skipped
Shard 1913 / 5040 skipped
Shard 1914 / 5040 skipped
Shard 1915 / 5040 skipped
Shard 1916 / 5040 skipped
Shard 1917 / 5040 skipped
Shard 1918 / 5040 skipped
Shard 1919 / 5040 skipped
Shard 1920 / 5040 skipped
Shard 1921 / 5040 skipped
Shard 1922 / 5040 skipped
Shard 1923 / 5040 skipped
Shard 1924 / 5040 skipped
Shard 1925 / 5040 skipped
Shard 1926 / 5040 skipped
Shard 1927 / 5040 skipped
Shard 1928 / 5040 skipped
Shard 1929 / 5040 skipped
Shard 1930 / 5040 skipped
Shard 1931 / 5040 skipped
Shard 1932 / 5040 skipped
Shard 1933 / 5040 skipped
Shard 1934 / 5040 skipped
Shard 1935 / 5040 skipped
Shard 1936 / 5040 skipped
Shard 1937 / 5040 skipped
Shard 1938 / 5040 skipped
Shard 1939 / 5040 skipped
Shard 1940 / 5040 skipped
Shard 1941 / 5040 skipped
Shard 1942 / 5040 skipped
Shard 1943 / 5040 skipped
Shard 1944 / 5040 skipped
Shard 1945 / 5040 skipped
Shard 1946 / 5040 skipped
Shard 1947 / 5040 skipped
Shard 1948 / 5040 skipped
Shard 1949 / 5040 skipped
Shard 1950 / 5040 skipped
Shard 1951 / 5040 skipped
Shard 1952 / 5040 skipped
Shard 1953 / 5040 skipped
Shard 1954 / 5040 skipped
Shard 1955 / 5040 skipped
Shard 1956 / 5040 skipped
Shard 1957 / 5040 skipped
Shard 1958 / 5040 skipped
Shard 1959 / 5040 skipped
Shard 1960 / 5040 skipped
Shard 1961 / 5040 skipped
Shard 1962 / 5040 skipped
Shard 1963 / 5040 skipped
Shard 1964 / 5040 skipped
Shard 1965 / 5040 skipped
Shard 1966 / 5040 skipped
Shard 1967 / 5040 skipped
Shard 1968 / 5040 skipped
Shard 1969 / 5040 skipped
Shard 1970 / 5040 skipped
Shard 1971 / 5040 skipped
Shard 1972 / 5040 skipped
Shard 1973 / 5040 skipped
Shard 1974 / 5040 skipped
Shard 1975 / 5040 skipped
Shard 1976 / 5040 skipped
Shard 1977 / 5040 skipped
Shard 1978 / 5040 skipped
Shard 1979 / 5040 skipped
Shard 1980 / 5040 skipped
Shard 1981 / 5040 skipped
Shard 1982 / 5040 skipped
Shard 1983 / 5040 skipped
Shard 1984 / 5040 skipped
Shard 1985 / 5040 skipped
Shard 1986 / 5040 skipped
Shard 1987 / 5040 skipped
Shard 1988 / 5040 skipped
Shard 1989 / 5040 skipped
Shard 1990 / 5040 skipped
Shard 1991 / 5040 skipped
Shard 1992 / 5040 skipped
Shard 1993 / 5040 skipped
Shard 1994 / 5040 skipped
Shard 1995 / 5040 skipped
Shard 1996 / 5040 skipped
Shard 1997 / 5040 skipped
Shard 1998 / 5040 skipped
Shard 1999 / 5040 skipped
Shard 2000 / 5040 skipped
Shard 2001 / 5040 skipped
Shard 2002 / 5040 skipped
Shard 2003 / 5040 skipped
Shard 2004 / 5040 skipped
Shard 2005 / 5040 skipped
Shard 2006 / 5040 skipped
Shard 2007 / 5040 skipped
Shard 2008 / 5040 skipped
Shard 2009 / 5040 skipped
Shard 2010 / 5040 skipped
Shard 2011 / 5040 skipped
Shard 2012 / 5040 skipped
Shard 2013 / 5040 skipped
Shard 2014 / 5040 skipped
Shard 2015 / 5040 skipped
Shard 2016 / 5040 skipped
Shard 2017 / 5040 skipped
Shard 2018 / 5040 skipped
Shard 2019 / 5040 skipped
Shard 2020 / 5040 skipped
Shard 2021 / 5040 skipped
Shard 2022 / 5040 skipped
Shard 2023 / 5040 skipped
Shard 2024 / 5040 skipped
Shard 2025 / 5040 skipped
Shard 2026 / 5040 skipped
Shard 2027 / 5040 skipped
Shard 2028 / 5040 skipped
Shard 2029 / 5040 skipped
Shard 2030 / 5040 skipped
Shard 2031 / 5040 skipped
Shard 2032 / 5040 skipped
Shard 2033 / 5040 skipped
Shard 2034 / 5040 skipped
Shard 2035 / 5040 skipped
Shard 2036 / 5040 skipped
Shard 2037 / 5040 skipped
Shard 2038 / 5040 skipped
Shard 2039 / 5040 skipped
Shard 2040 / 5040 skipped
Shard 2041 / 5040 skipped
Shard 2042 / 5040 skipped
Shard 2043 / 5040 skipped
Shard 2044 / 5040 skipped
Shard 2045 / 5040 skipped
Shard 2046 / 5040 skipped
Shard 2047 / 5040 skipped
Shard 2048 / 5040 skipped
Shard 2049 / 5040 skipped
Shard 2050 / 5040 skipped
Shard 2051 / 5040 skipped
Shard 2052 / 5040 skipped
Shard 2053 / 5040 skipped
Shard 2054 / 5040 skipped
Shard 2055 / 5040 skipped
Shard 2056 / 5040 skipped
Shard 2057 / 5040 skipped
Shard 2058 / 5040 skipped
Shard 2059 / 5040 skipped
Shard 2060 / 5040 skipped
Shard 2061 / 5040 skipped
Shard 2062 / 5040 skipped
Shard 2063 / 5040 skipped
Shard 2064 / 5040 skipped
Shard 2065 / 5040 skipped
Shard 2066 / 5040 skipped
Shard 2067 / 5040 skipped
Shard 2068 / 5040 skipped
Shard 2069 / 5040 skipped
Shard 2070 / 5040 skipped
Shard 2071 / 5040 skipped
Shard 2072 / 5040 skipped
Shard 2073 / 5040 skipped
Shard 2074 / 5040 skipped
Shard 2075 / 5040 skipped
Shard 2076 / 5040 skipped
Shard 2077 / 5040 skipped
Shard 2078 / 5040 skipped
Shard 2079 / 5040 skipped
Shard 2080 / 5040 skipped
Shard 2081 / 5040 skipped
Shard 2082 / 5040 skipped
Shard 2083 / 5040 skipped
Shard 2084 / 5040 skipped
Shard 2085 / 5040 skipped
Shard 2086 / 5040 skipped
Shard 2087 / 5040 skipped
Shard 2088 / 5040 skipped
Shard 2089 / 5040 skipped
Shard 2090 / 5040 skipped
Shard 2091 / 5040 skipped
Shard 2092 / 5040 skipped
Shard 2093 / 5040 skipped
Shard 2094 / 5040 skipped
Shard 2095 / 5040 skipped
Shard 2096 / 5040 skipped
Shard 2097 / 5040 skipped
Shard 2098 / 5040 skipped
Shard 2099 / 5040 skipped
Shard 2100 / 5040 skipped
Shard 2101 / 5040 skipped
Shard 2102 / 5040 skipped
Shard 2103 / 5040 skipped
Shard 2104 / 5040 skipped
Shard 2105 / 5040 skipped
Shard 2106 / 5040 skipped
Shard 2107 / 5040 skipped
Shard 2108 / 5040 skipped
Shard 2109 / 5040 skipped
Shard 2110 / 5040 skipped
Shard 2111 / 5040 skipped
Shard 2112 / 5040 skipped
Shard 2113 / 5040 skipped
Shard 2114 / 5040 skipped
Shard 2115 / 5040 skipped
Shard 2116 / 5040 skipped
Shard 2117 / 5040 skipped
Shard 2118 / 5040 skipped
Shard 2119 / 5040 skipped
Shard 2120 / 5040 skipped
Shard 2121 / 5040 skipped
Shard 2122 / 5040 skipped
Shard 2123 / 5040 skipped
Shard 2124 / 5040 skipped
Shard 2125 / 5040 skipped
Shard 2126 / 5040 skipped
Shard 2127 / 5040 skipped
Shard 2128 / 5040 skipped
Shard 2129 / 5040 skipped
Shard 2130 / 5040 skipped
Shard 2131 / 5040 skipped
Shard 2132 / 5040 skipped
Shard 2133 / 5040 skipped
Shard 2134 / 5040 skipped
Shard 2135 / 5040 skipped
Shard 2136 / 5040 skipped
Shard 2137 / 5040 skipped
Shard 2138 / 5040 skipped
Shard 2139 / 5040 skipped
Shard 2140 / 5040 skipped
Shard 2141 / 5040 skipped
Shard 2142 / 5040 skipped
Shard 2143 / 5040 skipped
Shard 2144 / 5040 skipped
Shard 2145 / 5040 skipped
Shard 2146 / 5040 skipped
Shard 2147 / 5040 skipped
Shard 2148 / 5040 skipped
Shard 2149 / 5040 skipped
Shard 2150 / 5040 skipped
Shard 2151 / 5040 skipped
Shard 2152 / 5040 skipped
Shard 2153 / 5040 skipped
Shard 2154 / 5040 skipped
Shard 2155 / 5040 skipped
Shard 2156 / 5040 skipped
Shard 2157 / 5040 skipped
Shard 2158 / 5040 skipped
Shard 2159 / 5040 skipped
Shard 2160 / 5040 skipped
Shard 2161 / 5040 skipped
Shard 2162 / 5040 skipped
Shard 2163 / 5040 skipped
Shard 2164 / 5040 skipped
Shard 2165 / 5040 skipped
Shard 2166 / 5040 skipped
Shard 2167 / 5040 skipped
Shard 2168 / 5040 skipped
Shard 2169 / 5040 skipped
Shard 2170 / 5040 skipped
Shard 2171 / 5040 skipped
Shard 2172 / 5040 skipped
Shard 2173 / 5040 skipped
Shard 2174 / 5040 skipped
Shard 2175 / 5040 skipped
Shard 2176 / 5040 skipped
Shard 2177 / 5040 skipped
Shard 2178 / 5040 skipped
Shard 2179 / 5040 skipped
Shard 2180 / 5040 skipped
Shard 2181 / 5040 skipped
Shard 2182 / 5040 skipped
Shard 2183 / 5040 skipped
Shard 2184 / 5040 skipped
Shard 2185 / 5040 skipped
Shard 2186 / 5040 skipped
Shard 2187 / 5040 skipped
Shard 2188 / 5040 skipped
Shard 2189 / 5040 skipped
Shard 2190 / 5040 skipped
Shard 2191 / 5040 skipped
Shard 2192 / 5040 skipped
Shard 2193 / 5040 skipped
Shard 2194 / 5040 skipped
Shard 2195 / 5040 skipped
Shard 2196 / 5040 skipped
Shard 2197 / 5040 skipped
Shard 2198 / 5040 skipped
Shard 2199 / 5040 skipped
Shard 2200 / 5040 skipped
Shard 2201 / 5040 skipped
Shard 2202 / 5040 skipped
Shard 2203 / 5040 skipped
Shard 2204 / 5040 skipped
Shard 2205 / 5040 skipped
Shard 2206 / 5040 skipped
Shard 2207 / 5040 skipped
Shard 2208 / 5040 skipped
Shard 2209 / 5040 skipped
Shard 2210 / 5040 skipped
Shard 2211 / 5040 skipped
Shard 2212 / 5040 skipped
Shard 2213 / 5040 skipped
Shard 2214 / 5040 skipped
Shard 2215 / 5040 skipped
Shard 2216 / 5040 skipped
Shard 2217 / 5040 skipped
Shard 2218 / 5040 skipped
Shard 2219 / 5040 skipped
Shard 2220 / 5040 skipped
Shard 2221 / 5040 skipped
Shard 2222 / 5040 skipped
Shard 2223 / 5040 skipped
Shard 2224 / 5040 skipped
Shard 2225 / 5040 skipped
Shard 2226 / 5040 skipped
Shard 2227 / 5040 skipped
Shard 2228 / 5040 skipped
Shard 2229 / 5040 skipped
Shard 2230 / 5040 skipped
Shard 2231 / 5040 skipped
Shard 2232 / 5040 skipped
Shard 2233 / 5040 skipped
Shard 2234 / 5040 skipped
Shard 2235 / 5040 skipped
Shard 2236 / 5040 skipped
Shard 2237 / 5040 skipped
Shard 2238 / 5040 skipped
Shard 2239 / 5040 skipped
Shard 2240 / 5040 skipped
Shard 2241 / 5040 skipped
Shard 2242 / 5040 skipped
Shard 2243 / 5040 skipped
Shard 2244 / 5040 skipped
Shard 2245 / 5040 skipped
Shard 2246 / 5040 skipped
Shard 2247 / 5040 skipped
Shard 2248 / 5040 skipped
Shard 2249 / 5040 skipped
Shard 2250 / 5040 skipped
Shard 2251 / 5040 skipped
Shard 2252 / 5040 skipped
Shard 2253 / 5040 skipped
Shard 2254 / 5040 skipped
Shard 2255 / 5040 skipped
Shard 2256 / 5040 skipped
Shard 2257 / 5040 skipped
Shard 2258 / 5040 skipped
Shard 2259 / 5040 skipped
Shard 2260 / 5040 skipped
Shard 2261 / 5040 skipped
Shard 2262 / 5040 skipped
Shard 2263 / 5040 skipped
Shard 2264 / 5040 skipped
Shard 2265 / 5040 skipped
Shard 2266 / 5040 skipped
Shard 2267 / 5040 skipped
Shard 2268 / 5040 skipped
Shard 2269 / 5040 skipped
Shard 2270 / 5040 skipped
Shard 2271 / 5040 skipped
Shard 2272 / 5040 skipped
Shard 2273 / 5040 skipped
Shard 2274 / 5040 skipped
Shard 2275 / 5040 skipped
Shard 2276 / 5040 skipped
Shard 2277 / 5040 skipped
Shard 2278 / 5040 skipped
Shard 2279 / 5040 skipped
Shard 2280 / 5040 skipped
Shard 2281 / 5040 skipped
Shard 2282 / 5040 skipped
Shard 2283 / 5040 skipped
Shard 2284 / 5040 skipped
Shard 2285 / 5040 skipped
Shard 2286 / 5040 skipped
Shard 2287 / 5040 skipped
Shard 2288 / 5040 skipped
Shard 2289 / 5040 skipped
Shard 2290 / 5040 skipped
Shard 2291 / 5040 skipped
Shard 2292 / 5040 skipped
Shard 2293 / 5040 skipped
Shard 2294 / 5040 skipped
Shard 2295 / 5040 skipped
Shard 2296 / 5040 skipped
Shard 2297 / 5040 skipped
Shard 2298 / 5040 skipped
Shard 2299 / 5040 skipped
Shard 2300 / 5040 skipped
Shard 2301 / 5040 skipped
Shard 2302 / 5040 skipped
Shard 2303 / 5040 skipped
Shard 2304 / 5040 skipped
Shard 2305 / 5040 skipped
Shard 2306 / 5040 skipped
Shard 2307 / 5040 skipped
Shard 2308 / 5040 skipped
Shard 2309 / 5040 skipped
Shard 2310 / 5040 skipped
Shard 2311 / 5040 skipped
Shard 2312 / 5040 skipped
Shard 2313 / 5040 skipped
Shard 2314 / 5040 skipped
Shard 2315 / 5040 skipped
Shard 2316 / 5040 skipped
Shard 2317 / 5040 skipped
Shard 2318 / 5040 skipped
Shard 2319 / 5040 skipped
Shard 2320 / 5040 skipped
Shard 2321 / 5040 skipped
Shard 2322 / 5040 skipped
Shard 2323 / 5040 skipped
Shard 2324 / 5040 skipped
Shard 2325 / 5040 skipped
Shard 2326 / 5040 skipped
Shard 2327 / 5040 skipped
Shard 2328 / 5040 skipped
Shard 2329 / 5040 skipped
Shard 2330 / 5040 skipped
Shard 2331 / 5040 skipped
Shard 2332 / 5040 skipped
Shard 2333 / 5040 skipped
Shard 2334 / 5040 skipped
Shard 2335 / 5040 skipped
Shard 2336 / 5040 skipped
Shard 2337 / 5040 skipped
Shard 2338 / 5040 skipped
Shard 2339 / 5040 skipped
Shard 2340 / 5040 skipped
Shard 2341 / 5040 skipped
Shard 2342 / 5040 skipped
Shard 2343 / 5040 skipped
Shard 2344 / 5040 skipped
Shard 2345 / 5040 skipped
Shard 2346 / 5040 skipped
Shard 2347 / 5040 skipped
Shard 2348 / 5040 skipped
Shard 2349 / 5040 skipped
Shard 2350 / 5040 skipped
Shard 2351 / 5040 skipped
Shard 2352 / 5040 skipped
Shard 2353 / 5040 skipped
Shard 2354 / 5040 skipped
Shard 2355 / 5040 skipped
Shard 2356 / 5040 skipped
Shard 2357 / 5040 skipped
Shard 2358 / 5040 skipped
Shard 2359 / 5040 skipped
Shard 2360 / 5040 skipped
Shard 2361 / 5040 skipped
Shard 2362 / 5040 skipped
Shard 2363 / 5040 skipped
Shard 2364 / 5040 skipped
Shard 2365 / 5040 skipped
Shard 2366 / 5040 skipped
Shard 2367 / 5040 skipped
Shard 2368 / 5040 skipped
Shard 2369 / 5040 skipped
Shard 2370 / 5040 skipped
Shard 2371 / 5040 skipped
Shard 2372 / 5040 skipped
Shard 2373 / 5040 skipped
Shard 2374 / 5040 skipped
Shard 2375 / 5040 skipped
Shard 2376 / 5040 skipped
Shard 2377 / 5040 skipped
Shard 2378 / 5040 skipped
Shard 2379 / 5040 skipped
Shard 2380 / 5040 skipped
Shard 2381 / 5040 skipped
Shard 2382 / 5040 skipped
Shard 2383 / 5040 skipped
Shard 2384 / 5040 skipped
Shard 2385 / 5040 skipped
Shard 2386 / 5040 skipped
Shard 2387 / 5040 skipped
Shard 2388 / 5040 skipped
Shard 2389 / 5040 skipped
Shard 2390 / 5040 skipped
Shard 2391 / 5040 skipped
Shard 2392 / 5040 skipped
Shard 2393 / 5040 skipped
Shard 2394 / 5040 skipped
Shard 2395 / 5040 skipped
Shard 2396 / 5040 skipped
Shard 2397 / 5040 skipped
Shard 2398 / 5040 skipped
Shard 2399 / 5040 skipped
Shard 2400 / 5040 skipped
Shard 2401 / 5040 skipped
Shard 2402 / 5040 skipped
Shard 2403 / 5040 skipped
Shard 2404 / 5040 skipped
Shard 2405 / 5040 skipped
Shard 2406 / 5040 skipped
Shard 2407 / 5040 skipped
Shard 2408 / 5040 skipped
Shard 2409 / 5040 skipped
Shard 2410 / 5040 skipped
Shard 2411 / 5040 skipped
Shard 2412 / 5040 skipped
Shard 2413 / 5040 skipped
Shard 2414 / 5040 skipped
Shard 2415 / 5040 skipped
Shard 2416 / 5040 skipped
Shard 2417 / 5040 skipped
Shard 2418 / 5040 skipped
Shard 2419 / 5040 skipped
Shard 2420 / 5040 skipped
Shard 2421 / 5040 skipped
Shard 2422 / 5040 skipped
Shard 2423 / 5040 skipped
Shard 2424 / 5040 skipped
Shard 2425 / 5040 skipped
Shard 2426 / 5040 skipped
Shard 2427 / 5040 skipped
Shard 2428 / 5040 skipped
Shard 2429 / 5040 skipped
Shard 2430 / 5040 skipped
Shard 2431 / 5040 skipped
Shard 2432 / 5040 skipped
Shard 2433 / 5040 skipped
Shard 2434 / 5040 skipped
Shard 2435 / 5040 skipped
Shard 2436 / 5040 skipped
Shard 2437 / 5040 skipped
Shard 2438 / 5040 skipped
Shard 2439 / 5040 skipped
Shard 2440 / 5040 skipped
Shard 2441 / 5040 skipped
Shard 2442 / 5040 skipped
Shard 2443 / 5040 skipped
Shard 2444 / 5040 skipped
Shard 2445 / 5040 skipped
Shard 2446 / 5040 skipped
Shard 2447 / 5040 skipped
Shard 2448 / 5040 skipped
Shard 2449 / 5040 skipped
Shard 2450 / 5040 skipped
Shard 2451 / 5040 skipped
Shard 2452 / 5040 skipped
Shard 2453 / 5040 skipped
Shard 2454 / 5040 skipped
Shard 2455 / 5040 skipped
Shard 2456 / 5040 skipped
Shard 2457 / 5040 skipped
Shard 2458 / 5040 skipped
Shard 2459 / 5040 skipped
Shard 2460 / 5040 skipped
Shard 2461 / 5040 skipped
Shard 2462 / 5040 skipped
Shard 2463 / 5040 skipped
Shard 2464 / 5040 skipped
Shard 2465 / 5040 skipped
Shard 2466 / 5040 skipped
Shard 2467 / 5040 skipped
Shard 2468 / 5040 skipped
Shard 2469 / 5040 skipped
Shard 2470 / 5040 skipped
Shard 2471 / 5040 skipped
Shard 2472 / 5040 skipped
Shard 2473 / 5040 skipped
Shard 2474 / 5040 skipped
Shard 2475 / 5040 skipped
Shard 2476 / 5040 skipped
Shard 2477 / 5040 skipped
Shard 2478 / 5040 skipped
Shard 2479 / 5040 skipped
Shard 2480 / 5040 skipped
Shard 2481 / 5040 skipped
Shard 2482 / 5040 skipped
Shard 2483 / 5040 skipped
Shard 2484 / 5040 skipped
Shard 2485 / 5040 skipped
Shard 2486 / 5040 skipped
Shard 2487 / 5040 skipped
Shard 2488 / 5040 skipped
Shard 2489 / 5040 skipped
Shard 2490 / 5040 skipped
Shard 2491 / 5040 skipped
Shard 2492 / 5040 skipped
Shard 2493 / 5040 skipped
Shard 2494 / 5040 skipped
Shard 2495 / 5040 skipped
Shard 2496 / 5040 skipped
Shard 2497 / 5040 skipped
Shard 2498 / 5040 skipped
Shard 2499 / 5040 skipped
Shard 2500 / 5040 skipped
Shard 2501 / 5040 skipped
Shard 2502 / 5040 skipped
Shard 2503 / 5040 skipped
Shard 2504 / 5040 skipped
Shard 2505 / 5040 skipped
Shard 2506 / 5040 skipped
Shard 2507 / 5040 skipped
Shard 2508 / 5040 skipped
Shard 2509 / 5040 skipped
Shard 2510 / 5040 skipped
Shard 2511 / 5040 skipped
Shard 2512 / 5040 skipped
Shard 2513 / 5040 skipped
Shard 2514 / 5040 skipped
Shard 2515 / 5040 skipped
Shard 2516 / 5040 skipped
Shard 2517 / 5040 skipped
Shard 2518 / 5040 skipped
Shard 2519 / 5040 skipped
Shard 2520 / 5040 skipped
Shard 2521 / 5040 skipped
Shard 2522 / 5040 skipped
Shard 2523 / 5040 skipped
Shard 2524 / 5040 skipped
Shard 2525 / 5040 skipped
Shard 2526 / 5040 skipped
Shard 2527 / 5040 skipped
Shard 2528 / 5040 skipped
Shard 2529 / 5040 skipped
Shard 2530 / 5040 skipped
Shard 2531 / 5040 skipped
Shard 2532 / 5040 skipped
Shard 2533 / 5040 skipped
Shard 2534 / 5040 skipped
Shard 2535 / 5040 skipped
Shard 2536 / 5040 skipped
Shard 2537 / 5040 skipped
Shard 2538 / 5040 skipped
Shard 2539 / 5040 skipped
Shard 2540 / 5040 skipped
Shard 2541 / 5040 skipped
Shard 2542 / 5040 skipped
Shard 2543 / 5040 skipped
Shard 2544 / 5040 skipped
Shard 2545 / 5040 skipped
Shard 2546 / 5040 skipped
Shard 2547 / 5040 skipped
Shard 2548 / 5040 skipped
Shard 2549 / 5040 skipped
Shard 2550 / 5040 skipped
Shard 2551 / 5040 skipped
Shard 2552 / 5040 skipped
Shard 2553 / 5040 skipped
Shard 2554 / 5040 skipped
Shard 2555 / 5040 skipped
Shard 2556 / 5040 skipped
Shard 2557 / 5040 skipped
Shard 2558 / 5040 skipped
Shard 2559 / 5040 skipped
Shard 2560 / 5040 skipped
Shard 2561 / 5040 skipped
Shard 2562 / 5040 skipped
Shard 2563 / 5040 skipped
Shard 2564 / 5040 skipped
Shard 2565 / 5040 skipped
Shard 2566 / 5040 skipped
Shard 2567 / 5040 skipped
Shard 2568 / 5040 skipped
Shard 2569 / 5040 skipped
Shard 2570 / 5040 skipped
Shard 2571 / 5040 skipped
Shard 2572 / 5040 skipped
Shard 2573 / 5040 skipped
Shard 2574 / 5040 skipped
Shard 2575 / 5040 skipped
Shard 2576 / 5040 skipped
Shard 2577 / 5040 skipped
Shard 2578 / 5040 skipped
Shard 2579 / 5040 skipped
Shard 2580 / 5040 skipped
Shard 2581 / 5040 skipped
Shard 2582 / 5040 skipped
Shard 2583 / 5040 skipped
Shard 2584 / 5040 skipped
Shard 2585 / 5040 skipped
Shard 2586 / 5040 skipped
Shard 2587 / 5040 skipped
Shard 2588 / 5040 skipped
Shard 2589 / 5040 skipped
Shard 2590 / 5040 skipped
Shard 2591 / 5040 skipped
Shard 2592 / 5040 skipped
Shard 2593 / 5040 skipped
Shard 2594 / 5040 skipped
Shard 2595 / 5040 skipped
Shard 2596 / 5040 skipped
Shard 2597 / 5040 skipped
Shard 2598 / 5040 skipped
Shard 2599 / 5040 skipped
Shard 2600 / 5040 skipped
Shard 2601 / 5040 skipped
Shard 2602 / 5040 skipped
Shard 2603 / 5040 skipped
Shard 2604 / 5040 skipped
Shard 2605 / 5040 skipped
Shard 2606 / 5040 skipped
Shard 2607 / 5040 skipped
Shard 2608 / 5040 skipped
Shard 2609 / 5040 skipped
Shard 2610 / 5040 skipped
Shard 2611 / 5040 skipped
Shard 2612 / 5040 skipped
Shard 2613 / 5040 skipped
Shard 2614 / 5040 skipped
Shard 2615 / 5040 skipped
Shard 2616 / 5040 skipped
Shard 2617 / 5040 skipped
Shard 2618 / 5040 skipped
Shard 2619 / 5040 skipped
Shard 2620 / 5040 skipped
Shard 2621 / 5040 skipped
Shard 2622 / 5040 skipped
Shard 2623 / 5040 skipped
Shard 2624 / 5040 skipped
Shard 2625 / 5040 skipped
Shard 2626 / 5040 skipped
Shard 2627 / 5040 skipped
Shard 2628 / 5040 skipped
Shard 2629 / 5040 skipped
Shard 2630 / 5040 skipped
Shard 2631 / 5040 skipped
Shard 2632 / 5040 skipped
Shard 2633 / 5040 skipped
Shard 2634 / 5040 skipped
Shard 2635 / 5040 skipped
Shard 2636 / 5040 skipped
Shard 2637 / 5040 skipped
Shard 2638 / 5040 skipped
Shard 2639 / 5040 skipped
Shard 2640 / 5040 skipped
Shard 2641 / 5040 skipped
Shard 2642 / 5040 skipped
Shard 2643 / 5040 skipped
Shard 2644 / 5040 skipped
Shard 2645 / 5040 skipped
Shard 2646 / 5040 skipped
Shard 2647 / 5040 skipped
Shard 2648 / 5040 skipped
Shard 2649 / 5040 skipped
Shard 2650 / 5040 skipped
Shard 2651 / 5040 skipped
Shard 2652 / 5040 skipped
Shard 2653 / 5040 skipped
Shard 2654 / 5040 skipped
Shard 2655 / 5040 skipped
Shard 2656 / 5040 skipped
Shard 2657 / 5040 skipped
Shard 2658 / 5040 skipped
Shard 2659 / 5040 skipped
Shard 2660 / 5040 skipped
Shard 2661 / 5040 skipped
Shard 2662 / 5040 skipped
Shard 2663 / 5040 skipped
Shard 2664 / 5040 skipped
Shard 2665 / 5040 skipped
Shard 2666 / 5040 skipped
Shard 2667 / 5040 skipped
Shard 2668 / 5040 skipped
Shard 2669 / 5040 skipped
Shard 2670 / 5040 skipped
Shard 2671 / 5040 skipped
Shard 2672 / 5040 skipped
Shard 2673 / 5040 skipped
Shard 2674 / 5040 skipped
Shard 2675 / 5040 skipped
Shard 2676 / 5040 skipped
Shard 2677 / 5040 skipped
Shard 2678 / 5040 skipped
Shard 2679 / 5040 skipped
Shard 2680 / 5040 skipped
Shard 2681 / 5040 skipped
Shard 2682 / 5040 skipped
Shard 2683 / 5040 skipped
Shard 2684 / 5040 skipped
Shard 2685 / 5040 skipped
Shard 2686 / 5040 skipped
Shard 2687 / 5040 skipped
Shard 2688 / 5040 skipped
Shard 2689 / 5040 skipped
Shard 2690 / 5040 skipped
Shard 2691 / 5040 skipped
Shard 2692 / 5040 skipped
Shard 2693 / 5040 skipped
Shard 2694 / 5040 skipped
Shard 2695 / 5040 skipped
Shard 2696 / 5040 skipped
Shard 2697 / 5040 skipped
Shard 2698 / 5040 skipped
Shard 2699 / 5040 skipped
Shard 2700 / 5040 skipped
Shard 2701 / 5040 skipped
Shard 2702 / 5040 skipped
Shard 2703 / 5040 skipped
Shard 2704 / 5040 skipped
Shard 2705 / 5040 skipped
Shard 2706 / 5040 skipped
Shard 2707 / 5040 skipped
Shard 2708 / 5040 skipped
Shard 2709 / 5040 skipped
Shard 2710 / 5040 skipped
Shard 2711 / 5040 skipped
Shard 2712 / 5040 skipped
Shard 2713 / 5040 skipped
Shard 2714 / 5040 skipped
Shard 2715 / 5040 skipped
Shard 2716 / 5040 skipped
Shard 2717 / 5040 skipped
Shard 2718 / 5040 skipped
Shard 2719 / 5040 skipped
Shard 2720 / 5040 skipped
Shard 2721 / 5040 skipped
Shard 2722 / 5040 skipped
Shard 2723 / 5040 skipped
Shard 2724 / 5040 skipped
Shard 2725 / 5040 skipped
Shard 2726 / 5040 skipped
Shard 2727 / 5040 skipped
Shard 2728 / 5040 skipped
Shard 2729 / 5040 skipped
Shard 2730 / 5040 skipped
Shard 2731 / 5040 skipped
Shard 2732 / 5040 skipped
Shard 2733 / 5040 skipped
Shard 2734 / 5040 skipped
Shard 2735 / 5040 skipped
Shard 2736 / 5040 skipped
Shard 2737 / 5040 skipped
Shard 2738 / 5040 skipped
Shard 2739 / 5040 skipped
Shard 2740 / 5040 skipped
Shard 2741 / 5040 skipped
Shard 2742 / 5040 skipped
Shard 2743 / 5040 skipped
Shard 2744 / 5040 skipped
Shard 2745 / 5040 skipped
Shard 2746 / 5040 skipped
Shard 2747 / 5040 skipped
Shard 2748 / 5040 skipped
Shard 2749 / 5040 skipped
Shard 2750 / 5040 skipped
Shard 2751 / 5040 skipped
Shard 2752 / 5040 skipped
Shard 2753 / 5040 skipped
Shard 2754 / 5040 skipped
Shard 2755 / 5040 skipped
Shard 2756 / 5040 skipped
Shard 2757 / 5040 skipped
Shard 2758 / 5040 skipped
Shard 2759 / 5040 skipped
Shard 2760 / 5040 skipped
Shard 2761 / 5040 skipped
Shard 2762 / 5040 skipped
Shard 2763 / 5040 skipped
Shard 2764 / 5040 skipped
Shard 2765 / 5040 skipped
Shard 2766 / 5040 skipped
Shard 2767 / 5040 skipped
Shard 2768 / 5040 skipped
Shard 2769 / 5040 skipped
Shard 2770 / 5040 skipped
Shard 2771 / 5040 skipped
Shard 2772 / 5040 skipped
Shard 2773 / 5040 skipped
Shard 2774 / 5040 skipped
Shard 2775 / 5040 skipped
Shard 2776 / 5040 skipped
Shard 2777 / 5040 skipped
Shard 2778 / 5040 skipped
Shard 2779 / 5040 skipped
Shard 2780 / 5040 skipped
Shard 2781 / 5040 skipped
Shard 2782 / 5040 skipped
Shard 2783 / 5040 skipped
Shard 2784 / 5040 skipped
Shard 2785 / 5040 skipped
Shard 2786 / 5040 skipped
Shard 2787 / 5040 skipped
Shard 2788 / 5040 skipped
Shard 2789 / 5040 skipped
Shard 2790 / 5040 skipped
Shard 2791 / 5040 skipped
Shard 2792 / 5040 skipped
Shard 2793 / 5040 skipped
Shard 2794 / 5040 skipped
Shard 2795 / 5040 skipped
Shard 2796 / 5040 skipped
Shard 2797 / 5040 skipped
Shard 2798 / 5040 skipped
Shard 2799 / 5040 skipped
Shard 2800 / 5040 skipped
Shard 2801 / 5040 skipped
Shard 2802 / 5040 skipped
Shard 2803 / 5040 skipped
Shard 2804 / 5040 skipped
Shard 2805 / 5040 skipped
Shard 2806 / 5040 skipped
Shard 2807 / 5040 skipped
Shard 2808 / 5040 skipped
Shard 2809 / 5040 skipped
Shard 2810 / 5040 skipped
Shard 2811 / 5040 skipped
Shard 2812 / 5040 skipped
Shard 2813 / 5040 skipped
Shard 2814 / 5040 skipped
Shard 2815 / 5040 skipped
Shard 2816 / 5040 skipped
Shard 2817 / 5040 skipped
Shard 2818 / 5040 skipped
Shard 2819 / 5040 skipped
Shard 2820 / 5040 skipped
Shard 2821 / 5040 skipped
Shard 2822 / 5040 skipped
Shard 2823 / 5040 skipped
Shard 2824 / 5040 skipped
Shard 2825 / 5040 skipped
Shard 2826 / 5040 skipped
Shard 2827 / 5040 skipped
Shard 2828 / 5040 skipped
Shard 2829 / 5040 skipped
Shard 2830 / 5040 skipped
Shard 2831 / 5040 skipped
Shard 2832 / 5040 skipped
Shard 2833 / 5040 skipped
Shard 2834 / 5040 skipped
Shard 2835 / 5040 skipped
Shard 2836 / 5040 skipped
Shard 2837 / 5040 skipped
Shard 2838 / 5040 skipped
Shard 2839 / 5040 skipped
Shard 2840 / 5040 skipped
Shard 2841 / 5040 skipped
Shard 2842 / 5040 skipped
Shard 2843 / 5040 skipped
Shard 2844 / 5040 skipped
Shard 2845 / 5040 skipped
Shard 2846 / 5040 skipped
Shard 2847 / 5040 skipped
Shard 2848 / 5040 skipped
Shard 2849 / 5040 skipped
Shard 2850 / 5040 skipped
Shard 2851 / 5040 skipped
Shard 2852 / 5040 skipped
Shard 2853 / 5040 skipped
Shard 2854 / 5040 skipped
Shard 2855 / 5040 skipped
Shard 2856 / 5040 skipped
Shard 2857 / 5040 skipped
Shard 2858 / 5040 skipped
Shard 2859 / 5040 skipped
Shard 2860 / 5040 skipped
Shard 2861 / 5040 skipped
Shard 2862 / 5040 skipped
Shard 2863 / 5040 skipped
Shard 2864 / 5040 skipped
Shard 2865 / 5040 skipped
Shard 2866 / 5040 skipped
Shard 2867 / 5040 skipped
Shard 2868 / 5040 skipped
Shard 2869 / 5040 skipped
Shard 2870 / 5040 skipped
Shard 2871 / 5040 skipped
Shard 2872 / 5040 skipped
Shard 2873 / 5040 skipped
Shard 2874 / 5040 skipped
Shard 2875 / 5040 skipped
Shard 2876 / 5040 skipped
Shard 2877 / 5040 skipped
Shard 2878 / 5040 skipped
Shard 2879 / 5040 skipped
Shard 2880 / 5040 skipped
Shard 2881 / 5040 skipped
Shard 2882 / 5040 skipped
Shard 2883 / 5040 skipped
Shard 2884 / 5040 skipped
Shard 2885 / 5040 skipped
Shard 2886 / 5040 skipped
Shard 2887 / 5040 skipped
Shard 2888 / 5040 skipped
Shard 2889 / 5040 skipped
Shard 2890 / 5040 skipped
Shard 2891 / 5040 skipped
Shard 2892 / 5040 skipped
Shard 2893 / 5040 skipped
Shard 2894 / 5040 skipped
Shard 2895 / 5040 skipped
Shard 2896 / 5040 skipped
Shard 2897 / 5040 skipped
Shard 2898 / 5040 skipped
Shard 2899 / 5040 skipped
Shard 2900 / 5040 skipped
Shard 2901 / 5040 skipped
Shard 2902 / 5040 skipped
Shard 2903 / 5040 skipped
Shard 2904 / 5040 skipped
Shard 2905 / 5040 skipped
Shard 2906 / 5040 skipped
Shard 2907 / 5040 skipped
Shard 2908 / 5040 skipped
Shard 2909 / 5040 skipped
Shard 2910 / 5040 skipped
Shard 2911 / 5040 skipped
Shard 2912 / 5040 skipped
Shard 2913 / 5040 skipped
Shard 2914 / 5040 skipped
Shard 2915 / 5040 skipped
Shard 2916 / 5040 skipped
Shard 2917 / 5040 skipped
Shard 2918 / 5040 skipped
Shard 2919 / 5040 skipped
Shard 2920 / 5040 skipped
Shard 2921 / 5040 skipped
Shard 2922 / 5040 skipped
Shard 2923 / 5040 skipped
Shard 2924 / 5040 skipped
Shard 2925 / 5040 skipped
Shard 2926 / 5040 skipped
Shard 2927 / 5040 skipped
Shard 2928 / 5040 skipped
Shard 2929 / 5040 skipped
Shard 2930 / 5040 skipped
Shard 2931 / 5040 skipped
Shard 2932 / 5040 skipped
Shard 2933 / 5040 skipped
Shard 2934 / 5040 skipped
Shard 2935 / 5040 skipped
Shard 2936 / 5040 skipped
Shard 2937 / 5040 skipped
Shard 2938 / 5040 skipped
Shard 2939 / 5040 skipped
Shard 2940 / 5040 skipped
Shard 2941 / 5040 skipped
Shard 2942 / 5040 skipped
Shard 2943 / 5040 skipped
Shard 2944 / 5040 skipped
Shard 2945 / 5040 skipped
Shard 2946 / 5040 skipped
Shard 2947 / 5040 skipped
Shard 2948 / 5040 skipped
Shard 2949 / 5040 skipped
Shard 2950 / 5040 skipped
Shard 2951 / 5040 skipped
Shard 2952 / 5040 skipped
Shard 2953 / 5040 skipped
Shard 2954 / 5040 skipped
Shard 2955 / 5040 skipped
Shard 2956 / 5040 skipped
Shard 2957 / 5040 skipped
Shard 2958 / 5040 skipped
Shard 2959 / 5040 skipped
Shard 2960 / 5040 skipped
Shard 2961 / 5040 skipped
Shard 2962 / 5040 skipped
Shard 2963 / 5040 skipped
Shard 2964 / 5040 skipped
Shard 2965 / 5040 skipped
Shard 2966 / 5040 skipped
Shard 2967 / 5040 skipped
Shard 2968 / 5040 skipped
Shard 2969 / 5040 skipped
Shard 2970 / 5040 skipped
Shard 2971 / 5040 skipped
Shard 2972 / 5040 skipped
Shard 2973 / 5040 skipped
Shard 2974 / 5040 skipped
Shard 2975 / 5040 skipped
Shard 2976 / 5040 skipped
Shard 2977 / 5040 skipped
Shard 2978 / 5040 skipped
Shard 2979 / 5040 skipped
Shard 2980 / 5040 skipped
Shard 2981 / 5040 skipped
Shard 2982 / 5040 skipped
Shard 2983 / 5040 skipped
Shard 2984 / 5040 skipped
Shard 2985 / 5040 skipped
Shard 2986 / 5040 skipped
Shard 2987 / 5040 skipped
Shard 2988 / 5040 skipped
Shard 2989 / 5040 skipped
Shard 2990 / 5040 skipped
Shard 2991 / 5040 skipped
Shard 2992 / 5040 skipped
Shard 2993 / 5040 skipped
Shard 2994 / 5040 skipped
Shard 2995 / 5040 skipped
Shard 2996 / 5040 skipped
Shard 2997 / 5040 skipped
Shard 2998 / 5040 skipped
Shard 2999 / 5040 skipped
Shard 3000 / 5040 skipped
Shard 3001 / 5040 skipped
Shard 3002 / 5040 skipped
Shard 3003 / 5040 skipped
Shard 3004 / 5040 skipped
Shard 3005 / 5040 skipped
Shard 3006 / 5040 skipped
Shard 3007 / 5040 skipped
Shard 3008 / 5040 skipped
Shard 3009 / 5040 skipped
Shard 3010 / 5040 skipped
Shard 3011 / 5040 skipped
Shard 3012 / 5040 skipped
Shard 3013 / 5040 skipped
Shard 3014 / 5040 skipped
Shard 3015 / 5040 skipped
Shard 3016 / 5040 skipped
Shard 3017 / 5040 skipped
Shard 3018 / 5040 skipped
Shard 3019 / 5040 skipped
Shard 3020 / 5040 skipped
Shard 3021 / 5040 skipped
Shard 3022 / 5040 skipped
Shard 3023 / 5040 skipped
Shard 3024 / 5040 skipped
Shard 3025 / 5040 skipped
Shard 3026 / 5040 skipped
Shard 3027 / 5040 skipped
Shard 3028 / 5040 skipped
Shard 3029 / 5040 skipped
Shard 3030 / 5040 skipped
Shard 3031 / 5040 skipped
Shard 3032 / 5040 skipped
Shard 3033 / 5040 skipped
Shard 3034 / 5040 skipped
Shard 3035 / 5040 skipped
Shard 3036 / 5040 skipped
Shard 3037 / 5040 skipped
Shard 3038 / 5040 skipped
Shard 3039 / 5040 skipped
Shard 3040 / 5040 skipped
Shard 3041 / 5040 skipped
Shard 3042 / 5040 skipped
Shard 3043 / 5040 skipped
Shard 3044 / 5040 skipped
Shard 3045 / 5040 skipped
Shard 3046 / 5040 skipped
Shard 3047 / 5040 skipped
Shard 3048 / 5040 skipped
Shard 3049 / 5040 skipped
Shard 3050 / 5040 skipped
Shard 3051 / 5040 skipped
Shard 3052 / 5040 skipped
Shard 3053 / 5040 skipped
Shard 3054 / 5040 skipped
Shard 3055 / 5040 skipped
Shard 3056 / 5040 skipped
Shard 3057 / 5040 skipped
Shard 3058 / 5040 skipped
Shard 3059 / 5040 skipped
Shard 3060 / 5040 skipped
Shard 3061 / 5040 skipped
Shard 3062 / 5040 skipped
Shard 3063 / 5040 skipped
Shard 3064 / 5040 skipped
Shard 3065 / 5040 skipped
Shard 3066 / 5040 skipped
Shard 3067 / 5040 skipped
Shard 3068 / 5040 skipped
Shard 3069 / 5040 skipped
Shard 3070 / 5040 skipped
Shard 3071 / 5040 skipped
Shard 3072 / 5040 skipped
Shard 3073 / 5040 skipped
Shard 3074 / 5040 skipped
Shard 3075 / 5040 skipped
Shard 3076 / 5040 skipped
Shard 3077 / 5040 skipped
Shard 3078 / 5040 skipped
Shard 3079 / 5040 skipped
Shard 3080 / 5040 skipped
Shard 3081 / 5040 skipped
Shard 3082 / 5040 skipped
Shard 3083 / 5040 skipped
Shard 3084 / 5040 skipped
Shard 3085 / 5040 skipped
Shard 3086 / 5040 skipped
Shard 3087 / 5040 skipped
Shard 3088 / 5040 skipped
Shard 3089 / 5040 skipped
Shard 3090 / 5040 skipped
Shard 3091 / 5040 skipped
Shard 3092 / 5040 skipped
Shard 3093 / 5040 skipped
Shard 3094 / 5040 skipped
Shard 3095 / 5040 skipped
Shard 3096 / 5040 skipped
Shard 3097 / 5040 skipped
Shard 3098 / 5040 skipped
Shard 3099 / 5040 skipped
Shard 3100 / 5040 skipped
Shard 3101 / 5040 skipped
Shard 3102 / 5040 skipped
Shard 3103 / 5040 skipped
Shard 3104 / 5040 skipped
Shard 3105 / 5040 skipped
Shard 3106 / 5040 skipped
Shard 3107 / 5040 skipped
Shard 3108 / 5040 skipped
Shard 3109 / 5040 skipped
Shard 3110 / 5040 skipped
Shard 3111 / 5040 skipped
Shard 3112 / 5040 skipped
Shard 3113 / 5040 skipped
Shard 3114 / 5040 skipped
Shard 3115 / 5040 skipped
Shard 3116 / 5040 skipped
Shard 3117 / 5040 skipped
Shard 3118 / 5040 skipped
Shard 3119 / 5040 skipped
Shard 3120 / 5040 skipped
Shard 3121 / 5040 skipped
Shard 3122 / 5040 skipped
Shard 3123 / 5040 skipped
Shard 3124 / 5040 skipped
Shard 3125 / 5040 skipped
Shard 3126 / 5040 skipped
Shard 3127 / 5040 skipped
Shard 3128 / 5040 skipped
Shard 3129 / 5040 skipped
Shard 3130 / 5040 skipped
Shard 3131 / 5040 skipped
Shard 3132 / 5040 skipped
Shard 3133 / 5040 skipped
Shard 3134 / 5040 skipped
Shard 3135 / 5040 skipped
Shard 3136 / 5040 skipped
Shard 3137 / 5040 skipped
Shard 3138 / 5040 skipped
Shard 3139 / 5040 skipped
Shard 3140 / 5040 skipped
Shard 3141 / 5040 skipped
Shard 3142 / 5040 skipped
Shard 3143 / 5040 skipped
Shard 3144 / 5040 skipped
Shard 3145 / 5040 skipped
Shard 3146 / 5040 skipped
Shard 3147 / 5040 skipped
Shard 3148 / 5040 skipped
Shard 3149 / 5040 skipped
Shard 3150 / 5040 skipped
Shard 3151 / 5040 skipped
Shard 3152 / 5040 skipped
Shard 3153 / 5040 skipped
Shard 3154 / 5040 skipped
Shard 3155 / 5040 skipped
Shard 3156 / 5040 skipped
Shard 3157 / 5040 skipped
Shard 3158 / 5040 skipped
Shard 3159 / 5040 skipped
Shard 3160 / 5040 skipped
Shard 3161 / 5040 skipped
Shard 3162 / 5040 skipped
Shard 3163 / 5040 skipped
Shard 3164 / 5040 skipped
Shard 3165 / 5040 skipped
Shard 3166 / 5040 skipped
Shard 3167 / 5040 skipped
Shard 3168 / 5040 skipped
Shard 3169 / 5040 skipped
Shard 3170 / 5040 skipped
Shard 3171 / 5040 skipped
Shard 3172 / 5040 skipped
Shard 3173 / 5040 skipped
Shard 3174 / 5040 skipped
Shard 3175 / 5040 skipped
Shard 3176 / 5040 skipped
Shard 3177 / 5040 skipped
Shard 3178 / 5040 skipped
Shard 3179 / 5040 skipped
Shard 3180 / 5040 skipped
Shard 3181 / 5040 skipped
Shard 3182 / 5040 skipped
Shard 3183 / 5040 skipped
Shard 3184 / 5040 skipped
Shard 3185 / 5040 skipped
Shard 3186 / 5040 skipped
Shard 3187 / 5040 skipped
Shard 3188 / 5040 skipped
Shard 3189 / 5040 skipped
Shard 3190 / 5040 skipped
Shard 3191 / 5040 skipped
Shard 3192 / 5040 skipped
Shard 3193 / 5040 skipped
Shard 3194 / 5040 skipped
Shard 3195 / 5040 skipped
Shard 3196 / 5040 skipped
Shard 3197 / 5040 skipped
Shard 3198 / 5040 skipped
Shard 3199 / 5040 skipped
Shard 3200 / 5040 skipped
Shard 3201 / 5040 skipped
Shard 3202 / 5040 skipped
Shard 3203 / 5040 skipped
Shard 3204 / 5040 skipped
Shard 3205 / 5040 skipped
Shard 3206 / 5040 skipped
Shard 3207 / 5040 skipped
Shard 3208 / 5040 skipped
Shard 3209 / 5040 skipped
Shard 3210 / 5040 skipped
Shard 3211 / 5040 skipped
Shard 3212 / 5040 skipped
Shard 3213 / 5040 skipped
Shard 3214 / 5040 skipped
Shard 3215 / 5040 skipped
Shard 3216 / 5040 skipped
Shard 3217 / 5040 skipped
Shard 3218 / 5040 skipped
Shard 3219 / 5040 skipped
Shard 3220 / 5040 skipped
Shard 3221 / 5040 skipped
Shard 3222 / 5040 skipped
Shard 3223 / 5040 skipped
Shard 3224 / 5040 skipped
Shard 3225 / 5040 skipped
Shard 3226 / 5040 skipped
Shard 3227 / 5040 skipped
Shard 3228 / 5040 skipped
Shard 3229 / 5040 skipped
Shard 3230 / 5040 skipped
Shard 3231 / 5040 skipped
Shard 3232 / 5040 skipped
Shard 3233 / 5040 skipped
Shard 3234 / 5040 skipped
Shard 3235 / 5040 skipped
Shard 3236 / 5040 skipped
Shard 3237 / 5040 skipped
Shard 3238 / 5040 skipped
Shard 3239 / 5040 skipped
Shard 3240 / 5040 skipped
Shard 3241 / 5040 skipped
Shard 3242 / 5040 skipped
Shard 3243 / 5040 skipped
Shard 3244 / 5040 skipped
Shard 3245 / 5040 skipped
Shard 3246 / 5040 skipped
Shard 3247 / 5040 skipped
Shard 3248 / 5040 skipped
Shard 3249 / 5040 skipped
Shard 3250 / 5040 skipped
Shard 3251 / 5040 skipped
Shard 3252 / 5040 skipped
Shard 3253 / 5040 skipped
Shard 3254 / 5040 skipped
Shard 3255 / 5040 skipped
Shard 3256 / 5040 skipped
Shard 3257 / 5040 skipped
Shard 3258 / 5040 skipped
Shard 3259 / 5040 skipped
Shard 3260 / 5040 skipped
Shard 3261 / 5040 skipped
Shard 3262 / 5040 skipped
Shard 3263 / 5040 skipped
Shard 3264 / 5040 skipped
Shard 3265 / 5040 skipped
Shard 3266 / 5040 skipped
Shard 3267 / 5040 skipped
Shard 3268 / 5040 skipped
Shard 3269 / 5040 skipped
Shard 3270 / 5040 skipped
Shard 3271 / 5040 skipped
Shard 3272 / 5040 skipped
Shard 3273 / 5040 skipped
Shard 3274 / 5040 skipped
Shard 3275 / 5040 skipped
Shard 3276 / 5040 skipped
Shard 3277 / 5040 skipped
Shard 3278 / 5040 skipped
Shard 3279 / 5040 skipped
Shard 3280 / 5040 skipped
Shard 3281 / 5040 skipped
Shard 3282 / 5040 skipped
Shard 3283 / 5040 skipped
Shard 3284 / 5040 skipped
Shard 3285 / 5040 skipped
Shard 3286 / 5040 skipped
Shard 3287 / 5040 skipped
Shard 3288 / 5040 skipped
Shard 3289 / 5040 skipped
Shard 3290 / 5040 skipped
Shard 3291 / 5040 skipped
Shard 3292 / 5040 skipped
Shard 3293 / 5040 skipped
Shard 3294 / 5040 skipped
Shard 3295 / 5040 skipped
Shard 3296 / 5040 skipped
Shard 3297 / 5040 skipped
Shard 3298 / 5040 skipped
Shard 3299 / 5040 skipped
Shard 3300 / 5040 skipped
Shard 3301 / 5040 skipped
Shard 3302 / 5040 skipped
Shard 3303 / 5040 skipped
Shard 3304 / 5040 skipped
Shard 3305 / 5040 skipped
Shard 3306 / 5040 skipped
Shard 3307 / 5040 skipped
Shard 3308 / 5040 skipped
Shard 3309 / 5040 skipped
Shard 3310 / 5040 skipped
Shard 3311 / 5040 skipped
Shard 3312 / 5040 skipped
Shard 3313 / 5040 skipped
Shard 3314 / 5040 skipped
Shard 3315 / 5040 skipped
Shard 3316 / 5040 skipped
Shard 3317 / 5040 skipped
Shard 3318 / 5040 skipped
Shard 3319 / 5040 skipped
Shard 3320 / 5040 skipped
Shard 3321 / 5040 skipped
Shard 3322 / 5040 skipped
Shard 3323 / 5040 skipped
Shard 3324 / 5040 skipped
Shard 3325 / 5040 skipped
Shard 3326 / 5040 skipped
Shard 3327 / 5040 skipped
Shard 3328 / 5040 skipped
Shard 3329 / 5040 skipped
Shard 3330 / 5040 skipped
Shard 3331 / 5040 skipped
Shard 3332 / 5040 skipped
Shard 3333 / 5040 skipped
Shard 3334 / 5040 skipped
Shard 3335 / 5040 skipped
Shard 3336 / 5040 skipped
Shard 3337 / 5040 skipped
Shard 3338 / 5040 skipped
Shard 3339 / 5040 skipped
Shard 3340 / 5040 skipped
Shard 3341 / 5040 skipped
Shard 3342 / 5040 skipped
Shard 3343 / 5040 skipped
Shard 3344 / 5040 skipped
Shard 3345 / 5040 skipped
Shard 3346 / 5040 skipped
Shard 3347 / 5040 skipped
Shard 3348 / 5040 skipped
Shard 3349 / 5040 skipped
Shard 3350 / 5040 skipped
Shard 3351 / 5040 skipped
Shard 3352 / 5040 skipped
Shard 3353 / 5040 skipped
Shard 3354 / 5040 skipped
Shard 3355 / 5040 skipped
Shard 3356 / 5040 skipped
Shard 3357 / 5040 skipped
Shard 3358 / 5040 skipped
Shard 3359 / 5040 skipped
Shard 3360 / 5040 skipped
Shard 3361 / 5040 skipped
Shard 3362 / 5040 skipped
Shard 3363 / 5040 skipped
Shard 3364 / 5040 skipped
Shard 3365 / 5040 skipped
Shard 3366 / 5040 skipped
Shard 3367 / 5040 skipped
Shard 3368 / 5040 skipped
Shard 3369 / 5040 skipped
Shard 3370 / 5040 skipped
Shard 3371 / 5040 skipped
Shard 3372 / 5040 skipped
Shard 3373 / 5040 skipped
Shard 3374 / 5040 skipped
Shard 3375 / 5040 skipped
Shard 3376 / 5040 skipped
Shard 3377 / 5040 skipped
Shard 3378 / 5040 skipped
Shard 3379 / 5040 skipped
Shard 3380 / 5040 skipped
Shard 3381 / 5040 skipped
Shard 3382 / 5040 skipped
Shard 3383 / 5040 skipped
Shard 3384 / 5040 skipped
Shard 3385 / 5040 skipped
Shard 3386 / 5040 skipped
Shard 3387 / 5040 skipped
Shard 3388 / 5040 skipped
Shard 3389 / 5040 skipped
Shard 3390 / 5040 skipped
Shard 3391 / 5040 skipped
Shard 3392 / 5040 skipped
Shard 3393 / 5040 skipped
Shard 3394 / 5040 skipped
Shard 3395 / 5040 skipped
Shard 3396 / 5040 skipped
Shard 3397 / 5040 skipped
Shard 3398 / 5040 skipped
Shard 3399 / 5040 skipped
Shard 3400 / 5040 skipped
Shard 3401 / 5040 skipped
Shard 3402 / 5040 skipped
Shard 3403 / 5040 skipped
Shard 3404 / 5040 skipped
Shard 3405 / 5040 skipped
Shard 3406 / 5040 skipped
Shard 3407 / 5040 skipped
Shard 3408 / 5040 skipped
Shard 3409 / 5040 skipped
Shard 3410 / 5040 skipped
Shard 3411 / 5040 skipped
Shard 3412 / 5040 skipped
Shard 3413 / 5040 skipped
Shard 3414 / 5040 skipped
Shard 3415 / 5040 skipped
Shard 3416 / 5040 skipped
Shard 3417 / 5040 skipped
Shard 3418 / 5040 skipped
Shard 3419 / 5040 skipped
Shard 3420 / 5040 skipped
Shard 3421 / 5040 skipped
Shard 3422 / 5040 skipped
Shard 3423 / 5040 skipped
Shard 3424 / 5040 skipped
Shard 3425 / 5040 skipped
Shard 3426 / 5040 skipped
Shard 3427 / 5040 skipped
Shard 3428 / 5040 skipped
Shard 3429 / 5040 skipped
Shard 3430 / 5040 skipped
Shard 3431 / 5040 skipped
Shard 3432 / 5040 skipped
Shard 3433 / 5040 skipped
Shard 3434 / 5040 skipped
Shard 3435 / 5040 skipped
Shard 3436 / 5040 skipped
Shard 3437 / 5040 skipped
Shard 3438 / 5040 skipped
Shard 3439 / 5040 skipped
Shard 3440 / 5040 skipped
Shard 3441 / 5040 skipped
Shard 3442 / 5040 skipped
Shard 3443 / 5040 skipped
Shard 3444 / 5040 skipped
Shard 3445 / 5040 skipped
Shard 3446 / 5040 skipped
Shard 3447 / 5040 skipped
Shard 3448 / 5040 skipped
Shard 3449 / 5040 skipped
Shard 3450 / 5040 skipped
Shard 3451 / 5040 skipped
Shard 3452 / 5040 skipped
Shard 3453 / 5040 skipped
Shard 3454 / 5040 skipped
Shard 3455 / 5040 skipped
Shard 3456 / 5040 skipped
Shard 3457 / 5040 skipped
Shard 3458 / 5040 skipped
Shard 3459 / 5040 skipped
Shard 3460 / 5040 skipped
Shard 3461 / 5040 skipped
Shard 3462 / 5040 skipped
Shard 3463 / 5040 skipped
Shard 3464 / 5040 skipped
Shard 3465 / 5040 skipped
Shard 3466 / 5040 skipped
Shard 3467 / 5040 skipped
Shard 3468 / 5040 skipped
Shard 3469 / 5040 skipped
Shard 3470 / 5040 skipped
Shard 3471 / 5040 skipped
Shard 3472 / 5040 skipped
Shard 3473 / 5040 skipped
Shard 3474 / 5040 skipped
Shard 3475 / 5040 skipped
Shard 3476 / 5040 skipped
Shard 3477 / 5040 skipped
Shard 3478 / 5040 skipped
Shard 3479 / 5040 skipped
Shard 3480 / 5040 skipped
Shard 3481 / 5040 skipped
Shard 3482 / 5040 skipped
Shard 3483 / 5040 skipped
Shard 3484 / 5040 skipped
Shard 3485 / 5040 skipped
Shard 3486 / 5040 skipped
Shard 3487 / 5040 skipped
Shard 3488 / 5040 skipped
Shard 3489 / 5040 skipped
Shard 3490 / 5040 skipped
Shard 3491 / 5040 skipped
Shard 3492 / 5040 skipped
Shard 3493 / 5040 skipped
Shard 3494 / 5040 skipped
Shard 3495 / 5040 skipped
Shard 3496 / 5040 skipped
Shard 3497 / 5040 skipped
Shard 3498 / 5040 skipped
Shard 3499 / 5040 skipped
Shard 3500 / 5040 skipped
Shard 3501 / 5040 skipped
Shard 3502 / 5040 skipped
Shard 3503 / 5040 skipped
Shard 3504 / 5040 skipped
Shard 3505 / 5040 skipped
Shard 3506 / 5040 skipped
Shard 3507 / 5040 skipped
Shard 3508 / 5040 skipped
Shard 3509 / 5040 skipped
Shard 3510 / 5040 skipped
Shard 3511 / 5040 skipped
Shard 3512 / 5040 skipped
Shard 3513 / 5040 skipped
Shard 3514 / 5040 skipped
Shard 3515 / 5040 skipped
Shard 3516 / 5040 skipped
Shard 3517 / 5040 skipped
Shard 3518 / 5040 skipped
Shard 3519 / 5040 skipped
Shard 3520 / 5040 skipped
Shard 3521 / 5040 skipped
Shard 3522 / 5040 skipped
Shard 3523 / 5040 skipped
Shard 3524 / 5040 skipped
Shard 3525 / 5040 skipped
Shard 3526 / 5040 skipped
Shard 3527 / 5040 skipped
Shard 3528 / 5040 skipped
Shard 3529 / 5040 skipped
Shard 3530 / 5040 skipped
Shard 3531 / 5040 skipped
Shard 3532 / 5040 skipped
Shard 3533 / 5040 skipped
Shard 3534 / 5040 skipped
Shard 3535 / 5040 skipped
Shard 3536 / 5040 skipped
Shard 3537 / 5040 skipped
Shard 3538 / 5040 skipped
Shard 3539 / 5040 skipped
Shard 3540 / 5040 skipped
Shard 3541 / 5040 skipped
Shard 3542 / 5040 skipped
Shard 3543 / 5040 skipped
Shard 3544 / 5040 skipped
Shard 3545 / 5040 skipped
Shard 3546 / 5040 skipped
Shard 3547 / 5040 skipped
Shard 3548 / 5040 skipped
Shard 3549 / 5040 skipped
Shard 3550 / 5040 skipped
Shard 3551 / 5040 skipped
Shard 3552 / 5040 skipped
Shard 3553 / 5040 skipped
Shard 3554 / 5040 skipped
Shard 3555 / 5040 skipped
Shard 3556 / 5040 skipped
Shard 3557 / 5040 skipped
Shard 3558 / 5040 skipped
Shard 3559 / 5040 skipped
Shard 3560 / 5040 skipped
Shard 3561 / 5040 skipped
Shard 3562 / 5040 skipped
Shard 3563 / 5040 skipped
Shard 3564 / 5040 skipped
Shard 3565 / 5040 skipped
Shard 3566 / 5040 skipped
Shard 3567 / 5040 skipped
Shard 3568 / 5040 skipped
Shard 3569 / 5040 skipped
Shard 3570 / 5040 skipped
Shard 3571 / 5040 skipped
Shard 3572 / 5040 skipped
Shard 3573 / 5040 skipped
Shard 3574 / 5040 skipped
Shard 3575 / 5040 skipped
Shard 3576 / 5040 skipped
Shard 3577 / 5040 skipped
Shard 3578 / 5040 skipped
Shard 3579 / 5040 skipped
Shard 3580 / 5040 skipped
Shard 3581 / 5040 skipped
Shard 3582 / 5040 skipped
Shard 3583 / 5040 skipped
Shard 3584 / 5040 skipped
Shard 3585 / 5040 skipped
Shard 3586 / 5040 skipped
Shard 3587 / 5040 skipped
Shard 3588 / 5040 skipped
Shard 3589 / 5040 skipped
Shard 3590 / 5040 skipped
Shard 3591 / 5040 skipped
Shard 3592 / 5040 skipped
Shard 3593 / 5040 skipped
Shard 3594 / 5040 skipped
Shard 3595 / 5040 skipped
Shard 3596 / 5040 skipped
Shard 3597 / 5040 skipped
Shard 3598 / 5040 skipped
Shard 3599 / 5040 skipped
Shard 3600 / 5040 skipped
Shard 3601 / 5040 skipped
Shard 3602 / 5040 skipped
Shard 3603 / 5040 skipped
Shard 3604 / 5040 skipped
Shard 3605 / 5040 skipped
Shard 3606 / 5040 skipped
Shard 3607 / 5040 skipped
Shard 3608 / 5040 skipped
Shard 3609 / 5040 skipped
Shard 3610 / 5040 skipped
Shard 3611 / 5040 skipped
Shard 3612 / 5040 skipped
Shard 3613 / 5040 skipped
Shard 3614 / 5040 skipped
Shard 3615 / 5040 skipped
Shard 3616 / 5040 skipped
Shard 3617 / 5040 skipped
Shard 3618 / 5040 skipped
Shard 3619 / 5040 skipped
Shard 3620 / 5040 skipped
Shard 3621 / 5040 skipped
Shard 3622 / 5040 skipped
Shard 3623 / 5040 skipped
Shard 3624 / 5040 skipped
Shard 3625 / 5040 skipped
Shard 3626 / 5040 skipped
Shard 3627 / 5040 skipped
Shard 3628 / 5040 skipped
Shard 3629 / 5040 skipped
Shard 3630 / 5040 skipped
Shard 3631 / 5040 skipped
Shard 3632 / 5040 skipped
Shard 3633 / 5040 skipped
Shard 3634 / 5040 skipped
Shard 3635 / 5040 skipped
Shard 3636 / 5040 skipped
Shard 3637 / 5040 skipped
Shard 3638 / 5040 skipped
Shard 3639 / 5040 skipped
Shard 3640 / 5040 skipped
Shard 3641 / 5040 skipped
Shard 3642 / 5040 skipped
Shard 3643 / 5040 skipped
Shard 3644 / 5040 skipped
Shard 3645 / 5040 skipped
Shard 3646 / 5040 skipped
Shard 3647 / 5040 skipped
Shard 3648 / 5040 skipped
Shard 3649 / 5040 skipped
Shard 3650 / 5040 skipped
Shard 3651 / 5040 skipped
Shard 3652 / 5040 skipped
Shard 3653 / 5040 skipped
Shard 3654 / 5040 skipped
Shard 3655 / 5040 skipped
Shard 3656 / 5040 skipped
Shard 3657 / 5040 skipped
Shard 3658 / 5040 skipped
Shard 3659 / 5040 skipped
Shard 3660 / 5040 skipped
Shard 3661 / 5040 skipped
Shard 3662 / 5040 skipped
Shard 3663 / 5040 skipped
Shard 3664 / 5040 skipped
Shard 3665 / 5040 skipped
Shard 3666 / 5040 skipped
Shard 3667 / 5040 skipped
Shard 3668 / 5040 skipped
Shard 3669 / 5040 skipped
Shard 3670 / 5040 skipped
Shard 3671 / 5040 skipped
Shard 3672 / 5040 skipped
Shard 3673 / 5040 skipped
Shard 3674 / 5040 skipped
Shard 3675 / 5040 skipped
Shard 3676 / 5040 skipped
Shard 3677 / 5040 skipped
Shard 3678 / 5040 skipped
Shard 3679 / 5040 skipped
Shard 3680 / 5040 skipped
Shard 3681 / 5040 skipped
Shard 3682 / 5040 skipped
Shard 3683 / 5040 skipped
Shard 3684 / 5040 skipped
Shard 3685 / 5040 skipped
Shard 3686 / 5040 skipped
Shard 3687 / 5040 skipped
Shard 3688 / 5040 skipped
Shard 3689 / 5040 skipped
Shard 3690 / 5040 skipped
Shard 3691 / 5040 skipped
Shard 3692 / 5040 skipped
Shard 3693 / 5040 skipped
Shard 3694 / 5040 skipped
Shard 3695 / 5040 skipped
Shard 3696 / 5040 skipped
Shard 3697 / 5040 skipped
Shard 3698 / 5040 skipped
Shard 3699 / 5040 skipped
Shard 3700 / 5040 skipped
Shard 3701 / 5040 skipped
Shard 3702 / 5040 skipped
Shard 3703 / 5040 skipped
Shard 3704 / 5040 skipped
Shard 3705 / 5040 skipped
Shard 3706 / 5040 skipped
Shard 3707 / 5040 skipped
Shard 3708 / 5040 skipped
Shard 3709 / 5040 skipped
Shard 3710 / 5040 skipped
Shard 3711 / 5040 skipped
Shard 3712 / 5040 skipped
Shard 3713 / 5040 skipped
Shard 3714 / 5040 skipped
Shard 3715 / 5040 skipped
Shard 3716 / 5040 skipped
Shard 3717 / 5040 skipped
Shard 3718 / 5040 skipped
Shard 3719 / 5040 skipped
Shard 3720 / 5040 skipped
Shard 3721 / 5040 skipped
Shard 3722 / 5040 skipped
Shard 3723 / 5040 skipped
Shard 3724 / 5040 skipped
Shard 3725 / 5040 skipped
Shard 3726 / 5040 skipped
Shard 3727 / 5040 skipped
Shard 3728 / 5040 skipped
Shard 3729 / 5040 skipped
Shard 3730 / 5040 skipped
Shard 3731 / 5040 skipped
Shard 3732 / 5040 skipped
Shard 3733 / 5040 skipped
Shard 3734 / 5040 skipped
Shard 3735 / 5040 skipped
Shard 3736 / 5040 skipped
Shard 3737 / 5040 skipped
Shard 3738 / 5040 skipped
Shard 3739 / 5040 skipped
Shard 3740 / 5040 skipped
Shard 3741 / 5040 skipped
Shard 3742 / 5040 skipped
Shard 3743 / 5040 skipped
Shard 3744 / 5040 skipped
Shard 3745 / 5040 skipped
Shard 3746 / 5040 skipped
Shard 3747 / 5040 skipped
Shard 3748 / 5040 skipped
Shard 3749 / 5040 skipped
Shard 3750 / 5040 skipped
Shard 3751 / 5040 skipped
Shard 3752 / 5040 skipped
Shard 3753 / 5040 skipped
Shard 3754 / 5040 skipped
Shard 3755 / 5040 skipped
Shard 3756 / 5040 skipped
Shard 3757 / 5040 skipped
Shard 3758 / 5040 skipped
Shard 3759 / 5040 skipped
Shard 3760 / 5040 skipped
Shard 3761 / 5040 skipped
Shard 3762 / 5040 skipped
Shard 3763 / 5040 skipped
Shard 3764 / 5040 skipped
Shard 3765 / 5040 skipped
Shard 3766 / 5040 skipped
Shard 3767 / 5040 skipped
Shard 3768 / 5040 skipped
Shard 3769 / 5040 skipped
Shard 3770 / 5040 skipped
Shard 3771 / 5040 skipped
Shard 3772 / 5040 skipped
Shard 3773 / 5040 skipped
Shard 3774 / 5040 skipped
Shard 3775 / 5040 skipped
Shard 3776 / 5040 skipped
Shard 3777 / 5040 skipped
Shard 3778 / 5040 skipped
Shard 3779 / 5040 skipped
Shard 3780 / 5040 skipped
Shard 3781 / 5040 skipped
Shard 3782 / 5040 skipped
Shard 3783 / 5040 skipped
Shard 3784 / 5040 skipped
Shard 3785 / 5040 skipped
Shard 3786 / 5040 skipped
Shard 3787 / 5040 skipped
Shard 3788 / 5040 skipped
Shard 3789 / 5040 skipped
Shard 3790 / 5040 skipped
Shard 3791 / 5040 skipped
Shard 3792 / 5040 skipped
Shard 3793 / 5040 skipped
Shard 3794 / 5040 skipped
Shard 3795 / 5040 skipped
Shard 3796 / 5040 skipped
Shard 3797 / 5040 skipped
Shard 3798 / 5040 skipped
Shard 3799 / 5040 skipped
Shard 3800 / 5040 skipped
Shard 3801 / 5040 skipped
Shard 3802 / 5040 skipped
Shard 3803 / 5040 skipped
Shard 3804 / 5040 skipped
Shard 3805 / 5040 skipped
Shard 3806 / 5040 skipped
Shard 3807 / 5040 skipped
Shard 3808 / 5040 skipped
Shard 3809 / 5040 skipped
Shard 3810 / 5040 skipped
Shard 3811 / 5040 skipped
Shard 3812 / 5040 skipped
Shard 3813 / 5040 skipped
Shard 3814 / 5040 skipped
Shard 3815 / 5040 skipped
Shard 3816 / 5040 skipped
Shard 3817 / 5040 skipped
Shard 3818 / 5040 skipped
Shard 3819 / 5040 skipped
Shard 3820 / 5040 skipped
Shard 3821 / 5040 skipped
Shard 3822 / 5040 skipped
Shard 3823 / 5040 skipped
Shard 3824 / 5040 skipped
Shard 3825 / 5040 skipped
Shard 3826 / 5040 skipped
Shard 3827 / 5040 skipped
Shard 3828 / 5040 skipped
Shard 3829 / 5040 skipped
Shard 3830 / 5040 skipped
Shard 3831 / 5040 skipped
Shard 3832 / 5040 skipped
Shard 3833 / 5040 skipped
Shard 3834 / 5040 skipped
Shard 3835 / 5040 skipped
Shard 3836 / 5040 skipped
Shard 3837 / 5040 skipped
Shard 3838 / 5040 skipped
Shard 3839 / 5040 skipped
Shard 3840 / 5040 skipped
Shard 3841 / 5040 skipped
Shard 3842 / 5040 skipped
Shard 3843 / 5040 skipped
Shard 3844 / 5040 skipped
Shard 3845 / 5040 skipped
Shard 3846 / 5040 skipped
Shard 3847 / 5040 skipped
Shard 3848 / 5040 skipped
Shard 3849 / 5040 skipped
Shard 3850 / 5040 skipped
Shard 3851 / 5040 skipped
Shard 3852 / 5040 skipped
Shard 3853 / 5040 skipped
Shard 3854 / 5040 skipped
Shard 3855 / 5040 skipped
Shard 3856 / 5040 skipped
Shard 3857 / 5040 skipped
Shard 3858 / 5040 skipped
Shard 3859 / 5040 skipped
Shard 3860 / 5040 skipped
Shard 3861 / 5040 skipped
Shard 3862 / 5040 skipped
Shard 3863 / 5040 skipped
Shard 3864 / 5040 skipped
Shard 3865 / 5040 skipped
Shard 3866 / 5040 skipped
Shard 3867 / 5040 skipped
Shard 3868 / 5040 skipped
Shard 3869 / 5040 skipped
Shard 3870 / 5040 skipped
Shard 3871 / 5040 skipped
Shard 3872 / 5040 skipped
Shard 3873 / 5040 skipped
Shard 3874 / 5040 skipped
Shard 3875 / 5040 skipped
Shard 3876 / 5040 skipped
Shard 3877 / 5040 skipped
Shard 3878 / 5040 skipped
Shard 3879 / 5040 skipped
Shard 3880 / 5040 skipped
Shard 3881 / 5040 skipped
Shard 3882 / 5040 skipped
Shard 3883 / 5040 skipped
Shard 3884 / 5040 skipped
Shard 3885 / 5040 skipped
Shard 3886 / 5040 skipped
Shard 3887 / 5040 skipped
Shard 3888 / 5040 skipped
Shard 3889 / 5040 skipped
Shard 3890 / 5040 skipped
Shard 3891 / 5040 skipped
Shard 3892 / 5040 skipped
Shard 3893 / 5040 skipped
Shard 3894 / 5040 skipped
Shard 3895 / 5040 skipped
Shard 3896 / 5040 skipped
Shard 3897 / 5040 skipped
Shard 3898 / 5040 skipped
Shard 3899 / 5040 skipped
Shard 3900 / 5040 skipped
Shard 3901 / 5040 skipped
Shard 3902 / 5040 skipped
Shard 3903 / 5040 skipped
Shard 3904 / 5040 skipped
Shard 3905 / 5040 skipped
Shard 3906 / 5040 skipped
Shard 3907 / 5040 skipped
Shard 3908 / 5040 skipped
Shard 3909 / 5040 skipped
Shard 3910 / 5040 skipped
Shard 3911 / 5040 skipped
Shard 3912 / 5040 skipped
Shard 3913 / 5040 skipped
Shard 3914 / 5040 skipped
Shard 3915 / 5040 skipped
Shard 3916 / 5040 skipped
Shard 3917 / 5040 skipped
Shard 3918 / 5040 skipped
Shard 3919 / 5040 skipped
Shard 3920 / 5040 skipped
Shard 3921 / 5040 skipped
Shard 3922 / 5040 skipped
Shard 3923 / 5040 skipped
Shard 3924 / 5040 skipped
Shard 3925 / 5040 skipped
Shard 3926 / 5040 skipped
Shard 3927 / 5040 skipped
Shard 3928 / 5040 skipped
Shard 3929 / 5040 skipped
Shard 3930 / 5040 skipped
Shard 3931 / 5040 skipped
Shard 3932 / 5040 skipped
Shard 3933 / 5040 skipped
Shard 3934 / 5040 skipped
Shard 3935 / 5040 skipped
Shard 3936 / 5040 skipped
Shard 3937 / 5040 skipped
Shard 3938 / 5040 skipped
Shard 3939 / 5040 skipped
Shard 3940 / 5040 skipped
Shard 3941 / 5040 skipped
Shard 3942 / 5040 skipped
Shard 3943 / 5040 skipped
Shard 3944 / 5040 skipped
Shard 3945 / 5040 skipped
Shard 3946 / 5040 skipped
Shard 3947 / 5040 skipped
Shard 3948 / 5040 skipped
Shard 3949 / 5040 skipped
Shard 3950 / 5040 skipped
Shard 3951 / 5040 skipped
Shard 3952 / 5040 skipped
Shard 3953 / 5040 skipped
Shard 3954 / 5040 skipped
Shard 3955 / 5040 skipped
Shard 3956 / 5040 skipped
Shard 3957 / 5040 skipped
Shard 3958 / 5040 skipped
Shard 3959 / 5040 skipped
Shard 3960 / 5040 skipped
Shard 3961 / 5040 skipped
Shard 3962 / 5040 skipped
Shard 3963 / 5040 skipped
Shard 3964 / 5040 skipped
Shard 3965 / 5040 skipped
Shard 3966 / 5040 skipped
Shard 3967 / 5040 skipped
Shard 3968 / 5040 skipped
Shard 3969 / 5040 skipped
Shard 3970 / 5040 skipped
Shard 3971 / 5040 skipped
Shard 3972 / 5040 skipped
Shard 3973 / 5040 skipped
Shard 3974 / 5040 skipped
Shard 3975 / 5040 skipped
Shard 3976 / 5040 skipped
Shard 3977 / 5040 skipped
Shard 3978 / 5040 skipped
Shard 3979 / 5040 skipped
Shard 3980 / 5040 skipped
Shard 3981 / 5040 skipped
Shard 3982 / 5040 skipped
Shard 3983 / 5040 skipped
Shard 3984 / 5040 skipped
Shard 3985 / 5040 skipped
Shard 3986 / 5040 skipped
Shard 3987 / 5040 skipped
Shard 3988 / 5040 skipped
Shard 3989 / 5040 skipped
Shard 3990 / 5040 skipped
Shard 3991 / 5040 skipped
Shard 3992 / 5040 skipped
Shard 3993 / 5040 skipped
Shard 3994 / 5040 skipped
Shard 3995 / 5040 skipped
Shard 3996 / 5040 skipped
Shard 3997 / 5040 skipped
Shard 3998 / 5040 skipped
Shard 3999 / 5040 skipped
Shard 4000 / 5040 skipped
Shard 4001 / 5040 skipped
Shard 4002 / 5040 skipped
Shard 4003 / 5040 skipped
Shard 4004 / 5040 skipped
Shard 4005 / 5040 skipped
Shard 4006 / 5040 skipped
Shard 4007 / 5040 skipped
Shard 4008 / 5040 skipped
Shard 4009 / 5040 skipped
Shard 4010 / 5040 skipped
Shard 4011 / 5040 skipped
Shard 4012 / 5040 skipped
Shard 4013 / 5040 skipped
Shard 4014 / 5040 skipped
Shard 4015 / 5040 skipped
Shard 4016 / 5040 skipped
Shard 4017 / 5040 skipped
Shard 4018 / 5040 skipped
Shard 4019 / 5040 skipped
Shard 4020 / 5040 skipped
Shard 4021 / 5040 skipped
Shard 4022 / 5040 skipped
Shard 4023 / 5040 skipped
Shard 4024 / 5040 skipped
Shard 4025 / 5040 skipped
Shard 4026 / 5040 skipped
Shard 4027 / 5040 skipped
Shard 4028 / 5040 skipped
Shard 4029 / 5040 skipped
Shard 4030 / 5040 skipped
Shard 4031 / 5040 skipped
Shard 4032 / 5040 skipped
Shard 4033 / 5040 skipped
Shard 4034 / 5040 skipped
Shard 4035 / 5040 skipped
Shard 4036 / 5040 skipped
Shard 4037 / 5040 skipped
Shard 4038 / 5040 skipped
Shard 4039 / 5040 skipped
Shard 4040 / 5040 skipped
Shard 4041 / 5040 skipped
Shard 4042 / 5040 skipped
Shard 4043 / 5040 skipped
Shard 4044 / 5040 skipped
Shard 4045 / 5040 skipped
Shard 4046 / 5040 skipped
Shard 4047 / 5040 skipped
Shard 4048 / 5040 skipped
Shard 4049 / 5040 skipped
Shard 4050 / 5040 skipped
Shard 4051 / 5040 skipped
Shard 4052 / 5040 skipped
Shard 4053 / 5040 skipped
Shard 4054 / 5040 skipped
Shard 4055 / 5040 skipped
Shard 4056 / 5040 skipped
Shard 4057 / 5040 skipped
Shard 4058 / 5040 skipped
Shard 4059 / 5040 skipped
Shard 4060 / 5040 skipped
Shard 4061 / 5040 skipped
Shard 4062 / 5040 skipped
Shard 4063 / 5040 skipped
Shard 4064 / 5040 skipped
Shard 4065 / 5040 skipped
Shard 4066 / 5040 skipped
Shard 4067 / 5040 skipped
Shard 4068 / 5040 skipped
Shard 4069 / 5040 skipped
Shard 4070 / 5040 skipped
Shard 4071 / 5040 skipped
Shard 4072 / 5040 skipped
Shard 4073 / 5040 skipped
Shard 4074 / 5040 skipped
Shard 4075 / 5040 skipped
Shard 4076 / 5040 skipped
Shard 4077 / 5040 skipped
Shard 4078 / 5040 skipped
Shard 4079 / 5040 skipped
Shard 4080 / 5040 skipped
Shard 4081 / 5040 skipped
Shard 4082 / 5040 skipped
Shard 4083 / 5040 skipped
Shard 4084 / 5040 skipped
Shard 4085 / 5040 skipped
Shard 4086 / 5040 skipped
Shard 4087 / 5040 skipped
Shard 4088 / 5040 skipped
Shard 4089 / 5040 skipped
Shard 4090 / 5040 skipped
Shard 4091 / 5040 skipped
Shard 4092 / 5040 skipped
Shard 4093 / 5040 skipped
Shard 4094 / 5040 skipped
Shard 4095 / 5040 skipped
Shard 4096 / 5040 skipped
Shard 4097 / 5040 skipped
Shard 4098 / 5040 skipped
Shard 4099 / 5040 skipped
Shard 4100 / 5040 skipped
Shard 4101 / 5040 skipped
Shard 4102 / 5040 skipped
Shard 4103 / 5040 skipped
Shard 4104 / 5040 skipped
Shard 4105 / 5040 skipped
Shard 4106 / 5040 skipped
Shard 4107 / 5040 skipped
Shard 4108 / 5040 skipped
Shard 4109 / 5040 skipped
Shard 4110 / 5040 skipped
Shard 4111 / 5040 skipped
Shard 4112 / 5040 skipped
Shard 4113 / 5040 skipped
Shard 4114 / 5040 skipped
Shard 4115 / 5040 skipped
Shard 4116 / 5040 skipped
Shard 4117 / 5040 skipped
Shard 4118 / 5040 skipped
Shard 4119 / 5040 skipped
Shard 4120 / 5040 skipped
Shard 4121 / 5040 skipped
Shard 4122 / 5040 skipped
Shard 4123 / 5040 skipped
Shard 4124 / 5040 skipped
Shard 4125 / 5040 skipped
Shard 4126 / 5040 skipped
Shard 4127 / 5040 skipped
Shard 4128 / 5040 skipped
Shard 4129 / 5040 skipped
Shard 4130 / 5040 skipped
Shard 4131 / 5040 skipped
Shard 4132 / 5040 skipped
Shard 4133 / 5040 skipped
Shard 4134 / 5040 skipped
Shard 4135 / 5040 skipped
Shard 4136 / 5040 skipped
Shard 4137 / 5040 skipped
Shard 4138 / 5040 skipped
Shard 4139 / 5040 skipped
Shard 4140 / 5040 skipped
Shard 4141 / 5040 skipped
Shard 4142 / 5040 skipped
Shard 4143 / 5040 skipped
Shard 4144 / 5040 skipped
Shard 4145 / 5040 skipped
Shard 4146 / 5040 skipped
Shard 4147 / 5040 skipped
Shard 4148 / 5040 skipped
Shard 4149 / 5040 skipped
Shard 4150 / 5040 skipped
Shard 4151 / 5040 skipped
Shard 4152 / 5040 skipped
Shard 4153 / 5040 skipped
Shard 4154 / 5040 skipped
Shard 4155 / 5040 skipped
Shard 4156 / 5040 skipped
Shard 4157 / 5040 skipped
Shard 4158 / 5040 skipped
Shard 4159 / 5040 skipped
Shard 4160 / 5040 skipped
Shard 4161 / 5040 skipped
Shard 4162 / 5040 skipped
Shard 4163 / 5040 skipped
Shard 4164 / 5040 skipped
Shard 4165 / 5040 skipped
Shard 4166 / 5040 skipped
Shard 4167 / 5040 skipped
Shard 4168 / 5040 skipped
Shard 4169 / 5040 skipped
Shard 4170 / 5040 skipped
Shard 4171 / 5040 skipped
Shard 4172 / 5040 skipped
Shard 4173 / 5040 skipped
Shard 4174 / 5040 skipped
Shard 4175 / 5040 skipped
Shard 4176 / 5040 skipped
Shard 4177 / 5040 skipped
Shard 4178 / 5040 skipped
Shard 4179 / 5040 skipped
Shard 4180 / 5040 skipped
Shard 4181 / 5040 skipped
Shard 4182 / 5040 skipped
Shard 4183 / 5040 skipped
Shard 4184 / 5040 skipped
Shard 4185 / 5040 skipped
Shard 4186 / 5040 skipped
Shard 4187 / 5040 skipped
Shard 4188 / 5040 skipped
Shard 4189 / 5040 skipped
Shard 4190 / 5040 skipped
Shard 4191 / 5040 skipped
Shard 4192 / 5040 skipped
Shard 4193 / 5040 skipped
Shard 4194 / 5040 skipped
Shard 4195 / 5040 skipped
Shard 4196 / 5040 skipped
Shard 4197 / 5040 skipped
Shard 4198 / 5040 skipped
Shard 4199 / 5040 skipped
Shard 4200 / 5040 skipped
Shard 4201 / 5040 skipped
Shard 4202 / 5040 skipped
Shard 4203 / 5040 skipped
Shard 4204 / 5040 skipped
Shard 4205 / 5040 skipped
Shard 4206 / 5040 skipped
Shard 4207 / 5040 skipped
Shard 4208 / 5040 skipped
Shard 4209 / 5040 skipped
Shard 4210 / 5040 skipped
Shard 4211 / 5040 skipped
Shard 4212 / 5040 skipped
Shard 4213 / 5040 skipped
Shard 4214 / 5040 skipped
Shard 4215 / 5040 skipped
Shard 4216 / 5040 skipped
Shard 4217 / 5040 skipped
Shard 4218 / 5040 skipped
Shard 4219 / 5040 skipped
Shard 4220 / 5040 skipped
Shard 4221 / 5040 skipped
Shard 4222 / 5040 skipped
Shard 4223 / 5040 skipped
Shard 4224 / 5040 skipped
Shard 4225 / 5040 skipped
Shard 4226 / 5040 skipped
Shard 4227 / 5040 skipped
Shard 4228 / 5040 skipped
Shard 4229 / 5040 skipped
Shard 4230 / 5040 skipped
Shard 4231 / 5040 skipped
Shard 4232 / 5040 skipped
Shard 4233 / 5040 skipped
Shard 4234 / 5040 skipped
Shard 4235 / 5040 skipped
Shard 4236 / 5040 skipped
Shard 4237 / 5040 skipped
Shard 4238 / 5040 skipped
Shard 4239 / 5040 skipped
Shard 4240 / 5040 skipped
Shard 4241 / 5040 skipped
Shard 4242 / 5040 skipped
Shard 4243 / 5040 skipped
Shard 4244 / 5040 skipped
Shard 4245 / 5040 skipped
Shard 4246 / 5040 skipped
Shard 4247 / 5040 skipped
Shard 4248 / 5040 skipped
Shard 4249 / 5040 skipped
Shard 4250 / 5040 skipped
Shard 4251 / 5040 skipped
Shard 4252 / 5040 skipped
Shard 4253 / 5040 skipped
Shard 4254 / 5040 skipped
Shard 4255 / 5040 skipped
Shard 4256 / 5040 skipped
Shard 4257 / 5040 skipped
Shard 4258 / 5040 skipped
Shard 4259 / 5040 skipped
Shard 4260 / 5040 skipped
Shard 4261 / 5040 skipped
Shard 4262 / 5040 skipped
Shard 4263 / 5040 skipped
Shard 4264 / 5040 skipped
Shard 4265 / 5040 skipped
Shard 4266 / 5040 skipped
Shard 4267 / 5040 skipped
Shard 4268 / 5040 skipped
Shard 4269 / 5040 skipped
Shard 4270 / 5040 skipped
Shard 4271 / 5040 skipped
Shard 4272 / 5040 skipped
Shard 4273 / 5040 skipped
Shard 4274 / 5040 skipped
Shard 4275 / 5040 skipped
Shard 4276 / 5040 skipped
Shard 4277 / 5040 skipped
Shard 4278 / 5040 skipped
Shard 4279 / 5040 skipped
Shard 4280 / 5040 skipped
Shard 4281 / 5040 skipped
Shard 4282 / 5040 skipped
Shard 4283 / 5040 skipped
Shard 4284 / 5040 skipped
Shard 4285 / 5040 skipped
Shard 4286 / 5040 skipped
Shard 4287 / 5040 skipped
Shard 4288 / 5040 skipped
Shard 4289 / 5040 skipped
Shard 4290 / 5040 skipped
Shard 4291 / 5040 skipped
Shard 4292 / 5040 skipped
Shard 4293 / 5040 skipped
Shard 4294 / 5040 skipped
Shard 4295 / 5040 skipped
Shard 4296 / 5040 skipped
Shard 4297 / 5040 skipped
Shard 4298 / 5040 skipped
Shard 4299 / 5040 skipped
Shard 4300 / 5040 skipped
Shard 4301 / 5040 skipped
Shard 4302 / 5040 skipped
Shard 4303 / 5040 skipped
Shard 4304 / 5040 skipped
Shard 4305 / 5040 skipped
Shard 4306 / 5040 skipped
Shard 4307 / 5040 skipped
Shard 4308 / 5040 skipped
Shard 4309 / 5040 skipped
Shard 4310 / 5040 skipped
Shard 4311 / 5040 skipped
Shard 4312 / 5040 skipped
Shard 4313 / 5040 skipped
Shard 4314 / 5040 skipped
Shard 4315 / 5040 skipped
Shard 4316 / 5040 skipped
Shard 4317 / 5040 skipped
Shard 4318 / 5040 skipped
Shard 4319 / 5040 skipped
Shard 4320 / 5040 skipped
Shard 4321 / 5040 skipped
Shard 4322 / 5040 skipped
Shard 4323 / 5040 skipped
Shard 4324 / 5040 skipped
Shard 4325 / 5040 skipped
Shard 4326 / 5040 skipped
Shard 4327 / 5040 skipped
Shard 4328 / 5040 skipped
Shard 4329 / 5040 skipped
Shard 4330 / 5040 skipped
Shard 4331 / 5040 skipped
Shard 4332 / 5040 skipped
Shard 4333 / 5040 skipped
Shard 4334 / 5040 skipped
Shard 4335 / 5040 skipped
Shard 4336 / 5040 skipped
Shard 4337 / 5040 skipped
Shard 4338 / 5040 skipped
Shard 4339 / 5040 skipped
Shard 4340 / 5040 skipped
Shard 4341 / 5040 skipped
Shard 4342 / 5040 skipped
Shard 4343 / 5040 skipped
Shard 4344 / 5040 skipped
Shard 4345 / 5040 skipped
Shard 4346 / 5040 skipped
Shard 4347 / 5040 skipped
Shard 4348 / 5040 skipped
Shard 4349 / 5040 skipped
Shard 4350 / 5040 skipped
Shard 4351 / 5040 skipped
Shard 4352 / 5040 skipped
Shard 4353 / 5040 skipped
Shard 4354 / 5040 skipped
Shard 4355 / 5040 skipped
Shard 4356 / 5040 skipped
Shard 4357 / 5040 skipped
Shard 4358 / 5040 skipped
Shard 4359 / 5040 skipped
Shard 4360 / 5040 skipped
Shard 4361 / 5040 skipped
Shard 4362 / 5040 skipped
Shard 4363 / 5040 skipped
Shard 4364 / 5040 skipped
Shard 4365 / 5040 skipped
Shard 4366 / 5040 skipped
Shard 4367 / 5040 skipped
Shard 4368 / 5040 skipped
Shard 4369 / 5040 skipped
Shard 4370 / 5040 skipped
Shard 4371 / 5040 skipped
Shard 4372 / 5040 skipped
Shard 4373 / 5040 skipped
Shard 4374 / 5040 skipped
Shard 4375 / 5040 skipped
Shard 4376 / 5040 skipped
Shard 4377 / 5040 skipped
Shard 4378 / 5040 skipped
Shard 4379 / 5040 skipped
Shard 4380 / 5040 skipped
Shard 4381 / 5040 skipped
Shard 4382 / 5040 skipped
Shard 4383 / 5040 skipped
Shard 4384 / 5040 skipped
Shard 4385 / 5040 skipped
Shard 4386 / 5040 skipped
Shard 4387 / 5040 skipped
Shard 4388 / 5040 skipped
Shard 4389 / 5040 skipped
Shard 4390 / 5040 skipped
Shard 4391 / 5040 skipped
Shard 4392 / 5040 skipped
Shard 4393 / 5040 skipped
Shard 4394 / 5040 skipped
Shard 4395 / 5040 skipped
Shard 4396 / 5040 skipped
Shard 4397 / 5040 skipped
Shard 4398 / 5040 skipped
Shard 4399 / 5040 skipped
Shard 4400 / 5040 skipped
Shard 4401 / 5040 skipped
Shard 4402 / 5040 skipped
Shard 4403 / 5040 skipped
Shard 4404 / 5040 skipped
Shard 4405 / 5040 skipped
Shard 4406 / 5040 skipped
Shard 4407 / 5040 skipped
Shard 4408 / 5040 skipped
Shard 4409 / 5040 skipped
Shard 4410 / 5040 skipped
Shard 4411 / 5040 skipped
Shard 4412 / 5040 skipped
Shard 4413 / 5040 skipped
Shard 4414 / 5040 skipped
Shard 4415 / 5040 skipped
Shard 4416 / 5040 skipped
Shard 4417 / 5040 skipped
Shard 4418 / 5040 skipped
Shard 4419 / 5040 skipped
Shard 4420 / 5040 skipped
Shard 4421 / 5040 skipped
Shard 4422 / 5040 skipped
Shard 4423 / 5040 skipped
Shard 4424 / 5040 skipped
Shard 4425 / 5040 skipped
Shard 4426 / 5040 skipped
Shard 4427 / 5040 skipped
Shard 4428 / 5040 skipped
Shard 4429 / 5040 skipped
Shard 4430 / 5040 skipped
Shard 4431 / 5040 skipped
Shard 4432 / 5040 skipped
Shard 4433 / 5040 skipped
Shard 4434 / 5040 skipped
Shard 4435 / 5040 skipped
Shard 4436 / 5040 skipped
Shard 4437 / 5040 skipped
Shard 4438 / 5040 skipped
Shard 4439 / 5040 skipped
Shard 4440 / 5040 skipped
Shard 4441 / 5040 skipped
Shard 4442 / 5040 skipped
Shard 4443 / 5040 skipped
Shard 4444 / 5040 skipped
Shard 4445 / 5040 skipped
Shard 4446 / 5040 skipped
Shard 4447 / 5040 skipped
Shard 4448 / 5040 skipped
Shard 4449 / 5040 skipped
Shard 4450 / 5040 skipped
Shard 4451 / 5040 skipped
Shard 4452 / 5040 skipped
Shard 4453 / 5040 skipped
Shard 4454 / 5040 skipped
Shard 4455 / 5040 skipped
Shard 4456 / 5040 skipped
Shard 4457 / 5040 skipped
Shard 4458 / 5040 skipped
Shard 4459 / 5040 skipped
Shard 4460 / 5040 skipped
Shard 4461 / 5040 skipped
Shard 4462 / 5040 skipped
Shard 4463 / 5040 skipped
Shard 4464 / 5040 skipped
Shard 4465 / 5040 skipped
Shard 4466 / 5040 skipped
Shard 4467 / 5040 skipped
Shard 4468 / 5040 skipped
Shard 4469 / 5040 skipped
Shard 4470 / 5040 skipped
Shard 4471 / 5040 skipped
Shard 4472 / 5040 skipped
Shard 4473 / 5040 skipped
Shard 4474 / 5040 skipped
Shard 4475 / 5040 skipped
Shard 4476 / 5040 skipped
Shard 4477 / 5040 skipped
Shard 4478 / 5040 skipped
Shard 4479 / 5040 skipped
Shard 4480 / 5040 skipped
Shard 4481 / 5040 skipped
Shard 4482 / 5040 skipped
Shard 4483 / 5040 skipped
Shard 4484 / 5040 skipped
Shard 4485 / 5040 skipped
Shard 4486 / 5040 skipped
Shard 4487 / 5040 skipped
Shard 4488 / 5040 skipped
Shard 4489 / 5040 skipped
Shard 4490 / 5040 skipped
Shard 4491 / 5040 skipped
Shard 4492 / 5040 skipped
Shard 4493 / 5040 skipped
Shard 4494 / 5040 skipped
Shard 4495 / 5040 skipped
Shard 4496 / 5040 skipped
Shard 4497 / 5040 skipped
Shard 4498 / 5040 skipped
Shard 4499 / 5040 skipped
Shard 4500 / 5040 skipped
Shard 4501 / 5040 skipped
Shard 4502 / 5040 skipped
Shard 4503 / 5040 skipped
Shard 4504 / 5040 skipped
Shard 4505 / 5040 skipped
Shard 4506 / 5040 skipped
Shard 4507 / 5040 skipped
Shard 4508 / 5040 skipped
Shard 4509 / 5040 skipped
Shard 4510 / 5040 skipped
Shard 4511 / 5040 skipped
Shard 4512 / 5040 skipped
Shard 4513 / 5040 skipped
Shard 4514 / 5040 skipped
Shard 4515 / 5040 skipped
Shard 4516 / 5040 skipped
Shard 4517 / 5040 skipped
Shard 4518 / 5040 skipped
Shard 4519 / 5040 skipped
Shard 4520 / 5040 skipped
Shard 4521 / 5040 skipped
Shard 4522 / 5040 skipped
Shard 4523 / 5040 skipped
Shard 4524 / 5040 skipped
Shard 4525 / 5040 skipped
Shard 4526 / 5040 skipped
Shard 4527 / 5040 skipped
Shard 4528 / 5040 skipped
Shard 4529 / 5040 skipped
Shard 4530 / 5040 skipped
Shard 4531 / 5040 skipped
Shard 4532 / 5040 skipped
Shard 4533 / 5040 skipped
Shard 4534 / 5040 skipped
Shard 4535 / 5040 skipped
Shard 4536 / 5040 skipped
Shard 4537 / 5040 skipped
Shard 4538 / 5040 skipped
Shard 4539 / 5040 skipped
Shard 4540 / 5040 skipped
Shard 4541 / 5040 skipped
Shard 4542 / 5040 skipped
Shard 4543 / 5040 skipped
Shard 4544 / 5040 skipped
Shard 4545 / 5040 skipped
Shard 4546 / 5040 skipped
Shard 4547 / 5040 skipped
Shard 4548 / 5040 skipped
Shard 4549 / 5040 skipped
Shard 4550 / 5040 skipped
Shard 4551 / 5040 skipped
Shard 4552 / 5040 skipped
Shard 4553 / 5040 skipped
Shard 4554 / 5040 skipped
Shard 4555 / 5040 skipped
Shard 4556 / 5040 skipped
Shard 4557 / 5040 skipped
Shard 4558 / 5040 skipped
Shard 4559 / 5040 skipped
Shard 4560 / 5040 skipped
Shard 4561 / 5040 skipped
Shard 4562 / 5040 skipped
Shard 4563 / 5040 skipped
Shard 4564 / 5040 skipped
Shard 4565 / 5040 skipped
Shard 4566 / 5040 skipped
Shard 4567 / 5040 skipped
Shard 4568 / 5040 skipped
Shard 4569 / 5040 skipped
Shard 4570 / 5040 skipped
Shard 4571 / 5040 skipped
Shard 4572 / 5040 skipped
Shard 4573 / 5040 skipped
Shard 4574 / 5040 skipped
Shard 4575 / 5040 skipped
Shard 4576 / 5040 skipped
Shard 4577 / 5040 skipped
Shard 4578 / 5040 skipped
Shard 4579 / 5040 skipped
Shard 4580 / 5040 skipped
Shard 4581 / 5040 skipped
Shard 4582 / 5040 skipped
Shard 4583 / 5040 skipped
Shard 4584 / 5040 skipped
Shard 4585 / 5040 skipped
Shard 4586 / 5040 skipped
Shard 4587 / 5040 skipped
Shard 4588 / 5040 skipped
Shard 4589 / 5040 skipped
Shard 4590 / 5040 skipped
Shard 4591 / 5040 skipped
Shard 4592 / 5040 skipped
Shard 4593 / 5040 skipped
Shard 4594 / 5040 skipped
Shard 4595 / 5040 skipped
Shard 4596 / 5040 skipped
Shard 4597 / 5040 skipped
Shard 4598 / 5040 skipped
Shard 4599 / 5040 skipped
Shard 4600 / 5040 skipped
Shard 4601 / 5040 skipped
Shard 4602 / 5040 skipped
Shard 4603 / 5040 skipped
Shard 4604 / 5040 skipped
Shard 4605 / 5040 skipped
Shard 4606 / 5040 skipped
Shard 4607 / 5040 skipped
Shard 4608 / 5040 skipped
Shard 4609 / 5040 skipped
Shard 4610 / 5040 skipped
Shard 4611 / 5040 skipped
Shard 4612 / 5040 skipped
Shard 4613 / 5040 skipped
Shard 4614 / 5040 skipped
Shard 4615 / 5040 skipped
Shard 4616 / 5040 skipped
Shard 4617 / 5040 skipped
Shard 4618 / 5040 skipped
Shard 4619 / 5040 skipped
Shard 4620 / 5040 skipped
Shard 4621 / 5040 skipped
Shard 4622 / 5040 skipped
Shard 4623 / 5040 skipped
Shard 4624 / 5040 skipped
Shard 4625 / 5040 skipped
Shard 4626 / 5040 skipped
Shard 4627 / 5040 skipped
Shard 4628 / 5040 skipped
Shard 4629 / 5040 skipped
Shard 4630 / 5040 skipped
Shard 4631 / 5040 skipped
Shard 4632 / 5040 skipped
Shard 4633 / 5040 skipped
Shard 4634 / 5040 skipped
Shard 4635 / 5040 skipped
Shard 4636 / 5040 skipped
Shard 4637 / 5040 skipped
Shard 4638 / 5040 skipped
Shard 4639 / 5040 skipped
Shard 4640 / 5040 skipped
Shard 4641 / 5040 skipped
Shard 4642 / 5040 skipped
Shard 4643 / 5040 skipped
Shard 4644 / 5040 skipped
Shard 4645 / 5040 skipped
Shard 4646 / 5040 skipped
Shard 4647 / 5040 skipped
Shard 4648 / 5040 skipped
Shard 4649 / 5040 skipped
Shard 4650 / 5040 skipped
Shard 4651 / 5040 skipped
Shard 4652 / 5040 skipped
Shard 4653 / 5040 skipped
Shard 4654 / 5040 skipped
Shard 4655 / 5040 skipped
Shard 4656 / 5040 skipped
Shard 4657 / 5040 skipped
Shard 4658 / 5040 skipped
Shard 4659 / 5040 skipped
Shard 4660 / 5040 skipped
Shard 4661 / 5040 skipped
Shard 4662 / 5040 skipped
Shard 4663 / 5040 skipped
Shard 4664 / 5040 skipped
Shard 4665 / 5040 skipped
Shard 4666 / 5040 skipped
Shard 4667 / 5040 skipped
Shard 4668 / 5040 skipped
Shard 4669 / 5040 skipped
Shard 4670 / 5040 skipped
Shard 4671 / 5040 skipped
Shard 4672 / 5040 skipped
Shard 4673 / 5040 skipped
Shard 4674 / 5040 skipped
Shard 4675 / 5040 skipped
Shard 4676 / 5040 skipped
Shard 4677 / 5040 skipped
Shard 4678 / 5040 skipped
Shard 4679 / 5040 skipped
Shard 4680 / 5040 skipped
Shard 4681 / 5040 skipped
Shard 4682 / 5040 skipped
Shard 4683 / 5040 skipped
Shard 4684 / 5040 skipped
Shard 4685 / 5040 skipped
Shard 4686 / 5040 skipped
Shard 4687 / 5040 skipped
Shard 4688 / 5040 skipped
Shard 4689 / 5040 skipped
Shard 4690 / 5040 skipped
Shard 4691 / 5040 skipped
Shard 4692 / 5040 skipped
Shard 4693 / 5040 skipped
Shard 4694 / 5040 skipped
Shard 4695 / 5040 skipped
Shard 4696 / 5040 skipped
Shard 4697 / 5040 skipped
Shard 4698 / 5040 skipped
Shard 4699 / 5040 skipped
Shard 4700 / 5040 skipped
Shard 4701 / 5040 skipped
Shard 4702 / 5040 skipped
Shard 4703 / 5040 skipped
Shard 4704 / 5040 skipped
Shard 4705 / 5040 skipped
Shard 4706 / 5040 skipped
Shard 4707 / 5040 skipped
Shard 4708 / 5040 skipped
Shard 4709 / 5040 skipped
Shard 4710 / 5040 skipped
Shard 4711 / 5040 skipped
Shard 4712 / 5040 skipped
Shard 4713 / 5040 skipped
Shard 4714 / 5040 skipped
Shard 4715 / 5040 skipped
Shard 4716 / 5040 skipped
Shard 4717 / 5040 skipped
Shard 4718 / 5040 skipped
Shard 4719 / 5040 skipped
Shard 4720 / 5040 skipped
Shard 4721 / 5040 skipped
Shard 4722 / 5040 skipped
Shard 4723 / 5040 skipped
Shard 4724 / 5040 skipped
Shard 4725 / 5040 skipped
Shard 4726 / 5040 skipped
Shard 4727 / 5040 skipped
Shard 4728 / 5040 skipped
Shard 4729 / 5040 skipped
Shard 4730 / 5040 skipped
Shard 4731 / 5040 skipped
Shard 4732 / 5040 skipped
Shard 4733 / 5040 skipped
Shard 4734 / 5040 skipped
Shard 4735 / 5040 skipped
Shard 4736 / 5040 skipped
Shard 4737 / 5040 skipped
Shard 4738 / 5040 skipped
Shard 4739 / 5040 skipped
Shard 4740 / 5040 skipped
Shard 4741 / 5040 skipped
Shard 4742 / 5040 skipped
Shard 4743 / 5040 skipped
Shard 4744 / 5040 skipped
Shard 4745 / 5040 skipped
Shard 4746 / 5040 skipped
Shard 4747 / 5040 skipped
Shard 4748 / 5040 skipped
Shard 4749 / 5040 skipped
Shard 4750 / 5040 skipped
Shard 4751 / 5040 skipped
Shard 4752 / 5040 skipped
Shard 4753 / 5040 skipped
Shard 4754 / 5040 skipped
Shard 4755 / 5040 skipped
Shard 4756 / 5040 skipped
Shard 4757 / 5040 skipped
Shard 4758 / 5040 skipped
Shard 4759 / 5040 skipped
Shard 4760 / 5040 skipped
Shard 4761 / 5040 skipped
Shard 4762 / 5040 skipped
Shard 4763 / 5040 skipped
Shard 4764 / 5040 skipped
Shard 4765 / 5040 skipped
Shard 4766 / 5040 skipped
Shard 4767 / 5040 skipped
Shard 4768 / 5040 skipped
Shard 4769 / 5040 skipped
Shard 4770 / 5040 skipped
Shard 4771 / 5040 skipped
Shard 4772 / 5040 skipped
Shard 4773 / 5040 skipped
Shard 4774 / 5040 skipped
Shard 4775 / 5040 skipped
Shard 4776 / 5040 skipped
Shard 4777 / 5040 skipped
Shard 4778 / 5040 skipped
Shard 4779 / 5040 skipped
Shard 4780 / 5040 skipped
Shard 4781 / 5040 skipped
Shard 4782 / 5040 skipped
Shard 4783 / 5040 skipped
Shard 4784 / 5040 skipped
Shard 4785 / 5040 skipped
Shard 4786 / 5040 skipped
Shard 4787 / 5040 skipped
Shard 4788 / 5040 skipped
Shard 4789 / 5040 skipped
Shard 4790 / 5040 skipped
Shard 4791 / 5040 skipped
Shard 4792 / 5040 skipped
Shard 4793 / 5040 skipped
Shard 4794 / 5040 skipped
Shard 4795 / 5040 skipped
Shard 4796 / 5040 skipped
Shard 4797 / 5040 skipped
Shard 4798 / 5040 skipped
Shard 4799 / 5040 skipped
Shard 4800 / 5040 skipped
Shard 4801 / 5040 skipped
Shard 4802 / 5040 skipped
Shard 4803 / 5040 skipped
Shard 4804 / 5040 skipped
Shard 4805 / 5040 skipped
Shard 4806 / 5040 skipped
Shard 4807 / 5040 skipped
Shard 4808 / 5040 skipped
Shard 4809 / 5040 skipped
Shard 4810 / 5040 skipped
Shard 4811 / 5040 skipped
Shard 4812 / 5040 skipped
Shard 4813 / 5040 skipped
Shard 4814 / 5040 skipped
Shard 4815 / 5040 skipped
Shard 4816 / 5040 skipped
Shard 4817 / 5040 skipped
Shard 4818 / 5040 skipped
Shard 4819 / 5040 skipped
Shard 4820 / 5040 skipped
Shard 4821 / 5040 skipped
Shard 4822 / 5040 skipped
Shard 4823 / 5040 skipped
Shard 4824 / 5040 skipped
Shard 4825 / 5040 skipped
Shard 4826 / 5040 skipped
Shard 4827 / 5040 skipped
Shard 4828 / 5040 skipped
Shard 4829 / 5040 skipped
Shard 4830 / 5040 skipped
Shard 4831 / 5040 skipped
Shard 4832 / 5040 skipped
Shard 4833 / 5040 skipped
Shard 4834 / 5040 skipped
Shard 4835 / 5040 skipped
Shard 4836 / 5040 skipped
Shard 4837 / 5040 skipped
Shard 4838 / 5040 skipped
Shard 4839 / 5040 skipped
Shard 4840 / 5040 skipped
Shard 4841 / 5040 skipped
Shard 4842 / 5040 skipped
Shard 4843 / 5040 skipped
Shard 4844 / 5040 skipped
Shard 4845 / 5040 skipped
Shard 4846 / 5040 skipped
Shard 4847 / 5040 skipped
Shard 4848 / 5040 skipped
Shard 4849 / 5040 skipped
Shard 4850 / 5040 skipped
Shard 4851 / 5040 skipped
Shard 4852 / 5040 skipped
Shard 4853 / 5040 skipped
Shard 4854 / 5040 skipped
Shard 4855 / 5040 skipped
Shard 4856 / 5040 skipped
Shard 4857 / 5040 skipped
Shard 4858 / 5040 skipped
Shard 4859 / 5040 skipped
Shard 4860 / 5040 skipped
Shard 4861 / 5040 skipped
Shard 4862 / 5040 skipped
Shard 4863 / 5040 skipped
Shard 4864 / 5040 skipped
Shard 4865 / 5040 skipped
Shard 4866 / 5040 skipped
Shard 4867 / 5040 skipped
Shard 4868 / 5040 skipped
Shard 4869 / 5040 skipped
Shard 4870 / 5040 skipped
Shard 4871 / 5040 skipped
Shard 4872 / 5040 skipped
Shard 4873 / 5040 skipped
Shard 4874 / 5040 skipped
Shard 4875 / 5040 skipped
Shard 4876 / 5040 skipped
Shard 4877 / 5040 skipped
Shard 4878 / 5040 skipped
Shard 4879 / 5040 skipped
Shard 4880 / 5040 skipped
Shard 4881 / 5040 skipped
Shard 4882 / 5040 skipped
Shard 4883 / 5040 skipped
Shard 4884 / 5040 skipped
Shard 4885 / 5040 skipped
Shard 4886 / 5040 skipped
Shard 4887 / 5040 skipped
Shard 4888 / 5040 skipped
Shard 4889 / 5040 skipped
Shard 4890 / 5040 skipped
Shard 4891 / 5040 skipped
Shard 4892 / 5040 skipped
Shard 4893 / 5040 skipped
Shard 4894 / 5040 skipped
Shard 4895 / 5040 skipped
Shard 4896 / 5040 skipped
Shard 4897 / 5040 skipped
Shard 4898 / 5040 skipped
Shard 4899 / 5040 skipped
Shard 4900 / 5040 skipped
Shard 4901 / 5040 skipped
Shard 4902 / 5040 skipped
Shard 4903 / 5040 skipped
Shard 4904 / 5040 skipped
Shard 4905 / 5040 skipped
Shard 4906 / 5040 skipped
Shard 4907 / 5040 skipped
Shard 4908 / 5040 skipped
Shard 4909 / 5040 skipped
Shard 4910 / 5040 skipped
Shard 4911 / 5040 skipped
Shard 4912 / 5040 skipped
Shard 4913 / 5040 skipped
Shard 4914 / 5040 skipped
Shard 4915 / 5040 skipped
Shard 4916 / 5040 skipped
Shard 4917 / 5040 skipped
Shard 4918 / 5040 skipped
Shard 4919 / 5040 skipped
Shard 4920 / 5040 skipped
Shard 4921 / 5040 skipped
Shard 4922 / 5040 skipped
Shard 4923 / 5040 skipped
Shard 4924 / 5040 skipped
Shard 4925 / 5040 skipped
Shard 4926 / 5040 skipped
Shard 4927 / 5040 skipped
Shard 4928 / 5040 skipped
Shard 4929 / 5040 skipped
Shard 4930 / 5040 skipped
Shard 4931 / 5040 skipped
Shard 4932 / 5040 skipped
Shard 4933 / 5040 skipped
Shard 4934 / 5040 skipped
Shard 4935 / 5040 skipped
Shard 4936 / 5040 skipped
Shard 4937 / 5040 skipped
Shard 4938 / 5040 skipped
Shard 4939 / 5040 skipped
Shard 4940 / 5040 skipped
Shard 4941 / 5040 skipped
Shard 4942 / 5040 skipped
Shard 4943 / 5040 skipped
Shard 4944 / 5040 skipped
Shard 4945 / 5040 skipped
Shard 4946 / 5040 skipped
Shard 4947 / 5040 skipped
Shard 4948 / 5040 skipped
Shard 4949 / 5040 skipped
Shard 4950 / 5040 skipped
Shard 4951 / 5040 skipped
Shard 4952 / 5040 skipped
Shard 4953 / 5040 skipped
Shard 4954 / 5040 skipped
Shard 4955 / 5040 skipped
Shard 4956 / 5040 skipped
Shard 4957 / 5040 skipped
Shard 4958 / 5040 skipped
Shard 4959 / 5040 skipped
Shard 4960 / 5040 skipped
Shard 4961 / 5040 skipped
Shard 4962 / 5040 skipped
Shard 4963 / 5040 skipped
Shard 4964 / 5040 skipped
Shard 4965 / 5040 skipped
Shard 4966 / 5040 skipped
Shard 4967 / 5040 skipped
Shard 4968 / 5040 skipped
Shard 4969 / 5040 skipped
Shard 4970 / 5040 skipped
Shard 4971 / 5040 skipped
Shard 4972 / 5040 skipped
Shard 4973 / 5040 skipped
Shard 4974 / 5040 skipped
Shard 4975 / 5040 skipped
Shard 4976 / 5040 skipped
Shard 4977 / 5040 skipped
Shard 4978 / 5040 skipped
Shard 4979 / 5040 skipped
Shard 4980 / 5040 skipped
Shard 4981 / 5040 skipped
Shard 4982 / 5040 skipped
Shard 4983 / 5040 skipped
Shard 4984 / 5040 skipped
Shard 4985 / 5040 skipped
Shard 4986 / 5040 skipped
Shard 4987 / 5040 skipped
Shard 4988 / 5040 skipped
Shard 4989 / 5040 skipped
Shard 4990 / 5040 skipped
Shard 4991 / 5040 skipped
Shard 4992 / 5040 skipped
Shard 4993 / 5040 skipped
Shard 4994 / 5040 skipped
Shard 4995 / 5040 skipped
Shard 4996 / 5040 skipped
Shard 4997 / 5040 skipped
Shard 4998 / 5040 skipped
Shard 4999 / 5040 skipped
Shard 5000 / 5040 skipped
Shard 5001 / 5040 skipped
Shard 5002 / 5040 skipped
Shard 5003 / 5040 skipped
Shard 5004 / 5040 skipped
Shard 5005 / 5040 skipped
Shard 5006 / 5040 skipped
Shard 5007 / 5040 skipped
Shard 5008 / 5040 skipped
Shard 5009 / 5040 skipped
Shard 5010 / 5040 skipped
Shard 5011 / 5040 skipped
Shard 5012 / 5040 skipped
Shard 5013 / 5040 skipped
Shard 5014 / 5040 skipped
Shard 5015 / 5040 skipped
Shard 5016 / 5040 skipped
Shard 5017 / 5040 skipped
Shard 5018 / 5040 skipped
Shard 5019 / 5040 skipped
Shard 5020 / 5040 skipped
Shard 5021 / 5040 skipped
Shard 5022 / 5040 skipped
Shard 5023 / 5040 skipped
Shard 5024 / 5040 skipped
Shard 5025 / 5040 skipped
Shard 5026 / 5040 skipped
Shard 5027 / 5040 skipped
Shard 5028 / 5040 skipped
Shard 5029 / 5040 skipped
Shard 5030 / 5040 skipped
Shard 5031 / 5040 skipped
Shard 5032 / 5040 skipped
Shard 5033 / 5040 skipped
Shard 5034 / 5040 skipped
Shard 5035 / 5040 skipped
Shard 5036 / 5040 skipped
Shard 5037 / 5040 skipped
Shard 5038 / 5040 skipped
Shard 5039 / 5040 skipped
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4725
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:40:40,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=5011, skipped=0, lr=[7.930826026931259e-07], mom=[(0.9, 0.999)]
steps: 5011 loss: 0.5729 iter time (s): 100.245 samples/sec: 1.277

100%|██████████| 1/1 [03:23<00:00, 203.36s/it][A100%|██████████| 1/1 [03:23<00:00, 203.36s/it]
 97%|█████████▋| 5041/5198 [03:23<00:06, 24.79it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [06:54<00:00, 414.66s/it][A100%|██████████| 1/1 [06:54<00:00, 414.66s/it]
 97%|█████████▋| 5041/5198 [06:54<00:12, 12.16it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [04:49<00:00, 289.15s/it][A100%|██████████| 1/1 [04:49<00:00, 289.15s/it]
 97%|█████████▋| 5041/5198 [04:49<00:09, 17.43it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [04:14<00:00, 254.18s/it][A100%|██████████| 1/1 [04:14<00:00, 254.18s/it]
 97%|█████████▋| 5041/5198 [04:14<00:07, 19.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [07:28<00:00, 448.58s/it][A100%|██████████| 1/1 [07:28<00:00, 448.58s/it]
 97%|█████████▋| 5041/5198 [07:29<00:13, 11.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [06:53<00:00, 413.82s/it][A100%|██████████| 1/1 [06:53<00:00, 413.82s/it]
 97%|█████████▋| 5041/5198 [06:53<00:12, 12.18it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [04:29<00:00, 269.55s/it][A100%|██████████| 1/1 [04:29<00:00, 269.55s/it]
 97%|█████████▋| 5041/5198 [04:29<00:08, 18.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.96s/it][A100%|██████████| 1/1 [01:40<00:00, 100.96s/it]
 97%|█████████▋| 5041/5198 [01:40<00:03, 49.92it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4726
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A 97%|█████████▋| 5041/5198 [04:40<00:08, 18.70it/s] 97%|█████████▋| 5041/5198 [05:00<00:09, 17.43it/s] 97%|█████████▋| 5041/5198 [07:40<00:13, 11.22it/s] 97%|█████████▋| 5041/5198 [03:37<00:06, 24.79it/s] 97%|█████████▋| 5041/5198 [04:28<00:07, 19.83it/s] 97%|█████████▋| 5041/5198 [07:08<00:12, 12.18it/s] 97%|█████████▋| 5041/5198 [07:10<00:12, 12.16it/s] 97%|█████████▋| 5041/5198 [01:58<00:03, 49.92it/s]
100%|██████████| 1/1 [02:01<00:00, 121.82s/it][A100%|██████████| 1/1 [02:01<00:00, 121.82s/it]
 97%|█████████▋| 5042/5198 [05:25<00:11, 13.35it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:42:49,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=5012, skipped=0, lr=[7.908762086545714e-07], mom=[(0.9, 0.999)]
steps: 5012 loss: 0.6255 iter time (s): 124.465 samples/sec: 1.028

100%|██████████| 1/1 [02:05<00:00, 125.18s/it][A100%|██████████| 1/1 [02:05<00:00, 125.18s/it]
 97%|█████████▋| 5042/5198 [08:59<00:18,  8.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.19s/it][A100%|██████████| 1/1 [02:05<00:00, 125.19s/it]
 97%|█████████▋| 5042/5198 [06:54<00:14, 10.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.25s/it][A100%|██████████| 1/1 [02:05<00:00, 125.25s/it]
 97%|█████████▋| 5042/5198 [06:19<00:13, 11.64it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.22s/it][A100%|██████████| 1/1 [02:05<00:00, 125.22s/it]
 97%|█████████▋| 5042/5198 [09:34<00:19,  8.03it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.24s/it][A100%|██████████| 1/1 [02:05<00:00, 125.24s/it]
 97%|█████████▋| 5042/5198 [08:59<00:18,  8.51it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.24s/it][A100%|██████████| 1/1 [02:05<00:00, 125.24s/it]
 97%|█████████▋| 5042/5198 [06:34<00:13, 11.24it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.24s/it][A100%|██████████| 1/1 [02:05<00:00, 125.24s/it]
 97%|█████████▋| 5042/5198 [03:46<00:08, 18.01it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4727
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.65s/it][A100%|██████████| 1/1 [01:35<00:00, 95.65s/it]
 97%|█████████▋| 5043/5198 [07:01<00:17,  8.80it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:44:24,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=5013, skipped=0, lr=[7.886727694523509e-07], mom=[(0.9, 0.999)]
steps: 5013 loss: 0.6153 iter time (s): 94.200 samples/sec: 1.359

100%|██████████| 1/1 [01:34<00:00, 94.92s/it][A100%|██████████| 1/1 [01:34<00:00, 94.92s/it]
 97%|█████████▋| 5043/5198 [10:34<00:24,  6.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.92s/it][A100%|██████████| 1/1 [01:34<00:00, 94.92s/it]
 97%|█████████▋| 5043/5198 [08:29<00:20,  7.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.89s/it][A100%|██████████| 1/1 [01:34<00:00, 94.89s/it]
 97%|█████████▋| 5043/5198 [07:54<00:19,  8.05it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.88s/it][A100%|██████████| 1/1 [01:34<00:00, 94.89s/it]
 97%|█████████▋| 5043/5198 [11:09<00:25,  6.14it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.90s/it][A100%|██████████| 1/1 [01:34<00:00, 94.90s/it]
 97%|█████████▋| 5043/5198 [10:33<00:24,  6.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.91s/it][A100%|██████████| 1/1 [01:34<00:00, 94.91s/it]
 97%|█████████▋| 5043/5198 [08:09<00:19,  7.86it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.90s/it][A100%|██████████| 1/1 [01:34<00:00, 94.90s/it]
 97%|█████████▋| 5043/5198 [05:21<00:14, 10.65it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4728
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.16s/it][A100%|██████████| 1/1 [01:37<00:00, 97.16s/it]
[2024-06-26 17:46:02,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=5014, skipped=0, lr=[7.864722857933834e-07], mom=[(0.9, 0.999)]
steps: 5014 loss: 0.5559 iter time (s): 96.828 samples/sec: 1.322

100%|██████████| 1/1 [01:37<00:00, 97.57s/it][A100%|██████████| 1/1 [01:37<00:00, 97.57s/it]

100%|██████████| 1/1 [01:37<00:00, 97.56s/it][A100%|██████████| 1/1 [01:37<00:00, 97.56s/it]

100%|██████████| 1/1 [01:37<00:00, 97.53s/it][A100%|██████████| 1/1 [01:37<00:00, 97.53s/it]

100%|██████████| 1/1 [01:37<00:00, 97.55s/it][A100%|██████████| 1/1 [01:37<00:00, 97.55s/it]

100%|██████████| 1/1 [01:37<00:00, 97.52s/it][A100%|██████████| 1/1 [01:37<00:00, 97.52s/it]

100%|██████████| 1/1 [01:37<00:00, 97.54s/it][A100%|██████████| 1/1 [01:37<00:00, 97.55s/it]

100%|██████████| 1/1 [01:37<00:00, 97.57s/it][A100%|██████████| 1/1 [01:37<00:00, 97.57s/it]
Checkpointing at shard 5043
[2024-06-26 17:46:08,035] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5014 is about to be saved!
[2024-06-26 17:46:08,991] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_00-model_states.pt...
[2024-06-26 17:46:12,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_08-model_states.pt...
[2024-06-26 17:46:14,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_02-model_states.pt...
[2024-06-26 17:46:15,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_07-model_states.pt...
[2024-06-26 17:46:19,764] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_06-model_states.pt...
[2024-06-26 17:46:19,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_05-model_states.pt...
[2024-06-26 17:46:23,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_03-model_states.pt...
[2024-06-26 17:46:24,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_04-model_states.pt...
[2024-06-26 17:46:30,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_00-model_states.pt.
[2024-06-26 17:46:35,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_01-model_states.pt...
[2024-06-26 17:50:32,852] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_08-model_states.pt.
[2024-06-26 17:50:32,858] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_05-model_states.pt.
[2024-06-26 17:50:32,864] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_01-model_states.pt.
[2024-06-26 17:50:32,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_02-model_states.pt.
[2024-06-26 17:50:32,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_06-model_states.pt.
[2024-06-26 17:50:32,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_03-model_states.pt.
[2024-06-26 17:50:32,927] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_04-model_states.pt.
[2024-06-26 17:50:32,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_02_model_states.pt...
[2024-06-26 17:50:32,982] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_09-model_states.pt...
[2024-06-26 17:50:32,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_03_model_states.pt...
[2024-06-26 17:50:33,106] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_02_model_states.pt.
[2024-06-26 17:50:33,106] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
[2024-06-26 17:50:33,122] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_03_model_states.pt.
[2024-06-26 17:50:33,122] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
[2024-06-26 17:50:33,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_04_model_states.pt...
[2024-06-26 17:50:33,162] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_01_model_states.pt
[2024-06-26 17:50:33,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_01_model_states.pt...
[2024-06-26 17:50:33,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_04_model_states.pt.
[2024-06-26 17:50:33,210] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
[2024-06-26 17:50:33,255] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_00_model_states.pt
[2024-06-26 17:50:33,256] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_00_model_states.pt...
[2024-06-26 17:50:33,263] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_01_model_states.pt.
[2024-06-26 17:50:33,265] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
[2024-06-26 17:50:33,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_05_model_states.pt...
[2024-06-26 17:50:33,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_07-model_states.pt.
[2024-06-26 17:50:33,403] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_05_model_states.pt.
[2024-06-26 17:50:33,404] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
[2024-06-26 17:50:33,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_06_model_states.pt...
[2024-06-26 17:50:33,645] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/layer_09-model_states.pt.
[2024-06-26 17:50:33,648] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_07_model_states.pt...
[2024-06-26 17:50:33,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_06_model_states.pt.
[2024-06-26 17:50:33,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
[2024-06-26 17:50:33,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_07_model_states.pt.
[2024-06-26 17:50:33,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
[2024-06-26 17:50:34,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5014/mp_rank_00_model_states.pt.
[2024-06-26 17:50:34,495] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5014 is ready now!
Checkpoint saved using --- 271.33396577835083 seconds ---
 97%|█████████▋| 5044/5198 [13:13<00:50,  3.05it/s] 97%|█████████▋| 5044/5198 [16:42<00:56,  2.71it/s] 97%|█████████▋| 5044/5198 [14:03<00:51,  2.96it/s] 97%|█████████▋| 5044/5198 [16:44<00:56,  2.71it/s] 97%|█████████▋| 5044/5198 [17:18<00:57,  2.66it/s] 97%|█████████▋| 5044/5198 [11:30<00:47,  3.26it/s] 97%|█████████▋| 5044/5198 [14:38<00:53,  2.90it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4729
 97%|█████████▋| 5044/5198 [14:18<00:52,  2.94it/s]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:27<00:00, 147.92s/it][A100%|██████████| 1/1 [02:27<00:00, 147.92s/it]
 97%|█████████▋| 5045/5198 [15:41<01:08,  2.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:53:06,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=5015, skipped=0, lr=[7.842747583836372e-07], mom=[(0.9, 0.999)]
steps: 5015 loss: 0.6181 iter time (s): 151.811 samples/sec: 0.843

100%|██████████| 1/1 [02:31<00:00, 151.85s/it][A100%|██████████| 1/1 [02:31<00:00, 151.85s/it]
 97%|█████████▋| 5045/5198 [14:02<01:06,  2.31it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4730
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.04s/it][A100%|██████████| 1/1 [02:32<00:00, 152.04s/it]
 97%|█████████▋| 5045/5198 [19:16<01:15,  2.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.21s/it][A100%|██████████| 1/1 [02:32<00:00, 152.21s/it]
 97%|█████████▋| 5045/5198 [17:10<01:11,  2.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.29s/it][A100%|██████████| 1/1 [02:32<00:00, 152.29s/it]
 97%|█████████▋| 5045/5198 [16:35<01:10,  2.16it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.36s/it][A100%|██████████| 1/1 [02:32<00:00, 152.36s/it]
 97%|█████████▋| 5045/5198 [19:51<01:16,  1.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.37s/it][A100%|██████████| 1/1 [02:32<00:00, 152.37s/it]
 97%|█████████▋| 5045/5198 [19:15<01:15,  2.02it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.32s/it][A100%|██████████| 1/1 [02:32<00:00, 152.32s/it]
 97%|█████████▋| 5045/5198 [16:51<01:11,  2.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.22s/it][A100%|██████████| 1/1 [01:22<00:00, 82.22s/it]
 97%|█████████▋| 5046/5198 [17:03<01:23,  1.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:54:26,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=5016, skipped=0, lr=[7.820801879281363e-07], mom=[(0.9, 0.999)]
steps: 5016 loss: 0.6132 iter time (s): 80.313 samples/sec: 1.594

100%|██████████| 1/1 [01:20<00:00, 80.44s/it][A100%|██████████| 1/1 [01:20<00:00, 80.44s/it]
 97%|█████████▋| 5046/5198 [20:36<01:29,  1.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.45s/it][A100%|██████████| 1/1 [01:20<00:00, 80.45s/it]
 97%|█████████▋| 5046/5198 [18:31<01:25,  1.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.44s/it][A100%|██████████| 1/1 [01:20<00:00, 80.44s/it]
 97%|█████████▋| 5046/5198 [17:56<01:24,  1.80it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.45s/it][A100%|██████████| 1/1 [01:20<00:00, 80.45s/it]
 97%|█████████▋| 5046/5198 [21:11<01:30,  1.68it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.47s/it][A100%|██████████| 1/1 [01:20<00:00, 80.47s/it]
 97%|█████████▋| 5046/5198 [20:36<01:29,  1.70it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.49s/it][A100%|██████████| 1/1 [01:20<00:00, 80.49s/it]
 97%|█████████▋| 5046/5198 [18:11<01:25,  1.79it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.04s/it][A100%|██████████| 1/1 [01:21<00:00, 81.04s/it]
 97%|█████████▋| 5046/5198 [15:23<01:20,  1.90it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4731
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:35<00:00, 155.16s/it][A100%|██████████| 1/1 [02:35<00:00, 155.16s/it]
 97%|█████████▋| 5047/5198 [19:39<02:01,  1.24it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:57:04,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=5017, skipped=0, lr=[7.798885751309496e-07], mom=[(0.9, 0.999)]
steps: 5017 loss: 0.6663 iter time (s): 156.648 samples/sec: 0.817

100%|██████████| 1/1 [02:37<00:00, 157.38s/it][A100%|██████████| 1/1 [02:37<00:00, 157.38s/it]
 97%|█████████▋| 5047/5198 [23:14<02:08,  1.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:37<00:00, 157.32s/it][A100%|██████████| 1/1 [02:37<00:00, 157.32s/it]
 97%|█████████▋| 5047/5198 [21:08<02:04,  1.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:37<00:00, 157.38s/it][A100%|██████████| 1/1 [02:37<00:00, 157.38s/it]
 97%|█████████▋| 5047/5198 [20:33<02:03,  1.22it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:37<00:00, 157.37s/it][A100%|██████████| 1/1 [02:37<00:00, 157.37s/it]
 97%|█████████▋| 5047/5198 [23:48<02:09,  1.16it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:37<00:00, 157.39s/it][A100%|██████████| 1/1 [02:37<00:00, 157.39s/it]
 97%|█████████▋| 5047/5198 [23:13<02:08,  1.17it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:37<00:00, 157.34s/it][A100%|██████████| 1/1 [02:37<00:00, 157.34s/it]
 97%|█████████▋| 5047/5198 [18:00<01:59,  1.27it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4732
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:37<00:00, 157.37s/it][A100%|██████████| 1/1 [02:37<00:00, 157.37s/it]
 97%|█████████▋| 5047/5198 [20:49<02:04,  1.21it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.44s/it][A100%|██████████| 1/1 [01:42<00:00, 102.44s/it]
 97%|█████████▋| 5048/5198 [21:21<02:37,  1.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 17:58:45,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=5018, skipped=0, lr=[7.776999206952028e-07], mom=[(0.9, 0.999)]
steps: 5018 loss: 0.6127 iter time (s): 100.499 samples/sec: 1.274

100%|██████████| 1/1 [01:41<00:00, 101.18s/it][A100%|██████████| 1/1 [01:41<00:00, 101.19s/it]
 97%|█████████▋| 5048/5198 [24:55<02:43,  1.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.23s/it][A100%|██████████| 1/1 [01:41<00:00, 101.23s/it]
 97%|█████████▋| 5048/5198 [22:50<02:40,  1.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.17s/it][A100%|██████████| 1/1 [01:41<00:00, 101.17s/it]
 97%|█████████▋| 5048/5198 [22:14<02:39,  1.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.18s/it][A100%|██████████| 1/1 [01:41<00:00, 101.18s/it]
 97%|█████████▋| 5048/5198 [25:30<02:44,  1.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.17s/it][A100%|██████████| 1/1 [01:41<00:00, 101.17s/it]
 97%|█████████▋| 5048/5198 [24:54<02:43,  1.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.19s/it][A100%|██████████| 1/1 [01:41<00:00, 101.19s/it]
 97%|█████████▋| 5048/5198 [22:30<02:39,  1.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.22s/it][A100%|██████████| 1/1 [01:41<00:00, 101.22s/it]
 97%|█████████▋| 5048/5198 [19:41<02:34,  1.03s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4733
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.89s/it][A100%|██████████| 1/1 [01:50<00:00, 110.89s/it]
 97%|█████████▋| 5049/5198 [23:12<03:32,  1.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:00:36,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=5019, skipped=0, lr=[7.755142253230709e-07], mom=[(0.9, 0.999)]
steps: 5019 loss: 0.5833 iter time (s): 110.665 samples/sec: 1.157

100%|██████████| 1/1 [01:51<00:00, 111.34s/it][A100%|██████████| 1/1 [01:51<00:00, 111.34s/it]
 97%|█████████▋| 5049/5198 [26:46<03:38,  1.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.34s/it][A100%|██████████| 1/1 [01:51<00:00, 111.34s/it]
 97%|█████████▋| 5049/5198 [24:41<03:34,  1.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.41s/it][A100%|██████████| 1/1 [01:51<00:00, 111.41s/it]
 97%|█████████▋| 5049/5198 [24:06<03:33,  1.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.39s/it][A100%|██████████| 1/1 [01:51<00:00, 111.39s/it]
 97%|█████████▋| 5049/5198 [27:21<03:39,  1.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.42s/it][A100%|██████████| 1/1 [01:51<00:00, 111.42s/it]
 97%|█████████▋| 5049/5198 [26:46<03:38,  1.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.43s/it][A100%|██████████| 1/1 [01:51<00:00, 111.43s/it]
 97%|█████████▋| 5049/5198 [24:21<03:34,  1.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.45s/it][A100%|██████████| 1/1 [01:51<00:00, 111.45s/it]
 97%|█████████▋| 5049/5198 [21:33<03:29,  1.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4734
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.44s/it][A100%|██████████| 1/1 [01:48<00:00, 108.45s/it]
 97%|█████████▋| 5050/5198 [25:01<04:47,  1.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:02:25,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=5020, skipped=0, lr=[7.733314897157782e-07], mom=[(0.9, 0.999)]
steps: 5020 loss: 0.6398 iter time (s): 107.745 samples/sec: 1.188

100%|██████████| 1/1 [01:48<00:00, 108.58s/it][A100%|██████████| 1/1 [01:48<00:00, 108.58s/it]
 97%|█████████▋| 5050/5198 [28:35<04:53,  1.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.57s/it][A100%|██████████| 1/1 [01:48<00:00, 108.57s/it]
 97%|█████████▋| 5050/5198 [26:29<04:50,  1.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.50s/it][A100%|██████████| 1/1 [01:48<00:00, 108.50s/it]
 97%|█████████▋| 5050/5198 [25:54<04:49,  1.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.55s/it][A100%|██████████| 1/1 [01:48<00:00, 108.55s/it]
 97%|█████████▋| 5050/5198 [29:09<04:54,  1.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.54s/it][A100%|██████████| 1/1 [01:48<00:00, 108.54s/it]
 97%|█████████▋| 5050/5198 [28:34<04:53,  1.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.56s/it][A100%|██████████| 1/1 [01:48<00:00, 108.56s/it]
 97%|█████████▋| 5050/5198 [26:10<04:49,  1.96s/it]
100%|██████████| 1/1 [01:48<00:00, 108.52s/it][A100%|██████████| 1/1 [01:48<00:00, 108.52s/it]
 97%|█████████▋| 5050/5198 [23:21<04:44,  1.92s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4735

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.09s/it][A100%|██████████| 1/1 [01:26<00:00, 86.09s/it]
 97%|█████████▋| 5051/5198 [26:27<06:10,  2.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:03:51,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=5021, skipped=0, lr=[7.711517145736022e-07], mom=[(0.9, 0.999)]
steps: 5021 loss: 0.6592 iter time (s): 84.796 samples/sec: 1.510

100%|██████████| 1/1 [01:25<00:00, 85.50s/it][A100%|██████████| 1/1 [01:25<00:00, 85.50s/it]
 97%|█████████▋| 5051/5198 [30:00<06:16,  2.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.51s/it][A100%|██████████| 1/1 [01:25<00:00, 85.51s/it]
 97%|█████████▋| 5051/5198 [27:55<06:12,  2.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.57s/it][A100%|██████████| 1/1 [01:25<00:00, 85.57s/it]
 97%|█████████▋| 5051/5198 [27:20<06:11,  2.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.51s/it][A100%|██████████| 1/1 [01:25<00:00, 85.51s/it]
 97%|█████████▋| 5051/5198 [30:35<06:17,  2.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.53s/it][A100%|██████████| 1/1 [01:25<00:00, 85.53s/it]
 97%|█████████▋| 5051/5198 [30:00<06:16,  2.56s/it]
100%|██████████| 1/1 [01:25<00:00, 85.48s/it][A100%|██████████| 1/1 [01:25<00:00, 85.48s/it]
 97%|█████████▋| 5051/5198 [27:35<06:12,  2.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.49s/it][A100%|██████████| 1/1 [01:25<00:00, 85.49s/it]
 97%|█████████▋| 5051/5198 [24:47<06:07,  2.50s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4736
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.92s/it][A100%|██████████| 1/1 [01:29<00:00, 89.92s/it]
 97%|█████████▋| 5052/5198 [27:57<08:12,  3.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:05:21,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=5022, skipped=0, lr=[7.68974900595867e-07], mom=[(0.9, 0.999)]
steps: 5022 loss: 0.6029 iter time (s): 89.541 samples/sec: 1.430

100%|██████████| 1/1 [01:30<00:00, 90.23s/it][A100%|██████████| 1/1 [01:30<00:00, 90.23s/it]
 97%|█████████▋| 5052/5198 [31:31<08:17,  3.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.24s/it][A100%|██████████| 1/1 [01:30<00:00, 90.24s/it]
 97%|█████████▋| 5052/5198 [29:25<08:14,  3.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.33s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
 97%|█████████▋| 5052/5198 [28:50<08:13,  3.38s/it]
100%|██████████| 1/1 [01:30<00:00, 90.30s/it][A100%|██████████| 1/1 [01:30<00:00, 90.30s/it]
 97%|█████████▋| 5052/5198 [32:05<08:19,  3.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.26s/it][A100%|██████████| 1/1 [01:30<00:00, 90.26s/it]
 97%|█████████▋| 5052/5198 [31:30<08:18,  3.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.32s/it][A100%|██████████| 1/1 [01:30<00:00, 90.32s/it]
 97%|█████████▋| 5052/5198 [29:06<08:14,  3.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.32s/it][A100%|██████████| 1/1 [01:30<00:00, 90.32s/it]
 97%|█████████▋| 5052/5198 [26:17<08:09,  3.35s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4737
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.97s/it][A100%|██████████| 1/1 [01:35<00:00, 95.98s/it]
 97%|█████████▋| 5053/5198 [29:33<11:13,  4.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:06:57,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=5023, skipped=0, lr=[7.668010484809484e-07], mom=[(0.9, 0.999)]
steps: 5023 loss: 0.6137 iter time (s): 95.566 samples/sec: 1.339

100%|██████████| 1/1 [01:36<00:00, 96.37s/it][A100%|██████████| 1/1 [01:36<00:00, 96.37s/it]
 97%|█████████▋| 5053/5198 [33:07<11:19,  4.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.36s/it][A100%|██████████| 1/1 [01:36<00:00, 96.36s/it]
 97%|█████████▋| 5053/5198 [31:02<11:15,  4.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.31s/it][A100%|██████████| 1/1 [01:36<00:00, 96.31s/it]
 97%|█████████▋| 5053/5198 [30:27<11:14,  4.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.38s/it][A100%|██████████| 1/1 [01:36<00:00, 96.38s/it]
 97%|█████████▋| 5053/5198 [33:42<11:20,  4.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.36s/it][A100%|██████████| 1/1 [01:36<00:00, 96.36s/it]
 97%|█████████▋| 5053/5198 [33:06<11:19,  4.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.35s/it][A100%|██████████| 1/1 [01:36<00:00, 96.35s/it]
 97%|█████████▋| 5053/5198 [30:42<11:15,  4.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.35s/it][A100%|██████████| 1/1 [01:36<00:00, 96.35s/it]
 97%|█████████▋| 5053/5198 [27:53<11:10,  4.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4738
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.10s/it][A100%|██████████| 1/1 [01:45<00:00, 105.10s/it]
 97%|█████████▋| 5054/5198 [31:19<15:46,  6.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:08:43,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=5024, skipped=0, lr=[7.646301589262742e-07], mom=[(0.9, 0.999)]
steps: 5024 loss: 0.5635 iter time (s): 104.751 samples/sec: 1.222

100%|██████████| 1/1 [01:45<00:00, 105.53s/it][A100%|██████████| 1/1 [01:45<00:00, 105.53s/it]
 97%|█████████▋| 5054/5198 [34:53<15:52,  6.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.59s/it][A100%|██████████| 1/1 [01:45<00:00, 105.59s/it]
 97%|█████████▋| 5054/5198 [32:47<15:49,  6.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.53s/it][A100%|██████████| 1/1 [01:45<00:00, 105.53s/it]
 97%|█████████▋| 5054/5198 [32:12<15:48,  6.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.47s/it][A100%|██████████| 1/1 [01:45<00:00, 105.47s/it]
 97%|█████████▋| 5054/5198 [35:27<15:53,  6.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.53s/it][A100%|██████████| 1/1 [01:45<00:00, 105.53s/it]
 97%|█████████▋| 5054/5198 [34:52<15:52,  6.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.57s/it][A100%|██████████| 1/1 [01:45<00:00, 105.57s/it]
 97%|█████████▋| 5054/5198 [32:28<15:49,  6.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.57s/it][A100%|██████████| 1/1 [01:45<00:00, 105.57s/it]
 97%|█████████▋| 5054/5198 [29:39<15:44,  6.56s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4739
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.67s/it][A100%|██████████| 1/1 [01:48<00:00, 108.67s/it]
 97%|█████████▋| 5055/5198 [33:08<22:10,  9.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:10:32,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=5025, skipped=0, lr=[7.624622326283188e-07], mom=[(0.9, 0.999)]
steps: 5025 loss: 0.6131 iter time (s): 108.112 samples/sec: 1.184

100%|██████████| 1/1 [01:48<00:00, 108.92s/it][A100%|██████████| 1/1 [01:48<00:00, 108.92s/it]
 97%|█████████▋| 5055/5198 [36:42<22:16,  9.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.87s/it][A100%|██████████| 1/1 [01:48<00:00, 108.87s/it]
 97%|█████████▋| 5055/5198 [34:36<22:13,  9.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.91s/it][A100%|██████████| 1/1 [01:48<00:00, 108.91s/it]
 97%|█████████▋| 5055/5198 [34:01<22:12,  9.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.05s/it][A100%|██████████| 1/1 [01:49<00:00, 109.05s/it]
 97%|█████████▋| 5055/5198 [37:16<22:17,  9.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.98s/it][A100%|██████████| 1/1 [01:48<00:00, 108.98s/it]
 97%|█████████▋| 5055/5198 [36:41<22:16,  9.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.94s/it][A100%|██████████| 1/1 [01:48<00:00, 108.94s/it]
 97%|█████████▋| 5055/5198 [34:17<22:12,  9.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.92s/it][A100%|██████████| 1/1 [01:48<00:00, 108.92s/it]
 97%|█████████▋| 5055/5198 [31:28<22:08,  9.29s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_315
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.05s/it][A100%|██████████| 1/1 [02:05<00:00, 125.05s/it]
 97%|█████████▋| 5056/5198 [35:13<32:05, 13.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:12:37,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=5026, skipped=0, lr=[7.60297270282605e-07], mom=[(0.9, 0.999)]
steps: 5026 loss: 0.8054 iter time (s): 124.905 samples/sec: 1.025

100%|██████████| 1/1 [02:05<00:00, 125.88s/it][A100%|██████████| 1/1 [02:05<00:00, 125.88s/it]
 97%|█████████▋| 5056/5198 [38:47<32:14, 13.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.90s/it][A100%|██████████| 1/1 [02:05<00:00, 125.90s/it]
 97%|█████████▋| 5056/5198 [36:42<32:11, 13.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.87s/it][A100%|██████████| 1/1 [02:05<00:00, 125.87s/it]
 97%|█████████▋| 5056/5198 [36:07<32:10, 13.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.78s/it][A100%|██████████| 1/1 [02:05<00:00, 125.79s/it]
 97%|█████████▋| 5056/5198 [39:22<32:15, 13.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.84s/it][A100%|██████████| 1/1 [02:05<00:00, 125.84s/it]
 97%|█████████▋| 5056/5198 [38:47<32:14, 13.62s/it]
100%|██████████| 1/1 [02:05<00:00, 125.80s/it][A100%|██████████| 1/1 [02:05<00:00, 125.80s/it]
 97%|█████████▋| 5056/5198 [33:34<32:06, 13.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4740

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.81s/it][A100%|██████████| 1/1 [02:05<00:00, 125.81s/it]
 97%|█████████▋| 5056/5198 [36:22<32:10, 13.60s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.74s/it][A100%|██████████| 1/1 [01:55<00:00, 115.75s/it]
 97%|█████████▋| 5057/5198 [37:09<43:50, 18.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:14:33,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=5027, skipped=0, lr=[7.581352725837111e-07], mom=[(0.9, 0.999)]
steps: 5027 loss: 0.6160 iter time (s): 114.766 samples/sec: 1.115

100%|██████████| 1/1 [01:55<00:00, 115.50s/it][A100%|██████████| 1/1 [01:55<00:00, 115.50s/it]
 97%|█████████▋| 5057/5198 [40:43<43:56, 18.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.48s/it][A100%|██████████| 1/1 [01:55<00:00, 115.48s/it]
 97%|█████████▋| 5057/5198 [38:37<43:53, 18.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.47s/it][A100%|██████████| 1/1 [01:55<00:00, 115.48s/it]
 97%|█████████▋| 5057/5198 [38:02<43:52, 18.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.48s/it][A100%|██████████| 1/1 [01:55<00:00, 115.48s/it]
 97%|█████████▋| 5057/5198 [41:18<43:57, 18.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.47s/it][A100%|██████████| 1/1 [01:55<00:00, 115.47s/it]
 97%|█████████▋| 5057/5198 [40:42<43:56, 18.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.50s/it][A100%|██████████| 1/1 [01:55<00:00, 115.50s/it]
 97%|█████████▋| 5057/5198 [38:18<43:52, 18.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.52s/it][A100%|██████████| 1/1 [01:55<00:00, 115.52s/it]
 97%|█████████▋| 5057/5198 [35:29<43:48, 18.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4741
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.77s/it][A100%|██████████| 1/1 [01:30<00:00, 90.78s/it]
 97%|█████████▋| 5058/5198 [38:40<54:44, 23.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:16:03,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=5028, skipped=0, lr=[7.55976240225254e-07], mom=[(0.9, 0.999)]
steps: 5028 loss: 0.6315 iter time (s): 88.887 samples/sec: 1.440

100%|██████████| 1/1 [01:29<00:00, 89.67s/it][A100%|██████████| 1/1 [01:29<00:00, 89.67s/it]
 97%|█████████▋| 5058/5198 [42:13<54:37, 23.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.71s/it][A100%|██████████| 1/1 [01:29<00:00, 89.71s/it]
 97%|█████████▋| 5058/5198 [40:07<54:35, 23.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.68s/it][A100%|██████████| 1/1 [01:29<00:00, 89.68s/it]
 97%|█████████▋| 5058/5198 [39:32<54:34, 23.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.69s/it][A100%|██████████| 1/1 [01:29<00:00, 89.69s/it]
 97%|█████████▋| 5058/5198 [42:47<54:38, 23.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.67s/it][A100%|██████████| 1/1 [01:29<00:00, 89.67s/it]
 97%|█████████▋| 5058/5198 [42:12<54:37, 23.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.68s/it][A100%|██████████| 1/1 [01:29<00:00, 89.69s/it]
 97%|█████████▋| 5058/5198 [39:48<54:34, 23.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.69s/it][A100%|██████████| 1/1 [01:29<00:00, 89.69s/it]
 97%|█████████▋| 5058/5198 [36:59<54:31, 23.37s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4742
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.96s/it][A100%|██████████| 1/1 [02:00<00:00, 120.96s/it]
 97%|█████████▋| 5059/5198 [40:41<1:13:57, 31.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:18:05,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=5029, skipped=0, lr=[7.538201738999078e-07], mom=[(0.9, 0.999)]
steps: 5029 loss: 0.6085 iter time (s): 121.788 samples/sec: 1.051

100%|██████████| 1/1 [02:02<00:00, 122.55s/it][A100%|██████████| 1/1 [02:02<00:00, 122.55s/it]
 97%|█████████▋| 5059/5198 [44:15<1:14:09, 32.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.54s/it][A100%|██████████| 1/1 [02:02<00:00, 122.54s/it]
 97%|█████████▋| 5059/5198 [42:10<1:14:06, 31.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.59s/it][A100%|██████████| 1/1 [02:02<00:00, 122.59s/it]
 97%|█████████▋| 5059/5198 [41:35<1:14:06, 31.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.57s/it][A100%|██████████| 1/1 [02:02<00:00, 122.57s/it]
 97%|█████████▋| 5059/5198 [44:50<1:14:10, 32.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.60s/it][A100%|██████████| 1/1 [02:02<00:00, 122.60s/it]
 97%|█████████▋| 5059/5198 [44:14<1:14:09, 32.01s/it]
100%|██████████| 1/1 [02:02<00:00, 122.55s/it][A100%|██████████| 1/1 [02:02<00:00, 122.55s/it]
 97%|█████████▋| 5059/5198 [41:50<1:14:06, 31.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.57s/it][A100%|██████████| 1/1 [02:02<00:00, 122.57s/it]
 97%|█████████▋| 5059/5198 [39:02<1:14:03, 31.97s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4743
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 102.00s/it][A100%|██████████| 1/1 [01:41<00:00, 102.00s/it]
 97%|█████████▋| 5060/5198 [42:23<1:31:13, 39.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:19:47,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=5030, skipped=0, lr=[7.516670742993913e-07], mom=[(0.9, 0.999)]
steps: 5030 loss: 0.6220 iter time (s): 100.775 samples/sec: 1.270

100%|██████████| 1/1 [01:41<00:00, 101.59s/it][A100%|██████████| 1/1 [01:41<00:00, 101.59s/it]
 97%|█████████▋| 5060/5198 [45:57<1:31:15, 39.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.57s/it][A100%|██████████| 1/1 [01:41<00:00, 101.57s/it]
 97%|█████████▋| 5060/5198 [43:51<1:31:12, 39.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.54s/it][A100%|██████████| 1/1 [01:41<00:00, 101.55s/it]
 97%|█████████▋| 5060/5198 [43:16<1:31:12, 39.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.55s/it][A100%|██████████| 1/1 [01:41<00:00, 101.55s/it]
 97%|█████████▋| 5060/5198 [46:31<1:31:15, 39.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.59s/it][A100%|██████████| 1/1 [01:41<00:00, 101.59s/it]
 97%|█████████▋| 5060/5198 [45:56<1:31:15, 39.68s/it]
100%|██████████| 1/1 [01:41<00:00, 101.59s/it][A100%|██████████| 1/1 [01:41<00:00, 101.59s/it]
 97%|█████████▋| 5060/5198 [43:32<1:31:12, 39.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.58s/it][A100%|██████████| 1/1 [01:41<00:00, 101.58s/it]
 97%|█████████▋| 5060/5198 [40:43<1:31:09, 39.64s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4744
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.19s/it][A100%|██████████| 1/1 [02:06<00:00, 126.19s/it]
 97%|█████████▋| 5061/5198 [44:29<1:57:30, 51.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:21:54,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=5031, skipped=0, lr=[7.495169421144717e-07], mom=[(0.9, 0.999)]
steps: 5031 loss: 0.6222 iter time (s): 126.369 samples/sec: 1.013

100%|██████████| 1/1 [02:07<00:00, 127.13s/it][A100%|██████████| 1/1 [02:07<00:00, 127.13s/it]
 97%|█████████▋| 5061/5198 [48:04<1:57:45, 51.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.17s/it][A100%|██████████| 1/1 [02:07<00:00, 127.17s/it]
 97%|█████████▋| 5061/5198 [45:58<1:57:43, 51.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.17s/it][A100%|██████████| 1/1 [02:07<00:00, 127.17s/it]
 97%|█████████▋| 5061/5198 [45:24<1:57:43, 51.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.12s/it][A100%|██████████| 1/1 [02:07<00:00, 127.12s/it]
 97%|█████████▋| 5061/5198 [48:38<1:57:45, 51.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.07s/it][A100%|██████████| 1/1 [02:07<00:00, 127.07s/it]
 97%|█████████▋| 5061/5198 [48:03<1:57:44, 51.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.11s/it][A100%|██████████| 1/1 [02:07<00:00, 127.11s/it]
 97%|█████████▋| 5061/5198 [45:39<1:57:42, 51.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.10s/it][A100%|██████████| 1/1 [02:07<00:00, 127.10s/it]
 97%|█████████▋| 5061/5198 [42:50<1:57:39, 51.53s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4745
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.57s/it][A100%|██████████| 1/1 [01:32<00:00, 92.57s/it]
 97%|█████████▋| 5062/5198 [46:02<2:11:51, 58.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:23:26,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=5032, skipped=0, lr=[7.47369778034967e-07], mom=[(0.9, 0.999)]
steps: 5032 loss: 0.5892 iter time (s): 90.912 samples/sec: 1.408

100%|██████████| 1/1 [01:31<00:00, 91.74s/it][A100%|██████████| 1/1 [01:31<00:00, 91.74s/it]
 97%|█████████▋| 5062/5198 [49:36<2:11:42, 58.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.69s/it][A100%|██████████| 1/1 [01:31<00:00, 91.69s/it]
 97%|█████████▋| 5062/5198 [47:30<2:11:40, 58.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.69s/it][A100%|██████████| 1/1 [01:31<00:00, 91.69s/it]
 97%|█████████▋| 5062/5198 [46:55<2:11:39, 58.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.71s/it][A100%|██████████| 1/1 [01:31<00:00, 91.71s/it]
 97%|█████████▋| 5062/5198 [50:10<2:11:41, 58.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.75s/it][A100%|██████████| 1/1 [01:31<00:00, 91.75s/it]
 97%|█████████▋| 5062/5198 [49:35<2:11:42, 58.10s/it]
100%|██████████| 1/1 [01:31<00:00, 91.72s/it][A100%|██████████| 1/1 [01:31<00:00, 91.72s/it]
 97%|█████████▋| 5062/5198 [47:11<2:11:39, 58.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.73s/it][A100%|██████████| 1/1 [01:31<00:00, 91.73s/it]
 97%|█████████▋| 5062/5198 [44:22<2:11:37, 58.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4746
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.11s/it][A100%|██████████| 1/1 [01:31<00:00, 91.11s/it]
 97%|█████████▋| 5063/5198 [47:33<2:24:55, 64.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:24:57,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=5033, skipped=0, lr=[7.452255827497405e-07], mom=[(0.9, 0.999)]
steps: 5033 loss: 0.6331 iter time (s): 90.479 samples/sec: 1.415

100%|██████████| 1/1 [01:31<00:00, 91.18s/it][A100%|██████████| 1/1 [01:31<00:00, 91.19s/it]
 97%|█████████▋| 5063/5198 [51:07<2:24:46, 64.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.21s/it][A100%|██████████| 1/1 [01:31<00:00, 91.21s/it]
 97%|█████████▋| 5063/5198 [49:01<2:24:45, 64.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.22s/it][A100%|██████████| 1/1 [01:31<00:00, 91.22s/it]
 97%|█████████▋| 5063/5198 [48:26<2:24:45, 64.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.24s/it][A100%|██████████| 1/1 [01:31<00:00, 91.24s/it]
 97%|█████████▋| 5063/5198 [51:41<2:24:47, 64.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.18s/it][A100%|██████████| 1/1 [01:31<00:00, 91.18s/it]
 97%|█████████▋| 5063/5198 [51:06<2:24:46, 64.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.23s/it][A100%|██████████| 1/1 [01:31<00:00, 91.23s/it]
 97%|█████████▋| 5063/5198 [48:42<2:24:45, 64.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.24s/it][A100%|██████████| 1/1 [01:31<00:00, 91.24s/it]
 97%|█████████▋| 5063/5198 [45:53<2:24:44, 64.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4747
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.96s/it][A100%|██████████| 1/1 [01:29<00:00, 89.96s/it]
 97%|█████████▋| 5064/5198 [49:04<2:36:02, 69.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:26:27,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=5034, skipped=0, lr=[7.430843569467008e-07], mom=[(0.9, 0.999)]
steps: 5034 loss: 0.5593 iter time (s): 89.294 samples/sec: 1.433

100%|██████████| 1/1 [01:30<00:00, 90.05s/it][A100%|██████████| 1/1 [01:30<00:00, 90.05s/it]
 97%|█████████▋| 5064/5198 [52:37<2:35:53, 69.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.98s/it][A100%|██████████| 1/1 [01:29<00:00, 89.98s/it]
 97%|█████████▋| 5064/5198 [50:31<2:35:50, 69.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.01s/it][A100%|██████████| 1/1 [01:30<00:00, 90.01s/it]
 97%|█████████▋| 5064/5198 [49:56<2:35:51, 69.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.05s/it][A100%|██████████| 1/1 [01:30<00:00, 90.05s/it]
 97%|█████████▋| 5064/5198 [53:12<2:35:54, 69.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.06s/it][A100%|██████████| 1/1 [01:30<00:00, 90.06s/it]
 97%|█████████▋| 5064/5198 [52:36<2:35:53, 69.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.04s/it][A100%|██████████| 1/1 [01:30<00:00, 90.04s/it]
 97%|█████████▋| 5064/5198 [50:12<2:35:52, 69.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.02s/it][A100%|██████████| 1/1 [01:30<00:00, 90.02s/it]
 97%|█████████▋| 5064/5198 [47:23<2:35:50, 69.78s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4748
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.68s/it][A100%|██████████| 1/1 [01:40<00:00, 100.68s/it]
 97%|█████████▋| 5065/5198 [50:44<2:50:50, 77.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:28:08,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=5035, skipped=0, lr=[7.409461013128078e-07], mom=[(0.9, 0.999)]
steps: 5035 loss: 0.6108 iter time (s): 100.360 samples/sec: 1.275

100%|██████████| 1/1 [01:41<00:00, 101.13s/it][A100%|██████████| 1/1 [01:41<00:00, 101.13s/it]
 97%|█████████▋| 5065/5198 [54:18<2:50:53, 77.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.18s/it][A100%|██████████| 1/1 [01:41<00:00, 101.18s/it]
 97%|█████████▋| 5065/5198 [52:13<2:50:52, 77.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.13s/it][A100%|██████████| 1/1 [01:41<00:00, 101.13s/it]
 97%|█████████▋| 5065/5198 [51:38<2:50:51, 77.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.08s/it][A100%|██████████| 1/1 [01:41<00:00, 101.08s/it]
 97%|█████████▋| 5065/5198 [54:53<2:50:52, 77.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.13s/it][A100%|██████████| 1/1 [01:41<00:00, 101.13s/it]
 97%|█████████▋| 5065/5198 [54:17<2:50:53, 77.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.13s/it][A100%|██████████| 1/1 [01:41<00:00, 101.13s/it]
 97%|█████████▋| 5065/5198 [51:53<2:50:52, 77.09s/it]
100%|██████████| 1/1 [01:41<00:00, 101.11s/it][A100%|██████████| 1/1 [01:41<00:00, 101.11s/it]
 97%|█████████▋| 5065/5198 [49:04<2:50:50, 77.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4749

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.67s/it][A100%|██████████| 1/1 [01:56<00:00, 116.67s/it]
 97%|█████████▋| 5066/5198 [52:41<3:11:25, 87.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:30:05,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=5036, skipped=0, lr=[7.388108165340669e-07], mom=[(0.9, 0.999)]
steps: 5036 loss: 0.6346 iter time (s): 116.624 samples/sec: 1.098

100%|██████████| 1/1 [01:57<00:00, 117.29s/it][A100%|██████████| 1/1 [01:57<00:00, 117.29s/it]
 97%|█████████▋| 5066/5198 [56:15<3:11:40, 87.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.39s/it][A100%|██████████| 1/1 [01:57<00:00, 117.39s/it]
 97%|█████████▋| 5066/5198 [54:10<3:11:43, 87.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.42s/it][A100%|██████████| 1/1 [01:57<00:00, 117.42s/it]
 97%|█████████▋| 5066/5198 [53:35<3:11:43, 87.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.39s/it][A100%|██████████| 1/1 [01:57<00:00, 117.39s/it]
 97%|█████████▋| 5066/5198 [56:50<3:11:43, 87.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.38s/it][A100%|██████████| 1/1 [01:57<00:00, 117.38s/it]
 97%|█████████▋| 5066/5198 [56:15<3:11:43, 87.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.37s/it][A100%|██████████| 1/1 [01:57<00:00, 117.37s/it]
 97%|█████████▋| 5066/5198 [53:50<3:11:42, 87.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.40s/it][A100%|██████████| 1/1 [01:57<00:00, 117.40s/it]
 97%|█████████▋| 5066/5198 [51:02<3:11:41, 87.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4750
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.75s/it][A100%|██████████| 1/1 [01:48<00:00, 108.75s/it]
 97%|█████████▋| 5067/5198 [54:30<3:22:31, 92.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:31:54,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=5037, skipped=0, lr=[7.366785032955307e-07], mom=[(0.9, 0.999)]
steps: 5037 loss: 0.6252 iter time (s): 107.887 samples/sec: 1.186

100%|██████████| 1/1 [01:48<00:00, 108.83s/it][A100%|██████████| 1/1 [01:48<00:00, 108.83s/it]
 97%|█████████▋| 5067/5198 [58:04<3:22:40, 92.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.70s/it][A100%|██████████| 1/1 [01:48<00:00, 108.71s/it]
 97%|█████████▋| 5067/5198 [55:59<3:22:38, 92.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.65s/it][A100%|██████████| 1/1 [01:48<00:00, 108.65s/it]
 97%|█████████▋| 5067/5198 [55:24<3:22:36, 92.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.73s/it][A100%|██████████| 1/1 [01:48<00:00, 108.73s/it]
 97%|█████████▋| 5067/5198 [58:39<3:22:38, 92.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.68s/it][A100%|██████████| 1/1 [01:48<00:00, 108.68s/it]
 97%|█████████▋| 5067/5198 [58:03<3:22:37, 92.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.70s/it][A100%|██████████| 1/1 [01:48<00:00, 108.70s/it]
 97%|█████████▋| 5067/5198 [55:39<3:22:37, 92.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.70s/it][A100%|██████████| 1/1 [01:48<00:00, 108.70s/it]
 97%|█████████▋| 5067/5198 [52:51<3:22:37, 92.80s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4751
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.39s/it][A100%|██████████| 1/1 [01:43<00:00, 103.39s/it]
 97%|█████████▋| 5068/5198 [56:14<3:27:21, 95.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:33:38,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=5038, skipped=0, lr=[7.34549162281299e-07], mom=[(0.9, 0.999)]
steps: 5038 loss: 0.5579 iter time (s): 102.585 samples/sec: 1.248

100%|██████████| 1/1 [01:43<00:00, 103.29s/it][A100%|██████████| 1/1 [01:43<00:00, 103.29s/it]
 97%|█████████▋| 5068/5198 [59:48<3:27:19, 95.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.31s/it][A100%|██████████| 1/1 [01:43<00:00, 103.31s/it]
 97%|█████████▋| 5068/5198 [57:42<3:27:18, 95.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.42s/it][A100%|██████████| 1/1 [01:43<00:00, 103.42s/it]
 97%|█████████▋| 5068/5198 [57:07<3:27:20, 95.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.33s/it][A100%|██████████| 1/1 [01:43<00:00, 103.33s/it]
 97%|█████████▋| 5068/5198 [1:00:22<3:27:19, 95.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [01:43<00:00, 103.34s/it][A100%|██████████| 1/1 [01:43<00:00, 103.35s/it]
100%|██████████| 1/1 [01:43<00:00, 103.39s/it][A 97%|█████████▋| 5068/5198 [57:22<3:27:18, 95.68s/it]100%|██████████| 1/1 [01:43<00:00, 103.39s/it]
 97%|█████████▋| 5068/5198 [59:47<3:27:20, 95.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.35s/it][A100%|██████████| 1/1 [01:43<00:00, 103.35s/it]
 97%|█████████▋| 5068/5198 [54:34<3:27:18, 95.68s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4752
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.80s/it][A100%|██████████| 1/1 [01:31<00:00, 91.80s/it]
 98%|█████████▊| 5069/5198 [57:46<3:23:31, 94.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:35:09,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=5039, skipped=0, lr=[7.324227941745146e-07], mom=[(0.9, 0.999)]
steps: 5039 loss: 0.5678 iter time (s): 90.897 samples/sec: 1.408

100%|██████████| 1/1 [01:31<00:00, 91.66s/it][A100%|██████████| 1/1 [01:31<00:00, 91.66s/it]
 98%|█████████▊| 5069/5198 [59:14<3:23:17, 94.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.75s/it][A100%|██████████| 1/1 [01:31<00:00, 91.75s/it]
 98%|█████████▊| 5069/5198 [1:01:19<3:23:21, 94.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.68s/it][A100%|██████████| 1/1 [01:31<00:00, 91.68s/it]
 98%|█████████▊| 5069/5198 [58:39<3:23:19, 94.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.69s/it][A100%|██████████| 1/1 [01:31<00:00, 91.69s/it]
 98%|█████████▊| 5069/5198 [1:01:54<3:23:19, 94.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.67s/it][A100%|██████████| 1/1 [01:31<00:00, 91.67s/it]
 98%|█████████▊| 5069/5198 [1:01:18<3:23:19, 94.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.73s/it][A100%|██████████| 1/1 [01:31<00:00, 91.73s/it]
 98%|█████████▊| 5069/5198 [58:54<3:23:20, 94.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.71s/it][A100%|██████████| 1/1 [01:31<00:00, 91.71s/it]
 98%|█████████▊| 5069/5198 [56:06<3:23:19, 94.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4753
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.37s/it][A100%|██████████| 1/1 [01:54<00:00, 114.37s/it]
[2024-06-26 18:37:04,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=5040, skipped=0, lr=[7.302993996573716e-07], mom=[(0.9, 0.999)]
steps: 5040 loss: 0.5946 iter time (s): 113.797 samples/sec: 1.125

100%|██████████| 1/1 [01:54<00:00, 114.51s/it][A100%|██████████| 1/1 [01:54<00:00, 114.51s/it]

100%|██████████| 1/1 [01:54<00:00, 114.58s/it][A100%|██████████| 1/1 [01:54<00:00, 114.58s/it]

100%|██████████| 1/1 [01:54<00:00, 114.53s/it][A100%|██████████| 1/1 [01:54<00:00, 114.53s/it]

100%|██████████| 1/1 [01:54<00:00, 114.56s/it][A100%|██████████| 1/1 [01:54<00:00, 114.56s/it]

100%|██████████| 1/1 [01:54<00:00, 114.60s/it][A100%|██████████| 1/1 [01:54<00:00, 114.60s/it]

100%|██████████| 1/1 [01:54<00:00, 114.57s/it][A100%|██████████| 1/1 [01:54<00:00, 114.57s/it]

100%|██████████| 1/1 [01:54<00:00, 114.57s/it][A100%|██████████| 1/1 [01:54<00:00, 114.57s/it]
Checkpointing at shard 5069
[2024-06-26 18:37:05,126] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5040 is about to be saved!
[2024-06-26 18:37:05,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_00-model_states.pt...
[2024-06-26 18:37:09,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_02-model_states.pt...
[2024-06-26 18:37:10,140] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_08-model_states.pt...
[2024-06-26 18:37:10,348] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_07-model_states.pt...
[2024-06-26 18:37:15,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_00-model_states.pt.
[2024-06-26 18:37:16,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_06-model_states.pt...
[2024-06-26 18:37:17,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_05-model_states.pt...
[2024-06-26 18:37:22,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_03-model_states.pt...
[2024-06-26 18:37:22,798] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_04-model_states.pt...
[2024-06-26 18:37:23,369] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_01-model_states.pt...
[2024-06-26 18:42:14,249] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_06-model_states.pt.
[2024-06-26 18:42:14,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_05_model_states.pt...
[2024-06-26 18:42:14,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_05_model_states.pt.
[2024-06-26 18:42:14,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
[2024-06-26 18:42:21,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_05-model_states.pt.
[2024-06-26 18:42:21,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_07-model_states.pt.
[2024-06-26 18:42:21,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_08-model_states.pt.
[2024-06-26 18:42:21,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_02-model_states.pt.
[2024-06-26 18:42:21,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_03-model_states.pt.
[2024-06-26 18:42:21,238] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_01-model_states.pt.
[2024-06-26 18:42:21,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_04-model_states.pt.
[2024-06-26 18:42:21,272] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_01_model_states.pt
[2024-06-26 18:42:21,272] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_01_model_states.pt...
[2024-06-26 18:42:21,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_06_model_states.pt...
[2024-06-26 18:42:21,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_09-model_states.pt...
[2024-06-26 18:42:21,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_06_model_states.pt.
[2024-06-26 18:42:21,365] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
[2024-06-26 18:42:21,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_01_model_states.pt.
[2024-06-26 18:42:21,370] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
[2024-06-26 18:42:21,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_02_model_states.pt...
[2024-06-26 18:42:21,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_04_model_states.pt...
[2024-06-26 18:42:21,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_04_model_states.pt.
[2024-06-26 18:42:21,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
[2024-06-26 18:42:21,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_02_model_states.pt.
[2024-06-26 18:42:21,856] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
[2024-06-26 18:42:21,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_03_model_states.pt...
[2024-06-26 18:42:21,975] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_03_model_states.pt.
[2024-06-26 18:42:21,976] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
[2024-06-26 18:42:22,077] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_00_model_states.pt
[2024-06-26 18:42:22,077] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_00_model_states.pt...
[2024-06-26 18:42:22,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/layer_09-model_states.pt.
[2024-06-26 18:42:22,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_07_model_states.pt...
[2024-06-26 18:42:22,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_07_model_states.pt.
[2024-06-26 18:42:22,186] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
[2024-06-26 18:42:22,877] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5040/mp_rank_00_model_states.pt.
[2024-06-26 18:42:22,877] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5040 is ready now!
Checkpoint saved using --- 317.7550222873688 seconds ---
 98%|█████████▊| 5070/5198 [1:05:01<6:49:46, 192.08s/it] 98%|█████████▊| 5070/5198 [1:05:51<6:48:00, 191.25s/it] 98%|█████████▊| 5070/5198 [1:09:06<6:47:58, 191.24s/it] 98%|█████████▊| 5070/5198 [1:03:18<6:47:53, 191.20s/it] 98%|█████████▊| 5070/5198 [1:08:31<6:47:56, 191.22s/it] 98%|█████████▊| 5070/5198 [1:06:07<6:47:54, 191.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4754
 98%|█████████▊| 5070/5198 [1:08:32<6:48:08, 191.32s/it] 98%|█████████▊| 5070/5198 [1:06:26<6:48:05, 191.29s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.75s/it][A100%|██████████| 1/1 [01:21<00:00, 81.75s/it]
 98%|█████████▊| 5071/5198 [1:06:23<5:38:55, 160.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:43:46,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=5041, skipped=0, lr=[7.281789794111058e-07], mom=[(0.9, 0.999)]
steps: 5041 loss: 0.6222 iter time (s): 83.639 samples/sec: 1.530

100%|██████████| 1/1 [01:23<00:00, 83.97s/it][A100%|██████████| 1/1 [01:23<00:00, 83.97s/it]
 98%|█████████▊| 5071/5198 [1:09:56<5:39:08, 160.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.14s/it][A100%|██████████| 1/1 [01:24<00:00, 84.14s/it]
 98%|█████████▊| 5071/5198 [1:07:51<5:39:12, 160.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.28s/it][A100%|██████████| 1/1 [01:24<00:00, 84.28s/it]
 98%|█████████▊| 5071/5198 [1:07:16<5:39:14, 160.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.39s/it][A100%|██████████| 1/1 [01:24<00:00, 84.39s/it]
 98%|█████████▊| 5071/5198 [1:10:31<5:39:16, 160.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.43s/it][A100%|██████████| 1/1 [01:24<00:00, 84.43s/it]
 98%|█████████▊| 5071/5198 [1:09:55<5:39:16, 160.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.45s/it][A100%|██████████| 1/1 [01:24<00:00, 84.45s/it]
 98%|█████████▊| 5071/5198 [1:07:31<5:39:16, 160.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.48s/it][A100%|██████████| 1/1 [01:24<00:00, 84.48s/it]
 98%|█████████▊| 5071/5198 [1:04:43<5:39:16, 160.29s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_316
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.24s/it][A100%|██████████| 1/1 [02:10<00:00, 130.25s/it]
 98%|█████████▊| 5072/5198 [1:08:33<5:17:57, 151.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:45:58,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=5042, skipped=0, lr=[7.260615341160024e-07], mom=[(0.9, 0.999)]
steps: 5042 loss: 0.7867 iter time (s): 130.763 samples/sec: 0.979

100%|██████████| 1/1 [02:11<00:00, 131.68s/it][A100%|██████████| 1/1 [02:11<00:00, 131.68s/it]
 98%|█████████▊| 5072/5198 [1:12:08<5:18:55, 151.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.62s/it][A100%|██████████| 1/1 [02:11<00:00, 131.62s/it]
 98%|█████████▊| 5072/5198 [1:10:02<5:18:55, 151.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.55s/it][A100%|██████████| 1/1 [02:11<00:00, 131.55s/it]
 98%|█████████▊| 5072/5198 [1:09:27<5:18:54, 151.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.53s/it][A100%|██████████| 1/1 [02:11<00:00, 131.53s/it]
 98%|█████████▊| 5072/5198 [1:12:42<5:18:55, 151.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.53s/it][A100%|██████████| 1/1 [02:11<00:00, 131.53s/it]
 98%|█████████▊| 5072/5198 [1:12:07<5:18:55, 151.87s/it]
100%|██████████| 1/1 [02:11<00:00, 131.52s/it][A100%|██████████| 1/1 [02:11<00:00, 131.52s/it]
 98%|█████████▊| 5072/5198 [1:09:43<5:18:54, 151.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.53s/it][A100%|██████████| 1/1 [02:11<00:00, 131.53s/it]
 98%|█████████▊| 5072/5198 [1:06:54<5:18:54, 151.87s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4755
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:13<00:00, 133.97s/it][A100%|██████████| 1/1 [02:13<00:00, 133.97s/it]
 98%|█████████▊| 5073/5198 [1:10:48<5:04:58, 146.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:48:12,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=5043, skipped=0, lr=[7.239470644513891e-07], mom=[(0.9, 0.999)]
steps: 5043 loss: 0.5987 iter time (s): 133.764 samples/sec: 0.957

100%|██████████| 1/1 [02:14<00:00, 134.42s/it][A100%|██████████| 1/1 [02:14<00:00, 134.42s/it]
 98%|█████████▊| 5073/5198 [1:14:22<5:05:40, 146.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.41s/it][A100%|██████████| 1/1 [02:14<00:00, 134.41s/it]
 98%|█████████▊| 5073/5198 [1:12:17<5:05:40, 146.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.42s/it][A100%|██████████| 1/1 [02:14<00:00, 134.42s/it]
 98%|█████████▊| 5073/5198 [1:11:42<5:05:39, 146.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.43s/it][A100%|██████████| 1/1 [02:14<00:00, 134.43s/it]
 98%|█████████▊| 5073/5198 [1:14:57<5:05:40, 146.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.48s/it][A100%|██████████| 1/1 [02:14<00:00, 134.48s/it]
 98%|█████████▊| 5073/5198 [1:14:21<5:05:42, 146.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.50s/it][A100%|██████████| 1/1 [02:14<00:00, 134.50s/it]
 98%|█████████▊| 5073/5198 [1:11:57<5:05:42, 146.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:14<00:00, 134.49s/it][A100%|██████████| 1/1 [02:14<00:00, 134.49s/it]
 98%|█████████▊| 5073/5198 [1:09:09<5:05:42, 146.74s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4756
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.67s/it][A100%|██████████| 1/1 [01:37<00:00, 97.67s/it]
 98%|█████████▊| 5074/5198 [1:12:26<4:32:49, 132.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:49:49,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=5044, skipped=0, lr=[7.21835571095642e-07], mom=[(0.9, 0.999)]
steps: 5044 loss: 0.6080 iter time (s): 96.096 samples/sec: 1.332

100%|██████████| 1/1 [01:36<00:00, 96.79s/it][A100%|██████████| 1/1 [01:36<00:00, 96.79s/it]
 98%|█████████▊| 5074/5198 [1:15:59<4:32:38, 131.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.85s/it][A100%|██████████| 1/1 [01:36<00:00, 96.85s/it]
 98%|█████████▊| 5074/5198 [1:13:54<4:32:40, 131.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.94s/it][A100%|██████████| 1/1 [01:36<00:00, 96.94s/it]
 98%|█████████▊| 5074/5198 [1:13:19<4:32:43, 131.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.89s/it][A100%|██████████| 1/1 [01:36<00:00, 96.89s/it]
 98%|█████████▊| 5074/5198 [1:16:34<4:32:41, 131.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.81s/it][A100%|██████████| 1/1 [01:36<00:00, 96.81s/it]
 98%|█████████▊| 5074/5198 [1:15:58<4:32:40, 131.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.86s/it][A100%|██████████| 1/1 [01:36<00:00, 96.86s/it]
 98%|█████████▊| 5074/5198 [1:13:34<4:32:41, 131.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.85s/it][A100%|██████████| 1/1 [01:36<00:00, 96.85s/it]
 98%|█████████▊| 5074/5198 [1:10:45<4:32:41, 131.95s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4757
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.68s/it][A100%|██████████| 1/1 [01:23<00:00, 83.68s/it]
 98%|█████████▊| 5075/5198 [1:13:49<4:01:16, 117.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:51:13,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=5045, skipped=0, lr=[7.19727054726179e-07], mom=[(0.9, 0.999)]
steps: 5045 loss: 0.6101 iter time (s): 82.785 samples/sec: 1.546

100%|██████████| 1/1 [01:23<00:00, 83.55s/it][A100%|██████████| 1/1 [01:23<00:00, 83.55s/it]
 98%|█████████▊| 5075/5198 [1:17:23<4:00:56, 117.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.48s/it][A100%|██████████| 1/1 [01:23<00:00, 83.49s/it]
 98%|█████████▊| 5075/5198 [1:15:17<4:00:55, 117.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.52s/it][A100%|██████████| 1/1 [01:23<00:00, 83.52s/it]
 98%|█████████▊| 5075/5198 [1:14:42<4:00:58, 117.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.53s/it][A100%|██████████| 1/1 [01:23<00:00, 83.53s/it]
 98%|█████████▊| 5075/5198 [1:17:57<4:00:57, 117.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.60s/it][A100%|██████████| 1/1 [01:23<00:00, 83.60s/it]
 98%|█████████▊| 5075/5198 [1:17:22<4:00:59, 117.56s/it]
100%|██████████| 1/1 [01:23<00:00, 83.55s/it][A100%|██████████| 1/1 [01:23<00:00, 83.55s/it]
 98%|█████████▊| 5075/5198 [1:14:58<4:00:58, 117.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.55s/it][A100%|██████████| 1/1 [01:23<00:00, 83.55s/it]
 98%|█████████▊| 5075/5198 [1:12:09<4:00:58, 117.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4758
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.36s/it][A100%|██████████| 1/1 [01:40<00:00, 100.36s/it]
 98%|█████████▊| 5076/5198 [1:15:30<3:48:55, 112.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:52:54,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=5046, skipped=0, lr=[7.176215160194637e-07], mom=[(0.9, 0.999)]
steps: 5046 loss: 0.6418 iter time (s): 100.238 samples/sec: 1.277

100%|██████████| 1/1 [01:41<00:00, 101.12s/it][A100%|██████████| 1/1 [01:41<00:00, 101.12s/it]
 98%|█████████▊| 5076/5198 [1:19:04<3:49:01, 112.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.09s/it][A100%|██████████| 1/1 [01:41<00:00, 101.09s/it]
 98%|█████████▊| 5076/5198 [1:16:58<3:49:00, 112.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.07s/it][A100%|██████████| 1/1 [01:41<00:00, 101.07s/it]
 98%|█████████▊| 5076/5198 [1:16:23<3:49:01, 112.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.09s/it][A100%|██████████| 1/1 [01:41<00:00, 101.09s/it]
 98%|█████████▊| 5076/5198 [1:19:38<3:49:01, 112.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.05s/it][A100%|██████████| 1/1 [01:41<00:00, 101.05s/it]
 98%|█████████▊| 5076/5198 [1:16:39<3:49:00, 112.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.07s/it][A100%|██████████| 1/1 [01:41<00:00, 101.07s/it]
 98%|█████████▊| 5076/5198 [1:19:03<3:49:02, 112.64s/it]
100%|██████████| 1/1 [01:41<00:00, 101.05s/it][A100%|██████████| 1/1 [01:41<00:00, 101.05s/it]
 98%|█████████▊| 5076/5198 [1:13:50<3:49:00, 112.63s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4759

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.64s/it][A100%|██████████| 1/1 [01:27<00:00, 87.64s/it]
 98%|█████████▊| 5077/5198 [1:16:58<3:32:12, 105.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:54:21,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=5047, skipped=0, lr=[7.155189556510081e-07], mom=[(0.9, 0.999)]
steps: 5047 loss: 0.6534 iter time (s): 86.803 samples/sec: 1.475

100%|██████████| 1/1 [01:27<00:00, 87.58s/it][A100%|██████████| 1/1 [01:27<00:00, 87.58s/it]
 98%|█████████▊| 5077/5198 [1:20:31<3:32:03, 105.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.68s/it][A100%|██████████| 1/1 [01:27<00:00, 87.68s/it]
 98%|█████████▊| 5077/5198 [1:18:26<3:32:06, 105.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.59s/it][A100%|██████████| 1/1 [01:27<00:00, 87.59s/it]
 98%|█████████▊| 5077/5198 [1:17:51<3:32:04, 105.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.62s/it][A100%|██████████| 1/1 [01:27<00:00, 87.62s/it]
 98%|█████████▊| 5077/5198 [1:21:06<3:32:04, 105.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.62s/it][A100%|██████████| 1/1 [01:27<00:00, 87.62s/it]
 98%|█████████▊| 5077/5198 [1:20:31<3:32:05, 105.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.65s/it][A100%|██████████| 1/1 [01:27<00:00, 87.65s/it]
 98%|█████████▊| 5077/5198 [1:15:18<3:32:05, 105.17s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4760

100%|██████████| 1/1 [01:27<00:00, 87.67s/it][A100%|██████████| 1/1 [01:27<00:00, 87.67s/it]
 98%|█████████▊| 5077/5198 [1:18:06<3:32:06, 105.18s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.33s/it][A100%|██████████| 1/1 [01:55<00:00, 115.33s/it]
 98%|█████████▊| 5078/5198 [1:18:53<3:36:36, 108.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:56:18,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=5048, skipped=0, lr=[7.134193742953629e-07], mom=[(0.9, 0.999)]
steps: 5048 loss: 0.6654 iter time (s): 115.413 samples/sec: 1.109

100%|██████████| 1/1 [01:56<00:00, 116.18s/it][A100%|██████████| 1/1 [01:56<00:00, 116.18s/it]
 98%|█████████▊| 5078/5198 [1:22:28<3:36:54, 108.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.18s/it][A100%|██████████| 1/1 [01:56<00:00, 116.19s/it]
 98%|█████████▊| 5078/5198 [1:20:22<3:36:56, 108.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.20s/it][A100%|██████████| 1/1 [01:56<00:00, 116.20s/it]
 98%|█████████▊| 5078/5198 [1:19:47<3:36:55, 108.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.16s/it][A100%|██████████| 1/1 [01:56<00:00, 116.16s/it]
 98%|█████████▊| 5078/5198 [1:23:02<3:36:54, 108.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.18s/it][A100%|██████████| 1/1 [01:56<00:00, 116.18s/it]
 98%|█████████▊| 5078/5198 [1:22:27<3:36:55, 108.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.19s/it][A100%|██████████| 1/1 [01:56<00:00, 116.19s/it]
 98%|█████████▊| 5078/5198 [1:20:03<3:36:56, 108.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.22s/it][A100%|██████████| 1/1 [01:56<00:00, 116.22s/it]
 98%|█████████▊| 5078/5198 [1:17:14<3:36:57, 108.48s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4761
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.04s/it][A100%|██████████| 1/1 [01:31<00:00, 91.04s/it]
 98%|█████████▊| 5079/5198 [1:20:25<3:25:07, 103.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:57:49,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=5049, skipped=0, lr=[7.113227726261296e-07], mom=[(0.9, 0.999)]
steps: 5049 loss: 0.6168 iter time (s): 90.505 samples/sec: 1.414

100%|██████████| 1/1 [01:31<00:00, 91.35s/it][A100%|██████████| 1/1 [01:31<00:00, 91.35s/it]
 98%|█████████▊| 5079/5198 [1:23:59<3:24:57, 103.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.23s/it][A100%|██████████| 1/1 [01:31<00:00, 91.23s/it]
 98%|█████████▊| 5079/5198 [1:21:53<3:24:54, 103.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.32s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 98%|█████████▊| 5079/5198 [1:21:18<3:24:56, 103.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.33s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 98%|█████████▊| 5079/5198 [1:24:33<3:24:56, 103.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.33s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 98%|█████████▊| 5079/5198 [1:23:58<3:24:57, 103.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.33s/it][A100%|██████████| 1/1 [01:31<00:00, 91.33s/it]
 98%|█████████▊| 5079/5198 [1:21:34<3:24:57, 103.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.32s/it][A100%|██████████| 1/1 [01:31<00:00, 91.32s/it]
 98%|█████████▊| 5079/5198 [1:18:45<3:24:57, 103.34s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4762
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.88s/it][A100%|██████████| 1/1 [01:40<00:00, 100.88s/it]
 98%|█████████▊| 5080/5198 [1:22:07<3:22:01, 102.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 18:59:30,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=5050, skipped=0, lr=[7.092291513159451e-07], mom=[(0.9, 0.999)]
steps: 5050 loss: 0.6633 iter time (s): 100.569 samples/sec: 1.273

100%|██████████| 1/1 [01:41<00:00, 101.37s/it][A100%|██████████| 1/1 [01:41<00:00, 101.37s/it]
 98%|█████████▊| 5080/5198 [1:25:40<3:22:05, 102.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.50s/it][A100%|██████████| 1/1 [01:41<00:00, 101.50s/it]
 98%|█████████▊| 5080/5198 [1:23:35<3:22:07, 102.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.35s/it][A100%|██████████| 1/1 [01:41<00:00, 101.36s/it]
 98%|█████████▊| 5080/5198 [1:23:00<3:22:03, 102.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.46s/it][A100%|██████████| 1/1 [01:41<00:00, 101.46s/it]
 98%|█████████▊| 5080/5198 [1:26:15<3:22:07, 102.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.43s/it][A100%|██████████| 1/1 [01:41<00:00, 101.43s/it]
 98%|█████████▊| 5080/5198 [1:25:40<3:22:06, 102.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [01:41<00:00, 101.42s/it][A100%|██████████| 1/1 [01:41<00:00, 101.42s/it]
 98%|█████████▊| 5080/5198 [1:23:15<3:22:06, 102.77s/it]100%|██████████| 1/1 [01:41<00:00, 101.41s/it][A100%|██████████| 1/1 [01:41<00:00, 101.41s/it]
 98%|█████████▊| 5080/5198 [1:20:27<3:22:06, 102.77s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4763

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.56s/it][A100%|██████████| 1/1 [01:31<00:00, 91.56s/it]
 98%|█████████▊| 5081/5198 [1:23:38<3:13:55, 99.45s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:01:02,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=5051, skipped=0, lr=[7.071385110364981e-07], mom=[(0.9, 0.999)]
steps: 5051 loss: 0.6119 iter time (s): 90.674 samples/sec: 1.412

100%|██████████| 1/1 [01:31<00:00, 91.48s/it][A100%|██████████| 1/1 [01:31<00:00, 91.48s/it]
 98%|█████████▊| 5081/5198 [1:27:12<3:13:47, 99.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.41s/it][A100%|██████████| 1/1 [01:31<00:00, 91.41s/it]
 98%|█████████▊| 5081/5198 [1:25:06<3:13:46, 99.37s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.53s/it][A100%|██████████| 1/1 [01:31<00:00, 91.53s/it]
 98%|█████████▊| 5081/5198 [1:24:31<3:13:48, 99.39s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.45s/it][A100%|██████████| 1/1 [01:31<00:00, 91.45s/it]
 98%|█████████▊| 5081/5198 [1:27:46<3:13:48, 99.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.43s/it][A100%|██████████| 1/1 [01:31<00:00, 91.43s/it]
 98%|█████████▊| 5081/5198 [1:27:11<3:13:46, 99.37s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.42s/it][A100%|██████████| 1/1 [01:31<00:00, 91.42s/it]
 98%|█████████▊| 5081/5198 [1:24:47<3:13:46, 99.37s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.44s/it][A100%|██████████| 1/1 [01:31<00:00, 91.44s/it]
 98%|█████████▊| 5081/5198 [1:21:58<3:13:46, 99.37s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4764
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.28s/it][A100%|██████████| 1/1 [01:32<00:00, 92.28s/it]
 98%|█████████▊| 5082/5198 [1:25:11<3:08:14, 97.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:02:34,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=5052, skipped=0, lr=[7.050508524585173e-07], mom=[(0.9, 0.999)]
steps: 5052 loss: 0.6411 iter time (s): 91.789 samples/sec: 1.394

100%|██████████| 1/1 [01:32<00:00, 92.60s/it][A100%|██████████| 1/1 [01:32<00:00, 92.60s/it]
 98%|█████████▊| 5082/5198 [1:28:44<3:08:12, 97.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.58s/it][A100%|██████████| 1/1 [01:32<00:00, 92.58s/it]
 98%|█████████▊| 5082/5198 [1:26:39<3:08:10, 97.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.51s/it][A100%|██████████| 1/1 [01:32<00:00, 92.51s/it]
 98%|█████████▊| 5082/5198 [1:26:04<3:08:10, 97.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.46s/it][A100%|██████████| 1/1 [01:32<00:00, 92.46s/it]
 98%|█████████▊| 5082/5198 [1:29:19<3:08:08, 97.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.53s/it][A100%|██████████| 1/1 [01:32<00:00, 92.53s/it]
 98%|█████████▊| 5082/5198 [1:28:44<3:08:09, 97.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.51s/it][A100%|██████████| 1/1 [01:32<00:00, 92.51s/it]
 98%|█████████▊| 5082/5198 [1:26:19<3:08:09, 97.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.54s/it][A100%|██████████| 1/1 [01:32<00:00, 92.54s/it]
 98%|█████████▊| 5082/5198 [1:23:31<3:08:09, 97.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4765
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.04s/it][A100%|██████████| 1/1 [01:32<00:00, 92.04s/it]
 98%|█████████▊| 5083/5198 [1:26:43<3:03:44, 95.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:04:07,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=5053, skipped=0, lr=[7.029661762517764e-07], mom=[(0.9, 0.999)]
steps: 5053 loss: 0.5759 iter time (s): 91.554 samples/sec: 1.398

100%|██████████| 1/1 [01:32<00:00, 92.32s/it][A100%|██████████| 1/1 [01:32<00:00, 92.32s/it]
 98%|█████████▊| 5083/5198 [1:30:17<3:03:42, 95.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.33s/it][A100%|██████████| 1/1 [01:32<00:00, 92.33s/it]
 98%|█████████▊| 5083/5198 [1:28:11<3:03:41, 95.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.39s/it][A100%|██████████| 1/1 [01:32<00:00, 92.39s/it]
 98%|█████████▊| 5083/5198 [1:27:36<3:03:42, 95.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.48s/it][A100%|██████████| 1/1 [01:32<00:00, 92.48s/it]
 98%|█████████▊| 5083/5198 [1:30:51<3:03:44, 95.87s/it]
100%|██████████| 1/1 [01:32<00:00, 92.38s/it][A100%|██████████| 1/1 [01:32<00:00, 92.38s/it]
 98%|█████████▊| 5083/5198 [1:30:16<3:03:42, 95.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.42s/it][A100%|██████████| 1/1 [01:32<00:00, 92.42s/it]
 98%|█████████▊| 5083/5198 [1:27:52<3:03:42, 95.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.40s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 98%|█████████▊| 5083/5198 [1:25:03<3:03:42, 95.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4766
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.88s/it][A100%|██████████| 1/1 [01:37<00:00, 97.88s/it]
 98%|█████████▊| 5084/5198 [1:28:21<3:03:25, 96.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:05:45,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=5054, skipped=0, lr=[7.008844830850892e-07], mom=[(0.9, 0.999)]
steps: 5054 loss: 0.6315 iter time (s): 97.421 samples/sec: 1.314

100%|██████████| 1/1 [01:38<00:00, 98.19s/it][A100%|██████████| 1/1 [01:38<00:00, 98.19s/it]
 98%|█████████▊| 5084/5198 [1:31:55<3:03:27, 96.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.27s/it][A100%|██████████| 1/1 [01:38<00:00, 98.27s/it]
 98%|█████████▊| 5084/5198 [1:29:50<3:03:28, 96.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.15s/it][A100%|██████████| 1/1 [01:38<00:00, 98.15s/it]
 98%|█████████▊| 5084/5198 [1:29:14<3:03:25, 96.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.12s/it][A100%|██████████| 1/1 [01:38<00:00, 98.12s/it]
 98%|█████████▊| 5084/5198 [1:32:30<3:03:25, 96.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.22s/it][A100%|██████████| 1/1 [01:38<00:00, 98.23s/it]
 98%|█████████▊| 5084/5198 [1:31:54<3:03:27, 96.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.19s/it][A100%|██████████| 1/1 [01:38<00:00, 98.19s/it]
 98%|█████████▊| 5084/5198 [1:29:30<3:03:27, 96.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.19s/it][A100%|██████████| 1/1 [01:38<00:00, 98.19s/it]
 98%|█████████▊| 5084/5198 [1:26:41<3:03:27, 96.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4767
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.14s/it][A100%|██████████| 1/1 [01:27<00:00, 87.14s/it]
 98%|█████████▊| 5085/5198 [1:29:49<2:56:37, 93.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:07:12,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=5055, skipped=0, lr=[6.988057736263166e-07], mom=[(0.9, 0.999)]
steps: 5055 loss: 0.5773 iter time (s): 86.282 samples/sec: 1.484

100%|██████████| 1/1 [01:27<00:00, 87.07s/it][A100%|██████████| 1/1 [01:27<00:00, 87.07s/it]
 98%|█████████▊| 5085/5198 [1:33:22<2:56:29, 93.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.97s/it][A100%|██████████| 1/1 [01:26<00:00, 86.97s/it]
 98%|█████████▊| 5085/5198 [1:31:16<2:56:27, 93.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.12s/it][A100%|██████████| 1/1 [01:27<00:00, 87.12s/it]
 98%|█████████▊| 5085/5198 [1:30:42<2:56:30, 93.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.15s/it][A100%|██████████| 1/1 [01:27<00:00, 87.15s/it]
 98%|█████████▊| 5085/5198 [1:33:57<2:56:31, 93.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.08s/it][A100%|██████████| 1/1 [01:27<00:00, 87.08s/it]
 98%|█████████▊| 5085/5198 [1:33:21<2:56:30, 93.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.11s/it][A100%|██████████| 1/1 [01:27<00:00, 87.11s/it]
 98%|█████████▊| 5085/5198 [1:30:57<2:56:30, 93.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.11s/it][A100%|██████████| 1/1 [01:27<00:00, 87.11s/it]
 98%|█████████▊| 5085/5198 [1:28:08<2:56:30, 93.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4768
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.44s/it][A100%|██████████| 1/1 [01:45<00:00, 105.45s/it]
 98%|█████████▊| 5086/5198 [1:31:34<3:01:42, 97.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:08:58,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=5056, skipped=0, lr=[6.967300485423587e-07], mom=[(0.9, 0.999)]
steps: 5056 loss: 0.6079 iter time (s): 105.333 samples/sec: 1.215

100%|██████████| 1/1 [01:46<00:00, 106.13s/it][A100%|██████████| 1/1 [01:46<00:00, 106.13s/it]
 98%|█████████▊| 5086/5198 [1:35:08<3:01:53, 97.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.21s/it][A100%|██████████| 1/1 [01:46<00:00, 106.21s/it]
 98%|█████████▊| 5086/5198 [1:33:03<3:01:54, 97.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.13s/it][A100%|██████████| 1/1 [01:46<00:00, 106.13s/it]
 98%|█████████▊| 5086/5198 [1:32:28<3:01:53, 97.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.08s/it][A100%|██████████| 1/1 [01:46<00:00, 106.08s/it]
 98%|█████████▊| 5086/5198 [1:35:43<3:01:52, 97.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.11s/it][A100%|██████████| 1/1 [01:46<00:00, 106.11s/it]
 98%|█████████▊| 5086/5198 [1:35:07<3:01:53, 97.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.14s/it][A100%|██████████| 1/1 [01:46<00:00, 106.15s/it]
 98%|█████████▊| 5086/5198 [1:32:43<3:01:54, 97.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.13s/it][A100%|██████████| 1/1 [01:46<00:00, 106.13s/it]
 98%|█████████▊| 5086/5198 [1:29:55<3:01:54, 97.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4769
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.54s/it][A100%|██████████| 1/1 [01:53<00:00, 113.55s/it]
 98%|█████████▊| 5087/5198 [1:33:28<3:09:10, 102.26s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
 98%|█████████▊| 5088/5198 [1:33:28<2:11:18, 71.62s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:10:52,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=5057, skipped=0, lr=[6.946573084991603e-07], mom=[(0.9, 0.999)]
steps: 5057 loss: 0.6117 iter time (s): 113.137 samples/sec: 1.131

100%|██████████| 1/1 [01:53<00:00, 113.96s/it][A100%|██████████| 1/1 [01:53<00:00, 113.96s/it]
 98%|█████████▊| 5087/5198 [1:37:02<3:09:26, 102.40s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.01s/it][A100%|██████████| 1/1 [01:54<00:00, 114.01s/it]
 98%|█████████▊| 5087/5198 [1:34:57<3:09:28, 102.42s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.94s/it][A100%|██████████| 1/1 [01:53<00:00, 113.94s/it]
 98%|█████████▊| 5087/5198 [1:34:22<3:09:25, 102.39s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.98s/it][A100%|██████████| 1/1 [01:53<00:00, 113.98s/it]
 98%|█████████▊| 5087/5198 [1:37:37<3:09:26, 102.40s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.95s/it][A100%|██████████| 1/1 [01:53<00:00, 113.95s/it]
 98%|█████████▊| 5087/5198 [1:34:37<3:09:26, 102.40s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.03s/it][A100%|██████████| 1/1 [01:54<00:00, 114.03s/it]
 98%|█████████▊| 5087/5198 [1:37:01<3:09:28, 102.42s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.97s/it][A100%|██████████| 1/1 [01:53<00:00, 113.97s/it]
 98%|█████████▊| 5087/5198 [1:31:49<3:09:27, 102.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_317
Training on 0 of 112 sentences.

0it [00:00, ?it/s][A0it [00:00, ?it/s]
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4770
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.51s/it][A100%|██████████| 1/1 [01:29<00:00, 89.51s/it]
 98%|█████████▊| 5089/5198 [1:34:58<2:19:59, 77.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:12:21,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=5058, skipped=0, lr=[6.925875541617074e-07], mom=[(0.9, 0.999)]
steps: 5058 loss: 0.5955 iter time (s): 88.351 samples/sec: 1.449

100%|██████████| 1/1 [01:29<00:00, 89.28s/it][A100%|██████████| 1/1 [01:29<00:00, 89.28s/it]
 98%|█████████▊| 5089/5198 [1:38:31<2:17:36, 75.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.22s/it][A100%|██████████| 1/1 [01:29<00:00, 89.22s/it]
 98%|█████████▊| 5089/5198 [1:36:26<2:17:36, 75.74s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.21s/it][A100%|██████████| 1/1 [01:29<00:00, 89.21s/it]
 98%|█████████▊| 5089/5198 [1:35:51<2:17:34, 75.73s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.28s/it][A100%|██████████| 1/1 [01:29<00:00, 89.28s/it]
 98%|█████████▊| 5089/5198 [1:39:06<2:17:36, 75.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.22s/it][A100%|██████████| 1/1 [01:29<00:00, 89.22s/it]
 98%|█████████▊| 5089/5198 [1:38:31<2:17:36, 75.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.31s/it][A100%|██████████| 1/1 [01:29<00:00, 89.31s/it]
 98%|█████████▊| 5089/5198 [1:36:06<2:17:37, 75.76s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.29s/it][A100%|██████████| 1/1 [01:29<00:00, 89.29s/it]
 98%|█████████▊| 5089/5198 [1:33:18<2:17:37, 75.75s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4771
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.45s/it][A100%|██████████| 1/1 [01:53<00:00, 113.45s/it]
 98%|█████████▊| 5090/5198 [1:36:52<2:38:28, 88.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:14:16,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=5059, skipped=0, lr=[6.905207861940292e-07], mom=[(0.9, 0.999)]
steps: 5059 loss: 0.6098 iter time (s): 113.378 samples/sec: 1.129

100%|██████████| 1/1 [01:54<00:00, 114.30s/it][A100%|██████████| 1/1 [01:54<00:00, 114.30s/it]
 98%|█████████▊| 5090/5198 [1:40:26<2:33:33, 85.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.28s/it][A100%|██████████| 1/1 [01:54<00:00, 114.28s/it]
 98%|█████████▊| 5090/5198 [1:38:20<2:33:32, 85.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.34s/it][A100%|██████████| 1/1 [01:54<00:00, 114.34s/it]
 98%|█████████▊| 5090/5198 [1:37:45<2:33:32, 85.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.24s/it][A100%|██████████| 1/1 [01:54<00:00, 114.24s/it]
 98%|█████████▊| 5090/5198 [1:41:00<2:33:31, 85.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.24s/it][A100%|██████████| 1/1 [01:54<00:00, 114.24s/it]
 98%|█████████▊| 5090/5198 [1:40:25<2:33:31, 85.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.22s/it][A100%|██████████| 1/1 [01:54<00:00, 114.22s/it]
 98%|█████████▊| 5090/5198 [1:38:01<2:33:31, 85.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.23s/it][A100%|██████████| 1/1 [01:54<00:00, 114.23s/it]
 98%|█████████▊| 5090/5198 [1:35:12<2:33:31, 85.30s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4772
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.55s/it][A100%|██████████| 1/1 [01:42<00:00, 102.55s/it]
 98%|█████████▊| 5091/5198 [1:38:34<2:44:52, 92.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:15:58,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=5060, skipped=0, lr=[6.884570052591966e-07], mom=[(0.9, 0.999)]
steps: 5060 loss: 0.5972 iter time (s): 101.554 samples/sec: 1.260

100%|██████████| 1/1 [01:42<00:00, 102.44s/it][A100%|██████████| 1/1 [01:42<00:00, 102.44s/it]
 98%|█████████▊| 5091/5198 [1:42:08<2:40:07, 89.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.55s/it][A100%|██████████| 1/1 [01:42<00:00, 102.55s/it]
 98%|█████████▊| 5091/5198 [1:40:03<2:40:09, 89.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.58s/it][A100%|██████████| 1/1 [01:42<00:00, 102.58s/it]
 98%|█████████▊| 5091/5198 [1:39:28<2:40:11, 89.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.50s/it][A100%|██████████| 1/1 [01:42<00:00, 102.50s/it]
 98%|█████████▊| 5091/5198 [1:42:43<2:40:08, 89.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.49s/it][A100%|██████████| 1/1 [01:42<00:00, 102.49s/it]
 98%|█████████▊| 5091/5198 [1:42:07<2:40:07, 89.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.49s/it][A100%|██████████| 1/1 [01:42<00:00, 102.49s/it]
 98%|█████████▊| 5091/5198 [1:39:43<2:40:07, 89.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.49s/it][A100%|██████████| 1/1 [01:42<00:00, 102.49s/it]
 98%|█████████▊| 5091/5198 [1:36:55<2:40:07, 89.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4773
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.11s/it][A100%|██████████| 1/1 [01:33<00:00, 93.12s/it]
 98%|█████████▊| 5092/5198 [1:40:08<2:43:50, 92.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:17:31,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=5061, skipped=0, lr=[6.863962120193204e-07], mom=[(0.9, 0.999)]
steps: 5061 loss: 0.5827 iter time (s): 92.231 samples/sec: 1.388

100%|██████████| 1/1 [01:33<00:00, 93.10s/it][A100%|██████████| 1/1 [01:33<00:00, 93.10s/it]
 98%|█████████▊| 5092/5198 [1:43:41<2:40:13, 90.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.99s/it][A100%|██████████| 1/1 [01:32<00:00, 92.99s/it]
 98%|█████████▊| 5092/5198 [1:41:36<2:40:12, 90.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.98s/it][A100%|██████████| 1/1 [01:32<00:00, 92.98s/it]
 98%|█████████▊| 5092/5198 [1:41:01<2:40:12, 90.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.06s/it][A100%|██████████| 1/1 [01:33<00:00, 93.07s/it]
 98%|█████████▊| 5092/5198 [1:44:16<2:40:12, 90.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.06s/it][A100%|██████████| 1/1 [01:33<00:00, 93.06s/it]
 98%|█████████▊| 5092/5198 [1:41:16<2:40:12, 90.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.13s/it][A100%|██████████| 1/1 [01:33<00:00, 93.13s/it]
 98%|█████████▊| 5092/5198 [1:43:41<2:40:14, 90.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.08s/it][A100%|██████████| 1/1 [01:33<00:00, 93.08s/it]
 98%|█████████▊| 5092/5198 [1:38:28<2:40:13, 90.69s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4774
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.32s/it][A100%|██████████| 1/1 [01:22<00:00, 82.32s/it]
 98%|█████████▊| 5093/5198 [1:41:30<2:36:56, 89.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:18:54,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=5062, skipped=0, lr=[6.843384071355544e-07], mom=[(0.9, 0.999)]
steps: 5062 loss: 0.6028 iter time (s): 81.361 samples/sec: 1.573

100%|██████████| 1/1 [01:22<00:00, 82.19s/it][A100%|██████████| 1/1 [01:22<00:00, 82.19s/it]
 98%|█████████▊| 5093/5198 [1:45:03<2:34:33, 88.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.25s/it][A100%|██████████| 1/1 [01:22<00:00, 82.25s/it]
 98%|█████████▊| 5093/5198 [1:42:58<2:34:33, 88.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.15s/it][A100%|██████████| 1/1 [01:22<00:00, 82.15s/it]
 98%|█████████▊| 5093/5198 [1:42:23<2:34:31, 88.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.18s/it][A100%|██████████| 1/1 [01:22<00:00, 82.18s/it]
 98%|█████████▊| 5093/5198 [1:45:38<2:34:32, 88.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.13s/it][A100%|██████████| 1/1 [01:22<00:00, 82.13s/it]
 98%|█████████▊| 5093/5198 [1:45:03<2:34:32, 88.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.18s/it][A100%|██████████| 1/1 [01:22<00:00, 82.18s/it]
 98%|█████████▊| 5093/5198 [1:42:38<2:34:32, 88.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.16s/it][A100%|██████████| 1/1 [01:22<00:00, 82.16s/it]
 98%|█████████▊| 5093/5198 [1:39:50<2:34:32, 88.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4775
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.99s/it][A100%|██████████| 1/1 [01:31<00:00, 91.99s/it]
 98%|█████████▊| 5094/5198 [1:43:02<2:36:43, 90.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:20:26,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=5063, skipped=0, lr=[6.822835912680945e-07], mom=[(0.9, 0.999)]
steps: 5063 loss: 0.5927 iter time (s): 91.682 samples/sec: 1.396

100%|██████████| 1/1 [01:32<00:00, 92.44s/it][A100%|██████████| 1/1 [01:32<00:00, 92.44s/it]
 98%|█████████▊| 5094/5198 [1:46:36<2:35:07, 89.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.36s/it][A100%|██████████| 1/1 [01:32<00:00, 92.36s/it]
 98%|█████████▊| 5094/5198 [1:44:30<2:35:05, 89.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.54s/it][A100%|██████████| 1/1 [01:32<00:00, 92.54s/it]
 98%|█████████▊| 5094/5198 [1:43:56<2:35:09, 89.52s/it]
100%|██████████| 1/1 [01:32<00:00, 92.42s/it][A100%|██████████| 1/1 [01:32<00:00, 92.42s/it]

 98%|█████████▊| 5094/5198 [1:47:11<2:35:06, 89.49s/it]  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.45s/it][A100%|██████████| 1/1 [01:32<00:00, 92.45s/it]
 98%|█████████▊| 5094/5198 [1:46:35<2:35:07, 89.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.47s/it][A100%|██████████| 1/1 [01:32<00:00, 92.47s/it]
 98%|█████████▊| 5094/5198 [1:44:11<2:35:07, 89.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.46s/it][A100%|██████████| 1/1 [01:32<00:00, 92.46s/it]
 98%|█████████▊| 5094/5198 [1:41:22<2:35:07, 89.49s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4776
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.29s/it][A100%|██████████| 1/1 [01:33<00:00, 93.29s/it]
 98%|█████████▊| 5095/5198 [1:44:36<2:36:50, 91.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:22:00,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=5064, skipped=0, lr=[6.80231765076175e-07], mom=[(0.9, 0.999)]
steps: 5064 loss: 0.5788 iter time (s): 92.762 samples/sec: 1.380

100%|██████████| 1/1 [01:33<00:00, 93.59s/it][A100%|██████████| 1/1 [01:33<00:00, 93.59s/it]
 98%|█████████▊| 5095/5198 [1:48:10<2:35:40, 90.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.69s/it][A100%|██████████| 1/1 [01:33<00:00, 93.69s/it]
 98%|█████████▊| 5095/5198 [1:46:04<2:35:42, 90.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.66s/it][A100%|██████████| 1/1 [01:33<00:00, 93.66s/it]
 98%|█████████▊| 5095/5198 [1:45:29<2:35:44, 90.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.79s/it][A100%|██████████| 1/1 [01:33<00:00, 93.79s/it]
 98%|█████████▊| 5095/5198 [1:48:44<2:35:45, 90.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.82s/it][A100%|██████████| 1/1 [01:33<00:00, 93.82s/it]
 98%|█████████▊| 5095/5198 [1:48:09<2:35:47, 90.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.79s/it][A100%|██████████| 1/1 [01:33<00:00, 93.79s/it]
 98%|█████████▊| 5095/5198 [1:45:45<2:35:46, 90.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.53s/it][A100%|██████████| 1/1 [01:34<00:00, 94.53s/it]
 98%|█████████▊| 5095/5198 [1:42:57<2:36:08, 90.96s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4777
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.92s/it][A100%|██████████| 1/1 [02:01<00:00, 121.92s/it]
[2024-06-26 19:24:03,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=5065, skipped=0, lr=[6.781829292180747e-07], mom=[(0.9, 0.999)]
steps: 5065 loss: 0.6086 iter time (s): 121.510 samples/sec: 1.053

100%|██████████| 1/1 [02:03<00:00, 123.27s/it][A100%|██████████| 1/1 [02:03<00:00, 123.27s/it]

100%|██████████| 1/1 [02:03<00:00, 123.31s/it][A100%|██████████| 1/1 [02:03<00:00, 123.31s/it]

100%|██████████| 1/1 [02:03<00:00, 123.23s/it][A100%|██████████| 1/1 [02:03<00:00, 123.23s/it]

100%|██████████| 1/1 [02:03<00:00, 123.22s/it][A100%|██████████| 1/1 [02:03<00:00, 123.23s/it]

100%|██████████| 1/1 [02:03<00:00, 123.12s/it][A100%|██████████| 1/1 [02:03<00:00, 123.12s/it]

100%|██████████| 1/1 [02:03<00:00, 123.15s/it][A100%|██████████| 1/1 [02:03<00:00, 123.15s/it]

100%|██████████| 1/1 [02:02<00:00, 122.41s/it][A100%|██████████| 1/1 [02:02<00:00, 122.41s/it]
Checkpointing at shard 5095
[2024-06-26 19:24:04,258] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5065 is about to be saved!
[2024-06-26 19:24:05,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_00-model_states.pt...
[2024-06-26 19:24:08,888] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_02-model_states.pt...
[2024-06-26 19:24:09,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_07-model_states.pt...
[2024-06-26 19:24:09,641] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_08-model_states.pt...
[2024-06-26 19:24:15,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_00-model_states.pt.
[2024-06-26 19:24:15,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_06-model_states.pt...
[2024-06-26 19:24:15,775] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_05-model_states.pt...
[2024-06-26 19:24:23,368] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_04-model_states.pt...
[2024-06-26 19:24:23,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_01-model_states.pt...
[2024-06-26 19:24:23,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_03-model_states.pt...
[2024-06-26 19:28:32,772] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_06-model_states.pt.
[2024-06-26 19:28:33,209] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_05_model_states.pt...
[2024-06-26 19:28:33,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_05_model_states.pt.
[2024-06-26 19:28:33,427] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
[2024-06-26 19:28:34,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_07-model_states.pt.
[2024-06-26 19:28:34,512] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_08-model_states.pt.
[2024-06-26 19:28:34,514] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_02-model_states.pt.
[2024-06-26 19:28:34,555] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_01_model_states.pt
[2024-06-26 19:28:34,556] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_01_model_states.pt...
[2024-06-26 19:28:34,559] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_06_model_states.pt...
[2024-06-26 19:28:34,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_09-model_states.pt...
[2024-06-26 19:28:34,802] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_01_model_states.pt.
[2024-06-26 19:28:34,802] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
[2024-06-26 19:28:34,815] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_06_model_states.pt.
[2024-06-26 19:28:34,815] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
[2024-06-26 19:28:35,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_09-model_states.pt.
[2024-06-26 19:28:35,636] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_07_model_states.pt...
[2024-06-26 19:28:35,858] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_07_model_states.pt.
[2024-06-26 19:28:35,858] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
[2024-06-26 19:28:41,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_04-model_states.pt.
[2024-06-26 19:28:41,605] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_03_model_states.pt...
[2024-06-26 19:28:41,895] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_03_model_states.pt.
[2024-06-26 19:28:41,895] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
[2024-06-26 19:28:42,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_01-model_states.pt.
[2024-06-26 19:28:42,748] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_03-model_states.pt.
[2024-06-26 19:28:43,189] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_00_model_states.pt
[2024-06-26 19:28:43,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_00_model_states.pt...
[2024-06-26 19:28:43,270] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_02_model_states.pt...
[2024-06-26 19:28:43,425] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_02_model_states.pt.
[2024-06-26 19:28:43,425] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
[2024-06-26 19:28:43,835] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_00_model_states.pt.
[2024-06-26 19:28:43,835] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
[2024-06-26 19:28:43,937] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/layer_05-model_states.pt.
[2024-06-26 19:28:44,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_04_model_states.pt...
[2024-06-26 19:28:44,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5065/mp_rank_04_model_states.pt.
[2024-06-26 19:28:44,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5065 is ready now!
 98%|█████████▊| 5096/5198 [1:52:13<5:10:08, 182.43s/it] 98%|█████████▊| 5096/5198 [1:54:54<5:10:15, 182.51s/it] 98%|█████████▊| 5096/5198 [1:51:23<5:16:11, 185.99s/it] 98%|█████████▊| 5096/5198 [1:52:28<5:10:01, 182.37s/it] 98%|█████████▊| 5096/5198 [1:55:28<5:10:05, 182.41s/it] 98%|█████████▊| 5096/5198 [1:52:48<5:10:12, 182.47s/it] 98%|█████████▊| 5096/5198 [1:54:53<5:10:02, 182.37s/it]Checkpoint saved using --- 280.51573514938354 seconds ---
 98%|█████████▊| 5096/5198 [1:49:40<5:09:54, 182.29s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4778

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.


  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.77s/it][A100%|██████████| 1/1 [01:47<00:00, 107.77s/it]
 98%|█████████▊| 5097/5198 [1:53:11<4:33:40, 162.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:30:35,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=5066, skipped=0, lr=[6.761370843511099e-07], mom=[(0.9, 0.999)]
steps: 5066 loss: 0.6210 iter time (s): 110.413 samples/sec: 1.159

100%|██████████| 1/1 [01:50<00:00, 110.75s/it][A100%|██████████| 1/1 [01:50<00:00, 110.75s/it]
 98%|█████████▊| 5097/5198 [1:56:45<4:31:41, 161.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.88s/it][A100%|██████████| 1/1 [01:50<00:00, 110.88s/it]
 98%|█████████▊| 5097/5198 [1:54:39<4:31:42, 161.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.09s/it][A100%|██████████| 1/1 [01:51<00:00, 111.09s/it]
 98%|█████████▊| 5097/5198 [1:57:19<4:31:44, 161.43s/it]
100%|██████████| 1/1 [01:51<00:00, 111.09s/it][A100%|██████████| 1/1 [01:51<00:00, 111.09s/it]
 98%|█████████▊| 5097/5198 [1:54:04<4:31:46, 161.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.21s/it][A100%|██████████| 1/1 [01:51<00:00, 111.21s/it]
 98%|█████████▊| 5097/5198 [1:56:44<4:31:45, 161.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.24s/it][A100%|██████████| 1/1 [01:51<00:00, 111.24s/it]
 98%|█████████▊| 5097/5198 [1:54:20<4:31:46, 161.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.30s/it][A100%|██████████| 1/1 [01:51<00:00, 111.30s/it]
 98%|█████████▊| 5097/5198 [1:51:31<4:31:42, 161.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4779
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.80s/it][A100%|██████████| 1/1 [01:32<00:00, 92.80s/it]
 98%|█████████▊| 5098/5198 [1:54:44<3:56:11, 141.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:32:07,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=5067, skipped=0, lr=[6.740942311316395e-07], mom=[(0.9, 0.999)]
steps: 5067 loss: 0.6151 iter time (s): 91.569 samples/sec: 1.398

100%|██████████| 1/1 [01:32<00:00, 92.48s/it][A100%|██████████| 1/1 [01:32<00:00, 92.48s/it]
 98%|█████████▊| 5098/5198 [1:58:17<3:54:57, 140.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.44s/it][A100%|██████████| 1/1 [01:32<00:00, 92.44s/it]
 98%|█████████▊| 5098/5198 [1:56:12<3:54:57, 140.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.35s/it][A100%|██████████| 1/1 [01:32<00:00, 92.35s/it]
 98%|█████████▊| 5098/5198 [1:55:37<3:54:57, 140.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.40s/it][A100%|██████████| 1/1 [01:32<00:00, 92.40s/it]
 98%|█████████▊| 5098/5198 [1:58:52<3:54:57, 140.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.40s/it][A100%|██████████| 1/1 [01:32<00:00, 92.40s/it]
 98%|█████████▊| 5098/5198 [1:58:17<3:54:58, 140.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 98%|█████████▊| 5098/5198 [1:55:52<3:54:58, 140.99s/it]
100%|██████████| 1/1 [01:32<00:00, 92.35s/it][A100%|██████████| 1/1 [01:32<00:00, 92.35s/it]
 98%|█████████▊| 5098/5198 [1:53:04<3:54:54, 140.95s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4780

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.38s/it][A100%|██████████| 1/1 [01:39<00:00, 99.38s/it]
 98%|█████████▊| 5099/5198 [1:56:23<3:32:58, 129.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:33:47,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=5068, skipped=0, lr=[6.720543702150596e-07], mom=[(0.9, 0.999)]
steps: 5068 loss: 0.6196 iter time (s): 99.021 samples/sec: 1.293

100%|██████████| 1/1 [01:39<00:00, 99.78s/it][A100%|██████████| 1/1 [01:39<00:00, 99.78s/it]
 98%|█████████▊| 5099/5198 [1:59:57<3:32:23, 128.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.80s/it][A100%|██████████| 1/1 [01:39<00:00, 99.80s/it]
 98%|█████████▊| 5099/5198 [1:57:52<3:32:23, 128.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.86s/it][A100%|██████████| 1/1 [01:39<00:00, 99.86s/it]
 98%|█████████▊| 5099/5198 [1:57:17<3:32:25, 128.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.90s/it][A100%|██████████| 1/1 [01:39<00:00, 99.90s/it]
 98%|█████████▊| 5099/5198 [2:00:32<3:32:26, 128.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.80s/it][A100%|██████████| 1/1 [01:39<00:00, 99.80s/it]
 98%|█████████▊| 5099/5198 [1:57:32<3:32:24, 128.74s/it]
100%|██████████| 1/1 [01:39<00:00, 99.85s/it][A100%|██████████| 1/1 [01:39<00:00, 99.85s/it]
 98%|█████████▊| 5099/5198 [1:59:56<3:32:26, 128.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.84s/it][A100%|██████████| 1/1 [01:39<00:00, 99.84s/it]
 98%|█████████▊| 5099/5198 [1:54:44<3:32:23, 128.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4781
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.47s/it][A100%|██████████| 1/1 [01:24<00:00, 84.47s/it]
 98%|█████████▊| 5100/5198 [1:57:48<3:09:02, 115.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:35:11,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=5069, skipped=0, lr=[6.700175022558086e-07], mom=[(0.9, 0.999)]
steps: 5069 loss: 0.5880 iter time (s): 83.344 samples/sec: 1.536

100%|██████████| 1/1 [01:24<00:00, 84.19s/it][A100%|██████████| 1/1 [01:24<00:00, 84.19s/it]
 98%|█████████▊| 5100/5198 [2:01:21<3:08:33, 115.45s/it]
100%|██████████| 1/1 [01:24<00:00, 84.10s/it][A100%|██████████| 1/1 [01:24<00:00, 84.10s/it]
 98%|█████████▊| 5100/5198 [1:59:16<3:08:31, 115.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.17s/it][A100%|██████████| 1/1 [01:24<00:00, 84.17s/it]
 98%|█████████▊| 5100/5198 [1:58:41<3:08:34, 115.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.16s/it][A100%|██████████| 1/1 [01:24<00:00, 84.16s/it]
 98%|█████████▊| 5100/5198 [2:01:56<3:08:34, 115.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.13s/it][A100%|██████████| 1/1 [01:24<00:00, 84.13s/it]
 98%|█████████▊| 5100/5198 [1:58:56<3:08:32, 115.44s/it]
100%|██████████| 1/1 [01:24<00:00, 84.13s/it][A100%|██████████| 1/1 [01:24<00:00, 84.13s/it]
 98%|█████████▊| 5100/5198 [2:01:21<3:08:33, 115.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.12s/it][A100%|██████████| 1/1 [01:24<00:00, 84.12s/it]
 98%|█████████▊| 5100/5198 [1:56:08<3:08:31, 115.42s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4782
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.41s/it][A100%|██████████| 1/1 [01:20<00:00, 80.41s/it]
 98%|█████████▊| 5101/5198 [1:59:09<2:50:04, 105.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:36:32,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=5070, skipped=0, lr=[6.679836279073665e-07], mom=[(0.9, 0.999)]
steps: 5070 loss: 0.6378 iter time (s): 79.716 samples/sec: 1.606

100%|██████████| 1/1 [01:20<00:00, 80.50s/it][A100%|██████████| 1/1 [01:20<00:00, 80.50s/it]
 98%|█████████▊| 5101/5198 [2:02:42<2:49:45, 105.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.63s/it][A100%|██████████| 1/1 [01:20<00:00, 80.63s/it]
 98%|█████████▊| 5101/5198 [2:00:36<2:49:47, 105.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.44s/it][A100%|██████████| 1/1 [01:20<00:00, 80.44s/it]
 98%|█████████▊| 5101/5198 [2:00:01<2:49:44, 105.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.45s/it][A100%|██████████| 1/1 [01:20<00:00, 80.45s/it]
 98%|█████████▊| 5101/5198 [2:03:16<2:49:45, 105.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.50s/it][A100%|██████████| 1/1 [01:20<00:00, 80.50s/it]
 98%|█████████▊| 5101/5198 [2:02:41<2:49:45, 105.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.54s/it][A100%|██████████| 1/1 [01:20<00:00, 80.54s/it]
 98%|█████████▊| 5101/5198 [2:00:17<2:49:46, 105.01s/it]
100%|██████████| 1/1 [01:20<00:00, 80.52s/it][A100%|██████████| 1/1 [01:20<00:00, 80.52s/it]
 98%|█████████▊| 5101/5198 [1:57:28<2:49:44, 105.00s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4783

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.09s/it][A100%|██████████| 1/1 [02:00<00:00, 120.09s/it]
 98%|█████████▊| 5102/5198 [2:01:09<2:55:35, 109.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:38:33,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=5071, skipped=0, lr=[6.65952747822248e-07], mom=[(0.9, 0.999)]
steps: 5071 loss: 0.6165 iter time (s): 120.636 samples/sec: 1.061

100%|██████████| 1/1 [02:01<00:00, 121.44s/it][A100%|██████████| 1/1 [02:01<00:00, 121.44s/it]
 98%|█████████▊| 5102/5198 [2:04:43<2:55:53, 109.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.46s/it][A100%|██████████| 1/1 [02:01<00:00, 121.46s/it]
 98%|█████████▊| 5102/5198 [2:02:38<2:55:54, 109.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.53s/it][A100%|██████████| 1/1 [02:01<00:00, 121.53s/it]
 98%|█████████▊| 5102/5198 [2:02:03<2:55:54, 109.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.57s/it][A100%|██████████| 1/1 [02:01<00:00, 121.57s/it]
 98%|█████████▊| 5102/5198 [2:05:18<2:55:56, 109.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.50s/it][A100%|██████████| 1/1 [02:01<00:00, 121.50s/it]
 98%|█████████▊| 5102/5198 [2:04:43<2:55:54, 109.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.52s/it][A100%|██████████| 1/1 [02:01<00:00, 121.52s/it]
 98%|█████████▊| 5102/5198 [2:02:18<2:55:55, 109.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.52s/it][A100%|██████████| 1/1 [02:01<00:00, 121.52s/it]
 98%|█████████▊| 5102/5198 [1:59:30<2:55:54, 109.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4784
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:31<00:00, 151.06s/it][A100%|██████████| 1/1 [02:31<00:00, 151.06s/it]
 98%|█████████▊| 5103/5198 [2:03:40<3:13:28, 122.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:41:05,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=5072, skipped=0, lr=[6.639248626520102e-07], mom=[(0.9, 0.999)]
steps: 5072 loss: 0.5788 iter time (s): 151.237 samples/sec: 0.846

100%|██████████| 1/1 [02:32<00:00, 152.03s/it][A100%|██████████| 1/1 [02:32<00:00, 152.03s/it]
 98%|█████████▊| 5103/5198 [2:07:15<3:14:00, 122.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.03s/it][A100%|██████████| 1/1 [02:32<00:00, 152.03s/it]
 98%|█████████▊| 5103/5198 [2:05:10<3:14:02, 122.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:32<00:00, 152.04s/it][A100%|██████████| 1/1 [02:32<00:00, 152.04s/it]
 98%|█████████▊| 5103/5198 [2:04:35<3:14:02, 122.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:31<00:00, 151.97s/it][A100%|██████████| 1/1 [02:31<00:00, 151.97s/it]
 98%|█████████▊| 5103/5198 [2:07:50<3:14:01, 122.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:31<00:00, 151.99s/it][A100%|██████████| 1/1 [02:31<00:00, 151.99s/it]
 98%|█████████▊| 5103/5198 [2:07:15<3:14:00, 122.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:31<00:00, 151.97s/it][A100%|██████████| 1/1 [02:31<00:00, 151.97s/it]
 98%|█████████▊| 5103/5198 [2:04:50<3:14:00, 122.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:31<00:00, 151.98s/it][A100%|██████████| 1/1 [02:31<00:00, 151.98s/it]
 98%|█████████▊| 5103/5198 [2:02:02<3:14:00, 122.53s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4785
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.30s/it][A100%|██████████| 1/1 [01:40<00:00, 100.30s/it]
 98%|█████████▊| 5104/5198 [2:05:21<3:01:16, 115.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:42:45,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=5073, skipped=0, lr=[6.618999730472508e-07], mom=[(0.9, 0.999)]
steps: 5073 loss: 0.5885 iter time (s): 98.362 samples/sec: 1.301

100%|██████████| 1/1 [01:39<00:00, 99.22s/it][A100%|██████████| 1/1 [01:39<00:00, 99.22s/it]
 98%|█████████▊| 5104/5198 [2:08:55<3:01:02, 115.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.20s/it][A100%|██████████| 1/1 [01:39<00:00, 99.20s/it]
 98%|█████████▊| 5104/5198 [2:06:49<3:01:02, 115.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.19s/it][A100%|██████████| 1/1 [01:39<00:00, 99.19s/it]
 98%|█████████▊| 5104/5198 [2:06:14<3:01:02, 115.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.23s/it][A100%|██████████| 1/1 [01:39<00:00, 99.23s/it]
 98%|█████████▊| 5104/5198 [2:09:29<3:01:02, 115.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.27s/it][A100%|██████████| 1/1 [01:39<00:00, 99.27s/it]
 98%|█████████▊| 5104/5198 [2:08:54<3:01:03, 115.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.24s/it][A100%|██████████| 1/1 [01:39<00:00, 99.24s/it]
 98%|█████████▊| 5104/5198 [2:06:30<3:01:02, 115.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.25s/it][A100%|██████████| 1/1 [01:39<00:00, 99.25s/it]
 98%|█████████▊| 5104/5198 [2:03:41<3:01:02, 115.56s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4786
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.89s/it][A100%|██████████| 1/1 [01:23<00:00, 83.89s/it]
 98%|█████████▊| 5105/5198 [2:06:45<2:44:38, 106.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:44:08,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=5074, skipped=0, lr=[6.598780796576039e-07], mom=[(0.9, 0.999)]
steps: 5074 loss: 0.6436 iter time (s): 82.768 samples/sec: 1.546

100%|██████████| 1/1 [01:23<00:00, 83.56s/it][A100%|██████████| 1/1 [01:23<00:00, 83.56s/it]
 98%|█████████▊| 5105/5198 [2:10:18<2:44:15, 105.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.57s/it][A100%|██████████| 1/1 [01:23<00:00, 83.57s/it]
 98%|█████████▊| 5105/5198 [2:08:13<2:44:15, 105.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.63s/it][A100%|██████████| 1/1 [01:23<00:00, 83.63s/it]
 98%|█████████▊| 5105/5198 [2:07:38<2:44:17, 105.99s/it]
100%|██████████| 1/1 [01:23<00:00, 83.56s/it][A100%|██████████| 1/1 [01:23<00:00, 83.56s/it]
 98%|█████████▊| 5105/5198 [2:10:53<2:44:15, 105.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.53s/it][A100%|██████████| 1/1 [01:23<00:00, 83.53s/it]
 98%|█████████▊| 5105/5198 [2:10:17<2:44:14, 105.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.56s/it][A100%|██████████| 1/1 [01:23<00:00, 83.56s/it]
 98%|█████████▊| 5105/5198 [2:07:53<2:44:15, 105.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.56s/it][A100%|██████████| 1/1 [01:23<00:00, 83.56s/it]
 98%|█████████▊| 5105/5198 [2:05:05<2:44:15, 105.97s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4787
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.70s/it][A100%|██████████| 1/1 [01:32<00:00, 92.70s/it]
 98%|█████████▊| 5106/5198 [2:08:18<2:36:45, 102.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:45:41,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=5075, skipped=0, lr=[6.578591831317414e-07], mom=[(0.9, 0.999)]
steps: 5075 loss: 0.5822 iter time (s): 92.379 samples/sec: 1.386

100%|██████████| 1/1 [01:33<00:00, 93.17s/it][A100%|██████████| 1/1 [01:33<00:00, 93.17s/it]
 98%|█████████▊| 5106/5198 [2:11:51<2:36:36, 102.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.17s/it][A100%|██████████| 1/1 [01:33<00:00, 93.17s/it]
 98%|█████████▊| 5106/5198 [2:09:46<2:36:36, 102.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.09s/it][A100%|██████████| 1/1 [01:33<00:00, 93.09s/it]
 98%|█████████▊| 5106/5198 [2:09:11<2:36:35, 102.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.14s/it][A100%|██████████| 1/1 [01:33<00:00, 93.14s/it]
 98%|█████████▊| 5106/5198 [2:12:26<2:36:35, 102.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.18s/it][A100%|██████████| 1/1 [01:33<00:00, 93.18s/it]
 98%|█████████▊| 5106/5198 [2:11:51<2:36:36, 102.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.15s/it][A100%|██████████| 1/1 [01:33<00:00, 93.15s/it]
 98%|█████████▊| 5106/5198 [2:09:26<2:36:36, 102.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.15s/it][A100%|██████████| 1/1 [01:33<00:00, 93.15s/it]
 98%|█████████▊| 5106/5198 [2:06:38<2:36:35, 102.13s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4788
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.29s/it][A100%|██████████| 1/1 [01:22<00:00, 82.29s/it]
 98%|█████████▊| 5107/5198 [2:09:40<2:26:03, 96.30s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:47:04,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=5076, skipped=0, lr=[6.558432841173783e-07], mom=[(0.9, 0.999)]
steps: 5076 loss: 0.5640 iter time (s): 81.364 samples/sec: 1.573

100%|██████████| 1/1 [01:22<00:00, 82.11s/it][A100%|██████████| 1/1 [01:22<00:00, 82.11s/it]
 98%|█████████▊| 5107/5198 [2:13:13<2:25:48, 96.13s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.07s/it][A100%|██████████| 1/1 [01:22<00:00, 82.07s/it]
 98%|█████████▊| 5107/5198 [2:11:08<2:25:47, 96.12s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.09s/it][A100%|██████████| 1/1 [01:22<00:00, 82.09s/it]
 98%|█████████▊| 5107/5198 [2:10:33<2:25:47, 96.12s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.09s/it][A100%|██████████| 1/1 [01:22<00:00, 82.09s/it]
 98%|█████████▊| 5107/5198 [2:13:48<2:25:47, 96.12s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.09s/it][A100%|██████████| 1/1 [01:22<00:00, 82.09s/it]
 98%|█████████▊| 5107/5198 [2:13:13<2:25:47, 96.13s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.11s/it][A100%|██████████| 1/1 [01:22<00:00, 82.11s/it]
 98%|█████████▊| 5107/5198 [2:10:48<2:25:47, 96.13s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.10s/it][A100%|██████████| 1/1 [01:22<00:00, 82.10s/it]
 98%|█████████▊| 5107/5198 [2:08:00<2:25:47, 96.13s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4789
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.01s/it][A100%|██████████| 1/1 [01:41<00:00, 101.01s/it]
 98%|█████████▊| 5108/5198 [2:11:21<2:26:37, 97.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:48:45,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=5077, skipped=0, lr=[6.538303832612642e-07], mom=[(0.9, 0.999)]
steps: 5077 loss: 0.6293 iter time (s): 100.900 samples/sec: 1.269

100%|██████████| 1/1 [01:41<00:00, 101.67s/it][A100%|██████████| 1/1 [01:41<00:00, 101.67s/it]
 98%|█████████▊| 5108/5198 [2:14:55<2:26:41, 97.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.80s/it][A100%|██████████| 1/1 [01:41<00:00, 101.80s/it]
 98%|█████████▊| 5108/5198 [2:12:50<2:26:44, 97.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.81s/it][A100%|██████████| 1/1 [01:41<00:00, 101.81s/it]
 98%|█████████▊| 5108/5198 [2:12:15<2:26:44, 97.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.79s/it][A100%|██████████| 1/1 [01:41<00:00, 101.79s/it]
 98%|█████████▊| 5108/5198 [2:15:30<2:26:44, 97.83s/it]
100%|██████████| 1/1 [01:41<00:00, 101.71s/it][A100%|██████████| 1/1 [01:41<00:00, 101.71s/it]
 98%|█████████▊| 5108/5198 [2:14:54<2:26:42, 97.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.75s/it][A100%|██████████| 1/1 [01:41<00:00, 101.75s/it]
 98%|█████████▊| 5108/5198 [2:12:30<2:26:43, 97.82s/it]
100%|██████████| 1/1 [01:41<00:00, 101.73s/it][A100%|██████████| 1/1 [01:41<00:00, 101.73s/it]
 98%|█████████▊| 5108/5198 [2:09:42<2:26:42, 97.81s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4790
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.43s/it][A100%|██████████| 1/1 [01:46<00:00, 106.43s/it]
 98%|█████████▊| 5109/5198 [2:13:08<2:28:58, 100.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:50:32,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=5078, skipped=0, lr=[6.518204812091852e-07], mom=[(0.9, 0.999)]
steps: 5078 loss: 0.6515 iter time (s): 106.009 samples/sec: 1.207

100%|██████████| 1/1 [01:46<00:00, 106.95s/it][A100%|██████████| 1/1 [01:46<00:00, 106.95s/it]
 98%|█████████▊| 5109/5198 [2:16:42<2:29:08, 100.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.85s/it][A100%|██████████| 1/1 [01:46<00:00, 106.85s/it]
 98%|█████████▊| 5109/5198 [2:14:37<2:29:07, 100.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.74s/it][A100%|██████████| 1/1 [01:46<00:00, 106.74s/it]
 98%|█████████▊| 5109/5198 [2:14:02<2:29:05, 100.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.92s/it][A100%|██████████| 1/1 [01:46<00:00, 106.92s/it]
 98%|█████████▊| 5109/5198 [2:17:17<2:29:09, 100.56s/it]
100%|██████████| 1/1 [01:46<00:00, 106.92s/it][A100%|██████████| 1/1 [01:46<00:00, 106.92s/it]
 98%|█████████▊| 5109/5198 [2:16:41<2:29:08, 100.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.87s/it][A100%|██████████| 1/1 [01:46<00:00, 106.87s/it]
 98%|█████████▊| 5109/5198 [2:14:17<2:29:07, 100.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.89s/it][A100%|██████████| 1/1 [01:46<00:00, 106.89s/it]
 98%|█████████▊| 5109/5198 [2:11:28<2:29:07, 100.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4791
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.77s/it][A100%|██████████| 1/1 [02:18<00:00, 138.77s/it]
 98%|█████████▊| 5110/5198 [2:15:27<2:44:15, 112.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:52:52,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=5079, skipped=0, lr=[6.498135786059705e-07], mom=[(0.9, 0.999)]
steps: 5079 loss: 0.6168 iter time (s): 139.065 samples/sec: 0.920

100%|██████████| 1/1 [02:19<00:00, 139.81s/it][A100%|██████████| 1/1 [02:19<00:00, 139.81s/it]
 98%|█████████▊| 5110/5198 [2:19:02<2:44:44, 112.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.91s/it][A100%|██████████| 1/1 [02:19<00:00, 139.91s/it]
 98%|█████████▊| 5110/5198 [2:16:57<2:44:46, 112.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.75s/it][A100%|██████████| 1/1 [02:19<00:00, 139.75s/it]
 98%|█████████▊| 5110/5198 [2:19:36<2:44:43, 112.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.97s/it][A100%|██████████| 1/1 [02:19<00:00, 139.97s/it]
 98%|█████████▊| 5110/5198 [2:16:22<2:44:46, 112.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.83s/it][A100%|██████████| 1/1 [02:19<00:00, 139.83s/it]
 98%|█████████▊| 5110/5198 [2:19:01<2:44:44, 112.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.86s/it][A100%|██████████| 1/1 [02:19<00:00, 139.86s/it]
 98%|█████████▊| 5110/5198 [2:16:37<2:44:45, 112.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.85s/it][A100%|██████████| 1/1 [02:19<00:00, 139.85s/it]
 98%|█████████▊| 5110/5198 [2:13:48<2:44:45, 112.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4792
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.29s/it][A100%|██████████| 1/1 [01:28<00:00, 88.29s/it]
 98%|█████████▊| 5111/5198 [2:16:56<2:32:12, 104.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:54:19,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=5080, skipped=0, lr=[6.47809676095483e-07], mom=[(0.9, 0.999)]
steps: 5080 loss: 0.6438 iter time (s): 86.296 samples/sec: 1.483

100%|██████████| 1/1 [01:27<00:00, 87.10s/it][A100%|██████████| 1/1 [01:27<00:00, 87.10s/it]
 98%|█████████▊| 5111/5198 [2:20:29<2:31:54, 104.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.97s/it][A100%|██████████| 1/1 [01:26<00:00, 86.97s/it]
 98%|█████████▊| 5111/5198 [2:18:23<2:31:52, 104.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.94s/it][A100%|██████████| 1/1 [01:26<00:00, 86.94s/it]
 98%|█████████▊| 5111/5198 [2:17:48<2:31:51, 104.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.05s/it][A100%|██████████| 1/1 [01:27<00:00, 87.05s/it]
 98%|█████████▊| 5111/5198 [2:21:04<2:31:52, 104.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.03s/it][A100%|██████████| 1/1 [01:27<00:00, 87.03s/it]
 98%|█████████▊| 5111/5198 [2:18:04<2:31:52, 104.74s/it]
100%|██████████| 1/1 [01:27<00:00, 87.07s/it][A100%|██████████| 1/1 [01:27<00:00, 87.07s/it]
 98%|█████████▊| 5111/5198 [2:20:28<2:31:53, 104.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.01s/it][A100%|██████████| 1/1 [01:27<00:00, 87.01s/it]

 98%|█████████▊| 5111/5198 [2:15:15<2:31:52, 104.74s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4793
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:09<00:00, 129.71s/it][A100%|██████████| 1/1 [02:09<00:00, 129.71s/it]
 98%|█████████▊| 5112/5198 [2:19:06<2:41:13, 112.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:56:30,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=5081, skipped=0, lr=[6.458087743206242e-07], mom=[(0.9, 0.999)]
steps: 5081 loss: 0.5918 iter time (s): 130.498 samples/sec: 0.981

100%|██████████| 1/1 [02:11<00:00, 131.24s/it][A100%|██████████| 1/1 [02:11<00:00, 131.24s/it]
 98%|█████████▊| 5112/5198 [2:22:40<2:41:32, 112.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.22s/it][A100%|██████████| 1/1 [02:11<00:00, 131.22s/it]
 98%|█████████▊| 5112/5198 [2:20:35<2:41:30, 112.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.36s/it][A100%|██████████| 1/1 [02:11<00:00, 131.36s/it]
 98%|█████████▊| 5112/5198 [2:20:00<2:41:34, 112.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.33s/it][A100%|██████████| 1/1 [02:11<00:00, 131.33s/it]
 98%|█████████▊| 5112/5198 [2:23:15<2:41:33, 112.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.25s/it][A100%|██████████| 1/1 [02:11<00:00, 131.25s/it]
 98%|█████████▊| 5112/5198 [2:22:39<2:41:32, 112.71s/it]
100%|██████████| 1/1 [02:11<00:00, 131.26s/it][A100%|██████████| 1/1 [02:11<00:00, 131.26s/it]
 98%|█████████▊| 5112/5198 [2:20:15<2:41:32, 112.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.28s/it][A100%|██████████| 1/1 [02:11<00:00, 131.28s/it]
 98%|█████████▊| 5112/5198 [2:17:27<2:41:32, 112.71s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4794
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.70s/it][A100%|██████████| 1/1 [02:11<00:00, 131.70s/it]
 98%|█████████▊| 5113/5198 [2:21:18<2:47:35, 118.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 19:58:42,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=5082, skipped=0, lr=[6.438108739233348e-07], mom=[(0.9, 0.999)]
steps: 5082 loss: 0.5813 iter time (s): 131.052 samples/sec: 0.977

100%|██████████| 1/1 [02:11<00:00, 131.85s/it][A100%|██████████| 1/1 [02:11<00:00, 131.85s/it]
 98%|█████████▊| 5113/5198 [2:24:52<2:47:48, 118.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.85s/it][A100%|██████████| 1/1 [02:11<00:00, 131.85s/it]
 98%|█████████▊| 5113/5198 [2:22:47<2:47:47, 118.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.81s/it][A100%|██████████| 1/1 [02:11<00:00, 131.81s/it]
 98%|█████████▊| 5113/5198 [2:22:12<2:47:48, 118.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.80s/it][A100%|██████████| 1/1 [02:11<00:00, 131.80s/it]
 98%|█████████▊| 5113/5198 [2:25:27<2:47:47, 118.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.80s/it][A100%|██████████| 1/1 [02:11<00:00, 131.80s/it]
 98%|█████████▊| 5113/5198 [2:24:51<2:47:47, 118.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.83s/it][A100%|██████████| 1/1 [02:11<00:00, 131.83s/it]
 98%|█████████▊| 5113/5198 [2:22:27<2:47:47, 118.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.84s/it][A100%|██████████| 1/1 [02:11<00:00, 131.84s/it]
 98%|█████████▊| 5113/5198 [2:19:38<2:47:47, 118.45s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4795
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.24s/it][A100%|██████████| 1/1 [01:35<00:00, 95.24s/it]
 98%|█████████▊| 5114/5198 [2:22:53<2:36:00, 111.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:00:17,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=5083, skipped=0, lr=[6.418159755445896e-07], mom=[(0.9, 0.999)]
steps: 5083 loss: 0.6045 iter time (s): 93.612 samples/sec: 1.367

100%|██████████| 1/1 [01:34<00:00, 94.40s/it][A100%|██████████| 1/1 [01:34<00:00, 94.40s/it]
 98%|█████████▊| 5114/5198 [2:26:26<2:35:44, 111.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.41s/it][A100%|██████████| 1/1 [01:34<00:00, 94.41s/it]
 98%|█████████▊| 5114/5198 [2:24:21<2:35:43, 111.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.35s/it][A100%|██████████| 1/1 [01:34<00:00, 94.36s/it]
 98%|█████████▊| 5114/5198 [2:23:46<2:35:43, 111.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.45s/it][A100%|██████████| 1/1 [01:34<00:00, 94.45s/it]
 98%|█████████▊| 5114/5198 [2:27:01<2:35:44, 111.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.44s/it][A100%|██████████| 1/1 [01:34<00:00, 94.45s/it]
 98%|█████████▊| 5114/5198 [2:26:26<2:35:44, 111.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.44s/it][A100%|██████████| 1/1 [01:34<00:00, 94.44s/it]
 98%|█████████▊| 5114/5198 [2:24:01<2:35:44, 111.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.41s/it][A100%|██████████| 1/1 [01:34<00:00, 94.41s/it]
 98%|█████████▊| 5114/5198 [2:21:13<2:35:44, 111.24s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4796
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.85s/it][A100%|██████████| 1/1 [01:50<00:00, 110.85s/it]
 98%|█████████▊| 5115/5198 [2:24:44<2:33:58, 111.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:02:08,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=5084, skipped=0, lr=[6.398240798244e-07], mom=[(0.9, 0.999)]
steps: 5084 loss: 0.6283 iter time (s): 110.686 samples/sec: 1.156

100%|██████████| 1/1 [01:51<00:00, 111.52s/it][A100%|██████████| 1/1 [01:51<00:00, 111.52s/it]
 98%|█████████▊| 5115/5198 [2:28:18<2:34:00, 111.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.57s/it][A100%|██████████| 1/1 [01:51<00:00, 111.57s/it]
 98%|█████████▊| 5115/5198 [2:26:13<2:34:00, 111.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.53s/it][A100%|██████████| 1/1 [01:51<00:00, 111.53s/it]
 98%|█████████▊| 5115/5198 [2:25:38<2:33:59, 111.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.46s/it][A100%|██████████| 1/1 [01:51<00:00, 111.46s/it]
 98%|█████████▊| 5115/5198 [2:28:53<2:33:59, 111.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.49s/it][A100%|██████████| 1/1 [01:51<00:00, 111.49s/it]
 98%|█████████▊| 5115/5198 [2:28:17<2:33:59, 111.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.53s/it][A100%|██████████| 1/1 [01:51<00:00, 111.53s/it]
 98%|█████████▊| 5115/5198 [2:25:53<2:34:00, 111.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.54s/it][A100%|██████████| 1/1 [01:51<00:00, 111.54s/it]
 98%|█████████▊| 5115/5198 [2:23:04<2:34:00, 111.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4797
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.64s/it][A100%|██████████| 1/1 [01:32<00:00, 92.64s/it]
 98%|█████████▊| 5116/5198 [2:26:17<2:24:32, 105.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:03:40,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=5085, skipped=0, lr=[6.378351874018176e-07], mom=[(0.9, 0.999)]
steps: 5085 loss: 0.6771 iter time (s): 91.434 samples/sec: 1.400

100%|██████████| 1/1 [01:32<00:00, 92.23s/it][A100%|██████████| 1/1 [01:32<00:00, 92.23s/it]
 98%|█████████▊| 5116/5198 [2:29:50<2:24:19, 105.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.16s/it][A100%|██████████| 1/1 [01:32<00:00, 92.16s/it]
 98%|█████████▊| 5116/5198 [2:27:45<2:24:18, 105.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.17s/it][A100%|██████████| 1/1 [01:32<00:00, 92.17s/it]
 98%|█████████▊| 5116/5198 [2:27:10<2:24:17, 105.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.27s/it][A100%|██████████| 1/1 [01:32<00:00, 92.28s/it]
 98%|█████████▊| 5116/5198 [2:30:25<2:24:19, 105.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.22s/it][A100%|██████████| 1/1 [01:32<00:00, 92.22s/it]
 98%|█████████▊| 5116/5198 [2:29:49<2:24:18, 105.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.24s/it][A100%|██████████| 1/1 [01:32<00:00, 92.24s/it]
 98%|█████████▊| 5116/5198 [2:27:25<2:24:19, 105.61s/it]
100%|██████████| 1/1 [01:32<00:00, 92.22s/it][A100%|██████████| 1/1 [01:32<00:00, 92.22s/it]
 98%|█████████▊| 5116/5198 [2:24:37<2:24:19, 105.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4798

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.57s/it][A100%|██████████| 1/1 [01:26<00:00, 86.57s/it]
 98%|█████████▊| 5117/5198 [2:27:44<2:15:05, 100.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:05:07,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=5086, skipped=0, lr=[6.358492989149292e-07], mom=[(0.9, 0.999)]
steps: 5086 loss: 0.5576 iter time (s): 85.819 samples/sec: 1.492

100%|██████████| 1/1 [01:26<00:00, 86.54s/it][A100%|██████████| 1/1 [01:26<00:00, 86.55s/it]
 98%|█████████▊| 5117/5198 [2:31:17<2:14:51, 99.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.59s/it][A100%|██████████| 1/1 [01:26<00:00, 86.59s/it]
 98%|█████████▊| 5117/5198 [2:29:11<2:14:51, 99.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.60s/it][A100%|██████████| 1/1 [01:26<00:00, 86.60s/it]
 98%|█████████▊| 5117/5198 [2:28:36<2:14:51, 99.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.58s/it][A100%|██████████| 1/1 [01:26<00:00, 86.58s/it]
 98%|█████████▊| 5117/5198 [2:31:51<2:14:52, 99.90s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.59s/it][A100%|██████████| 1/1 [01:26<00:00, 86.59s/it]
 98%|█████████▊| 5117/5198 [2:31:16<2:14:51, 99.90s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.55s/it][A100%|██████████| 1/1 [01:26<00:00, 86.55s/it]
 98%|█████████▊| 5117/5198 [2:28:52<2:14:51, 99.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.55s/it][A100%|██████████| 1/1 [01:26<00:00, 86.55s/it]
 98%|█████████▊| 5117/5198 [2:26:03<2:14:51, 99.89s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4799
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.79s/it][A100%|██████████| 1/1 [01:30<00:00, 90.79s/it]
 98%|█████████▊| 5118/5198 [2:29:15<2:09:48, 97.35s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:06:38,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=5087, skipped=0, lr=[6.33866415000855e-07], mom=[(0.9, 0.999)]
steps: 5087 loss: 0.5805 iter time (s): 90.369 samples/sec: 1.416

100%|██████████| 1/1 [01:31<00:00, 91.15s/it][A100%|██████████| 1/1 [01:31<00:00, 91.15s/it]
 98%|█████████▊| 5118/5198 [2:32:48<2:09:41, 97.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.30s/it][A100%|██████████| 1/1 [01:31<00:00, 91.30s/it]
 98%|█████████▊| 5118/5198 [2:30:43<2:09:45, 97.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.20s/it][A100%|██████████| 1/1 [01:31<00:00, 91.20s/it]
 98%|█████████▊| 5118/5198 [2:30:08<2:09:42, 97.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.18s/it][A100%|██████████| 1/1 [01:31<00:00, 91.18s/it]
 98%|█████████▊| 5118/5198 [2:33:23<2:09:43, 97.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.18s/it][A100%|██████████| 1/1 [01:31<00:00, 91.18s/it]
 98%|█████████▊| 5118/5198 [2:32:47<2:09:42, 97.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.21s/it][A100%|██████████| 1/1 [01:31<00:00, 91.21s/it]
 98%|█████████▊| 5118/5198 [2:30:23<2:09:43, 97.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.23s/it][A100%|██████████| 1/1 [01:31<00:00, 91.23s/it]
 98%|█████████▊| 5118/5198 [2:27:34<2:09:43, 97.29s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4800
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.11s/it][A100%|██████████| 1/1 [01:30<00:00, 90.11s/it]
 98%|█████████▊| 5119/5198 [2:30:45<2:05:23, 95.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:08:08,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=5088, skipped=0, lr=[6.318865362957553e-07], mom=[(0.9, 0.999)]
steps: 5088 loss: 0.6212 iter time (s): 89.393 samples/sec: 1.432

100%|██████████| 1/1 [01:30<00:00, 90.30s/it][A100%|██████████| 1/1 [01:30<00:00, 90.30s/it]
 98%|█████████▊| 5119/5198 [2:34:18<2:05:19, 95.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.15s/it][A100%|██████████| 1/1 [01:30<00:00, 90.15s/it]
 98%|█████████▊| 5119/5198 [2:32:13<2:05:18, 95.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.33s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
 98%|█████████▊| 5119/5198 [2:31:38<2:05:20, 95.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.30s/it][A100%|██████████| 1/1 [01:30<00:00, 90.30s/it]
 98%|█████████▊| 5119/5198 [2:34:53<2:05:20, 95.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.31s/it][A100%|██████████| 1/1 [01:30<00:00, 90.31s/it]
 98%|█████████▊| 5119/5198 [2:34:18<2:05:20, 95.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.29s/it][A100%|██████████| 1/1 [01:30<00:00, 90.29s/it]
 98%|█████████▊| 5119/5198 [2:31:53<2:05:20, 95.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.27s/it][A100%|██████████| 1/1 [01:30<00:00, 90.27s/it]
 98%|█████████▊| 5119/5198 [2:29:05<2:05:19, 95.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4801
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.78s/it][A100%|██████████| 1/1 [01:27<00:00, 87.78s/it]
 98%|█████████▊| 5120/5198 [2:32:13<2:00:57, 93.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:09:36,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=5089, skipped=0, lr=[6.299096634348221e-07], mom=[(0.9, 0.999)]
steps: 5089 loss: 0.6195 iter time (s): 87.018 samples/sec: 1.471

100%|██████████| 1/1 [01:27<00:00, 87.79s/it][A100%|██████████| 1/1 [01:27<00:00, 87.79s/it]
 98%|█████████▊| 5120/5198 [2:35:46<2:00:51, 92.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.91s/it][A100%|██████████| 1/1 [01:27<00:00, 87.91s/it]
 98%|█████████▊| 5120/5198 [2:33:41<2:00:53, 92.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.74s/it][A100%|██████████| 1/1 [01:27<00:00, 87.74s/it]
 98%|█████████▊| 5120/5198 [2:33:06<2:00:51, 92.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.83s/it][A100%|██████████| 1/1 [01:27<00:00, 87.83s/it]
 98%|█████████▊| 5120/5198 [2:36:21<2:00:53, 92.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.85s/it][A100%|██████████| 1/1 [01:27<00:00, 87.85s/it]
 98%|█████████▊| 5120/5198 [2:35:45<2:00:53, 92.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.84s/it][A100%|██████████| 1/1 [01:27<00:00, 87.84s/it]
 98%|█████████▊| 5120/5198 [2:33:21<2:00:53, 92.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.85s/it][A100%|██████████| 1/1 [01:27<00:00, 87.85s/it]
 98%|█████████▊| 5120/5198 [2:30:33<2:00:53, 92.99s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4802
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.89s/it][A100%|██████████| 1/1 [02:10<00:00, 130.89s/it]
 99%|█████████▊| 5121/5198 [2:34:24<2:14:02, 104.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:11:49,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=5090, skipped=0, lr=[6.279357970522867e-07], mom=[(0.9, 0.999)]
steps: 5090 loss: 0.6172 iter time (s): 131.461 samples/sec: 0.974

100%|██████████| 1/1 [02:12<00:00, 132.25s/it][A100%|██████████| 1/1 [02:12<00:00, 132.25s/it]
 99%|█████████▊| 5121/5198 [2:37:58<2:14:26, 104.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.25s/it][A100%|██████████| 1/1 [02:12<00:00, 132.25s/it]
 99%|█████████▊| 5121/5198 [2:35:53<2:14:27, 104.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.35s/it][A100%|██████████| 1/1 [02:12<00:00, 132.35s/it]
 99%|█████████▊| 5121/5198 [2:35:18<2:14:28, 104.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.19s/it][A100%|██████████| 1/1 [02:12<00:00, 132.20s/it]
 99%|█████████▊| 5121/5198 [2:38:33<2:14:26, 104.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.22s/it][A100%|██████████| 1/1 [02:12<00:00, 132.22s/it]
 99%|█████████▊| 5121/5198 [2:37:58<2:14:26, 104.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.22s/it][A100%|██████████| 1/1 [02:12<00:00, 132.22s/it]
 99%|█████████▊| 5121/5198 [2:35:33<2:14:26, 104.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:12<00:00, 132.21s/it][A100%|██████████| 1/1 [02:12<00:00, 132.21s/it]
 99%|█████████▊| 5121/5198 [2:32:45<2:14:26, 104.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4803
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.11s/it][A100%|██████████| 1/1 [01:28<00:00, 88.11s/it]
[2024-06-26 20:13:16,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=5091, skipped=0, lr=[6.259649377814135e-07], mom=[(0.9, 0.999)]
steps: 5091 loss: 0.6063 iter time (s): 86.379 samples/sec: 1.482

100%|██████████| 1/1 [01:27<00:00, 87.22s/it][A100%|██████████| 1/1 [01:27<00:00, 87.22s/it]

100%|██████████| 1/1 [01:27<00:00, 87.19s/it][A100%|██████████| 1/1 [01:27<00:00, 87.19s/it]

100%|██████████| 1/1 [01:27<00:00, 87.10s/it][A100%|██████████| 1/1 [01:27<00:00, 87.10s/it]

100%|██████████| 1/1 [01:27<00:00, 87.22s/it][A100%|██████████| 1/1 [01:27<00:00, 87.22s/it]

100%|██████████| 1/1 [01:27<00:00, 87.18s/it][A100%|██████████| 1/1 [01:27<00:00, 87.18s/it]

100%|██████████| 1/1 [01:27<00:00, 87.17s/it][A100%|██████████| 1/1 [01:27<00:00, 87.17s/it]

100%|██████████| 1/1 [01:27<00:00, 87.21s/it][A100%|██████████| 1/1 [01:27<00:00, 87.21s/it]
Checkpointing at shard 5121
[2024-06-26 20:13:16,951] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5091 is about to be saved!
[2024-06-26 20:13:17,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_00-model_states.pt...
[2024-06-26 20:13:22,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_08-model_states.pt...
[2024-06-26 20:13:22,070] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_02-model_states.pt...
[2024-06-26 20:13:22,160] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_07-model_states.pt...
[2024-06-26 20:13:27,588] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_00-model_states.pt.
[2024-06-26 20:13:28,453] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_06-model_states.pt...
[2024-06-26 20:13:28,714] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_05-model_states.pt...
[2024-06-26 20:13:30,607] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_01-model_states.pt...
[2024-06-26 20:13:34,328] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_03-model_states.pt...
[2024-06-26 20:13:34,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_04-model_states.pt...
[2024-06-26 20:17:02,821] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_07-model_states.pt.
[2024-06-26 20:17:02,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_08-model_states.pt.
[2024-06-26 20:17:02,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_06_model_states.pt...
[2024-06-26 20:17:03,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_09-model_states.pt...
[2024-06-26 20:17:03,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_06_model_states.pt.
[2024-06-26 20:17:03,029] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
[2024-06-26 20:17:06,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_09-model_states.pt.
[2024-06-26 20:17:06,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_07_model_states.pt...
[2024-06-26 20:17:07,021] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_07_model_states.pt.
[2024-06-26 20:17:07,021] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
[2024-06-26 20:17:14,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_02-model_states.pt.
[2024-06-26 20:17:14,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_04-model_states.pt.
[2024-06-26 20:17:14,760] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_01_model_states.pt
[2024-06-26 20:17:14,760] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_01_model_states.pt...
[2024-06-26 20:17:14,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_01_model_states.pt.
[2024-06-26 20:17:14,894] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
[2024-06-26 20:17:15,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_03_model_states.pt...
[2024-06-26 20:17:15,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_03_model_states.pt.
[2024-06-26 20:17:15,178] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
[2024-06-26 20:17:16,728] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_01-model_states.pt.
[2024-06-26 20:17:16,765] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_00_model_states.pt
[2024-06-26 20:17:16,765] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_00_model_states.pt...
[2024-06-26 20:17:20,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_00_model_states.pt.
[2024-06-26 20:17:20,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
[2024-06-26 20:17:22,224] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_05-model_states.pt.
[2024-06-26 20:17:22,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_04_model_states.pt...
[2024-06-26 20:17:23,638] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_04_model_states.pt.
[2024-06-26 20:17:23,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
[2024-06-26 20:17:24,945] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_03-model_states.pt.
[2024-06-26 20:17:25,007] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/layer_06-model_states.pt.
[2024-06-26 20:17:25,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_05_model_states.pt...
[2024-06-26 20:17:25,235] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_02_model_states.pt...
[2024-06-26 20:17:25,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_05_model_states.pt.
[2024-06-26 20:17:25,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
[2024-06-26 20:17:25,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5091/mp_rank_02_model_states.pt.
[2024-06-26 20:17:25,334] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5091 is ready now!
Checkpoint saved using --- 248.38580632209778 seconds ---
 99%|█████████▊| 5122/5198 [2:43:33<3:40:25, 174.02s/it] 99%|█████████▊| 5122/5198 [2:40:03<3:41:37, 174.97s/it] 99%|█████████▊| 5122/5198 [2:40:54<3:40:29, 174.07s/it] 99%|█████████▊| 5122/5198 [2:41:29<3:40:31, 174.10s/it] 99%|█████████▊| 5122/5198 [2:44:09<3:40:27, 174.04s/it] 99%|█████████▊| 5122/5198 [2:38:20<3:40:24, 174.01s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4804
 99%|█████████▊| 5122/5198 [2:43:34<3:40:35, 174.15s/it] 99%|█████████▊| 5122/5198 [2:41:09<3:40:25, 174.01s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.10s/it][A100%|██████████| 1/1 [01:43<00:00, 103.10s/it]
 99%|█████████▊| 5123/5198 [2:41:47<3:11:54, 153.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:19:11,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=5092, skipped=0, lr=[6.239970862545048e-07], mom=[(0.9, 0.999)]
steps: 5092 loss: 0.6275 iter time (s): 105.598 samples/sec: 1.212

100%|██████████| 1/1 [01:45<00:00, 105.96s/it][A100%|██████████| 1/1 [01:45<00:00, 105.96s/it]
 99%|█████████▊| 5123/5198 [2:45:21<3:12:15, 153.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.11s/it][A100%|██████████| 1/1 [01:46<00:00, 106.11s/it]
 99%|█████████▊| 5123/5198 [2:43:15<3:12:16, 153.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.15s/it][A100%|██████████| 1/1 [01:46<00:00, 106.15s/it]
 99%|█████████▊| 5123/5198 [2:42:40<3:12:15, 153.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.30s/it][A100%|██████████| 1/1 [01:46<00:00, 106.30s/it]
 99%|█████████▊| 5123/5198 [2:45:55<3:12:17, 153.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.38s/it][A100%|██████████| 1/1 [01:46<00:00, 106.39s/it]
 99%|█████████▊| 5123/5198 [2:45:20<3:12:18, 153.85s/it]
100%|██████████| 1/1 [01:46<00:00, 106.39s/it][A100%|██████████| 1/1 [01:46<00:00, 106.39s/it]
 99%|█████████▊| 5123/5198 [2:42:56<3:12:17, 153.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.43s/it][A100%|██████████| 1/1 [01:46<00:00, 106.43s/it]
 99%|█████████▊| 5123/5198 [2:40:07<3:12:18, 153.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4805
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.65s/it][A100%|██████████| 1/1 [01:55<00:00, 115.65s/it]
 99%|█████████▊| 5124/5198 [2:43:43<2:55:24, 142.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:21:07,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=5093, skipped=0, lr=[6.220322431028945e-07], mom=[(0.9, 0.999)]
steps: 5093 loss: 0.6097 iter time (s): 115.247 samples/sec: 1.111

100%|██████████| 1/1 [01:56<00:00, 116.01s/it][A100%|██████████| 1/1 [01:56<00:00, 116.01s/it]
 99%|█████████▊| 5124/5198 [2:47:17<2:55:42, 142.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.02s/it][A100%|██████████| 1/1 [01:56<00:00, 116.02s/it]
 99%|█████████▊| 5124/5198 [2:45:11<2:55:43, 142.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.16s/it][A100%|██████████| 1/1 [01:56<00:00, 116.16s/it]
 99%|█████████▊| 5124/5198 [2:44:36<2:55:45, 142.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.03s/it][A100%|██████████| 1/1 [01:56<00:00, 116.03s/it]
 99%|█████████▊| 5124/5198 [2:47:51<2:55:44, 142.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.04s/it][A100%|██████████| 1/1 [01:56<00:00, 116.04s/it]
 99%|█████████▊| 5124/5198 [2:47:16<2:55:45, 142.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.06s/it][A100%|██████████| 1/1 [01:56<00:00, 116.06s/it]
 99%|█████████▊| 5124/5198 [2:44:52<2:55:45, 142.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.04s/it][A100%|██████████| 1/1 [01:56<00:00, 116.04s/it]
 99%|█████████▊| 5124/5198 [2:42:03<2:55:45, 142.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4806
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.93s/it][A100%|██████████| 1/1 [01:34<00:00, 94.93s/it]
 99%|█████████▊| 5125/5198 [2:45:18<2:35:51, 128.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:22:41,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=5094, skipped=0, lr=[6.200704089569551e-07], mom=[(0.9, 0.999)]
steps: 5094 loss: 0.6080 iter time (s): 93.738 samples/sec: 1.366

100%|██████████| 1/1 [01:34<00:00, 94.59s/it][A100%|██████████| 1/1 [01:34<00:00, 94.59s/it]
 99%|█████████▊| 5125/5198 [2:48:51<2:35:51, 128.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.54s/it][A100%|██████████| 1/1 [01:34<00:00, 94.54s/it]
 99%|█████████▊| 5125/5198 [2:46:46<2:35:51, 128.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.49s/it][A100%|██████████| 1/1 [01:34<00:00, 94.49s/it]
 99%|█████████▊| 5125/5198 [2:46:11<2:35:52, 128.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.63s/it][A100%|██████████| 1/1 [01:34<00:00, 94.63s/it]
 99%|█████████▊| 5125/5198 [2:49:26<2:35:54, 128.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.60s/it][A100%|██████████| 1/1 [01:34<00:00, 94.60s/it]
 99%|█████████▊| 5125/5198 [2:48:51<2:35:53, 128.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.59s/it][A100%|██████████| 1/1 [01:34<00:00, 94.59s/it]
 99%|█████████▊| 5125/5198 [2:46:26<2:35:53, 128.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.60s/it][A100%|██████████| 1/1 [01:34<00:00, 94.60s/it]
 99%|█████████▊| 5125/5198 [2:43:38<2:35:53, 128.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4807
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.58s/it][A100%|██████████| 1/1 [01:45<00:00, 105.58s/it]
 99%|█████████▊| 5126/5198 [2:47:04<2:25:42, 121.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:24:28,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=5095, skipped=0, lr=[6.181115844460892e-07], mom=[(0.9, 0.999)]
steps: 5095 loss: 0.5928 iter time (s): 105.282 samples/sec: 1.216

100%|██████████| 1/1 [01:46<00:00, 106.17s/it][A100%|██████████| 1/1 [01:46<00:00, 106.17s/it]
 99%|█████████▊| 5126/5198 [2:50:38<2:25:50, 121.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.10s/it][A100%|██████████| 1/1 [01:46<00:00, 106.10s/it]
 99%|█████████▊| 5126/5198 [2:48:32<2:25:48, 121.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.04s/it][A100%|██████████| 1/1 [01:46<00:00, 106.04s/it]
 99%|█████████▊| 5126/5198 [2:51:12<2:25:48, 121.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.27s/it][A100%|██████████| 1/1 [01:46<00:00, 106.27s/it]
 99%|█████████▊| 5126/5198 [2:47:57<2:25:52, 121.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.12s/it][A100%|██████████| 1/1 [01:46<00:00, 106.12s/it]
 99%|█████████▊| 5126/5198 [2:50:37<2:25:50, 121.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.13s/it][A100%|██████████| 1/1 [01:46<00:00, 106.13s/it]
 99%|█████████▊| 5126/5198 [2:48:13<2:25:50, 121.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.14s/it][A100%|██████████| 1/1 [01:46<00:00, 106.14s/it]
 99%|█████████▊| 5126/5198 [2:45:24<2:25:50, 121.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4808
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.81s/it][A100%|██████████| 1/1 [01:30<00:00, 90.81s/it]
 99%|█████████▊| 5127/5198 [2:48:35<2:12:52, 112.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:25:58,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=5096, skipped=0, lr=[6.161557701987377e-07], mom=[(0.9, 0.999)]
steps: 5096 loss: 0.6002 iter time (s): 89.720 samples/sec: 1.427

100%|██████████| 1/1 [01:30<00:00, 90.52s/it][A100%|██████████| 1/1 [01:30<00:00, 90.52s/it]
 99%|█████████▊| 5127/5198 [2:52:08<2:12:48, 112.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.65s/it][A100%|██████████| 1/1 [01:30<00:00, 90.65s/it]
 99%|█████████▊| 5127/5198 [2:50:03<2:12:49, 112.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.52s/it][A100%|██████████| 1/1 [01:30<00:00, 90.52s/it]
 99%|█████████▊| 5127/5198 [2:49:28<2:12:49, 112.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.67s/it][A100%|██████████| 1/1 [01:30<00:00, 90.67s/it]
 99%|█████████▊| 5127/5198 [2:52:43<2:12:50, 112.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.64s/it][A100%|██████████| 1/1 [01:30<00:00, 90.64s/it]
 99%|█████████▊| 5127/5198 [2:52:07<2:12:51, 112.27s/it]
100%|██████████| 1/1 [01:30<00:00, 90.62s/it][A100%|██████████| 1/1 [01:30<00:00, 90.62s/it]
 99%|█████████▊| 5127/5198 [2:49:43<2:12:50, 112.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.63s/it][A100%|██████████| 1/1 [01:30<00:00, 90.63s/it]
 99%|█████████▊| 5127/5198 [2:46:55<2:12:51, 112.27s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4809
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.77s/it][A100%|██████████| 1/1 [01:24<00:00, 84.77s/it]
 99%|█████████▊| 5128/5198 [2:50:00<2:01:25, 104.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:27:23,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=5097, skipped=0, lr=[6.14202966842374e-07], mom=[(0.9, 0.999)]
steps: 5097 loss: 0.6001 iter time (s): 83.816 samples/sec: 1.527

100%|██████████| 1/1 [01:24<00:00, 84.74s/it][A100%|██████████| 1/1 [01:24<00:00, 84.74s/it]
 99%|█████████▊| 5128/5198 [2:53:33<2:01:19, 103.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.69s/it][A100%|██████████| 1/1 [01:24<00:00, 84.69s/it]
 99%|█████████▊| 5128/5198 [2:51:27<2:01:19, 103.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.69s/it][A100%|██████████| 1/1 [01:24<00:00, 84.69s/it]
 99%|█████████▊| 5128/5198 [2:50:52<2:01:19, 103.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.63s/it][A100%|██████████| 1/1 [01:24<00:00, 84.63s/it]
 99%|█████████▊| 5128/5198 [2:54:07<2:01:18, 103.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.55s/it][A100%|██████████| 1/1 [01:24<00:00, 84.55s/it]
 99%|█████████▊| 5128/5198 [2:53:32<2:01:17, 103.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.64s/it][A100%|██████████| 1/1 [01:24<00:00, 84.65s/it]
 99%|█████████▊| 5128/5198 [2:51:08<2:01:18, 103.98s/it]
100%|██████████| 1/1 [01:24<00:00, 84.61s/it][A100%|██████████| 1/1 [01:24<00:00, 84.61s/it]
 99%|█████████▊| 5128/5198 [2:48:19<2:01:18, 103.97s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4810
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.90s/it][A100%|██████████| 1/1 [01:45<00:00, 105.90s/it]
 99%|█████████▊| 5129/5198 [2:51:46<2:00:23, 104.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:29:10,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=5098, skipped=0, lr=[6.122531750035061e-07], mom=[(0.9, 0.999)]
steps: 5098 loss: 0.5865 iter time (s): 105.934 samples/sec: 1.208

100%|██████████| 1/1 [01:46<00:00, 106.72s/it][A100%|██████████| 1/1 [01:46<00:00, 106.72s/it]
 99%|█████████▊| 5129/5198 [2:55:20<2:00:32, 104.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.77s/it][A100%|██████████| 1/1 [01:46<00:00, 106.77s/it]
 99%|█████████▊| 5129/5198 [2:53:14<2:00:33, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.74s/it][A100%|██████████| 1/1 [01:46<00:00, 106.74s/it]
 99%|█████████▊| 5129/5198 [2:52:39<2:00:32, 104.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.81s/it][A100%|██████████| 1/1 [01:46<00:00, 106.81s/it]
 99%|█████████▊| 5129/5198 [2:55:54<2:00:33, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.84s/it][A100%|██████████| 1/1 [01:46<00:00, 106.84s/it]
 99%|█████████▊| 5129/5198 [2:55:19<2:00:32, 104.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.78s/it][A100%|██████████| 1/1 [01:46<00:00, 106.78s/it]
 99%|█████████▊| 5129/5198 [2:52:55<2:00:32, 104.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.81s/it][A100%|██████████| 1/1 [01:46<00:00, 106.81s/it]
 99%|█████████▊| 5129/5198 [2:50:06<2:00:33, 104.83s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4811
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.92s/it][A100%|██████████| 1/1 [01:40<00:00, 100.92s/it]
 99%|█████████▊| 5130/5198 [2:53:27<1:57:26, 103.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:30:51,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=5099, skipped=0, lr=[6.103063953076777e-07], mom=[(0.9, 0.999)]
steps: 5099 loss: 0.5711 iter time (s): 100.123 samples/sec: 1.278

100%|██████████| 1/1 [01:40<00:00, 100.94s/it][A100%|██████████| 1/1 [01:40<00:00, 100.94s/it]
 99%|█████████▊| 5130/5198 [2:57:01<1:57:28, 103.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.97s/it][A100%|██████████| 1/1 [01:40<00:00, 100.97s/it]
 99%|█████████▊| 5130/5198 [2:54:55<1:57:29, 103.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.92s/it][A100%|██████████| 1/1 [01:40<00:00, 100.93s/it]
 99%|█████████▊| 5130/5198 [2:54:20<1:57:28, 103.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.87s/it][A100%|██████████| 1/1 [01:40<00:00, 100.87s/it]
 99%|█████████▊| 5130/5198 [2:57:35<1:57:27, 103.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.95s/it][A100%|██████████| 1/1 [01:40<00:00, 100.95s/it]
 99%|█████████▊| 5130/5198 [2:57:00<1:57:29, 103.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.92s/it][A100%|██████████| 1/1 [01:40<00:00, 100.92s/it]
 99%|█████████▊| 5130/5198 [2:54:36<1:57:28, 103.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.93s/it][A100%|██████████| 1/1 [01:40<00:00, 100.93s/it]
 99%|█████████▊| 5130/5198 [2:51:47<1:57:28, 103.66s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4812
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.70s/it][A100%|██████████| 1/1 [01:28<00:00, 88.70s/it]
 99%|█████████▊| 5131/5198 [2:54:56<1:50:47, 99.21s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:32:19,686] [INFO] [logging.py:96:log_dist] [Rank 0] step=5100, skipped=0, lr=[6.083626283794593e-07], mom=[(0.9, 0.999)]
steps: 5100 loss: 0.6195 iter time (s): 87.716 samples/sec: 1.459

100%|██████████| 1/1 [01:28<00:00, 88.57s/it][A100%|██████████| 1/1 [01:28<00:00, 88.57s/it]
 99%|█████████▊| 5131/5198 [2:58:29<1:50:41, 99.13s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.45s/it][A100%|██████████| 1/1 [01:28<00:00, 88.45s/it]
 99%|█████████▊| 5131/5198 [2:56:24<1:50:40, 99.11s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.55s/it][A100%|██████████| 1/1 [01:28<00:00, 88.55s/it]
 99%|█████████▊| 5131/5198 [2:55:49<1:50:41, 99.12s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.53s/it][A100%|██████████| 1/1 [01:28<00:00, 88.53s/it]
 99%|█████████▊| 5131/5198 [2:59:04<1:50:40, 99.11s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.47s/it][A100%|██████████| 1/1 [01:28<00:00, 88.47s/it]
 99%|█████████▊| 5131/5198 [2:58:28<1:50:40, 99.11s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.52s/it][A100%|██████████| 1/1 [01:28<00:00, 88.52s/it]
 99%|█████████▊| 5131/5198 [2:56:04<1:50:40, 99.12s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.49s/it][A100%|██████████| 1/1 [01:28<00:00, 88.49s/it]
 99%|█████████▊| 5131/5198 [2:53:15<1:50:40, 99.11s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4813
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.63s/it][A100%|██████████| 1/1 [01:46<00:00, 106.63s/it]
 99%|█████████▊| 5132/5198 [2:56:43<1:51:38, 101.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:34:07,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=5101, skipped=0, lr=[6.064218748424635e-07], mom=[(0.9, 0.999)]
steps: 5101 loss: 0.6052 iter time (s): 106.562 samples/sec: 1.201

100%|██████████| 1/1 [01:47<00:00, 107.41s/it][A100%|██████████| 1/1 [01:47<00:00, 107.41s/it]
 99%|█████████▊| 5132/5198 [3:00:17<1:51:46, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.42s/it][A100%|██████████| 1/1 [01:47<00:00, 107.42s/it]
 99%|█████████▊| 5132/5198 [2:58:11<1:51:45, 101.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.40s/it][A100%|██████████| 1/1 [01:47<00:00, 107.40s/it]
 99%|█████████▊| 5132/5198 [3:00:51<1:51:45, 101.60s/it]
100%|██████████| 1/1 [01:47<00:00, 107.44s/it][A100%|██████████| 1/1 [01:47<00:00, 107.44s/it]
 99%|█████████▊| 5132/5198 [2:57:36<1:51:46, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.43s/it][A100%|██████████| 1/1 [01:47<00:00, 107.43s/it]
 99%|█████████▊| 5132/5198 [2:57:52<1:51:46, 101.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.51s/it][A100%|██████████| 1/1 [01:47<00:00, 107.51s/it]
 99%|█████████▊| 5132/5198 [3:00:16<1:51:47, 101.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.46s/it][A100%|██████████| 1/1 [01:47<00:00, 107.47s/it]
 99%|█████████▊| 5132/5198 [2:55:03<1:51:46, 101.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4814
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.30s/it][A100%|██████████| 1/1 [01:28<00:00, 88.30s/it]
 99%|█████████▊| 5133/5198 [2:58:11<1:45:44, 97.61s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:35:35,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=5102, skipped=0, lr=[6.044841353193302e-07], mom=[(0.9, 0.999)]
steps: 5102 loss: 0.5997 iter time (s): 87.078 samples/sec: 1.470

100%|██████████| 1/1 [01:27<00:00, 87.87s/it][A100%|██████████| 1/1 [01:27<00:00, 87.87s/it]
 99%|█████████▊| 5133/5198 [3:01:44<1:45:37, 97.50s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.89s/it][A100%|██████████| 1/1 [01:27<00:00, 87.89s/it]
 99%|█████████▊| 5133/5198 [2:59:39<1:45:37, 97.50s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.90s/it][A100%|██████████| 1/1 [01:27<00:00, 87.90s/it]
 99%|█████████▊| 5133/5198 [2:59:04<1:45:37, 97.51s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.94s/it][A100%|██████████| 1/1 [01:27<00:00, 87.94s/it]
 99%|█████████▊| 5133/5198 [3:02:19<1:45:37, 97.51s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.84s/it][A100%|██████████| 1/1 [01:27<00:00, 87.84s/it]
 99%|█████████▊| 5133/5198 [3:01:44<1:45:37, 97.50s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.90s/it][A100%|██████████| 1/1 [01:27<00:00, 87.90s/it]
 99%|█████████▊| 5133/5198 [2:59:19<1:45:37, 97.50s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.89s/it][A100%|██████████| 1/1 [01:27<00:00, 87.89s/it]
 99%|█████████▊| 5133/5198 [2:56:31<1:45:37, 97.50s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4815
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.79s/it][A100%|██████████| 1/1 [01:54<00:00, 114.79s/it]
 99%|█████████▉| 5134/5198 [3:00:06<1:49:40, 102.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:37:30,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=5103, skipped=0, lr=[6.025494104317345e-07], mom=[(0.9, 0.999)]
steps: 5103 loss: 0.6163 iter time (s): 114.968 samples/sec: 1.113

100%|██████████| 1/1 [01:55<00:00, 115.80s/it][A100%|██████████| 1/1 [01:55<00:00, 115.80s/it]
 99%|█████████▉| 5134/5198 [3:03:40<1:49:51, 102.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.80s/it][A100%|██████████| 1/1 [01:55<00:00, 115.80s/it]
 99%|█████████▉| 5134/5198 [3:01:35<1:49:51, 102.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.82s/it][A100%|██████████| 1/1 [01:55<00:00, 115.82s/it]
 99%|█████████▉| 5134/5198 [3:01:00<1:49:52, 103.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.92s/it][A100%|██████████| 1/1 [01:55<00:00, 115.92s/it]
 99%|█████████▉| 5134/5198 [3:04:15<1:49:54, 103.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.86s/it][A100%|██████████| 1/1 [01:55<00:00, 115.86s/it]
 99%|█████████▉| 5134/5198 [3:03:40<1:49:52, 103.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.87s/it][A100%|██████████| 1/1 [01:55<00:00, 115.87s/it]
 99%|█████████▉| 5134/5198 [3:01:15<1:49:52, 103.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.86s/it][A100%|██████████| 1/1 [01:55<00:00, 115.86s/it]
 99%|█████████▉| 5134/5198 [2:58:27<1:49:52, 103.01s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4816
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.12s/it][A100%|██████████| 1/1 [01:41<00:00, 101.12s/it]
 99%|█████████▉| 5135/5198 [3:01:48<1:47:31, 102.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:39:11,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=5104, skipped=0, lr=[6.006177008003875e-07], mom=[(0.9, 0.999)]
steps: 5104 loss: 0.6088 iter time (s): 100.160 samples/sec: 1.278

100%|██████████| 1/1 [01:40<00:00, 100.96s/it][A100%|██████████| 1/1 [01:40<00:00, 100.96s/it]
 99%|█████████▉| 5135/5198 [3:05:21<1:47:30, 102.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.06s/it][A100%|██████████| 1/1 [01:41<00:00, 101.06s/it]
 99%|█████████▉| 5135/5198 [3:03:16<1:47:31, 102.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.97s/it][A100%|██████████| 1/1 [01:40<00:00, 100.97s/it]
 99%|█████████▉| 5135/5198 [3:02:41<1:47:30, 102.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.93s/it][A100%|██████████| 1/1 [01:40<00:00, 100.94s/it]
 99%|█████████▉| 5135/5198 [3:05:56<1:47:31, 102.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.98s/it][A100%|██████████| 1/1 [01:40<00:00, 100.98s/it]
 99%|█████████▉| 5135/5198 [3:05:21<1:47:31, 102.40s/it]
100%|██████████| 1/1 [01:40<00:00, 100.94s/it][A100%|██████████| 1/1 [01:40<00:00, 100.94s/it]
 99%|█████████▉| 5135/5198 [3:02:56<1:47:30, 102.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.94s/it][A100%|██████████| 1/1 [01:40<00:00, 100.94s/it]
 99%|█████████▉| 5135/5198 [3:00:08<1:47:30, 102.39s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4817
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.86s/it][A100%|██████████| 1/1 [01:56<00:00, 116.86s/it]
 99%|█████████▉| 5136/5198 [3:03:45<1:50:20, 106.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:41:09,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=5105, skipped=0, lr=[5.986890070450249e-07], mom=[(0.9, 0.999)]
steps: 5105 loss: 0.5750 iter time (s): 116.639 samples/sec: 1.097

100%|██████████| 1/1 [01:57<00:00, 117.52s/it][A100%|██████████| 1/1 [01:57<00:00, 117.52s/it]
 99%|█████████▉| 5136/5198 [3:07:19<1:50:29, 106.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.47s/it][A100%|██████████| 1/1 [01:57<00:00, 117.47s/it]
 99%|█████████▉| 5136/5198 [3:05:13<1:50:29, 106.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.51s/it][A100%|██████████| 1/1 [01:57<00:00, 117.51s/it]
 99%|█████████▉| 5136/5198 [3:04:38<1:50:29, 106.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.51s/it][A100%|██████████| 1/1 [01:57<00:00, 117.51s/it]
 99%|█████████▉| 5136/5198 [3:07:53<1:50:30, 106.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.47s/it][A100%|██████████| 1/1 [01:57<00:00, 117.47s/it]
 99%|█████████▉| 5136/5198 [3:07:18<1:50:29, 106.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.49s/it][A100%|██████████| 1/1 [01:57<00:00, 117.49s/it]
 99%|█████████▉| 5136/5198 [3:04:54<1:50:29, 106.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.50s/it][A100%|██████████| 1/1 [01:57<00:00, 117.50s/it]
 99%|█████████▉| 5136/5198 [3:02:05<1:50:29, 106.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4818
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.26s/it][A100%|██████████| 1/1 [01:30<00:00, 90.26s/it]
 99%|█████████▉| 5137/5198 [3:05:15<1:43:33, 101.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:42:38,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=5106, skipped=0, lr=[5.967633297844209e-07], mom=[(0.9, 0.999)]
steps: 5106 loss: 0.6120 iter time (s): 88.762 samples/sec: 1.442

100%|██████████| 1/1 [01:29<00:00, 89.78s/it][A100%|██████████| 1/1 [01:29<00:00, 89.78s/it]
 99%|█████████▉| 5137/5198 [3:08:49<1:43:29, 101.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.73s/it][A100%|██████████| 1/1 [01:29<00:00, 89.73s/it]
 99%|█████████▉| 5137/5198 [3:06:43<1:43:28, 101.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.68s/it][A100%|██████████| 1/1 [01:29<00:00, 89.68s/it]
 99%|█████████▉| 5137/5198 [3:06:08<1:43:27, 101.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.66s/it][A100%|██████████| 1/1 [01:29<00:00, 89.66s/it]
 99%|█████████▉| 5137/5198 [3:09:23<1:43:27, 101.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.76s/it][A100%|██████████| 1/1 [01:29<00:00, 89.76s/it]
 99%|█████████▉| 5137/5198 [3:08:48<1:43:28, 101.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.76s/it][A100%|██████████| 1/1 [01:29<00:00, 89.76s/it]
 99%|█████████▉| 5137/5198 [3:06:24<1:43:28, 101.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.75s/it][A100%|██████████| 1/1 [01:29<00:00, 89.75s/it]
 99%|█████████▉| 5137/5198 [3:03:35<1:43:28, 101.78s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4819
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.64s/it][A100%|██████████| 1/1 [01:27<00:00, 87.64s/it]
 99%|█████████▉| 5138/5198 [3:06:43<1:37:38, 97.64s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:44:06,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=5107, skipped=0, lr=[5.948406696363815e-07], mom=[(0.9, 0.999)]
steps: 5107 loss: 0.6448 iter time (s): 86.720 samples/sec: 1.476

100%|██████████| 1/1 [01:27<00:00, 87.48s/it][A100%|██████████| 1/1 [01:27<00:00, 87.48s/it]
 99%|█████████▉| 5138/5198 [3:10:16<1:37:29, 97.50s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.61s/it][A100%|██████████| 1/1 [01:27<00:00, 87.61s/it]
 99%|█████████▉| 5138/5198 [3:08:11<1:37:31, 97.53s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.59s/it][A100%|██████████| 1/1 [01:27<00:00, 87.59s/it]
 99%|█████████▉| 5138/5198 [3:07:36<1:37:30, 97.51s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.61s/it][A100%|██████████| 1/1 [01:27<00:00, 87.61s/it]
 99%|█████████▉| 5138/5198 [3:10:51<1:37:30, 97.52s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.59s/it][A100%|██████████| 1/1 [01:27<00:00, 87.60s/it]
 99%|█████████▉| 5138/5198 [3:10:15<1:37:31, 97.53s/it] 
100%|██████████| 1/1 [01:27<00:00, 87.58s/it][A100%|██████████| 1/1 [01:27<00:00, 87.58s/it]

 99%|█████████▉| 5138/5198 [3:07:51<1:37:31, 97.52s/it]   0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.57s/it][A100%|██████████| 1/1 [01:27<00:00, 87.57s/it]
 99%|█████████▉| 5138/5198 [3:05:03<1:37:30, 97.52s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4820
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.82s/it][A100%|██████████| 1/1 [01:32<00:00, 92.82s/it]
 99%|█████████▉| 5139/5198 [3:08:16<1:34:38, 96.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:45:39,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=5108, skipped=0, lr=[5.929210272177441e-07], mom=[(0.9, 0.999)]
steps: 5108 loss: 0.5983 iter time (s): 92.280 samples/sec: 1.387

100%|██████████| 1/1 [01:33<00:00, 93.17s/it][A100%|██████████| 1/1 [01:33<00:00, 93.17s/it]
 99%|█████████▉| 5139/5198 [3:11:49<1:34:35, 96.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.95s/it][A100%|██████████| 1/1 [01:32<00:00, 92.95s/it]
 99%|█████████▉| 5139/5198 [3:09:44<1:34:33, 96.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.07s/it][A100%|██████████| 1/1 [01:33<00:00, 93.07s/it]
 99%|█████████▉| 5139/5198 [3:09:09<1:34:34, 96.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.04s/it][A100%|██████████| 1/1 [01:33<00:00, 93.04s/it]
 99%|█████████▉| 5139/5198 [3:12:24<1:34:34, 96.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.02s/it][A100%|██████████| 1/1 [01:33<00:00, 93.02s/it]
 99%|█████████▉| 5139/5198 [3:11:48<1:34:34, 96.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.03s/it][A100%|██████████| 1/1 [01:33<00:00, 93.03s/it]
 99%|█████████▉| 5139/5198 [3:09:24<1:34:34, 96.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.04s/it][A100%|██████████| 1/1 [01:33<00:00, 93.04s/it]
 99%|█████████▉| 5139/5198 [3:06:36<1:34:34, 96.18s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4821
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.38s/it][A100%|██████████| 1/1 [01:37<00:00, 97.38s/it]
 99%|█████████▉| 5140/5198 [3:09:53<1:33:24, 96.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:47:17,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=5109, skipped=0, lr=[5.910044031443753e-07], mom=[(0.9, 0.999)]
steps: 5109 loss: 0.6116 iter time (s): 96.873 samples/sec: 1.321

100%|██████████| 1/1 [01:37<00:00, 97.59s/it][A100%|██████████| 1/1 [01:37<00:00, 97.59s/it]
 99%|█████████▉| 5140/5198 [3:13:27<1:33:24, 96.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.71s/it][A100%|██████████| 1/1 [01:37<00:00, 97.71s/it]
 99%|█████████▉| 5140/5198 [3:11:21<1:33:24, 96.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.74s/it][A100%|██████████| 1/1 [01:37<00:00, 97.74s/it]
 99%|█████████▉| 5140/5198 [3:10:47<1:33:25, 96.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.68s/it][A100%|██████████| 1/1 [01:37<00:00, 97.69s/it]
 99%|█████████▉| 5140/5198 [3:14:01<1:33:24, 96.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.68s/it][A100%|██████████| 1/1 [01:37<00:00, 97.68s/it]
 99%|█████████▉| 5140/5198 [3:13:26<1:33:24, 96.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.69s/it][A100%|██████████| 1/1 [01:37<00:00, 97.69s/it]
 99%|█████████▉| 5140/5198 [3:11:02<1:33:24, 96.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.69s/it][A100%|██████████| 1/1 [01:37<00:00, 97.69s/it]
 99%|█████████▉| 5140/5198 [3:08:13<1:33:24, 96.63s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4822
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.16s/it][A100%|██████████| 1/1 [01:30<00:00, 90.16s/it]
 99%|█████████▉| 5141/5198 [3:11:24<1:30:02, 94.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:48:47,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=5110, skipped=0, lr=[5.890907980311806e-07], mom=[(0.9, 0.999)]
steps: 5110 loss: 0.6112 iter time (s): 89.454 samples/sec: 1.431

100%|██████████| 1/1 [01:30<00:00, 90.33s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
 99%|█████████▉| 5141/5198 [3:14:57<1:29:59, 94.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.28s/it][A100%|██████████| 1/1 [01:30<00:00, 90.28s/it]
 99%|█████████▉| 5141/5198 [3:12:52<1:29:59, 94.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.32s/it][A100%|██████████| 1/1 [01:30<00:00, 90.32s/it]
 99%|█████████▉| 5141/5198 [3:12:17<1:30:01, 94.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.33s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
 99%|█████████▉| 5141/5198 [3:15:32<1:30:00, 94.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.32s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
 99%|█████████▉| 5141/5198 [3:14:56<1:30:00, 94.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.33s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
 99%|█████████▉| 5141/5198 [3:12:32<1:30:00, 94.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.34s/it][A100%|██████████| 1/1 [01:30<00:00, 90.34s/it]
 99%|█████████▉| 5141/5198 [3:09:44<1:30:00, 94.75s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4823
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.49s/it][A100%|██████████| 1/1 [01:36<00:00, 96.49s/it]
 99%|█████████▉| 5142/5198 [3:13:00<1:28:58, 95.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:50:24,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=5111, skipped=0, lr=[5.871802124920865e-07], mom=[(0.9, 0.999)]
steps: 5111 loss: 0.6346 iter time (s): 95.879 samples/sec: 1.335

100%|██████████| 1/1 [01:36<00:00, 96.85s/it][A100%|██████████| 1/1 [01:36<00:00, 96.85s/it]
 99%|█████████▉| 5142/5198 [3:16:34<1:29:01, 95.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.83s/it][A100%|██████████| 1/1 [01:36<00:00, 96.83s/it]
 99%|█████████▉| 5142/5198 [3:14:29<1:29:00, 95.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.73s/it][A100%|██████████| 1/1 [01:36<00:00, 96.73s/it]
 99%|█████████▉| 5142/5198 [3:13:54<1:28:59, 95.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.84s/it][A100%|██████████| 1/1 [01:36<00:00, 96.84s/it]
 99%|█████████▉| 5142/5198 [3:17:09<1:29:00, 95.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.81s/it][A100%|██████████| 1/1 [01:36<00:00, 96.81s/it]
 99%|█████████▉| 5142/5198 [3:16:33<1:29:00, 95.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.78s/it][A100%|██████████| 1/1 [01:36<00:00, 96.78s/it]
 99%|█████████▉| 5142/5198 [3:14:09<1:29:00, 95.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [01:36<00:00, 96.77s/it][A100%|██████████| 1/1 [01:36<00:00, 96.77s/it]
 99%|█████████▉| 5142/5198 [3:11:20<1:28:59, 95.36s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4824
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.40s/it][A100%|██████████| 1/1 [01:28<00:00, 88.40s/it]
 99%|█████████▉| 5143/5198 [3:14:29<1:25:31, 93.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:51:52,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=5112, skipped=0, lr=[5.852726471400588e-07], mom=[(0.9, 0.999)]
steps: 5112 loss: 0.6265 iter time (s): 87.499 samples/sec: 1.463

100%|██████████| 1/1 [01:28<00:00, 88.27s/it][A100%|██████████| 1/1 [01:28<00:00, 88.27s/it]
 99%|█████████▉| 5143/5198 [3:18:02<1:25:28, 93.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.33s/it][A100%|██████████| 1/1 [01:28<00:00, 88.33s/it]
 99%|█████████▉| 5143/5198 [3:15:57<1:25:29, 93.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.39s/it][A100%|██████████| 1/1 [01:28<00:00, 88.39s/it]
 99%|█████████▉| 5143/5198 [3:15:22<1:25:29, 93.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 99%|█████████▉| 5143/5198 [3:18:37<1:25:29, 93.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.28s/it][A100%|██████████| 1/1 [01:28<00:00, 88.28s/it]
 99%|█████████▉| 5143/5198 [3:18:02<1:25:28, 93.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.30s/it][A100%|██████████| 1/1 [01:28<00:00, 88.30s/it]
 99%|█████████▉| 5143/5198 [3:15:37<1:25:28, 93.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 99%|█████████▉| 5143/5198 [3:12:49<1:25:28, 93.24s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4825
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.33s/it][A100%|██████████| 1/1 [01:30<00:00, 90.33s/it]
 99%|█████████▉| 5144/5198 [3:15:59<1:23:12, 92.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:53:23,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=5113, skipped=0, lr=[5.833681025870903e-07], mom=[(0.9, 0.999)]
steps: 5113 loss: 0.6022 iter time (s): 89.738 samples/sec: 1.426

100%|██████████| 1/1 [01:30<00:00, 90.56s/it][A100%|██████████| 1/1 [01:30<00:00, 90.56s/it]
 99%|█████████▉| 5144/5198 [3:19:33<1:23:11, 92.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.61s/it][A100%|██████████| 1/1 [01:30<00:00, 90.61s/it]
 99%|█████████▉| 5144/5198 [3:17:27<1:23:13, 92.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.53s/it][A100%|██████████| 1/1 [01:30<00:00, 90.54s/it]
 99%|█████████▉| 5144/5198 [3:16:53<1:23:12, 92.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.52s/it][A100%|██████████| 1/1 [01:30<00:00, 90.52s/it]
 99%|█████████▉| 5144/5198 [3:20:07<1:23:11, 92.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.52s/it][A100%|██████████| 1/1 [01:30<00:00, 90.52s/it]
 99%|█████████▉| 5144/5198 [3:19:32<1:23:11, 92.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.56s/it][A100%|██████████| 1/1 [01:30<00:00, 90.56s/it]
 99%|█████████▉| 5144/5198 [3:14:19<1:23:11, 92.44s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4826

100%|██████████| 1/1 [01:30<00:00, 90.58s/it][A100%|██████████| 1/1 [01:30<00:00, 90.58s/it]
 99%|█████████▉| 5144/5198 [3:17:08<1:23:12, 92.45s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.66s/it][A100%|██████████| 1/1 [01:27<00:00, 87.66s/it]
 99%|█████████▉| 5145/5198 [3:17:27<1:20:27, 91.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:54:51,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=5114, skipped=0, lr=[5.814665794442088e-07], mom=[(0.9, 0.999)]
steps: 5114 loss: 0.6075 iter time (s): 86.911 samples/sec: 1.473

100%|██████████| 1/1 [01:27<00:00, 87.81s/it][A100%|██████████| 1/1 [01:27<00:00, 87.81s/it]
 99%|█████████▉| 5145/5198 [3:21:01<1:20:25, 91.06s/it]
100%|██████████| 1/1 [01:27<00:00, 87.64s/it][A100%|██████████| 1/1 [01:27<00:00, 87.64s/it]
 99%|█████████▉| 5145/5198 [3:18:55<1:20:24, 91.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.63s/it][A100%|██████████| 1/1 [01:27<00:00, 87.63s/it]
 99%|█████████▉| 5145/5198 [3:18:20<1:20:23, 91.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.67s/it][A100%|██████████| 1/1 [01:27<00:00, 87.67s/it]
 99%|█████████▉| 5145/5198 [3:21:35<1:20:23, 91.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.76s/it][A100%|██████████| 1/1 [01:27<00:00, 87.76s/it]
 99%|█████████▉| 5145/5198 [3:21:00<1:20:24, 91.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.76s/it][A100%|██████████| 1/1 [01:27<00:00, 87.76s/it]
 99%|█████████▉| 5145/5198 [3:18:36<1:20:25, 91.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.78s/it][A100%|██████████| 1/1 [01:27<00:00, 87.78s/it]
 99%|█████████▉| 5145/5198 [3:15:47<1:20:25, 91.05s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4827
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.12s/it][A100%|██████████| 1/1 [01:31<00:00, 91.12s/it]
 99%|█████████▉| 5146/5198 [3:18:59<1:19:00, 91.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:56:22,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=5115, skipped=0, lr=[5.795680783214684e-07], mom=[(0.9, 0.999)]
steps: 5115 loss: 0.5853 iter time (s): 90.577 samples/sec: 1.413

100%|██████████| 1/1 [01:31<00:00, 91.34s/it][A100%|██████████| 1/1 [01:31<00:00, 91.35s/it]
 99%|█████████▉| 5146/5198 [3:22:32<1:18:59, 91.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.41s/it][A100%|██████████| 1/1 [01:31<00:00, 91.41s/it]
 99%|█████████▉| 5146/5198 [3:20:27<1:18:59, 91.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.42s/it][A100%|██████████| 1/1 [01:31<00:00, 91.42s/it]
 99%|█████████▉| 5146/5198 [3:19:52<1:18:58, 91.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.44s/it][A100%|██████████| 1/1 [01:31<00:00, 91.44s/it]
 99%|█████████▉| 5146/5198 [3:23:07<1:18:59, 91.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.41s/it][A100%|██████████| 1/1 [01:31<00:00, 91.41s/it]
 99%|█████████▉| 5146/5198 [3:22:31<1:18:59, 91.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.35s/it][A100%|██████████| 1/1 [01:31<00:00, 91.35s/it]
 99%|█████████▉| 5146/5198 [3:20:07<1:18:59, 91.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.36s/it][A100%|██████████| 1/1 [01:31<00:00, 91.36s/it]
 99%|█████████▉| 5146/5198 [3:17:18<1:18:59, 91.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4828
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.28s/it][A100%|██████████| 1/1 [01:25<00:00, 85.28s/it]
 99%|█████████▉| 5147/5198 [3:20:24<1:16:01, 89.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 20:57:47,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=5116, skipped=0, lr=[5.776725998279558e-07], mom=[(0.9, 0.999)]
steps: 5116 loss: 0.6735 iter time (s): 84.445 samples/sec: 1.516

100%|██████████| 1/1 [01:25<00:00, 85.19s/it][A100%|██████████| 1/1 [01:25<00:00, 85.19s/it]
 99%|█████████▉| 5147/5198 [3:23:57<1:15:57, 89.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.22s/it][A100%|██████████| 1/1 [01:25<00:00, 85.22s/it]
 99%|█████████▉| 5147/5198 [3:21:52<1:15:57, 89.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.32s/it][A100%|██████████| 1/1 [01:25<00:00, 85.32s/it]
 99%|█████████▉| 5147/5198 [3:21:17<1:15:58, 89.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.24s/it][A100%|██████████| 1/1 [01:25<00:00, 85.25s/it]
 99%|█████████▉| 5147/5198 [3:24:32<1:15:58, 89.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.23s/it][A100%|██████████| 1/1 [01:25<00:00, 85.23s/it]
 99%|█████████▉| 5147/5198 [3:23:57<1:15:58, 89.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.23s/it][A100%|██████████| 1/1 [01:25<00:00, 85.23s/it]
 99%|█████████▉| 5147/5198 [3:18:44<1:15:57, 89.37s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4829

100%|██████████| 1/1 [01:25<00:00, 85.25s/it][A100%|██████████| 1/1 [01:25<00:00, 85.25s/it]
 99%|█████████▉| 5147/5198 [3:21:32<1:15:58, 89.38s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.62s/it][A100%|██████████| 1/1 [01:35<00:00, 95.62s/it]
[2024-06-26 20:59:23,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=5117, skipped=0, lr=[5.757801445717875e-07], mom=[(0.9, 0.999)]
steps: 5117 loss: 0.6133 iter time (s): 95.285 samples/sec: 1.343

100%|██████████| 1/1 [01:36<00:00, 96.15s/it][A100%|██████████| 1/1 [01:36<00:00, 96.15s/it]

100%|██████████| 1/1 [01:36<00:00, 96.15s/it][A100%|██████████| 1/1 [01:36<00:00, 96.15s/it]

100%|██████████| 1/1 [01:36<00:00, 96.14s/it][A100%|██████████| 1/1 [01:36<00:00, 96.14s/it]

100%|██████████| 1/1 [01:36<00:00, 96.16s/it][A100%|██████████| 1/1 [01:36<00:00, 96.16s/it]

100%|██████████| 1/1 [01:36<00:00, 96.16s/it][A100%|██████████| 1/1 [01:36<00:00, 96.17s/it]

100%|██████████| 1/1 [01:36<00:00, 96.13s/it][A100%|██████████| 1/1 [01:36<00:00, 96.13s/it]

100%|██████████| 1/1 [01:36<00:00, 96.16s/it][A100%|██████████| 1/1 [01:36<00:00, 96.16s/it]
Checkpointing at shard 5147
[2024-06-26 20:59:24,784] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5117 is about to be saved!
[2024-06-26 20:59:25,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_00-model_states.pt...
[2024-06-26 20:59:30,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_02-model_states.pt...
[2024-06-26 20:59:30,948] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_08-model_states.pt...
[2024-06-26 20:59:31,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_07-model_states.pt...
[2024-06-26 20:59:35,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_00-model_states.pt.
[2024-06-26 20:59:40,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_01-model_states.pt...
[2024-06-26 20:59:40,513] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_05-model_states.pt...
[2024-06-26 20:59:40,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_06-model_states.pt...
[2024-06-26 20:59:44,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_03-model_states.pt...
[2024-06-26 20:59:44,435] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_04-model_states.pt...
[2024-06-26 21:02:56,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_07-model_states.pt.
[2024-06-26 21:02:56,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_06_model_states.pt...
[2024-06-26 21:02:57,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_06_model_states.pt.
[2024-06-26 21:02:57,361] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
[2024-06-26 21:02:58,868] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_02-model_states.pt.
[2024-06-26 21:02:59,015] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_01_model_states.pt
[2024-06-26 21:02:59,015] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_01_model_states.pt...
[2024-06-26 21:02:59,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_01_model_states.pt.
[2024-06-26 21:02:59,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
[2024-06-26 21:03:01,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_08-model_states.pt.
[2024-06-26 21:03:02,208] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_09-model_states.pt...
[2024-06-26 21:03:03,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_09-model_states.pt.
[2024-06-26 21:03:03,408] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_07_model_states.pt...
[2024-06-26 21:03:03,548] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_07_model_states.pt.
[2024-06-26 21:03:03,548] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
[2024-06-26 21:03:13,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_04-model_states.pt.
[2024-06-26 21:03:13,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_03_model_states.pt...
[2024-06-26 21:03:13,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_03_model_states.pt.
[2024-06-26 21:03:13,873] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
[2024-06-26 21:03:14,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_03-model_states.pt.
[2024-06-26 21:03:14,388] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_02_model_states.pt...
[2024-06-26 21:03:14,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_02_model_states.pt.
[2024-06-26 21:03:14,781] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
[2024-06-26 21:03:15,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_06-model_states.pt.
[2024-06-26 21:03:15,757] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_05_model_states.pt...
[2024-06-26 21:03:15,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_05_model_states.pt.
[2024-06-26 21:03:15,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
[2024-06-26 21:03:16,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_05-model_states.pt.
[2024-06-26 21:03:16,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/layer_01-model_states.pt.
[2024-06-26 21:03:16,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_04_model_states.pt...
[2024-06-26 21:03:16,711] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_00_model_states.pt
[2024-06-26 21:03:16,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_00_model_states.pt...
[2024-06-26 21:03:16,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_04_model_states.pt.
[2024-06-26 21:03:16,746] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
[2024-06-26 21:03:17,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5117/mp_rank_00_model_states.pt.
[2024-06-26 21:03:17,162] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5117 is ready now!
Checkpoint saved using --- 232.38071751594543 seconds ---
 99%|█████████▉| 5148/5198 [3:26:46<2:14:18, 161.18s/it] 99%|█████████▉| 5148/5198 [3:27:21<2:14:21, 161.24s/it] 99%|█████████▉| 5148/5198 [3:24:12<2:14:16, 161.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4830
 99%|█████████▉| 5148/5198 [3:30:01<2:14:18, 161.17s/it] 99%|█████████▉| 5148/5198 [3:25:55<2:14:58, 161.96s/it] 99%|█████████▉| 5148/5198 [3:29:26<2:14:23, 161.26s/it] 99%|█████████▉| 5148/5198 [3:29:25<2:14:16, 161.14s/it] 99%|█████████▉| 5148/5198 [3:27:01<2:14:16, 161.13s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.16s/it][A100%|██████████| 1/1 [01:53<00:00, 113.16s/it]
 99%|█████████▉| 5149/5198 [3:27:49<2:00:23, 147.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:05:13,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=5118, skipped=0, lr=[5.738907131601099e-07], mom=[(0.9, 0.999)]
steps: 5118 loss: 0.5889 iter time (s): 115.933 samples/sec: 1.104

100%|██████████| 1/1 [01:56<00:00, 116.42s/it][A100%|██████████| 1/1 [01:56<00:00, 116.42s/it]
 99%|█████████▉| 5149/5198 [3:29:17<2:00:46, 147.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.43s/it][A100%|██████████| 1/1 [01:56<00:00, 116.43s/it]
 99%|█████████▉| 5149/5198 [3:31:23<2:00:47, 147.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.55s/it][A100%|██████████| 1/1 [01:56<00:00, 116.55s/it]
 99%|█████████▉| 5149/5198 [3:28:42<2:00:46, 147.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.67s/it][A100%|██████████| 1/1 [01:56<00:00, 116.67s/it]
 99%|█████████▉| 5149/5198 [3:31:58<2:00:47, 147.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.79s/it][A100%|██████████| 1/1 [01:56<00:00, 116.79s/it]
 99%|█████████▉| 5149/5198 [3:31:22<2:00:48, 147.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.83s/it][A100%|██████████| 1/1 [01:56<00:00, 116.83s/it]
 99%|█████████▉| 5149/5198 [3:28:58<2:00:48, 147.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.86s/it][A100%|██████████| 1/1 [01:56<00:00, 116.86s/it]
 99%|█████████▉| 5149/5198 [3:26:09<2:00:48, 147.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4831
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.23s/it][A100%|██████████| 1/1 [01:33<00:00, 93.23s/it]
 99%|█████████▉| 5150/5198 [3:29:22<1:44:59, 131.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:06:46,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=5119, skipped=0, lr=[5.720043061991003e-07], mom=[(0.9, 0.999)]
steps: 5119 loss: 0.6100 iter time (s): 91.862 samples/sec: 1.393

100%|██████████| 1/1 [01:32<00:00, 92.69s/it][A100%|██████████| 1/1 [01:32<00:00, 92.69s/it]
 99%|█████████▉| 5150/5198 [3:32:56<1:45:04, 131.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.83s/it][A100%|██████████| 1/1 [01:32<00:00, 92.83s/it]
 99%|█████████▉| 5150/5198 [3:30:50<1:45:05, 131.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.85s/it][A100%|██████████| 1/1 [01:32<00:00, 92.85s/it]
 99%|█████████▉| 5150/5198 [3:30:15<1:45:06, 131.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.81s/it][A100%|██████████| 1/1 [01:32<00:00, 92.81s/it]
 99%|█████████▉| 5150/5198 [3:33:30<1:45:06, 131.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.70s/it][A100%|██████████| 1/1 [01:32<00:00, 92.70s/it]
 99%|█████████▉| 5150/5198 [3:32:55<1:45:05, 131.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.70s/it][A100%|██████████| 1/1 [01:32<00:00, 92.70s/it]
 99%|█████████▉| 5150/5198 [3:30:31<1:45:05, 131.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.70s/it][A100%|██████████| 1/1 [01:32<00:00, 92.70s/it]
 99%|█████████▉| 5150/5198 [3:27:42<1:45:05, 131.37s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4832
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.17s/it][A100%|██████████| 1/1 [01:25<00:00, 85.17s/it]
 99%|█████████▉| 5151/5198 [3:30:47<1:32:00, 117.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:08:11,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=5120, skipped=0, lr=[5.701209242939631e-07], mom=[(0.9, 0.999)]
steps: 5120 loss: 0.5863 iter time (s): 84.258 samples/sec: 1.519

100%|██████████| 1/1 [01:25<00:00, 85.01s/it][A100%|██████████| 1/1 [01:25<00:00, 85.01s/it]
 99%|█████████▉| 5151/5198 [3:34:21<1:32:00, 117.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.94s/it][A100%|██████████| 1/1 [01:24<00:00, 84.94s/it]
 99%|█████████▉| 5151/5198 [3:32:15<1:31:59, 117.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.96s/it][A100%|██████████| 1/1 [01:24<00:00, 84.96s/it]
 99%|█████████▉| 5151/5198 [3:34:55<1:32:00, 117.46s/it]
100%|██████████| 1/1 [01:25<00:00, 85.06s/it][A100%|██████████| 1/1 [01:25<00:00, 85.06s/it]
 99%|█████████▉| 5151/5198 [3:31:40<1:32:01, 117.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.07s/it][A100%|██████████| 1/1 [01:25<00:00, 85.07s/it]
 99%|█████████▉| 5151/5198 [3:34:20<1:32:01, 117.48s/it]
100%|██████████| 1/1 [01:25<00:00, 85.02s/it][A100%|██████████| 1/1 [01:25<00:00, 85.02s/it]
 99%|█████████▉| 5151/5198 [3:29:07<1:32:00, 117.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4833

100%|██████████| 1/1 [01:25<00:00, 85.05s/it][A100%|██████████| 1/1 [01:25<00:00, 85.05s/it]
 99%|█████████▉| 5151/5198 [3:31:56<1:32:01, 117.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.20s/it][A100%|██████████| 1/1 [01:23<00:00, 83.20s/it]
 99%|█████████▉| 5152/5198 [3:32:11<1:22:12, 107.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:09:34,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=5121, skipped=0, lr=[5.68240568048938e-07], mom=[(0.9, 0.999)]
steps: 5121 loss: 0.6484 iter time (s): 82.573 samples/sec: 1.550

100%|██████████| 1/1 [01:23<00:00, 83.41s/it][A100%|██████████| 1/1 [01:23<00:00, 83.41s/it]
 99%|█████████▉| 5152/5198 [3:35:44<1:22:12, 107.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.41s/it][A100%|██████████| 1/1 [01:23<00:00, 83.41s/it]
 99%|█████████▉| 5152/5198 [3:33:39<1:22:12, 107.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.38s/it][A100%|██████████| 1/1 [01:23<00:00, 83.38s/it]
 99%|█████████▉| 5152/5198 [3:33:04<1:22:13, 107.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.42s/it][A100%|██████████| 1/1 [01:23<00:00, 83.42s/it]
 99%|█████████▉| 5152/5198 [3:36:19<1:22:13, 107.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.36s/it][A100%|██████████| 1/1 [01:23<00:00, 83.36s/it]
 99%|█████████▉| 5152/5198 [3:33:19<1:22:13, 107.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.38s/it][A100%|██████████| 1/1 [01:23<00:00, 83.38s/it]
 99%|█████████▉| 5152/5198 [3:35:43<1:22:13, 107.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.41s/it][A100%|██████████| 1/1 [01:23<00:00, 83.41s/it]
 99%|█████████▉| 5152/5198 [3:30:31<1:22:13, 107.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4834
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.98s/it][A100%|██████████| 1/1 [01:20<00:00, 80.98s/it]
 99%|█████████▉| 5153/5198 [3:33:32<1:14:33, 99.41s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:10:55,728] [INFO] [logging.py:96:log_dist] [Rank 0] step=5122, skipped=0, lr=[5.663632380672851e-07], mom=[(0.9, 0.999)]
steps: 5122 loss: 0.5940 iter time (s): 80.227 samples/sec: 1.595

100%|██████████| 1/1 [01:21<00:00, 81.01s/it][A100%|██████████| 1/1 [01:21<00:00, 81.01s/it]
 99%|█████████▉| 5153/5198 [3:37:05<1:14:31, 99.37s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.03s/it][A100%|██████████| 1/1 [01:21<00:00, 81.03s/it]
 99%|█████████▉| 5153/5198 [3:35:00<1:14:32, 99.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.96s/it][A100%|██████████| 1/1 [01:20<00:00, 80.96s/it]
 99%|█████████▉| 5153/5198 [3:34:25<1:14:31, 99.37s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.08s/it][A100%|██████████| 1/1 [01:21<00:00, 81.08s/it]
 99%|█████████▉| 5153/5198 [3:37:40<1:14:33, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.07s/it][A100%|██████████| 1/1 [01:21<00:00, 81.07s/it]
 99%|█████████▉| 5153/5198 [3:37:04<1:14:32, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.10s/it][A100%|██████████| 1/1 [01:21<00:00, 81.10s/it]
 99%|█████████▉| 5153/5198 [3:34:40<1:14:33, 99.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.08s/it][A100%|██████████| 1/1 [01:21<00:00, 81.08s/it]
 99%|█████████▉| 5153/5198 [3:31:52<1:14:33, 99.40s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4835
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.19s/it][A100%|██████████| 1/1 [01:51<00:00, 111.19s/it]
 99%|█████████▉| 5154/5198 [3:35:23<1:15:31, 102.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:12:47,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=5123, skipped=0, lr=[5.644889349513001e-07], mom=[(0.9, 0.999)]
steps: 5123 loss: 0.6128 iter time (s): 111.327 samples/sec: 1.150

100%|██████████| 1/1 [01:52<00:00, 112.24s/it][A100%|██████████| 1/1 [01:52<00:00, 112.25s/it]
 99%|█████████▉| 5154/5198 [3:38:57<1:15:42, 103.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.18s/it][A100%|██████████| 1/1 [01:52<00:00, 112.18s/it]
 99%|█████████▉| 5154/5198 [3:36:52<1:15:41, 103.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.23s/it][A100%|██████████| 1/1 [01:52<00:00, 112.23s/it]
 99%|█████████▉| 5154/5198 [3:36:17<1:15:42, 103.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.13s/it][A100%|██████████| 1/1 [01:52<00:00, 112.13s/it]
 99%|█████████▉| 5154/5198 [3:39:32<1:15:41, 103.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.14s/it][A100%|██████████| 1/1 [01:52<00:00, 112.14s/it]
 99%|█████████▉| 5154/5198 [3:38:57<1:15:41, 103.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.15s/it][A100%|██████████| 1/1 [01:52<00:00, 112.15s/it]
 99%|█████████▉| 5154/5198 [3:36:32<1:15:42, 103.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.16s/it][A100%|██████████| 1/1 [01:52<00:00, 112.16s/it]
 99%|█████████▉| 5154/5198 [3:33:44<1:15:42, 103.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4836
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.19s/it][A100%|██████████| 1/1 [01:32<00:00, 92.19s/it]
 99%|█████████▉| 5155/5198 [3:36:56<1:11:31, 99.81s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:14:19,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=5124, skipped=0, lr=[5.626176593023065e-07], mom=[(0.9, 0.999)]
steps: 5124 loss: 0.6032 iter time (s): 90.994 samples/sec: 1.407

100%|██████████| 1/1 [01:31<00:00, 91.73s/it][A100%|██████████| 1/1 [01:31<00:00, 91.73s/it]
 99%|█████████▉| 5155/5198 [3:40:29<1:11:30, 99.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.85s/it][A100%|██████████| 1/1 [01:31<00:00, 91.85s/it]
 99%|█████████▉| 5155/5198 [3:38:24<1:11:32, 99.82s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.70s/it][A100%|██████████| 1/1 [01:31<00:00, 91.70s/it]
 99%|█████████▉| 5155/5198 [3:37:49<1:11:30, 99.77s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.78s/it][A100%|██████████| 1/1 [01:31<00:00, 91.78s/it]
 99%|█████████▉| 5155/5198 [3:41:04<1:11:30, 99.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.77s/it][A100%|██████████| 1/1 [01:31<00:00, 91.77s/it]
 99%|█████████▉| 5155/5198 [3:40:28<1:11:31, 99.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.77s/it][A100%|██████████| 1/1 [01:31<00:00, 91.77s/it]
 99%|█████████▉| 5155/5198 [3:38:04<1:11:31, 99.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.77s/it][A100%|██████████| 1/1 [01:31<00:00, 91.77s/it]
 99%|█████████▉| 5155/5198 [3:35:16<1:11:31, 99.79s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4837
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.69s/it][A100%|██████████| 1/1 [01:28<00:00, 88.69s/it]
 99%|█████████▉| 5156/5198 [3:38:25<1:07:33, 96.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:15:48,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=5125, skipped=0, lr=[5.60749411720656e-07], mom=[(0.9, 0.999)]
steps: 5125 loss: 0.6142 iter time (s): 87.943 samples/sec: 1.455

100%|██████████| 1/1 [01:28<00:00, 88.74s/it][A100%|██████████| 1/1 [01:28<00:00, 88.74s/it]
 99%|█████████▉| 5156/5198 [3:41:58<1:07:31, 96.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.69s/it][A100%|██████████| 1/1 [01:28<00:00, 88.70s/it]
 99%|█████████▉| 5156/5198 [3:39:52<1:07:32, 96.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.82s/it][A100%|██████████| 1/1 [01:28<00:00, 88.82s/it]
 99%|█████████▉| 5156/5198 [3:39:18<1:07:32, 96.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.73s/it][A100%|██████████| 1/1 [01:28<00:00, 88.73s/it]
 99%|█████████▉| 5156/5198 [3:42:33<1:07:31, 96.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.77s/it][A100%|██████████| 1/1 [01:28<00:00, 88.77s/it]
 99%|█████████▉| 5156/5198 [3:41:57<1:07:32, 96.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.77s/it][A100%|██████████| 1/1 [01:28<00:00, 88.77s/it]
 99%|█████████▉| 5156/5198 [3:39:33<1:07:32, 96.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.76s/it][A100%|██████████| 1/1 [01:28<00:00, 88.76s/it]
 99%|█████████▉| 5156/5198 [3:36:44<1:07:32, 96.49s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4838
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.90s/it][A100%|██████████| 1/1 [01:33<00:00, 93.90s/it]
 99%|█████████▉| 5157/5198 [3:39:59<1:05:27, 95.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:17:22,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=5126, skipped=0, lr=[5.588841928057293e-07], mom=[(0.9, 0.999)]
steps: 5126 loss: 0.6115 iter time (s): 93.441 samples/sec: 1.370

100%|██████████| 1/1 [01:34<00:00, 94.21s/it][A100%|██████████| 1/1 [01:34<00:00, 94.21s/it]
 99%|█████████▉| 5157/5198 [3:43:32<1:05:27, 95.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.20s/it][A100%|██████████| 1/1 [01:34<00:00, 94.20s/it]
 99%|█████████▉| 5157/5198 [3:41:27<1:05:27, 95.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.11s/it][A100%|██████████| 1/1 [01:34<00:00, 94.11s/it]
 99%|█████████▉| 5157/5198 [3:40:52<1:05:26, 95.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.24s/it][A100%|██████████| 1/1 [01:34<00:00, 94.24s/it]
 99%|█████████▉| 5157/5198 [3:44:07<1:05:28, 95.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.19s/it][A100%|██████████| 1/1 [01:34<00:00, 94.19s/it]
 99%|█████████▉| 5157/5198 [3:41:07<1:05:27, 95.80s/it]
100%|██████████| 1/1 [01:34<00:00, 94.21s/it][A100%|██████████| 1/1 [01:34<00:00, 94.21s/it]
 99%|█████████▉| 5157/5198 [3:43:31<1:05:28, 95.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.21s/it][A100%|██████████| 1/1 [01:34<00:00, 94.21s/it]
 99%|█████████▉| 5157/5198 [3:38:19<1:05:27, 95.80s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4839
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.66s/it][A100%|██████████| 1/1 [01:30<00:00, 90.66s/it]
 99%|█████████▉| 5158/5198 [3:41:29<1:02:51, 94.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:18:53,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=5127, skipped=0, lr=[5.570220031559343e-07], mom=[(0.9, 0.999)]
steps: 5127 loss: 0.5552 iter time (s): 89.919 samples/sec: 1.424

100%|██████████| 1/1 [01:30<00:00, 90.66s/it][A100%|██████████| 1/1 [01:30<00:00, 90.66s/it]
 99%|█████████▉| 5158/5198 [3:45:03<1:02:50, 94.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.75s/it][A100%|██████████| 1/1 [01:30<00:00, 90.76s/it]
 99%|█████████▉| 5158/5198 [3:42:57<1:02:51, 94.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.77s/it][A100%|██████████| 1/1 [01:30<00:00, 90.77s/it]
 99%|█████████▉| 5158/5198 [3:42:22<1:02:51, 94.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.64s/it][A100%|██████████| 1/1 [01:30<00:00, 90.64s/it]
 99%|█████████▉| 5158/5198 [3:45:37<1:02:50, 94.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.68s/it][A100%|██████████| 1/1 [01:30<00:00, 90.68s/it]
 99%|█████████▉| 5158/5198 [3:45:02<1:02:50, 94.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.69s/it][A100%|██████████| 1/1 [01:30<00:00, 90.69s/it]
 99%|█████████▉| 5158/5198 [3:42:38<1:02:50, 94.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.68s/it][A100%|██████████| 1/1 [01:30<00:00, 90.68s/it]
 99%|█████████▉| 5158/5198 [3:39:49<1:02:50, 94.27s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4840
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.65s/it][A100%|██████████| 1/1 [01:28<00:00, 88.65s/it]
 99%|█████████▉| 5159/5198 [3:42:58<1:00:13, 92.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:20:22,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=5128, skipped=0, lr=[5.551628433687068e-07], mom=[(0.9, 0.999)]
steps: 5128 loss: 0.5942 iter time (s): 88.042 samples/sec: 1.454

100%|██████████| 1/1 [01:28<00:00, 88.87s/it][A100%|██████████| 1/1 [01:28<00:00, 88.87s/it]
 99%|█████████▉| 5159/5198 [3:46:32<1:00:13, 92.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.76s/it][A100%|██████████| 1/1 [01:28<00:00, 88.77s/it]
 99%|█████████▉| 5159/5198 [3:44:26<1:00:12, 92.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.78s/it][A100%|██████████| 1/1 [01:28<00:00, 88.78s/it]
 99%|█████████▉| 5159/5198 [3:43:51<1:00:12, 92.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.86s/it][A100%|██████████| 1/1 [01:28<00:00, 88.86s/it]
 99%|█████████▉| 5159/5198 [3:47:06<1:00:12, 92.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.74s/it][A100%|██████████| 1/1 [01:28<00:00, 88.74s/it]
 99%|█████████▉| 5159/5198 [3:46:31<1:00:11, 92.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.79s/it][A100%|██████████| 1/1 [01:28<00:00, 88.79s/it]
 99%|█████████▉| 5159/5198 [3:44:07<1:00:12, 92.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.80s/it][A100%|██████████| 1/1 [01:28<00:00, 88.80s/it]
 99%|█████████▉| 5159/5198 [3:41:18<1:00:12, 92.63s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4841
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.27s/it][A100%|██████████| 1/1 [01:26<00:00, 86.27s/it]
 99%|█████████▉| 5160/5198 [3:44:25<57:30, 90.80s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:21:48,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=5129, skipped=0, lr=[5.533067140405121e-07], mom=[(0.9, 0.999)]
steps: 5129 loss: 0.6191 iter time (s): 85.628 samples/sec: 1.495

100%|██████████| 1/1 [01:26<00:00, 86.45s/it][A100%|██████████| 1/1 [01:26<00:00, 86.45s/it]
 99%|█████████▉| 5160/5198 [3:47:58<57:30, 90.79s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.46s/it][A100%|██████████| 1/1 [01:26<00:00, 86.46s/it]
 99%|█████████▉| 5160/5198 [3:45:53<57:29, 90.78s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.46s/it][A100%|██████████| 1/1 [01:26<00:00, 86.46s/it]
 99%|█████████▉| 5160/5198 [3:45:18<57:29, 90.78s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.46s/it][A100%|██████████| 1/1 [01:26<00:00, 86.46s/it]
 99%|█████████▉| 5160/5198 [3:48:33<57:29, 90.79s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.45s/it][A100%|██████████| 1/1 [01:26<00:00, 86.45s/it]
 99%|█████████▉| 5160/5198 [3:47:57<57:29, 90.77s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.44s/it][A100%|██████████| 1/1 [01:26<00:00, 86.44s/it]
 99%|█████████▉| 5160/5198 [3:45:33<57:29, 90.77s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.44s/it][A100%|██████████| 1/1 [01:26<00:00, 86.44s/it]
 99%|█████████▉| 5160/5198 [3:42:44<57:29, 90.78s/it]  Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4842
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.26s/it][A100%|██████████| 1/1 [01:28<00:00, 88.26s/it]
 99%|█████████▉| 5161/5198 [3:45:53<55:33, 90.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:23:17,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=5130, skipped=0, lr=[5.514536157668421e-07], mom=[(0.9, 0.999)]
steps: 5130 loss: 0.6136 iter time (s): 87.704 samples/sec: 1.459

100%|██████████| 1/1 [01:28<00:00, 88.37s/it][A100%|██████████| 1/1 [01:28<00:00, 88.37s/it]
 99%|█████████▉| 5161/5198 [3:49:27<55:32, 90.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.45s/it][A100%|██████████| 1/1 [01:28<00:00, 88.45s/it]
 99%|█████████▉| 5161/5198 [3:47:21<55:33, 90.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.48s/it][A100%|██████████| 1/1 [01:28<00:00, 88.48s/it]
 99%|█████████▉| 5161/5198 [3:46:46<55:33, 90.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.58s/it][A100%|██████████| 1/1 [01:28<00:00, 88.58s/it]
 99%|█████████▉| 5161/5198 [3:50:01<55:34, 90.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.58s/it][A100%|██████████| 1/1 [01:28<00:00, 88.58s/it]
 99%|█████████▉| 5161/5198 [3:49:26<55:34, 90.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.58s/it][A100%|██████████| 1/1 [01:28<00:00, 88.58s/it]
 99%|█████████▉| 5161/5198 [3:47:02<55:34, 90.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.57s/it][A100%|██████████| 1/1 [01:28<00:00, 88.57s/it]
 99%|█████████▉| 5161/5198 [3:44:13<55:34, 90.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4843
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.91s/it][A100%|██████████| 1/1 [01:42<00:00, 102.91s/it]
 99%|█████████▉| 5162/5198 [3:47:36<56:23, 93.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:25:00,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=5131, skipped=0, lr=[5.496035491422185e-07], mom=[(0.9, 0.999)]
steps: 5131 loss: 0.5592 iter time (s): 102.656 samples/sec: 1.247

100%|██████████| 1/1 [01:43<00:00, 103.80s/it][A100%|██████████| 1/1 [01:43<00:00, 103.80s/it]
 99%|█████████▉| 5162/5198 [3:51:10<56:31, 94.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.75s/it][A100%|██████████| 1/1 [01:43<00:00, 103.75s/it]
 99%|█████████▉| 5162/5198 [3:49:05<56:30, 94.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.69s/it][A100%|██████████| 1/1 [01:43<00:00, 103.69s/it]
 99%|█████████▉| 5162/5198 [3:48:30<56:30, 94.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.64s/it][A100%|██████████| 1/1 [01:43<00:00, 103.64s/it]
 99%|█████████▉| 5162/5198 [3:51:45<56:30, 94.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.71s/it][A100%|██████████| 1/1 [01:43<00:00, 103.72s/it]
 99%|█████████▉| 5162/5198 [3:48:45<56:31, 94.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.31s/it][A100%|██████████| 1/1 [01:44<00:00, 104.31s/it]
 99%|█████████▉| 5162/5198 [3:45:57<56:37, 94.38s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4844
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.40s/it][A100%|██████████| 1/1 [01:44<00:00, 104.40s/it]
 99%|█████████▉| 5162/5198 [3:51:10<56:38, 94.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 138.00s/it][A100%|██████████| 1/1 [02:17<00:00, 138.00s/it]
 99%|█████████▉| 5163/5198 [3:49:54<1:02:33, 107.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:27:19,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=5132, skipped=0, lr=[5.477565147601879e-07], mom=[(0.9, 0.999)]
steps: 5132 loss: 0.5958 iter time (s): 137.512 samples/sec: 0.931

100%|██████████| 1/1 [02:18<00:00, 138.95s/it][A100%|██████████| 1/1 [02:18<00:00, 138.95s/it]
 99%|█████████▉| 5163/5198 [3:53:29<1:02:47, 107.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.94s/it][A100%|██████████| 1/1 [02:18<00:00, 138.94s/it]
 99%|█████████▉| 5163/5198 [3:51:24<1:02:46, 107.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.95s/it][A100%|██████████| 1/1 [02:18<00:00, 138.95s/it]
 99%|█████████▉| 5163/5198 [3:50:49<1:02:46, 107.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.93s/it][A100%|██████████| 1/1 [02:18<00:00, 138.94s/it]
 99%|█████████▉| 5163/5198 [3:54:04<1:02:46, 107.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
 99%|█████████▉| 5163/5198 [3:53:29<1:02:44, 107.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.89s/it][A100%|██████████| 1/1 [02:18<00:00, 138.89s/it]
 99%|█████████▉| 5163/5198 [3:51:04<1:02:46, 107.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.28s/it][A100%|██████████| 1/1 [02:18<00:00, 138.28s/it]
 99%|█████████▉| 5163/5198 [3:48:16<1:02:44, 107.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4845
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.43s/it][A100%|██████████| 1/1 [01:35<00:00, 95.43s/it]
 99%|█████████▉| 5164/5198 [3:51:30<58:48, 103.78s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:28:54,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=5133, skipped=0, lr=[5.459125132133228e-07], mom=[(0.9, 0.999)]
steps: 5133 loss: 0.6102 iter time (s): 93.626 samples/sec: 1.367

100%|██████████| 1/1 [01:34<00:00, 94.42s/it][A100%|██████████| 1/1 [01:34<00:00, 94.42s/it]
 99%|█████████▉| 5164/5198 [3:55:04<58:44, 103.67s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.47s/it][A100%|██████████| 1/1 [01:34<00:00, 94.47s/it]
 99%|█████████▉| 5164/5198 [3:52:58<58:45, 103.68s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.47s/it][A100%|██████████| 1/1 [01:34<00:00, 94.47s/it]
 99%|█████████▉| 5164/5198 [3:52:23<58:44, 103.68s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.46s/it][A100%|██████████| 1/1 [01:34<00:00, 94.46s/it]
 99%|█████████▉| 5164/5198 [3:55:38<58:44, 103.67s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.47s/it][A100%|██████████| 1/1 [01:34<00:00, 94.47s/it]
 99%|█████████▉| 5164/5198 [3:55:03<58:43, 103.63s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.48s/it][A100%|██████████| 1/1 [01:34<00:00, 94.48s/it]
 99%|█████████▉| 5164/5198 [3:52:39<58:44, 103.67s/it]  
100%|██████████| 1/1 [01:34<00:00, 94.47s/it][A100%|██████████| 1/1 [01:34<00:00, 94.48s/it]
 99%|█████████▉| 5164/5198 [3:49:50<58:43, 103.63s/it]  Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4846
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.54s/it][A100%|██████████| 1/1 [01:40<00:00, 100.54s/it]
 99%|█████████▉| 5165/5198 [3:53:11<56:35, 102.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:30:35,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=5134, skipped=0, lr=[5.440715450932273e-07], mom=[(0.9, 0.999)]
steps: 5134 loss: 0.6467 iter time (s): 100.155 samples/sec: 1.278

100%|██████████| 1/1 [01:40<00:00, 100.97s/it][A100%|██████████| 1/1 [01:40<00:00, 100.98s/it]
 99%|█████████▉| 5165/5198 [3:56:45<56:34, 102.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.97s/it][A100%|██████████| 1/1 [01:40<00:00, 100.97s/it]
 99%|█████████▉| 5165/5198 [3:54:39<56:34, 102.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.01s/it][A100%|██████████| 1/1 [01:41<00:00, 101.01s/it]
 99%|█████████▉| 5165/5198 [3:54:04<56:34, 102.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.98s/it][A100%|██████████| 1/1 [01:40<00:00, 100.98s/it]
 99%|█████████▉| 5165/5198 [3:57:19<56:34, 102.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 101.00s/it][A100%|██████████| 1/1 [01:40<00:00, 101.00s/it]
 99%|█████████▉| 5165/5198 [3:54:20<56:34, 102.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.08s/it][A100%|██████████| 1/1 [01:41<00:00, 101.08s/it]
 99%|█████████▉| 5165/5198 [3:56:44<56:34, 102.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.05s/it][A100%|██████████| 1/1 [01:41<00:00, 101.05s/it]
 99%|█████████▉| 5165/5198 [3:51:31<56:34, 102.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4847
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.69s/it][A100%|██████████| 1/1 [01:22<00:00, 82.69s/it]
 99%|█████████▉| 5166/5198 [3:54:34<51:40, 96.88s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:31:57,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=5135, skipped=0, lr=[5.422336109905277e-07], mom=[(0.9, 0.999)]
steps: 5135 loss: 0.5981 iter time (s): 81.524 samples/sec: 1.570

100%|██████████| 1/1 [01:22<00:00, 82.49s/it][A100%|██████████| 1/1 [01:22<00:00, 82.49s/it]
 99%|█████████▉| 5166/5198 [3:58:07<51:36, 96.75s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.37s/it][A100%|██████████| 1/1 [01:22<00:00, 82.37s/it]
 99%|█████████▉| 5166/5198 [3:56:02<51:35, 96.72s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.40s/it][A100%|██████████| 1/1 [01:22<00:00, 82.40s/it]
 99%|█████████▉| 5166/5198 [3:55:27<51:35, 96.74s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.40s/it][A100%|██████████| 1/1 [01:22<00:00, 82.40s/it]
 99%|█████████▉| 5166/5198 [3:58:42<51:35, 96.73s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.31s/it][A100%|██████████| 1/1 [01:22<00:00, 82.31s/it]
 99%|█████████▉| 5166/5198 [3:58:06<51:34, 96.70s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.37s/it][A100%|██████████| 1/1 [01:22<00:00, 82.38s/it]
 99%|█████████▉| 5166/5198 [3:55:42<51:35, 96.73s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.34s/it][A100%|██████████| 1/1 [01:22<00:00, 82.34s/it]
 99%|█████████▉| 5166/5198 [3:52:54<51:34, 96.71s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4848
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.66s/it][A100%|██████████| 1/1 [01:25<00:00, 85.66s/it]
 99%|█████████▉| 5167/5198 [3:56:00<48:20, 93.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:33:23,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=5136, skipped=0, lr=[5.403987114948799e-07], mom=[(0.9, 0.999)]
steps: 5136 loss: 0.5580 iter time (s): 85.018 samples/sec: 1.506

100%|██████████| 1/1 [01:25<00:00, 85.69s/it][A100%|██████████| 1/1 [01:25<00:00, 85.69s/it]
 99%|█████████▉| 5167/5198 [3:59:33<48:16, 93.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.83s/it][A100%|██████████| 1/1 [01:25<00:00, 85.83s/it]
 99%|█████████▉| 5167/5198 [3:57:28<48:17, 93.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.77s/it][A100%|██████████| 1/1 [01:25<00:00, 85.77s/it]
 99%|█████████▉| 5167/5198 [3:56:53<48:16, 93.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.83s/it][A100%|██████████| 1/1 [01:25<00:00, 85.83s/it]
 99%|█████████▉| 5167/5198 [4:00:08<48:17, 93.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.82s/it][A100%|██████████| 1/1 [01:25<00:00, 85.82s/it]
 99%|█████████▉| 5167/5198 [3:59:32<48:16, 93.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.82s/it][A100%|██████████| 1/1 [01:25<00:00, 85.82s/it]
 99%|█████████▉| 5167/5198 [3:57:08<48:17, 93.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.82s/it][A100%|██████████| 1/1 [01:25<00:00, 85.82s/it]
 99%|█████████▉| 5167/5198 [3:54:19<48:16, 93.44s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4849
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.40s/it][A100%|██████████| 1/1 [01:40<00:00, 100.40s/it]
 99%|█████████▉| 5168/5198 [3:57:40<47:49, 95.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:35:04,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=5137, skipped=0, lr=[5.385668471949641e-07], mom=[(0.9, 0.999)]
steps: 5137 loss: 0.6244 iter time (s): 100.187 samples/sec: 1.278

100%|██████████| 1/1 [01:41<00:00, 101.07s/it][A100%|██████████| 1/1 [01:41<00:00, 101.07s/it]
 99%|█████████▉| 5168/5198 [4:01:14<47:51, 95.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.00s/it][A100%|██████████| 1/1 [01:41<00:00, 101.00s/it]
 99%|█████████▉| 5168/5198 [3:59:09<47:51, 95.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.04s/it][A100%|██████████| 1/1 [01:41<00:00, 101.04s/it]
 99%|█████████▉| 5168/5198 [3:58:34<47:51, 95.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.02s/it][A100%|██████████| 1/1 [01:41<00:00, 101.02s/it]
 99%|█████████▉| 5168/5198 [4:01:49<47:51, 95.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.98s/it][A100%|██████████| 1/1 [01:40<00:00, 100.98s/it]
 99%|█████████▉| 5168/5198 [4:01:13<47:51, 95.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.01s/it][A100%|██████████| 1/1 [01:41<00:00, 101.01s/it]
 99%|█████████▉| 5168/5198 [3:58:49<47:51, 95.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.02s/it][A100%|██████████| 1/1 [01:41<00:00, 101.02s/it]
 99%|█████████▉| 5168/5198 [3:56:00<47:51, 95.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4850
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.12s/it][A100%|██████████| 1/1 [01:34<00:00, 94.12s/it]
 99%|█████████▉| 5169/5198 [3:59:15<46:02, 95.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:36:38,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=5138, skipped=0, lr=[5.367380186784903e-07], mom=[(0.9, 0.999)]
steps: 5138 loss: 0.6412 iter time (s): 93.204 samples/sec: 1.373

100%|██████████| 1/1 [01:33<00:00, 93.97s/it][A100%|██████████| 1/1 [01:33<00:00, 93.97s/it]
 99%|█████████▉| 5169/5198 [4:02:48<46:00, 95.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.96s/it][A100%|██████████| 1/1 [01:33<00:00, 93.96s/it]
 99%|█████████▉| 5169/5198 [4:00:42<46:00, 95.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.97s/it][A100%|██████████| 1/1 [01:33<00:00, 93.97s/it]
 99%|█████████▉| 5169/5198 [4:00:08<46:00, 95.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.96s/it][A100%|██████████| 1/1 [01:33<00:00, 93.96s/it]
 99%|█████████▉| 5169/5198 [4:03:23<46:00, 95.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.96s/it][A100%|██████████| 1/1 [01:33<00:00, 93.96s/it]
 99%|█████████▉| 5169/5198 [4:02:47<46:00, 95.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:33<00:00, 93.96s/it][A100%|██████████| 1/1 [01:33<00:00, 93.96s/it]
 99%|█████████▉| 5169/5198 [3:57:34<46:00, 95.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4851

100%|██████████| 1/1 [01:33<00:00, 93.99s/it][A100%|██████████| 1/1 [01:33<00:00, 93.99s/it]
 99%|█████████▉| 5169/5198 [4:00:23<46:00, 95.21s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.43s/it][A100%|██████████| 1/1 [01:49<00:00, 109.43s/it]
 99%|█████████▉| 5170/5198 [4:01:04<46:27, 99.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:38:28,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=5139, skipped=0, lr=[5.349122265321892e-07], mom=[(0.9, 0.999)]
steps: 5139 loss: 0.6326 iter time (s): 109.285 samples/sec: 1.171

100%|██████████| 1/1 [01:50<00:00, 110.06s/it][A100%|██████████| 1/1 [01:50<00:00, 110.06s/it]
 99%|█████████▉| 5170/5198 [4:04:38<46:30, 99.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.07s/it][A100%|██████████| 1/1 [01:50<00:00, 110.07s/it]
 99%|█████████▉| 5170/5198 [4:02:33<46:30, 99.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.07s/it][A100%|██████████| 1/1 [01:50<00:00, 110.07s/it]
 99%|█████████▉| 5170/5198 [4:01:58<46:30, 99.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.14s/it][A100%|██████████| 1/1 [01:50<00:00, 110.14s/it]
 99%|█████████▉| 5170/5198 [4:05:13<46:31, 99.69s/it]
100%|██████████| 1/1 [01:50<00:00, 110.11s/it][A100%|██████████| 1/1 [01:50<00:00, 110.11s/it]
 99%|█████████▉| 5170/5198 [4:04:37<46:30, 99.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.08s/it][A100%|██████████| 1/1 [01:50<00:00, 110.08s/it]
 99%|█████████▉| 5170/5198 [4:02:13<46:30, 99.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.09s/it][A100%|██████████| 1/1 [01:50<00:00, 110.09s/it]
 99%|█████████▉| 5170/5198 [3:59:24<46:30, 99.66s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4852
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.43s/it][A100%|██████████| 1/1 [01:35<00:00, 95.43s/it]
 99%|█████████▉| 5171/5198 [4:02:40<44:15, 98.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:40:02,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=5140, skipped=0, lr=[5.330894713418226e-07], mom=[(0.9, 0.999)]
steps: 5140 loss: 0.5954 iter time (s): 93.382 samples/sec: 1.371

100%|██████████| 1/1 [01:34<00:00, 94.20s/it][A100%|██████████| 1/1 [01:34<00:00, 94.20s/it]
 99%|█████████▉| 5171/5198 [4:06:12<44:06, 98.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.19s/it][A100%|██████████| 1/1 [01:34<00:00, 94.19s/it]
 99%|█████████▉| 5171/5198 [4:04:07<44:06, 98.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.17s/it][A100%|██████████| 1/1 [01:34<00:00, 94.17s/it]
 99%|█████████▉| 5171/5198 [4:03:32<44:06, 98.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.08s/it][A100%|██████████| 1/1 [01:34<00:00, 94.08s/it]
 99%|█████████▉| 5171/5198 [4:06:47<44:06, 98.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.19s/it][A100%|██████████| 1/1 [01:34<00:00, 94.20s/it]
 99%|█████████▉| 5171/5198 [4:06:12<44:06, 98.03s/it]
100%|██████████| 1/1 [01:34<00:00, 94.16s/it][A100%|██████████| 1/1 [01:34<00:00, 94.16s/it]
 99%|█████████▉| 5171/5198 [4:03:47<44:06, 98.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.18s/it][A100%|██████████| 1/1 [01:34<00:00, 94.18s/it]
 99%|█████████▉| 5171/5198 [4:00:59<44:06, 98.02s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4853
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.17s/it][A100%|██████████| 1/1 [01:22<00:00, 82.17s/it]
 99%|█████████▉| 5172/5198 [4:04:02<40:32, 93.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:41:25,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=5141, skipped=0, lr=[5.312697536921742e-07], mom=[(0.9, 0.999)]
steps: 5141 loss: 0.6246 iter time (s): 82.178 samples/sec: 1.558

100%|██████████| 1/1 [01:22<00:00, 82.94s/it][A100%|██████████| 1/1 [01:22<00:00, 82.94s/it]
 99%|█████████▉| 5172/5198 [4:07:35<40:31, 93.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.02s/it][A100%|██████████| 1/1 [01:23<00:00, 83.02s/it]
 99%|█████████▉| 5172/5198 [4:05:30<40:31, 93.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.97s/it][A100%|██████████| 1/1 [01:22<00:00, 82.97s/it]
 99%|█████████▉| 5172/5198 [4:04:55<40:31, 93.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.06s/it][A100%|██████████| 1/1 [01:23<00:00, 83.06s/it]
 99%|█████████▉| 5172/5198 [4:08:10<40:31, 93.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.98s/it][A100%|██████████| 1/1 [01:22<00:00, 82.98s/it]
 99%|█████████▉| 5172/5198 [4:07:35<40:31, 93.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.02s/it][A100%|██████████| 1/1 [01:23<00:00, 83.02s/it]
 99%|█████████▉| 5172/5198 [4:05:10<40:31, 93.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.01s/it][A100%|██████████| 1/1 [01:23<00:00, 83.01s/it]
 99%|█████████▉| 5172/5198 [4:02:22<40:31, 93.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4854
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.14s/it][A100%|██████████| 1/1 [01:37<00:00, 97.14s/it]
100%|█████████▉| 5173/5198 [4:05:39<39:27, 94.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:43:02,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=5142, skipped=0, lr=[5.294530741670552e-07], mom=[(0.9, 0.999)]
steps: 5142 loss: 0.6306 iter time (s): 96.260 samples/sec: 1.330

100%|██████████| 1/1 [01:37<00:00, 97.14s/it][A100%|██████████| 1/1 [01:37<00:00, 97.14s/it]
100%|█████████▉| 5173/5198 [4:09:12<39:24, 94.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.07s/it][A100%|██████████| 1/1 [01:37<00:00, 97.07s/it]
100%|█████████▉| 5173/5198 [4:07:07<39:24, 94.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.08s/it][A100%|██████████| 1/1 [01:37<00:00, 97.08s/it]
100%|█████████▉| 5173/5198 [4:06:32<39:24, 94.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.01s/it][A100%|██████████| 1/1 [01:37<00:00, 97.01s/it]
100%|█████████▉| 5173/5198 [4:09:47<39:24, 94.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.06s/it][A100%|██████████| 1/1 [01:37<00:00, 97.06s/it]
100%|█████████▉| 5173/5198 [4:09:12<39:24, 94.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.06s/it][A100%|██████████| 1/1 [01:37<00:00, 97.06s/it]
100%|█████████▉| 5173/5198 [4:06:47<39:24, 94.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.06s/it][A100%|██████████| 1/1 [01:37<00:00, 97.06s/it]
100%|█████████▉| 5173/5198 [4:03:59<39:24, 94.58s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4855
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.32s/it][A100%|██████████| 1/1 [01:30<00:00, 90.32s/it]
[2024-06-26 21:44:33,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=5143, skipped=0, lr=[5.276394333493021e-07], mom=[(0.9, 0.999)]
steps: 5143 loss: 0.6405 iter time (s): 90.107 samples/sec: 1.421

100%|██████████| 1/1 [01:30<00:00, 90.87s/it][A100%|██████████| 1/1 [01:30<00:00, 90.87s/it]

100%|██████████| 1/1 [01:30<00:00, 90.92s/it][A100%|██████████| 1/1 [01:30<00:00, 90.92s/it]

100%|██████████| 1/1 [01:30<00:00, 90.91s/it][A100%|██████████| 1/1 [01:30<00:00, 90.91s/it]

100%|██████████| 1/1 [01:30<00:00, 90.96s/it][A100%|██████████| 1/1 [01:30<00:00, 90.96s/it]

100%|██████████| 1/1 [01:30<00:00, 90.95s/it][A100%|██████████| 1/1 [01:30<00:00, 90.95s/it]

100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]

100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
Checkpointing at shard 5173
[2024-06-26 21:44:34,639] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5143 is about to be saved!
[2024-06-26 21:44:35,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_00-model_states.pt...
[2024-06-26 21:44:40,138] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_07-model_states.pt...
[2024-06-26 21:44:40,345] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_02-model_states.pt...
[2024-06-26 21:44:40,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_08-model_states.pt...
[2024-06-26 21:44:44,455] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_00-model_states.pt.
[2024-06-26 21:44:49,812] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_05-model_states.pt...
[2024-06-26 21:44:49,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_06-model_states.pt...
[2024-06-26 21:44:53,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_01-model_states.pt...
[2024-06-26 21:44:53,974] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_04-model_states.pt...
[2024-06-26 21:44:54,228] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_03-model_states.pt...
[2024-06-26 21:47:59,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_02-model_states.pt.
[2024-06-26 21:47:59,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_07-model_states.pt.
[2024-06-26 21:47:59,583] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_08-model_states.pt.
[2024-06-26 21:47:59,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_06_model_states.pt...
[2024-06-26 21:47:59,721] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_01_model_states.pt
[2024-06-26 21:47:59,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_01_model_states.pt...
[2024-06-26 21:47:59,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_06_model_states.pt.
[2024-06-26 21:47:59,809] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
[2024-06-26 21:47:59,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_01_model_states.pt.
[2024-06-26 21:47:59,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
[2024-06-26 21:47:59,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_09-model_states.pt...
[2024-06-26 21:48:01,083] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_09-model_states.pt.
[2024-06-26 21:48:01,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_07_model_states.pt...
[2024-06-26 21:48:01,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_07_model_states.pt.
[2024-06-26 21:48:01,296] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
[2024-06-26 21:48:05,926] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_01-model_states.pt.
[2024-06-26 21:48:06,814] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_00_model_states.pt
[2024-06-26 21:48:06,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_00_model_states.pt...
[2024-06-26 21:48:07,621] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_00_model_states.pt.
[2024-06-26 21:48:07,621] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
[2024-06-26 21:48:23,748] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_06-model_states.pt.
[2024-06-26 21:48:23,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_05-model_states.pt.
[2024-06-26 21:48:23,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_03-model_states.pt.
[2024-06-26 21:48:23,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/layer_04-model_states.pt.
[2024-06-26 21:48:24,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_02_model_states.pt...
[2024-06-26 21:48:24,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_05_model_states.pt...
[2024-06-26 21:48:24,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_04_model_states.pt...
[2024-06-26 21:48:24,439] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_02_model_states.pt.
[2024-06-26 21:48:24,439] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
[2024-06-26 21:48:24,442] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_05_model_states.pt.
[2024-06-26 21:48:24,442] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
[2024-06-26 21:48:24,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_03_model_states.pt...
[2024-06-26 21:48:24,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_04_model_states.pt.
[2024-06-26 21:48:24,494] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
[2024-06-26 21:48:24,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5143/mp_rank_03_model_states.pt.
[2024-06-26 21:48:24,522] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5143 is ready now!
Checkpoint saved using --- 229.88425636291504 seconds ---
100%|█████████▉| 5174/5198 [4:12:28<1:05:01, 162.56s/it]100%|█████████▉| 5174/5198 [4:12:08<1:04:59, 162.46s/it]100%|█████████▉| 5174/5198 [4:11:03<1:05:17, 163.25s/it]100%|█████████▉| 5174/5198 [4:14:32<1:04:59, 162.47s/it]100%|█████████▉| 5174/5198 [4:11:53<1:05:00, 162.53s/it]100%|█████████▉| 5174/5198 [4:15:08<1:04:59, 162.50s/it]100%|█████████▉| 5174/5198 [4:09:20<1:04:58, 162.46s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4856
100%|█████████▉| 5174/5198 [4:14:34<1:05:02, 162.59s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.32s/it][A100%|██████████| 1/1 [01:24<00:00, 84.32s/it]
100%|█████████▉| 5175/5198 [4:12:27<53:31, 139.64s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:49:51,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=5144, skipped=0, lr=[5.258288318207735e-07], mom=[(0.9, 0.999)]
steps: 5144 loss: 0.6559 iter time (s): 86.281 samples/sec: 1.484

100%|██████████| 1/1 [01:26<00:00, 86.58s/it][A100%|██████████| 1/1 [01:26<00:00, 86.58s/it]
100%|█████████▉| 5175/5198 [4:16:00<53:36, 139.86s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.71s/it][A100%|██████████| 1/1 [01:26<00:00, 86.71s/it]
100%|█████████▉| 5175/5198 [4:13:55<53:37, 139.87s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.86s/it][A100%|██████████| 1/1 [01:26<00:00, 86.86s/it]
100%|█████████▉| 5175/5198 [4:13:20<53:37, 139.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.94s/it][A100%|██████████| 1/1 [01:26<00:00, 86.94s/it]
100%|█████████▉| 5175/5198 [4:16:35<53:37, 139.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.02s/it][A100%|██████████| 1/1 [01:27<00:00, 87.02s/it]
100%|█████████▉| 5175/5198 [4:16:00<53:37, 139.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.03s/it][A100%|██████████| 1/1 [01:27<00:00, 87.03s/it]
100%|█████████▉| 5175/5198 [4:13:35<53:37, 139.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.07s/it][A100%|██████████| 1/1 [01:27<00:00, 87.07s/it]
100%|█████████▉| 5175/5198 [4:10:47<53:37, 139.91s/it]  Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4857
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.84s/it][A100%|██████████| 1/1 [01:31<00:00, 91.84s/it]
100%|█████████▉| 5176/5198 [4:13:59<45:58, 125.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:51:23,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=5145, skipped=0, lr=[5.240212701623579e-07], mom=[(0.9, 0.999)]
steps: 5145 loss: 0.6193 iter time (s): 91.414 samples/sec: 1.400

100%|██████████| 1/1 [01:32<00:00, 92.29s/it][A100%|██████████| 1/1 [01:32<00:00, 92.29s/it]
100%|█████████▉| 5176/5198 [4:17:33<46:02, 125.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.26s/it][A100%|██████████| 1/1 [01:32<00:00, 92.26s/it]
100%|█████████▉| 5176/5198 [4:15:27<46:03, 125.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.22s/it][A100%|██████████| 1/1 [01:32<00:00, 92.22s/it]
100%|█████████▉| 5176/5198 [4:14:52<46:03, 125.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.31s/it][A100%|██████████| 1/1 [01:32<00:00, 92.31s/it]
100%|█████████▉| 5176/5198 [4:18:07<46:03, 125.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.26s/it][A100%|██████████| 1/1 [01:32<00:00, 92.26s/it]
100%|█████████▉| 5176/5198 [4:17:32<46:03, 125.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.28s/it][A100%|██████████| 1/1 [01:32<00:00, 92.28s/it]
100%|█████████▉| 5176/5198 [4:15:08<46:03, 125.62s/it]
100%|██████████| 1/1 [01:32<00:00, 92.25s/it][A100%|██████████| 1/1 [01:32<00:00, 92.25s/it]
100%|█████████▉| 5176/5198 [4:12:19<46:03, 125.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4858

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.71s/it][A100%|██████████| 1/1 [01:36<00:00, 96.71s/it]
100%|█████████▉| 5177/5198 [4:15:36<40:53, 116.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:53:00,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=5146, skipped=0, lr=[5.22216748953964e-07], mom=[(0.9, 0.999)]
steps: 5146 loss: 0.5979 iter time (s): 96.143 samples/sec: 1.331

100%|██████████| 1/1 [01:36<00:00, 96.97s/it][A100%|██████████| 1/1 [01:36<00:00, 96.97s/it]
100%|█████████▉| 5177/5198 [4:19:10<40:57, 117.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.03s/it][A100%|██████████| 1/1 [01:37<00:00, 97.03s/it]
100%|█████████▉| 5177/5198 [4:17:04<40:57, 117.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.03s/it][A100%|██████████| 1/1 [01:37<00:00, 97.03s/it]
100%|█████████▉| 5177/5198 [4:16:29<40:57, 117.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.95s/it][A100%|██████████| 1/1 [01:36<00:00, 96.95s/it]
100%|█████████▉| 5177/5198 [4:19:44<40:57, 117.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.00s/it][A100%|██████████| 1/1 [01:37<00:00, 97.00s/it]
100%|█████████▉| 5177/5198 [4:16:45<40:57, 117.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.47s/it][A100%|██████████| 1/1 [01:37<00:00, 97.47s/it]
100%|█████████▉| 5177/5198 [4:13:57<41:00, 117.17s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4859
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.72s/it][A100%|██████████| 1/1 [01:37<00:00, 97.72s/it]
100%|█████████▉| 5177/5198 [4:19:10<41:02, 117.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.29s/it][A100%|██████████| 1/1 [01:54<00:00, 114.29s/it]
100%|█████████▉| 5178/5198 [4:17:30<38:41, 116.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:54:55,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=5147, skipped=0, lr=[5.204152687745279e-07], mom=[(0.9, 0.999)]
steps: 5147 loss: 0.6311 iter time (s): 113.557 samples/sec: 1.127

100%|██████████| 1/1 [01:54<00:00, 114.81s/it][A100%|██████████| 1/1 [01:54<00:00, 114.81s/it]
100%|█████████▉| 5178/5198 [4:18:59<38:47, 116.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.04s/it][A100%|██████████| 1/1 [01:55<00:00, 115.04s/it]
100%|█████████▉| 5178/5198 [4:21:05<38:48, 116.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.95s/it][A100%|██████████| 1/1 [01:54<00:00, 114.95s/it]
100%|█████████▉| 5178/5198 [4:18:24<38:48, 116.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.91s/it][A100%|██████████| 1/1 [01:54<00:00, 114.91s/it]
100%|█████████▉| 5178/5198 [4:21:39<38:47, 116.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.20s/it][A100%|██████████| 1/1 [01:54<00:00, 114.21s/it]
100%|█████████▉| 5178/5198 [4:21:04<38:46, 116.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.90s/it][A100%|██████████| 1/1 [01:54<00:00, 114.90s/it]
100%|█████████▉| 5178/5198 [4:18:40<38:48, 116.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.46s/it][A100%|██████████| 1/1 [01:54<00:00, 114.46s/it]
100%|█████████▉| 5178/5198 [4:15:51<38:47, 116.36s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4860
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:18<00:00, 138.64s/it][A100%|██████████| 1/1 [02:18<00:00, 138.64s/it]
100%|█████████▉| 5179/5198 [4:19:49<38:55, 122.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:57:14,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=5148, skipped=0, lr=[5.186168302020114e-07], mom=[(0.9, 0.999)]
steps: 5148 loss: 0.6264 iter time (s): 138.842 samples/sec: 0.922

100%|██████████| 1/1 [02:18<00:00, 138.87s/it][A100%|██████████| 1/1 [02:18<00:00, 138.87s/it]
100%|█████████▉| 5179/5198 [4:18:10<38:59, 123.12s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4861
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.47s/it][A100%|██████████| 1/1 [02:19<00:00, 139.47s/it]
100%|█████████▉| 5179/5198 [4:23:24<39:03, 123.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.64s/it][A100%|██████████| 1/1 [02:19<00:00, 139.64s/it]
100%|█████████▉| 5179/5198 [4:21:19<39:03, 123.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.54s/it][A100%|██████████| 1/1 [02:19<00:00, 139.54s/it]
100%|█████████▉| 5179/5198 [4:20:44<39:03, 123.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.53s/it][A100%|██████████| 1/1 [02:19<00:00, 139.54s/it]
100%|█████████▉| 5179/5198 [4:23:59<39:03, 123.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.47s/it][A100%|██████████| 1/1 [02:19<00:00, 139.47s/it]
100%|█████████▉| 5179/5198 [4:23:23<39:02, 123.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:19<00:00, 139.46s/it][A100%|██████████| 1/1 [02:19<00:00, 139.46s/it]
100%|█████████▉| 5179/5198 [4:20:59<39:03, 123.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.05s/it][A100%|██████████| 1/1 [01:39<00:00, 99.05s/it]
100%|█████████▉| 5180/5198 [4:21:29<34:44, 115.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 21:58:52,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=5149, skipped=0, lr=[5.168214338133942e-07], mom=[(0.9, 0.999)]
steps: 5149 loss: 0.5959 iter time (s): 97.960 samples/sec: 1.307

100%|██████████| 1/1 [01:38<00:00, 98.10s/it][A100%|██████████| 1/1 [01:38<00:00, 98.10s/it]
100%|█████████▉| 5180/5198 [4:25:02<34:43, 115.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.17s/it][A100%|██████████| 1/1 [01:38<00:00, 98.17s/it]
100%|█████████▉| 5180/5198 [4:22:57<34:44, 115.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.09s/it][A100%|██████████| 1/1 [01:38<00:00, 98.09s/it]
100%|█████████▉| 5180/5198 [4:25:37<34:43, 115.77s/it]
100%|██████████| 1/1 [01:38<00:00, 98.14s/it][A100%|██████████| 1/1 [01:38<00:00, 98.14s/it]
100%|█████████▉| 5180/5198 [4:22:22<34:44, 115.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.22s/it][A100%|██████████| 1/1 [01:38<00:00, 98.22s/it]
100%|█████████▉| 5180/5198 [4:25:02<34:43, 115.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.21s/it][A100%|██████████| 1/1 [01:38<00:00, 98.21s/it]
100%|█████████▉| 5180/5198 [4:22:37<34:44, 115.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.80s/it][A100%|██████████| 1/1 [01:38<00:00, 98.80s/it]
100%|█████████▉| 5180/5198 [4:19:49<34:44, 115.82s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4862
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.41s/it][A100%|██████████| 1/1 [01:47<00:00, 107.41s/it]
100%|█████████▉| 5181/5198 [4:23:16<32:06, 113.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:00:40,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=5150, skipped=0, lr=[5.150290801846857e-07], mom=[(0.9, 0.999)]
steps: 5150 loss: 0.5940 iter time (s): 106.909 samples/sec: 1.197

100%|██████████| 1/1 [01:47<00:00, 107.75s/it][A100%|██████████| 1/1 [01:47<00:00, 107.75s/it]
100%|█████████▉| 5181/5198 [4:26:50<32:07, 113.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.70s/it][A100%|██████████| 1/1 [01:47<00:00, 107.70s/it]
100%|█████████▉| 5181/5198 [4:24:45<32:07, 113.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.72s/it][A100%|██████████| 1/1 [01:47<00:00, 107.72s/it]
100%|█████████▉| 5181/5198 [4:24:10<32:07, 113.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.83s/it][A100%|██████████| 1/1 [01:47<00:00, 107.83s/it]
100%|█████████▉| 5181/5198 [4:27:25<32:07, 113.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.73s/it][A100%|██████████| 1/1 [01:47<00:00, 107.73s/it]
100%|█████████▉| 5181/5198 [4:26:49<32:07, 113.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.75s/it][A100%|██████████| 1/1 [01:47<00:00, 107.75s/it]
100%|█████████▉| 5181/5198 [4:24:25<32:07, 113.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:47<00:00, 107.77s/it][A100%|██████████| 1/1 [01:47<00:00, 107.77s/it]
100%|█████████▉| 5181/5198 [4:21:37<32:07, 113.41s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4863
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.59s/it][A100%|██████████| 1/1 [01:56<00:00, 116.59s/it]
100%|█████████▉| 5182/5198 [4:25:13<30:29, 114.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:02:37,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=5151, skipped=0, lr=[5.132397698909171e-07], mom=[(0.9, 0.999)]
steps: 5151 loss: 0.5731 iter time (s): 116.106 samples/sec: 1.102

100%|██████████| 1/1 [01:56<00:00, 116.96s/it][A100%|██████████| 1/1 [01:56<00:00, 116.96s/it]
100%|█████████▉| 5182/5198 [4:28:47<30:31, 114.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.88s/it][A100%|██████████| 1/1 [01:56<00:00, 116.89s/it]
100%|█████████▉| 5182/5198 [4:26:42<30:30, 114.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.97s/it][A100%|██████████| 1/1 [01:56<00:00, 116.97s/it]
100%|█████████▉| 5182/5198 [4:26:07<30:31, 114.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.89s/it][A100%|██████████| 1/1 [01:56<00:00, 116.89s/it]
100%|█████████▉| 5182/5198 [4:29:22<30:31, 114.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.91s/it][A100%|██████████| 1/1 [01:56<00:00, 116.91s/it]
100%|█████████▉| 5182/5198 [4:28:46<30:30, 114.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.95s/it][A100%|██████████| 1/1 [01:56<00:00, 116.95s/it]
100%|█████████▉| 5182/5198 [4:26:22<30:31, 114.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.93s/it][A100%|██████████| 1/1 [01:56<00:00, 116.93s/it]
100%|█████████▉| 5182/5198 [4:23:33<30:31, 114.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4864
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.05s/it][A100%|██████████| 1/1 [01:32<00:00, 92.05s/it]
100%|█████████▉| 5183/5198 [4:26:45<26:55, 107.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:04:09,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=5152, skipped=0, lr=[5.114535035061452e-07], mom=[(0.9, 0.999)]
steps: 5152 loss: 0.5684 iter time (s): 90.651 samples/sec: 1.412

100%|██████████| 1/1 [01:31<00:00, 91.48s/it][A100%|██████████| 1/1 [01:31<00:00, 91.48s/it]
100%|█████████▉| 5183/5198 [4:30:19<26:53, 107.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.50s/it][A100%|██████████| 1/1 [01:31<00:00, 91.50s/it]
100%|█████████▉| 5183/5198 [4:28:13<26:53, 107.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.48s/it][A100%|██████████| 1/1 [01:31<00:00, 91.48s/it]
100%|█████████▉| 5183/5198 [4:27:38<26:53, 107.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.52s/it][A100%|██████████| 1/1 [01:31<00:00, 91.52s/it]
100%|█████████▉| 5183/5198 [4:30:53<26:53, 107.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.51s/it][A100%|██████████| 1/1 [01:31<00:00, 91.51s/it]
100%|█████████▉| 5183/5198 [4:30:18<26:53, 107.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.48s/it][A100%|██████████| 1/1 [01:31<00:00, 91.48s/it]
100%|█████████▉| 5183/5198 [4:27:54<26:53, 107.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.48s/it][A100%|██████████| 1/1 [01:31<00:00, 91.48s/it]
100%|█████████▉| 5183/5198 [4:25:05<26:53, 107.57s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4865
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.70s/it][A100%|██████████| 1/1 [01:23<00:00, 83.70s/it]
100%|█████████▉| 5184/5198 [4:28:09<23:28, 100.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:05:32,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=5153, skipped=0, lr=[5.096702816034464e-07], mom=[(0.9, 0.999)]
steps: 5153 loss: 0.6324 iter time (s): 82.910 samples/sec: 1.544

100%|██████████| 1/1 [01:23<00:00, 83.70s/it][A100%|██████████| 1/1 [01:23<00:00, 83.70s/it]
100%|█████████▉| 5184/5198 [4:31:42<23:25, 100.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.76s/it][A100%|██████████| 1/1 [01:23<00:00, 83.76s/it]
100%|█████████▉| 5184/5198 [4:29:37<23:25, 100.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.65s/it][A100%|██████████| 1/1 [01:23<00:00, 83.65s/it]
100%|█████████▉| 5184/5198 [4:29:02<23:25, 100.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.73s/it][A100%|██████████| 1/1 [01:23<00:00, 83.73s/it]
100%|█████████▉| 5184/5198 [4:32:17<23:25, 100.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.71s/it][A100%|██████████| 1/1 [01:23<00:00, 83.71s/it]
100%|█████████▉| 5184/5198 [4:31:42<23:25, 100.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.70s/it][A100%|██████████| 1/1 [01:23<00:00, 83.70s/it]
100%|█████████▉| 5184/5198 [4:29:17<23:25, 100.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.72s/it][A100%|██████████| 1/1 [01:23<00:00, 83.72s/it]
100%|█████████▉| 5184/5198 [4:26:29<23:25, 100.42s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4866
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.69s/it][A100%|██████████| 1/1 [01:43<00:00, 103.69s/it]
100%|█████████▉| 5185/5198 [4:29:53<22:00, 101.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:07:17,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=5154, skipped=0, lr=[5.078901047549258e-07], mom=[(0.9, 0.999)]
steps: 5154 loss: 0.6018 iter time (s): 103.605 samples/sec: 1.235

100%|██████████| 1/1 [01:44<00:00, 104.38s/it][A100%|██████████| 1/1 [01:44<00:00, 104.38s/it]
100%|█████████▉| 5185/5198 [4:33:27<22:00, 101.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.30s/it][A100%|██████████| 1/1 [01:44<00:00, 104.30s/it]
100%|█████████▉| 5185/5198 [4:31:21<22:00, 101.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.48s/it][A100%|██████████| 1/1 [01:44<00:00, 104.48s/it]
100%|█████████▉| 5185/5198 [4:30:46<22:01, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.37s/it][A100%|██████████| 1/1 [01:44<00:00, 104.37s/it]
100%|█████████▉| 5185/5198 [4:34:01<22:00, 101.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.43s/it][A100%|██████████| 1/1 [01:44<00:00, 104.43s/it]
100%|█████████▉| 5185/5198 [4:33:26<22:00, 101.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.45s/it][A100%|██████████| 1/1 [01:44<00:00, 104.45s/it]
100%|█████████▉| 5185/5198 [4:31:02<22:01, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.43s/it][A100%|██████████| 1/1 [01:44<00:00, 104.43s/it]
100%|█████████▉| 5185/5198 [4:28:13<22:01, 101.62s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4867
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.52s/it][A100%|██████████| 1/1 [01:32<00:00, 92.52s/it]
100%|█████████▉| 5186/5198 [4:31:26<19:46, 98.89s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:08:49,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=5155, skipped=0, lr=[5.061129735317039e-07], mom=[(0.9, 0.999)]
steps: 5155 loss: 0.6491 iter time (s): 91.507 samples/sec: 1.399

100%|██████████| 1/1 [01:32<00:00, 92.38s/it][A100%|██████████| 1/1 [01:32<00:00, 92.38s/it]
100%|█████████▉| 5186/5198 [4:34:59<19:46, 98.84s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.40s/it][A100%|██████████| 1/1 [01:32<00:00, 92.40s/it]
100%|█████████▉| 5186/5198 [4:32:54<19:45, 98.83s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.34s/it][A100%|██████████| 1/1 [01:32<00:00, 92.34s/it]
100%|█████████▉| 5186/5198 [4:32:19<19:46, 98.84s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.36s/it][A100%|██████████| 1/1 [01:32<00:00, 92.36s/it]
100%|█████████▉| 5186/5198 [4:35:34<19:46, 98.83s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.33s/it][A100%|██████████| 1/1 [01:32<00:00, 92.33s/it]
100%|█████████▉| 5186/5198 [4:34:58<19:45, 98.83s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.34s/it][A100%|██████████| 1/1 [01:32<00:00, 92.34s/it]
100%|█████████▉| 5186/5198 [4:32:34<19:46, 98.84s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.34s/it][A100%|██████████| 1/1 [01:32<00:00, 92.34s/it]
100%|█████████▉| 5186/5198 [4:29:45<19:46, 98.84s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4868
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.19s/it][A100%|██████████| 1/1 [01:45<00:00, 105.19s/it]
100%|█████████▉| 5187/5198 [4:33:11<18:29, 100.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:10:35,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=5156, skipped=0, lr=[5.04338888503932e-07], mom=[(0.9, 0.999)]
steps: 5156 loss: 0.6177 iter time (s): 104.927 samples/sec: 1.220

100%|██████████| 1/1 [01:45<00:00, 105.77s/it][A100%|██████████| 1/1 [01:45<00:00, 105.77s/it]
100%|█████████▉| 5187/5198 [4:36:45<18:30, 100.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.81s/it][A100%|██████████| 1/1 [01:45<00:00, 105.81s/it]
100%|█████████▉| 5187/5198 [4:34:39<18:30, 100.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.82s/it][A100%|██████████| 1/1 [01:45<00:00, 105.82s/it]
100%|█████████▉| 5187/5198 [4:34:04<18:30, 100.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.84s/it][A100%|██████████| 1/1 [01:45<00:00, 105.84s/it]
100%|█████████▉| 5187/5198 [4:37:20<18:30, 100.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.79s/it][A100%|██████████| 1/1 [01:45<00:00, 105.79s/it]
100%|█████████▉| 5187/5198 [4:36:44<18:30, 100.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.78s/it][A100%|██████████| 1/1 [01:45<00:00, 105.78s/it]
100%|█████████▉| 5187/5198 [4:34:20<18:30, 100.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.80s/it][A100%|██████████| 1/1 [01:45<00:00, 105.80s/it]
100%|█████████▉| 5187/5198 [4:31:31<18:30, 100.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4869
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.85s/it][A100%|██████████| 1/1 [01:38<00:00, 98.85s/it]
100%|█████████▉| 5188/5198 [4:34:50<16:42, 100.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:12:14,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=5157, skipped=0, lr=[5.025678502407778e-07], mom=[(0.9, 0.999)]
steps: 5157 loss: 0.6141 iter time (s): 97.951 samples/sec: 1.307

100%|██████████| 1/1 [01:38<00:00, 98.76s/it][A100%|██████████| 1/1 [01:38<00:00, 98.76s/it]
100%|█████████▉| 5188/5198 [4:38:24<16:42, 100.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.80s/it][A100%|██████████| 1/1 [01:38<00:00, 98.80s/it]
100%|█████████▉| 5188/5198 [4:36:18<16:42, 100.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.70s/it][A100%|██████████| 1/1 [01:38<00:00, 98.70s/it]
100%|█████████▉| 5188/5198 [4:35:43<16:42, 100.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.74s/it][A100%|██████████| 1/1 [01:38<00:00, 98.74s/it]
100%|█████████▉| 5188/5198 [4:38:58<16:42, 100.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.77s/it][A100%|██████████| 1/1 [01:38<00:00, 98.77s/it]
100%|█████████▉| 5188/5198 [4:35:59<16:42, 100.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.82s/it][A100%|██████████| 1/1 [01:38<00:00, 98.82s/it]
100%|█████████▉| 5188/5198 [4:38:23<16:42, 100.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.76s/it][A100%|██████████| 1/1 [01:38<00:00, 98.76s/it]
100%|█████████▉| 5188/5198 [4:33:10<16:42, 100.28s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4870
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:15<00:00, 135.85s/it][A100%|██████████| 1/1 [02:15<00:00, 135.85s/it]
100%|█████████▉| 5189/5198 [4:37:06<16:38, 110.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:14:31,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=5158, skipped=0, lr=[5.007998593104365e-07], mom=[(0.9, 0.999)]
steps: 5158 loss: 0.6148 iter time (s): 136.211 samples/sec: 0.940

100%|██████████| 1/1 [02:17<00:00, 137.03s/it][A100%|██████████| 1/1 [02:17<00:00, 137.03s/it]
100%|█████████▉| 5189/5198 [4:40:41<16:41, 111.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:16<00:00, 136.95s/it][A100%|██████████| 1/1 [02:16<00:00, 136.95s/it]
100%|█████████▉| 5189/5198 [4:38:35<16:41, 111.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.06s/it][A100%|██████████| 1/1 [02:17<00:00, 137.06s/it]
100%|█████████▉| 5189/5198 [4:38:00<16:41, 111.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.04s/it][A100%|██████████| 1/1 [02:17<00:00, 137.04s/it]
100%|█████████▉| 5189/5198 [4:41:15<16:41, 111.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.01s/it][A100%|██████████| 1/1 [02:17<00:00, 137.01s/it]
100%|█████████▉| 5189/5198 [4:38:16<16:41, 111.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.02s/it][A100%|██████████| 1/1 [02:17<00:00, 137.02s/it]
100%|█████████▉| 5189/5198 [4:40:40<16:41, 111.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:17<00:00, 137.02s/it][A100%|██████████| 1/1 [02:17<00:00, 137.03s/it]
100%|█████████▉| 5189/5198 [4:35:27<16:41, 111.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4871
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.52s/it][A100%|██████████| 1/1 [01:45<00:00, 105.52s/it]
100%|█████████▉| 5190/5198 [4:38:52<14:35, 109.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:16:16,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=5159, skipped=0, lr=[4.990349162801244e-07], mom=[(0.9, 0.999)]
steps: 5159 loss: 0.6130 iter time (s): 103.956 samples/sec: 1.231

100%|██████████| 1/1 [01:44<00:00, 104.80s/it][A100%|██████████| 1/1 [01:44<00:00, 104.80s/it]
100%|█████████▉| 5190/5198 [4:42:25<14:34, 109.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.82s/it][A100%|██████████| 1/1 [01:44<00:00, 104.82s/it]
100%|█████████▉| 5190/5198 [4:40:20<14:34, 109.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.78s/it][A100%|██████████| 1/1 [01:44<00:00, 104.78s/it]
100%|█████████▉| 5190/5198 [4:39:45<14:34, 109.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.76s/it][A100%|██████████| 1/1 [01:44<00:00, 104.76s/it]
100%|█████████▉| 5190/5198 [4:43:00<14:34, 109.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.75s/it][A100%|██████████| 1/1 [01:44<00:00, 104.75s/it]
100%|█████████▉| 5190/5198 [4:42:25<14:34, 109.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.81s/it][A100%|██████████| 1/1 [01:44<00:00, 104.81s/it]
100%|█████████▉| 5190/5198 [4:40:00<14:34, 109.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.80s/it][A100%|██████████| 1/1 [01:44<00:00, 104.80s/it]
100%|█████████▉| 5190/5198 [4:37:12<14:34, 109.36s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4872
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.39s/it][A100%|██████████| 1/1 [01:24<00:00, 84.39s/it]
100%|█████████▉| 5191/5198 [4:40:16<11:53, 101.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:17:40,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=5160, skipped=0, lr=[4.972730217160763e-07], mom=[(0.9, 0.999)]
steps: 5160 loss: 0.6135 iter time (s): 83.159 samples/sec: 1.539

100%|██████████| 1/1 [01:24<00:00, 84.12s/it][A100%|██████████| 1/1 [01:24<00:00, 84.13s/it]
100%|█████████▉| 5191/5198 [4:43:50<11:52, 101.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.06s/it][A100%|██████████| 1/1 [01:24<00:00, 84.06s/it]
100%|█████████▉| 5191/5198 [4:41:44<11:52, 101.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.02s/it][A100%|██████████| 1/1 [01:24<00:00, 84.02s/it]
100%|█████████▉| 5191/5198 [4:41:09<11:52, 101.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.07s/it][A100%|██████████| 1/1 [01:24<00:00, 84.07s/it]
100%|█████████▉| 5191/5198 [4:44:24<11:52, 101.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.02s/it][A100%|██████████| 1/1 [01:24<00:00, 84.02s/it]
100%|█████████▉| 5191/5198 [4:43:49<11:52, 101.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.06s/it][A100%|██████████| 1/1 [01:24<00:00, 84.06s/it]
100%|█████████▉| 5191/5198 [4:41:25<11:52, 101.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.03s/it][A100%|██████████| 1/1 [01:24<00:00, 84.03s/it]
100%|█████████▉| 5191/5198 [4:38:36<11:52, 101.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4873
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.14s/it][A100%|██████████| 1/1 [01:36<00:00, 96.14s/it]
100%|█████████▉| 5192/5198 [4:41:53<10:01, 100.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:19:16,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=5161, skipped=0, lr=[4.955141761835523e-07], mom=[(0.9, 0.999)]
steps: 5161 loss: 0.6463 iter time (s): 95.793 samples/sec: 1.336

100%|██████████| 1/1 [01:36<00:00, 96.61s/it][A100%|██████████| 1/1 [01:36<00:00, 96.61s/it]
100%|█████████▉| 5192/5198 [4:45:26<10:01, 100.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.65s/it][A100%|██████████| 1/1 [01:36<00:00, 96.65s/it]
100%|█████████▉| 5192/5198 [4:43:21<10:01, 100.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.60s/it][A100%|██████████| 1/1 [01:36<00:00, 96.60s/it]
100%|█████████▉| 5192/5198 [4:42:46<10:01, 100.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.60s/it][A100%|██████████| 1/1 [01:36<00:00, 96.60s/it]
100%|█████████▉| 5192/5198 [4:46:01<10:01, 100.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.65s/it][A100%|██████████| 1/1 [01:36<00:00, 96.65s/it]
100%|█████████▉| 5192/5198 [4:45:25<10:01, 100.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.63s/it][A100%|██████████| 1/1 [01:36<00:00, 96.63s/it]
100%|█████████▉| 5192/5198 [4:43:01<10:01, 100.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.64s/it][A100%|██████████| 1/1 [01:36<00:00, 96.64s/it]
100%|█████████▉| 5192/5198 [4:40:13<10:01, 100.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4874
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.04s/it][A100%|██████████| 1/1 [01:29<00:00, 89.04s/it]
100%|█████████▉| 5193/5198 [4:43:22<08:04, 96.96s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:20:45,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=5162, skipped=0, lr=[4.937583802468346e-07], mom=[(0.9, 0.999)]
steps: 5162 loss: 0.5872 iter time (s): 88.224 samples/sec: 1.451

100%|██████████| 1/1 [01:28<00:00, 88.95s/it][A100%|██████████| 1/1 [01:28<00:00, 88.95s/it]
100%|█████████▉| 5193/5198 [4:46:55<08:04, 96.86s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.03s/it][A100%|██████████| 1/1 [01:29<00:00, 89.03s/it]
100%|█████████▉| 5193/5198 [4:44:50<08:04, 96.88s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.10s/it][A100%|██████████| 1/1 [01:29<00:00, 89.10s/it]
100%|█████████▉| 5193/5198 [4:44:15<08:04, 96.88s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.05s/it][A100%|██████████| 1/1 [01:29<00:00, 89.05s/it]
100%|█████████▉| 5193/5198 [4:47:30<08:04, 96.87s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.03s/it][A100%|██████████| 1/1 [01:29<00:00, 89.03s/it]
100%|█████████▉| 5193/5198 [4:46:54<08:04, 96.87s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.05s/it][A100%|██████████| 1/1 [01:29<00:00, 89.05s/it]
100%|█████████▉| 5193/5198 [4:44:30<08:04, 96.88s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.04s/it][A100%|██████████| 1/1 [01:29<00:00, 89.04s/it]
100%|█████████▉| 5193/5198 [4:41:42<08:04, 96.87s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4875
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.10s/it][A100%|██████████| 1/1 [01:44<00:00, 104.10s/it]
100%|█████████▉| 5194/5198 [4:45:06<06:36, 99.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:22:30,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=5163, skipped=0, lr=[4.920056344692262e-07], mom=[(0.9, 0.999)]
steps: 5163 loss: 0.6114 iter time (s): 103.963 samples/sec: 1.231

100%|██████████| 1/1 [01:44<00:00, 104.81s/it][A100%|██████████| 1/1 [01:44<00:00, 104.81s/it]
100%|█████████▉| 5194/5198 [4:48:40<06:36, 99.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.75s/it][A100%|██████████| 1/1 [01:44<00:00, 104.76s/it]
100%|█████████▉| 5194/5198 [4:46:35<06:36, 99.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.81s/it][A100%|██████████| 1/1 [01:44<00:00, 104.81s/it]
100%|█████████▉| 5194/5198 [4:46:00<06:37, 99.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.80s/it][A100%|██████████| 1/1 [01:44<00:00, 104.80s/it]
100%|█████████▉| 5194/5198 [4:49:15<06:36, 99.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.78s/it][A100%|██████████| 1/1 [01:44<00:00, 104.78s/it]
100%|█████████▉| 5194/5198 [4:48:39<06:36, 99.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.77s/it][A100%|██████████| 1/1 [01:44<00:00, 104.77s/it]
100%|█████████▉| 5194/5198 [4:46:15<06:36, 99.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.78s/it][A100%|██████████| 1/1 [01:44<00:00, 104.78s/it]
100%|█████████▉| 5194/5198 [4:43:26<06:36, 99.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4876
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.96s/it][A100%|██████████| 1/1 [01:31<00:00, 91.96s/it]
100%|█████████▉| 5195/5198 [4:46:38<04:51, 97.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:24:02,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=5164, skipped=0, lr=[4.902559394130511e-07], mom=[(0.9, 0.999)]
steps: 5164 loss: 0.5683 iter time (s): 90.893 samples/sec: 1.408

100%|██████████| 1/1 [01:31<00:00, 91.67s/it][A100%|██████████| 1/1 [01:31<00:00, 91.67s/it]
100%|█████████▉| 5195/5198 [4:50:12<04:50, 96.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.66s/it][A100%|██████████| 1/1 [01:31<00:00, 91.66s/it]
100%|█████████▉| 5195/5198 [4:48:06<04:50, 96.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.64s/it][A100%|██████████| 1/1 [01:31<00:00, 91.64s/it]
100%|█████████▉| 5195/5198 [4:47:31<04:50, 96.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.69s/it][A100%|██████████| 1/1 [01:31<00:00, 91.69s/it]
100%|█████████▉| 5195/5198 [4:50:46<04:50, 96.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.68s/it][A100%|██████████| 1/1 [01:31<00:00, 91.68s/it]
100%|█████████▉| 5195/5198 [4:50:11<04:50, 96.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.68s/it][A100%|██████████| 1/1 [01:31<00:00, 91.68s/it]
100%|█████████▉| 5195/5198 [4:47:47<04:50, 96.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.67s/it][A100%|██████████| 1/1 [01:31<00:00, 91.67s/it]
100%|█████████▉| 5195/5198 [4:44:58<04:50, 96.98s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4877
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.48s/it][A100%|██████████| 1/1 [01:25<00:00, 85.48s/it]
100%|█████████▉| 5196/5198 [4:48:04<03:07, 93.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-06-26 22:25:27,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=5165, skipped=0, lr=[4.885092956396539e-07], mom=[(0.9, 0.999)]
steps: 5165 loss: 0.5790 iter time (s): 84.656 samples/sec: 1.512

100%|██████████| 1/1 [01:25<00:00, 85.51s/it][A100%|██████████| 1/1 [01:25<00:00, 85.51s/it]
100%|█████████▉| 5196/5198 [4:51:37<03:07, 93.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.49s/it][A100%|██████████| 1/1 [01:25<00:00, 85.49s/it]
100%|█████████▉| 5196/5198 [4:49:32<03:07, 93.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.53s/it][A100%|██████████| 1/1 [01:25<00:00, 85.53s/it]
100%|█████████▉| 5196/5198 [4:48:57<03:07, 93.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.47s/it][A100%|██████████| 1/1 [01:25<00:00, 85.47s/it]
100%|█████████▉| 5196/5198 [4:51:36<03:07, 93.53s/it]
100%|██████████| 1/1 [01:25<00:00, 85.52s/it][A100%|██████████| 1/1 [01:25<00:00, 85.52s/it]
100%|█████████▉| 5196/5198 [4:52:12<03:07, 93.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.49s/it][A100%|██████████| 1/1 [01:25<00:00, 85.49s/it]
100%|█████████▉| 5196/5198 [4:49:12<03:07, 93.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [01:25<00:00, 85.48s/it][A100%|██████████| 1/1 [01:25<00:00, 85.48s/it]
100%|█████████▉| 5196/5198 [4:46:24<03:07, 93.53s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4878
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.76s/it][A100%|██████████| 1/1 [01:45<00:00, 105.76s/it]
100%|█████████▉| 5197/5198 [4:49:50<01:37, 97.33s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:49:50<00:00, 68.18s/it]100%|██████████| 5198/5198 [4:49:50<00:00,  3.35s/it]
[2024-06-26 22:27:14,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=5166, skipped=0, lr=[4.867657037094025e-07], mom=[(0.9, 0.999)]
steps: 5166 loss: 0.6070 iter time (s): 105.762 samples/sec: 1.210

100%|██████████| 1/1 [01:46<00:00, 106.51s/it][A100%|██████████| 1/1 [01:46<00:00, 106.51s/it]
100%|█████████▉| 5197/5198 [4:53:24<01:37, 97.43s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:53:24<00:00,  3.39s/it]

100%|██████████| 1/1 [01:46<00:00, 106.58s/it][A100%|██████████| 1/1 [01:46<00:00, 106.58s/it]
100%|█████████▉| 5197/5198 [4:51:18<01:37, 97.45s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:51:18<00:00,  3.36s/it]

100%|██████████| 1/1 [01:46<00:00, 106.51s/it][A100%|██████████| 1/1 [01:46<00:00, 106.51s/it]
100%|█████████▉| 5197/5198 [4:50:43<01:37, 97.44s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:50:43<00:00,  3.36s/it]

100%|██████████| 1/1 [01:46<00:00, 106.52s/it][A100%|██████████| 1/1 [01:46<00:00, 106.52s/it]
100%|█████████▉| 5197/5198 [4:53:58<01:37, 97.44s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:53:58<00:00,  3.39s/it]

100%|██████████| 1/1 [01:46<00:00, 106.63s/it][A100%|██████████| 1/1 [01:46<00:00, 106.63s/it]
100%|█████████▉| 5197/5198 [4:53:23<01:37, 97.46s/it]
0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:53:23<00:00,  3.39s/it]

100%|██████████| 1/1 [01:46<00:00, 106.61s/it][A100%|██████████| 1/1 [01:46<00:00, 106.61s/it]
100%|█████████▉| 5197/5198 [4:50:59<01:37, 97.46s/it]
100%|██████████| 1/1 [01:46<00:00, 106.61s/it][A100%|██████████| 1/1 [01:46<00:00, 106.61s/it]
100%|█████████▉| 5197/5198 [4:48:10<01:37, 97.46s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_4879

0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:50:59<00:00,  3.36s/it]
Training on 0 of 98 sentences.

0it [00:00, ?it/s][A0it [00:00, ?it/s]
100%|██████████| 5198/5198 [4:48:10<00:00,  3.33s/it]
--- FINISHED ---
Checkpointing at shard 5197
[2024-06-26 22:27:15,151] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5166 is about to be saved!
[2024-06-26 22:27:16,012] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_00-model_states.pt...
[2024-06-26 22:27:21,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_02-model_states.pt...
[2024-06-26 22:27:21,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_07-model_states.pt...
[2024-06-26 22:27:22,509] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_08-model_states.pt...
[2024-06-26 22:27:24,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_00-model_states.pt.
[2024-06-26 22:27:28,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_06-model_states.pt...
[2024-06-26 22:27:29,348] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_05-model_states.pt...
[2024-06-26 22:27:32,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_01-model_states.pt...
[2024-06-26 22:27:34,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_04-model_states.pt...
[2024-06-26 22:27:34,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_03-model_states.pt...
[2024-06-26 22:29:20,653] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_06-model_states.pt.
[2024-06-26 22:29:21,237] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_05_model_states.pt...
[2024-06-26 22:29:21,391] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_05_model_states.pt.
[2024-06-26 22:29:21,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
[2024-06-26 22:30:18,538] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_08-model_states.pt.
[2024-06-26 22:30:18,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_02-model_states.pt.
[2024-06-26 22:30:18,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_07-model_states.pt.
[2024-06-26 22:30:18,637] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_01_model_states.pt
[2024-06-26 22:30:18,637] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_01_model_states.pt...
[2024-06-26 22:30:18,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_06_model_states.pt...
[2024-06-26 22:30:18,787] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_01_model_states.pt.
[2024-06-26 22:30:18,787] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
[2024-06-26 22:30:18,847] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_06_model_states.pt.
[2024-06-26 22:30:18,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
[2024-06-26 22:30:19,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_09-model_states.pt...
[2024-06-26 22:30:22,744] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_09-model_states.pt.
[2024-06-26 22:30:22,788] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_07_model_states.pt...
[2024-06-26 22:30:22,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_07_model_states.pt.
[2024-06-26 22:30:22,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
[2024-06-26 22:30:38,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_03-model_states.pt.
[2024-06-26 22:30:38,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_02_model_states.pt...
[2024-06-26 22:30:39,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_02_model_states.pt.
[2024-06-26 22:30:39,652] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
[2024-06-26 22:30:43,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_04-model_states.pt.
[2024-06-26 22:30:43,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_01-model_states.pt.
[2024-06-26 22:30:43,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/layer_05-model_states.pt.
[2024-06-26 22:30:44,079] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_03_model_states.pt...
[2024-06-26 22:30:44,175] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_03_model_states.pt.
[2024-06-26 22:30:44,175] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
[2024-06-26 22:30:44,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_04_model_states.pt...
[2024-06-26 22:30:44,537] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_04_model_states.pt.
[2024-06-26 22:30:44,538] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
[2024-06-26 22:30:44,665] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_00_model_states.pt
[2024-06-26 22:30:44,665] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_00_model_states.pt...
[2024-06-26 22:30:45,194] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step5166/mp_rank_00_model_states.pt.
[2024-06-26 22:30:45,194] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5166 is ready now!
Checkpoint saved using --- 210.04627132415771 seconds ---
[2024-06-26 22:31:02,433] [INFO] [launch.py:348:main] Process 432674 exits successfully.
[2024-06-26 22:31:02,446] [INFO] [launch.py:348:main] Process 432673 exits successfully.
[2024-06-26 22:31:02,446] [INFO] [launch.py:348:main] Process 432675 exits successfully.
[2024-06-26 22:31:02,446] [INFO] [launch.py:348:main] Process 432671 exits successfully.
[2024-06-26 22:31:02,446] [INFO] [launch.py:348:main] Process 432669 exits successfully.
[2024-06-26 22:31:03,448] [INFO] [launch.py:348:main] Process 432672 exits successfully.
[2024-06-26 22:31:03,448] [INFO] [launch.py:348:main] Process 432670 exits successfully.
[2024-06-26 22:31:03,448] [INFO] [launch.py:348:main] Process 432668 exits successfully.
