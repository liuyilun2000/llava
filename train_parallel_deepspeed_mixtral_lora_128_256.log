[2024-08-18 00:58:52,428] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:58:59,933] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-08-18 00:58:59,933] [INFO] [runner.py:568:main] cmd = /home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train_parallel_deepspeed_mixtral_lora.py --num_stages=8 --lora_r=128 --lora_alpha=256 --save_model_shard=100 --skip_shard=860 --checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint
[2024-08-18 00:59:03,085] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:04,621] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-08-18 00:59:04,621] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-08-18 00:59:04,621] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-08-18 00:59:04,621] [INFO] [launch.py:163:main] dist_world_size=8
[2024-08-18 00:59:04,621] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-08-18 00:59:04,627] [INFO] [launch.py:253:main] process 56145 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=0', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:04,635] [INFO] [launch.py:253:main] process 56146 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=1', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:04,642] [INFO] [launch.py:253:main] process 56147 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=2', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:04,652] [INFO] [launch.py:253:main] process 56148 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=3', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:04,662] [INFO] [launch.py:253:main] process 56149 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=4', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:04,672] [INFO] [launch.py:253:main] process 56150 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=5', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:04,680] [INFO] [launch.py:253:main] process 56151 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=6', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:04,687] [INFO] [launch.py:253:main] process 56152 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_lora.py', '--local_rank=7', '--num_stages=8', '--lora_r=128', '--lora_alpha=256', '--save_model_shard=100', '--skip_shard=860', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint']
[2024-08-18 00:59:17,913] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:20,189] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 00:59:22,258] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:22,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:22,290] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:22,863] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 00:59:22,864] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 00:59:22,978] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 00:59:27,582] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:28,165] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 00:59:28,165] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-08-18 00:59:29,045] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:29,052] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:29,186] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-18 00:59:30,631] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-18 00:59:30,922] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s][2024-08-18 00:59:31,128] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:02<00:41,  2.17s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:02<00:39,  2.08s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:30,  1.61s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:33,  1.86s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:27,  1.46s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:28,  1.58s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:27,  1.43s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:34,  1.91s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:30,  1.70s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:37,  1.96s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:28,  1.68s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:06<00:34,  2.06s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:01<00:37,  1.99s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:31,  1.75s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:02<00:54,  2.87s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:06<00:36,  2.13s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:03<00:32,  1.80s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:08<00:31,  2.00s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:28,  1.77s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:04<00:39,  2.20s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:34,  2.05s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:31,  1.86s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:05<00:29,  1.75s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:05<00:51,  2.84s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:08<00:27,  1.82s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:31,  1.95s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:08<00:37,  2.32s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:28,  1.80s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:10<00:32,  2.19s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:06<00:39,  2.31s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:07<00:30,  1.93s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:27,  1.96s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:08<00:45,  2.67s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:10<00:32,  2.14s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:08<00:32,  2.01s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:08<00:27,  1.82s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:11<00:29,  2.13s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:10<00:32,  2.14s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:14<00:25,  1.96s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:09<00:30,  2.06s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:10<00:25,  1.81s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:29,  2.12s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:10<00:29,  1.97s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:10<00:39,  2.50s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:29,  2.13s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:13<00:27,  2.15s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:11<00:26,  1.89s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:12<00:33,  2.24s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:12<00:29,  2.10s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:12<00:25,  1.93s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:14<00:27,  2.13s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:25,  2.10s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:15<00:23,  2.00s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:15<00:31,  2.39s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:13<00:24,  1.87s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:13<00:25,  1.97s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:14<00:23,  1.92s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:22,  2.03s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:16<00:24,  2.07s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:14<00:31,  2.27s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:17<00:23,  2.09s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:15<00:23,  1.97s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:17<00:28,  2.36s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:16<00:20,  1.87s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:15<00:24,  2.04s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:22,  2.04s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:16<00:27,  2.15s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:22,  2.24s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:21,  2.19s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:18<00:18,  1.88s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:19<00:24,  2.23s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:21,  1.99s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:19,  1.96s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:18<00:23,  2.15s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:19<00:27,  2.33s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:23<00:20,  2.30s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:20<00:15,  1.77s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:22<00:19,  2.21s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:21<00:21,  2.14s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:19,  1.99s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:22<00:18,  2.09s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:20<00:21,  2.17s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:21<00:14,  1.78s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:25<00:18,  2.30s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:21<00:26,  2.44s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:24<00:18,  2.26s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:21<00:17,  1.96s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:23<00:19,  2.12s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:22<00:18,  2.08s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:25<00:17,  2.24s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:23<00:12,  1.79s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:23<00:15,  1.95s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:27<00:15,  2.27s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:26<00:15,  2.21s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:25<00:16,  2.10s/it]Loading checkpoint shards:  50%|█████     | 10/20 [00:24<00:24,  2.48s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:24<00:16,  2.02s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:27<00:15,  2.25s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:26<00:12,  2.01s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:25<00:13,  1.94s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:28<00:12,  2.12s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:27<00:14,  2.04s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:30<00:13,  2.21s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [00:26<00:20,  2.30s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:26<00:15,  2.19s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:29<00:13,  2.23s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:27<00:11,  1.92s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:28<00:10,  2.03s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:29<00:11,  1.99s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:30<00:10,  2.07s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:32<00:11,  2.28s/it]Loading checkpoint shards:  60%|██████    | 12/20 [00:28<00:18,  2.28s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:29<00:09,  1.87s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:29<00:13,  2.27s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:30<00:08,  2.01s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:31<00:09,  1.95s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:32<00:08,  2.00s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:32<00:11,  2.29s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:34<00:09,  2.33s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [00:30<00:16,  2.33s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:31<00:07,  1.86s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:34<00:05,  1.89s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:32<00:05,  1.97s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:31<00:11,  2.21s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:34<00:08,  2.14s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:33<00:08,  2.00s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:37<00:06,  2.32s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:33<00:08,  2.04s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:33<00:05,  1.93s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:35<00:06,  2.00s/it]Loading checkpoint shards:  70%|███████   | 14/20 [00:33<00:14,  2.44s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:34<00:04,  2.05s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:36<00:04,  2.07s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:35<00:06,  2.16s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [00:35<00:06,  2.12s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:35<00:04,  2.04s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [00:35<00:11,  2.30s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:39<00:04,  2.39s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [00:38<00:04,  2.10s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:36<00:02,  2.06s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:39<00:02,  2.31s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:37<00:00,  1.70s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:37<00:00,  1.86s/it]
Loading checkpoint shards:  90%|█████████ | 18/20 [00:38<00:04,  2.29s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:37<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:40<00:00,  1.81s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]
Loading checkpoint shards:  90%|█████████ | 18/20 [00:37<00:04,  2.02s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:41<00:02,  2.17s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:39<00:01,  1.97s/it]Loading checkpoint shards:  80%|████████  | 16/20 [00:37<00:08,  2.22s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:40<00:00,  1.57s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:40<00:00,  2.02s/it]
Loading checkpoint shards: 100%|██████████| 20/20 [00:38<00:00,  1.61s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:38<00:00,  1.91s/it]
Loading checkpoint shards: 100%|██████████| 20/20 [00:42<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:42<00:00,  2.12s/it]
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:40<00:02,  2.12s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [00:39<00:02,  2.06s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  1.80s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:41<00:00,  2.05s/it]
Loading checkpoint shards:  85%|████████▌ | 17/20 [00:39<00:06,  2.19s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:39<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:39<00:00,  2.00s/it]
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Loading checkpoint shards:  90%|█████████ | 18/20 [00:42<00:04,  2.26s/it]Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1, ProcessCoord(pipe=2, data=0): 2, ProcessCoord(pipe=3, data=0): 3, ProcessCoord(pipe=4, data=0): 4, ProcessCoord(pipe=5, data=0): 5, ProcessCoord(pipe=6, data=0): 6, ProcessCoord(pipe=7, data=0): 7}
[2024-08-18 01:00:19,399] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:LanguageModelLayerWrapper
stage=0 layers=2
     0: LlavaMultiModalModuleWrapper
     1: LanguageModelLayerWrapper
stage=1 layers=1
     2: LanguageModelLayerWrapper
stage=2 layers=1
     3: LanguageModelLayerWrapper
stage=3 layers=1
     4: LanguageModelLayerWrapper
stage=4 layers=1
     5: LanguageModelLayerWrapper
stage=5 layers=1
     6: LanguageModelLayerWrapper
stage=6 layers=1
     7: LanguageModelLayerWrapper
stage=7 layers=2
     8: LanguageModelLayerWrapper
     9: LanguageModelFinalWrapper
  loss: loss_fn
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Loading checkpoint shards:  95%|█████████▌| 19/20 [00:44<00:02,  2.26s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:44<00:00,  1.71s/it]Loading checkpoint shards: 100%|██████████| 20/20 [00:44<00:00,  2.25s/it]
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Print trainable params: 96485376 || all params: 47115384832 || trainable%: 0.20
Rank 4 initialized with CUDA_MEM (60868526080, 85097971712)
Deepspeed engine initializing at --- RANK 4 --- ...
Rank 5 initialized with CUDA_MEM (60868526080, 85097971712)
Deepspeed engine initializing at --- RANK 5 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 1 initialized with CUDA_MEM (60868526080, 85097971712)
Deepspeed engine initializing at --- RANK 1 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Rank 2 initialized with CUDA_MEM (60868526080, 85097971712)
Deepspeed engine initializing at --- RANK 2 --- ...
Rank 7 initialized with CUDA_MEM (60342140928, 85097971712)
Deepspeed engine initializing at --- RANK 7 --- ...
Rank 3 initialized with CUDA_MEM (60868526080, 85097971712)
Deepspeed engine initializing at --- RANK 3 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 6 initialized with CUDA_MEM (60868526080, 85097971712)
Deepspeed engine initializing at --- RANK 6 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 0.1375105381011963 seconds
Time to load fused_adam op: 1.7970387935638428 seconds
[2024-08-18 01:00:25,338] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-08-18 01:00:25,338] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.7499089241027832 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.6035764217376709 seconds
[2024-08-18 01:00:26,110] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-08-18 01:00:26,111] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 2.0070860385894775 seconds
Loading extension module fused_adam...
[2024-08-18 01:00:26,173] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Time to load fused_adam op: 1.5054283142089844 seconds
[2024-08-18 01:00:26,180] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 1.0044217109680176 seconds
[2024-08-18 01:00:26,190] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Rank 0 initialized with CUDA_MEM (58907688960, 85097971712)
Deepspeed engine initializing at --- RANK 0 --- ...
[2024-08-18 01:00:26,214] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-08-18 01:00:26,499] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 2.12226939201355 seconds
[2024-08-18 01:00:29,248] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-08-18 01:00:29,248] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-08-18 01:00:29,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-08-18 01:00:29,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-08-18 01:00:29,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupCosineLR
[2024-08-18 01:00:29,258] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupCosineLR object at 0x14b7c9c3ca60>
[2024-08-18 01:00:29,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-08-18 01:00:29,259] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14b7cb435a90>
[2024-08-18 01:00:29,260] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 128
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=True, output_path='/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint', job_name='deepspeed_monitor_logs') enabled=True
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   optimizer_name ............... adam
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 2e-05}
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True, 'use_reentrant': False}
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-08-18 01:00:29,261] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupCosineLR
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 5718, 'warmup_min_ratio': 0.1, 'warmup_num_steps': 171.54}
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   steps_per_print .............. 1
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-08-18 01:00:29,262] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-08-18 01:00:29,262] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 128, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupCosineLR", 
        "params": {
            "total_num_steps": 5.718000e+03, 
            "warmup_min_ratio": 0.1, 
            "warmup_num_steps": 171.54
        }
    }, 
    "pipeline": {
        "use_reentrant": false
    }, 
    "csv_monitor": {
        "enabled": true, 
        "output_path": "/home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint", 
        "job_name": "deepspeed_monitor_logs"
    }
}
[2024-08-18 01:00:29,262] [INFO] [engine.py:101:__init__] CONFIG: micro_batches=128 micro_batch_size=1
[2024-08-18 01:00:29,262] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=6 STAGE=6 LAYERS=1 [7, 8) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=4 STAGE=4 LAYERS=1 [5, 6) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=2 STAGE=2 LAYERS=1 [3, 4) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=3 STAGE=3 LAYERS=1 [4, 5) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=0 STAGE=0 LAYERS=2 [0, 2) STAGE_PARAMS=48775168 (48.775M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=5 STAGE=5 LAYERS=1 [6, 7) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=7 STAGE=7 LAYERS=2 [8, 10) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
[2024-08-18 01:00:31,799] [INFO] [engine.py:160:__init__] RANK=1 STAGE=1 LAYERS=1 [2, 3) STAGE_PARAMS=6815744 (6.816M) TOTAL_PARAMS=96485376 (96.485M) UNIQUE_PARAMS=96485376 (96.485M)
Deepspeed engine successfully initialized at --- RANK 0 --- hosting 24 of 136 trainable parameters
Loading latest model checkpoint at shard 860
[2024-08-18 01:00:33,137] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 7 --- hosting 16 of 136 trainable parameters
[2024-08-18 01:00:33,393] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
[2024-08-18 01:00:33,531] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:33,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
[2024-08-18 01:00:33,532] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:33,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_07_model_states.pt...
[2024-08-18 01:00:33,592] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:33,593] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_00-model_states.pt...
[2024-08-18 01:00:33,600] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_07_model_states.pt.
[2024-08-18 01:00:33,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_08-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 2 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 1 --- hosting 16 of 136 trainable parameters
[2024-08-18 01:00:34,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
[2024-08-18 01:00:34,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 3 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 4 --- hosting 16 of 136 trainable parameters
[2024-08-18 01:00:34,586] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
[2024-08-18 01:00:34,586] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
[2024-08-18 01:00:34,602] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:34,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_02_model_states.pt...
[2024-08-18 01:00:34,603] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:34,603] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_01_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 6 --- hosting 16 of 136 trainable parameters
Deepspeed engine successfully initialized at --- RANK 5 --- hosting 16 of 136 trainable parameters
[2024-08-18 01:00:34,633] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
[2024-08-18 01:00:34,633] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt...
[2024-08-18 01:00:34,656] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:34,657] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_04_model_states.pt...
[2024-08-18 01:00:34,681] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:34,682] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_03_model_states.pt...
[2024-08-18 01:00:34,696] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_01_model_states.pt.
[2024-08-18 01:00:34,697] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_02-model_states.pt...
[2024-08-18 01:00:34,721] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_02_model_states.pt.
[2024-08-18 01:00:34,722] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_03-model_states.pt...
[2024-08-18 01:00:34,725] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:34,725] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_06_model_states.pt...
[2024-08-18 01:00:34,737] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_00_model_states.pt.
[2024-08-18 01:00:34,738] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_05_model_states.pt...
[2024-08-18 01:00:34,755] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_04_model_states.pt.
[2024-08-18 01:00:34,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_05-model_states.pt...
[2024-08-18 01:00:34,779] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_03_model_states.pt.
[2024-08-18 01:00:34,780] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_04-model_states.pt...
[2024-08-18 01:00:34,815] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_06_model_states.pt.
[2024-08-18 01:00:34,816] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_07-model_states.pt...
[2024-08-18 01:00:34,822] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/mp_rank_05_model_states.pt.
[2024-08-18 01:00:34,822] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_06-model_states.pt...
[2024-08-18 01:01:05,386] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_00-model_states.pt.
[2024-08-18 01:01:05,397] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_00-model_states.pt...
[2024-08-18 01:01:05,742] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_00-model_states.pt.
[2024-08-18 01:01:05,943] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_01-model_states.pt...
[2024-08-18 01:02:02,798] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_07-model_states.pt.
[2024-08-18 01:02:02,974] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_07-model_states.pt...
[2024-08-18 01:02:06,826] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_08-model_states.pt.
[2024-08-18 01:02:07,048] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_08-model_states.pt...
[2024-08-18 01:02:08,976] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_07-model_states.pt.
[2024-08-18 01:02:09,271] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_05-model_states.pt.
[2024-08-18 01:02:09,305] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_05-model_states.pt...
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:02:12,329] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_08-model_states.pt.
[2024-08-18 01:02:12,975] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_05-model_states.pt.
[2024-08-18 01:02:13,687] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_09-model_states.pt...
[2024-08-18 01:02:14,186] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_09-model_states.pt.
[2024-08-18 01:02:14,199] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_09-model_states.pt...
[2024-08-18 01:02:14,363] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_09-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:02:22,043] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_02-model_states.pt.
[2024-08-18 01:02:22,204] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_02-model_states.pt...
[2024-08-18 01:02:22,240] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_06-model_states.pt.
[2024-08-18 01:02:22,347] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_06-model_states.pt...
[2024-08-18 01:02:23,175] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_03-model_states.pt.
[2024-08-18 01:02:23,209] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_03-model_states.pt...
[2024-08-18 01:02:23,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_04-model_states.pt.
[2024-08-18 01:02:24,077] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_04-model_states.pt...
[2024-08-18 01:02:26,431] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_02-model_states.pt.
[2024-08-18 01:02:26,941] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_03-model_states.pt.
[2024-08-18 01:02:27,487] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_01-model_states.pt.
[2024-08-18 01:02:27,916] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_01-model_states.pt...
[2024-08-18 01:02:28,163] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_04-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:02:29,063] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_06-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:02:35,971] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step852/layer_01-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]Shard 0 / 860 skipped
Shard 1 / 860 skipped
Shard 2 / 860 skipped
Shard 3 / 860 skipped
Shard 4 / 860 skipped
Shard 5 / 860 skipped
Shard 6 / 860 skipped
Shard 7 / 860 skipped
Shard 8 / 860 skipped
Shard 9 / 860 skipped
Shard 10 / 860 skipped
Shard 11 / 860 skipped
Shard 12 / 860 skipped
Shard 13 / 860 skipped
Shard 14 / 860 skipped
Shard 15 / 860 skipped
Shard 16 / 860 skipped
Shard 17 / 860 skipped
Shard 18 / 860 skipped
Shard 19 / 860 skipped
Shard 20 / 860 skipped
Shard 21 / 860 skipped
Shard 22 / 860 skipped
Shard 23 / 860 skipped
Shard 24 / 860 skipped
Shard 25 / 860 skipped
Shard 26 / 860 skipped
Shard 27 / 860 skipped
Shard 28 / 860 skipped
Shard 29 / 860 skipped
Shard 30 / 860 skipped
Shard 31 / 860 skipped
Shard 32 / 860 skipped
Shard 33 / 860 skipped
Shard 34 / 860 skipped
Shard 35 / 860 skipped
Shard 36 / 860 skipped
Shard 37 / 860 skipped
Shard 38 / 860 skipped
Shard 39 / 860 skipped
Shard 40 / 860 skipped
Shard 41 / 860 skipped
Shard 42 / 860 skipped
Shard 43 / 860 skipped
Shard 44 / 860 skipped
Shard 45 / 860 skipped
Shard 46 / 860 skipped
Shard 47 / 860 skipped
Shard 48 / 860 skipped
Shard 49 / 860 skipped
Shard 50 / 860 skipped
Shard 51 / 860 skipped
Shard 52 / 860 skipped
Shard 53 / 860 skipped
Shard 54 / 860 skipped
Shard 55 / 860 skipped
Shard 56 / 860 skipped
Shard 57 / 860 skipped
Shard 58 / 860 skipped
Shard 59 / 860 skipped
Shard 60 / 860 skipped
Shard 61 / 860 skipped
Shard 62 / 860 skipped
Shard 63 / 860 skipped
Shard 64 / 860 skipped
Shard 65 / 860 skipped
Shard 66 / 860 skipped
Shard 67 / 860 skipped
Shard 68 / 860 skipped
Shard 69 / 860 skipped
Shard 70 / 860 skipped
Shard 71 / 860 skipped
Shard 72 / 860 skipped
Shard 73 / 860 skipped
Shard 74 / 860 skipped
Shard 75 / 860 skipped
Shard 76 / 860 skipped
Shard 77 / 860 skipped
Shard 78 / 860 skipped
Shard 79 / 860 skipped
Shard 80 / 860 skipped
Shard 81 / 860 skipped
Shard 82 / 860 skipped
Shard 83 / 860 skipped
Shard 84 / 860 skipped
Shard 85 / 860 skipped
Shard 86 / 860 skipped
Shard 87 / 860 skipped
Shard 88 / 860 skipped
Shard 89 / 860 skipped
Shard 90 / 860 skipped
Shard 91 / 860 skipped
Shard 92 / 860 skipped
Shard 93 / 860 skipped
Shard 94 / 860 skipped
Shard 95 / 860 skipped
Shard 96 / 860 skipped
Shard 97 / 860 skipped
Shard 98 / 860 skipped
Shard 99 / 860 skipped
Shard 100 / 860 skipped
Shard 101 / 860 skipped
Shard 102 / 860 skipped
Shard 103 / 860 skipped
Shard 104 / 860 skipped
Shard 105 / 860 skipped
Shard 106 / 860 skipped
Shard 107 / 860 skipped
Shard 108 / 860 skipped
Shard 109 / 860 skipped
Shard 110 / 860 skipped
Shard 111 / 860 skipped
Shard 112 / 860 skipped
Shard 113 / 860 skipped
Shard 114 / 860 skipped
Shard 115 / 860 skipped
Shard 116 / 860 skipped
Shard 117 / 860 skipped
Shard 118 / 860 skipped
Shard 119 / 860 skipped
Shard 120 / 860 skipped
Shard 121 / 860 skipped
Shard 122 / 860 skipped
Shard 123 / 860 skipped
Shard 124 / 860 skipped
Shard 125 / 860 skipped
Shard 126 / 860 skipped
Shard 127 / 860 skipped
Shard 128 / 860 skipped
Shard 129 / 860 skipped
Shard 130 / 860 skipped
Shard 131 / 860 skipped
Shard 132 / 860 skipped
Shard 133 / 860 skipped
Shard 134 / 860 skipped
Shard 135 / 860 skipped
Shard 136 / 860 skipped
Shard 137 / 860 skipped
Shard 138 / 860 skipped
Shard 139 / 860 skipped
Shard 140 / 860 skipped
Shard 141 / 860 skipped
Shard 142 / 860 skipped
Shard 143 / 860 skipped
Shard 144 / 860 skipped
Shard 145 / 860 skipped
Shard 146 / 860 skipped
Shard 147 / 860 skipped
Shard 148 / 860 skipped
Shard 149 / 860 skipped
Shard 150 / 860 skipped
Shard 151 / 860 skipped
Shard 152 / 860 skipped
Shard 153 / 860 skipped
Shard 154 / 860 skipped
Shard 155 / 860 skipped
Shard 156 / 860 skipped
Shard 157 / 860 skipped
Shard 158 / 860 skipped
Shard 159 / 860 skipped
Shard 160 / 860 skipped
Shard 161 / 860 skipped
Shard 162 / 860 skipped
Shard 163 / 860 skipped
Shard 164 / 860 skipped
Shard 165 / 860 skipped
Shard 166 / 860 skipped
Shard 167 / 860 skipped
Shard 168 / 860 skipped
Shard 169 / 860 skipped
Shard 170 / 860 skipped
Shard 171 / 860 skipped
Shard 172 / 860 skipped
Shard 173 / 860 skipped
Shard 174 / 860 skipped
Shard 175 / 860 skipped
Shard 176 / 860 skipped
Shard 177 / 860 skipped
Shard 178 / 860 skipped
Shard 179 / 860 skipped
Shard 180 / 860 skipped
Shard 181 / 860 skipped
Shard 182 / 860 skipped
Shard 183 / 860 skipped
Shard 184 / 860 skipped
Shard 185 / 860 skipped
Shard 186 / 860 skipped
Shard 187 / 860 skipped
Shard 188 / 860 skipped
Shard 189 / 860 skipped
Shard 190 / 860 skipped
Shard 191 / 860 skipped
Shard 192 / 860 skipped
Shard 193 / 860 skipped
Shard 194 / 860 skipped
Shard 195 / 860 skipped
Shard 196 / 860 skipped
Shard 197 / 860 skipped
Shard 198 / 860 skipped
Shard 199 / 860 skipped
Shard 200 / 860 skipped
Shard 201 / 860 skipped
Shard 202 / 860 skipped
Shard 203 / 860 skipped
Shard 204 / 860 skipped
Shard 205 / 860 skipped
Shard 206 / 860 skipped
Shard 207 / 860 skipped
Shard 208 / 860 skipped
Shard 209 / 860 skipped
Shard 210 / 860 skipped
Shard 211 / 860 skipped
Shard 212 / 860 skipped
Shard 213 / 860 skipped
Shard 214 / 860 skipped
Shard 215 / 860 skipped
Shard 216 / 860 skipped
Shard 217 / 860 skipped
Shard 218 / 860 skipped
Shard 219 / 860 skipped
Shard 220 / 860 skipped
Shard 221 / 860 skipped
Shard 222 / 860 skipped
Shard 223 / 860 skipped
Shard 224 / 860 skipped
Shard 225 / 860 skipped
Shard 226 / 860 skipped
Shard 227 / 860 skipped
Shard 228 / 860 skipped
Shard 229 / 860 skipped
Shard 230 / 860 skipped
Shard 231 / 860 skipped
Shard 232 / 860 skipped
Shard 233 / 860 skipped
Shard 234 / 860 skipped
Shard 235 / 860 skipped
Shard 236 / 860 skipped
Shard 237 / 860 skipped
Shard 238 / 860 skipped
Shard 239 / 860 skipped
Shard 240 / 860 skipped
Shard 241 / 860 skipped
Shard 242 / 860 skipped
Shard 243 / 860 skipped
Shard 244 / 860 skipped
Shard 245 / 860 skipped
Shard 246 / 860 skipped
Shard 247 / 860 skipped
Shard 248 / 860 skipped
Shard 249 / 860 skipped
Shard 250 / 860 skipped
Shard 251 / 860 skipped
Shard 252 / 860 skipped
Shard 253 / 860 skipped
Shard 254 / 860 skipped
Shard 255 / 860 skipped
Shard 256 / 860 skipped
Shard 257 / 860 skipped
Shard 258 / 860 skipped
Shard 259 / 860 skipped
Shard 260 / 860 skipped
Shard 261 / 860 skipped
Shard 262 / 860 skipped
Shard 263 / 860 skipped
Shard 264 / 860 skipped
Shard 265 / 860 skipped
Shard 266 / 860 skipped
Shard 267 / 860 skipped
Shard 268 / 860 skipped
Shard 269 / 860 skipped
Shard 270 / 860 skipped
Shard 271 / 860 skipped
Shard 272 / 860 skipped
Shard 273 / 860 skipped
Shard 274 / 860 skipped
Shard 275 / 860 skipped
Shard 276 / 860 skipped
Shard 277 / 860 skipped
Shard 278 / 860 skipped
Shard 279 / 860 skipped
Shard 280 / 860 skipped
Shard 281 / 860 skipped
Shard 282 / 860 skipped
Shard 283 / 860 skipped
Shard 284 / 860 skipped
Shard 285 / 860 skipped
Shard 286 / 860 skipped
Shard 287 / 860 skipped
Shard 288 / 860 skipped
Shard 289 / 860 skipped
Shard 290 / 860 skipped
Shard 291 / 860 skipped
Shard 292 / 860 skipped
Shard 293 / 860 skipped
Shard 294 / 860 skipped
Shard 295 / 860 skipped
Shard 296 / 860 skipped
Shard 297 / 860 skipped
Shard 298 / 860 skipped
Shard 299 / 860 skipped
Shard 300 / 860 skipped
Shard 301 / 860 skipped
Shard 302 / 860 skipped
Shard 303 / 860 skipped
Shard 304 / 860 skipped
Shard 305 / 860 skipped
Shard 306 / 860 skipped
Shard 307 / 860 skipped
Shard 308 / 860 skipped
Shard 309 / 860 skipped
Shard 310 / 860 skipped
Shard 311 / 860 skipped
Shard 312 / 860 skipped
Shard 313 / 860 skipped
Shard 314 / 860 skipped
Shard 315 / 860 skipped
Shard 316 / 860 skipped
Shard 317 / 860 skipped
Shard 318 / 860 skipped
Shard 319 / 860 skipped
Shard 320 / 860 skipped
Shard 321 / 860 skipped
Shard 322 / 860 skipped
Shard 323 / 860 skipped
Shard 324 / 860 skipped
Shard 325 / 860 skipped
Shard 326 / 860 skipped
Shard 327 / 860 skipped
Shard 328 / 860 skipped
Shard 329 / 860 skipped
Shard 330 / 860 skipped
Shard 331 / 860 skipped
Shard 332 / 860 skipped
Shard 333 / 860 skipped
Shard 334 / 860 skipped
Shard 335 / 860 skipped
Shard 336 / 860 skipped
Shard 337 / 860 skipped
Shard 338 / 860 skipped
Shard 339 / 860 skipped
Shard 340 / 860 skipped
Shard 341 / 860 skipped
Shard 342 / 860 skipped
Shard 343 / 860 skipped
Shard 344 / 860 skipped
Shard 345 / 860 skipped
Shard 346 / 860 skipped
Shard 347 / 860 skipped
Shard 348 / 860 skipped
Shard 349 / 860 skipped
Shard 350 / 860 skipped
Shard 351 / 860 skipped
Shard 352 / 860 skipped
Shard 353 / 860 skipped
Shard 354 / 860 skipped
Shard 355 / 860 skipped
Shard 356 / 860 skipped
Shard 357 / 860 skipped
Shard 358 / 860 skipped
Shard 359 / 860 skipped
Shard 360 / 860 skipped
Shard 361 / 860 skipped
Shard 362 / 860 skipped
Shard 363 / 860 skipped
Shard 364 / 860 skipped
Shard 365 / 860 skipped
Shard 366 / 860 skipped
Shard 367 / 860 skipped
Shard 368 / 860 skipped
Shard 369 / 860 skipped
Shard 370 / 860 skipped
Shard 371 / 860 skipped
Shard 372 / 860 skipped
Shard 373 / 860 skipped
Shard 374 / 860 skipped
Shard 375 / 860 skipped
Shard 376 / 860 skipped
Shard 377 / 860 skipped
Shard 378 / 860 skipped
Shard 379 / 860 skipped
Shard 380 / 860 skipped
Shard 381 / 860 skipped
Shard 382 / 860 skipped
Shard 383 / 860 skipped
Shard 384 / 860 skipped
Shard 385 / 860 skipped
Shard 386 / 860 skipped
Shard 387 / 860 skipped
Shard 388 / 860 skipped
Shard 389 / 860 skipped
Shard 390 / 860 skipped
Shard 391 / 860 skipped
Shard 392 / 860 skipped
Shard 393 / 860 skipped
Shard 394 / 860 skipped
Shard 395 / 860 skipped
Shard 396 / 860 skipped
Shard 397 / 860 skipped
Shard 398 / 860 skipped
Shard 399 / 860 skipped
Shard 400 / 860 skipped
Shard 401 / 860 skipped
Shard 402 / 860 skipped
Shard 403 / 860 skipped
Shard 404 / 860 skipped
Shard 405 / 860 skipped
Shard 406 / 860 skipped
Shard 407 / 860 skipped
Shard 408 / 860 skipped
Shard 409 / 860 skipped
Shard 410 / 860 skipped
Shard 411 / 860 skipped
Shard 412 / 860 skipped
Shard 413 / 860 skipped
Shard 414 / 860 skipped
Shard 415 / 860 skipped
Shard 416 / 860 skipped
Shard 417 / 860 skipped
Shard 418 / 860 skipped
Shard 419 / 860 skipped
Shard 420 / 860 skipped
Shard 421 / 860 skipped
Shard 422 / 860 skipped
Shard 423 / 860 skipped
Shard 424 / 860 skipped
Shard 425 / 860 skipped
Shard 426 / 860 skipped
Shard 427 / 860 skipped
Shard 428 / 860 skipped
Shard 429 / 860 skipped
Shard 430 / 860 skipped
Shard 431 / 860 skipped
Shard 432 / 860 skipped
Shard 433 / 860 skipped
Shard 434 / 860 skipped
Shard 435 / 860 skipped
Shard 436 / 860 skipped
Shard 437 / 860 skipped
Shard 438 / 860 skipped
Shard 439 / 860 skipped
Shard 440 / 860 skipped
Shard 441 / 860 skipped
Shard 442 / 860 skipped
Shard 443 / 860 skipped
Shard 444 / 860 skipped
Shard 445 / 860 skipped
Shard 446 / 860 skipped
Shard 447 / 860 skipped
Shard 448 / 860 skipped
Shard 449 / 860 skipped
Shard 450 / 860 skipped
Shard 451 / 860 skipped
Shard 452 / 860 skipped
Shard 453 / 860 skipped
Shard 454 / 860 skipped
Shard 455 / 860 skipped
Shard 456 / 860 skipped
Shard 457 / 860 skipped
Shard 458 / 860 skipped
Shard 459 / 860 skipped
Shard 460 / 860 skipped
Shard 461 / 860 skipped
Shard 462 / 860 skipped
Shard 463 / 860 skipped
Shard 464 / 860 skipped
Shard 465 / 860 skipped
Shard 466 / 860 skipped
Shard 467 / 860 skipped
Shard 468 / 860 skipped
Shard 469 / 860 skipped
Shard 470 / 860 skipped
Shard 471 / 860 skipped
Shard 472 / 860 skipped
Shard 473 / 860 skipped
Shard 474 / 860 skipped
Shard 475 / 860 skipped
Shard 476 / 860 skipped
Shard 477 / 860 skipped
Shard 478 / 860 skipped
Shard 479 / 860 skipped
Shard 480 / 860 skipped
Shard 481 / 860 skipped
Shard 482 / 860 skipped
Shard 483 / 860 skipped
Shard 484 / 860 skipped
Shard 485 / 860 skipped
Shard 486 / 860 skipped
Shard 487 / 860 skipped
Shard 488 / 860 skipped
Shard 489 / 860 skipped
Shard 490 / 860 skipped
Shard 491 / 860 skipped
Shard 492 / 860 skipped
Shard 493 / 860 skipped
Shard 494 / 860 skipped
Shard 495 / 860 skipped
Shard 496 / 860 skipped
Shard 497 / 860 skipped
Shard 498 / 860 skipped
Shard 499 / 860 skipped
Shard 500 / 860 skipped
Shard 501 / 860 skipped
Shard 502 / 860 skipped
Shard 503 / 860 skipped
Shard 504 / 860 skipped
Shard 505 / 860 skipped
Shard 506 / 860 skipped
Shard 507 / 860 skipped
Shard 508 / 860 skipped
Shard 509 / 860 skipped
Shard 510 / 860 skipped
Shard 511 / 860 skipped
Shard 512 / 860 skipped
Shard 513 / 860 skipped
Shard 514 / 860 skipped
Shard 515 / 860 skipped
Shard 516 / 860 skipped
Shard 517 / 860 skipped
Shard 518 / 860 skipped
Shard 519 / 860 skipped
Shard 520 / 860 skipped
Shard 521 / 860 skipped
Shard 522 / 860 skipped
Shard 523 / 860 skipped
Shard 524 / 860 skipped
Shard 525 / 860 skipped
Shard 526 / 860 skipped
Shard 527 / 860 skipped
Shard 528 / 860 skipped
Shard 529 / 860 skipped
Shard 530 / 860 skipped
Shard 531 / 860 skipped
Shard 532 / 860 skipped
Shard 533 / 860 skipped
Shard 534 / 860 skipped
Shard 535 / 860 skipped
Shard 536 / 860 skipped
Shard 537 / 860 skipped
Shard 538 / 860 skipped
Shard 539 / 860 skipped
Shard 540 / 860 skipped
Shard 541 / 860 skipped
Shard 542 / 860 skipped
Shard 543 / 860 skipped
Shard 544 / 860 skipped
Shard 545 / 860 skipped
Shard 546 / 860 skipped
Shard 547 / 860 skipped
Shard 548 / 860 skipped
Shard 549 / 860 skipped
Shard 550 / 860 skipped
Shard 551 / 860 skipped
Shard 552 / 860 skipped
Shard 553 / 860 skipped
Shard 554 / 860 skipped
Shard 555 / 860 skipped
Shard 556 / 860 skipped
Shard 557 / 860 skipped
Shard 558 / 860 skipped
Shard 559 / 860 skipped
Shard 560 / 860 skipped
Shard 561 / 860 skipped
Shard 562 / 860 skipped
Shard 563 / 860 skipped
Shard 564 / 860 skipped
Shard 565 / 860 skipped
Shard 566 / 860 skipped
Shard 567 / 860 skipped
Shard 568 / 860 skipped
Shard 569 / 860 skipped
Shard 570 / 860 skipped
Shard 571 / 860 skipped
Shard 572 / 860 skipped
Shard 573 / 860 skipped
Shard 574 / 860 skipped
Shard 575 / 860 skipped
Shard 576 / 860 skipped
Shard 577 / 860 skipped
Shard 578 / 860 skipped
Shard 579 / 860 skipped
Shard 580 / 860 skipped
Shard 581 / 860 skipped
Shard 582 / 860 skipped
Shard 583 / 860 skipped
Shard 584 / 860 skipped
Shard 585 / 860 skipped
Shard 586 / 860 skipped
Shard 587 / 860 skipped
Shard 588 / 860 skipped
Shard 589 / 860 skipped
Shard 590 / 860 skipped
Shard 591 / 860 skipped
Shard 592 / 860 skipped
Shard 593 / 860 skipped
Shard 594 / 860 skipped
Shard 595 / 860 skipped
Shard 596 / 860 skipped
Shard 597 / 860 skipped
Shard 598 / 860 skipped
Shard 599 / 860 skipped
Shard 600 / 860 skipped
Shard 601 / 860 skipped
Shard 602 / 860 skipped
Shard 603 / 860 skipped
Shard 604 / 860 skipped
Shard 605 / 860 skipped
Shard 606 / 860 skipped
Shard 607 / 860 skipped
Shard 608 / 860 skipped
Shard 609 / 860 skipped
Shard 610 / 860 skipped
Shard 611 / 860 skipped
Shard 612 / 860 skipped
Shard 613 / 860 skipped
Shard 614 / 860 skipped
Shard 615 / 860 skipped
Shard 616 / 860 skipped
Shard 617 / 860 skipped
Shard 618 / 860 skipped
Shard 619 / 860 skipped
Shard 620 / 860 skipped
Shard 621 / 860 skipped
Shard 622 / 860 skipped
Shard 623 / 860 skipped
Shard 624 / 860 skipped
Shard 625 / 860 skipped
Shard 626 / 860 skipped
Shard 627 / 860 skipped
Shard 628 / 860 skipped
Shard 629 / 860 skipped
Shard 630 / 860 skipped
Shard 631 / 860 skipped
Shard 632 / 860 skipped
Shard 633 / 860 skipped
Shard 634 / 860 skipped
Shard 635 / 860 skipped
Shard 636 / 860 skipped
Shard 637 / 860 skipped
Shard 638 / 860 skipped
Shard 639 / 860 skipped
Shard 640 / 860 skipped
Shard 641 / 860 skipped
Shard 642 / 860 skipped
Shard 643 / 860 skipped
Shard 644 / 860 skipped
Shard 645 / 860 skipped
Shard 646 / 860 skipped
Shard 647 / 860 skipped
Shard 648 / 860 skipped
Shard 649 / 860 skipped
Shard 650 / 860 skipped
Shard 651 / 860 skipped
Shard 652 / 860 skipped
Shard 653 / 860 skipped
Shard 654 / 860 skipped
Shard 655 / 860 skipped
Shard 656 / 860 skipped
Shard 657 / 860 skipped
Shard 658 / 860 skipped
Shard 659 / 860 skipped
Shard 660 / 860 skipped
Shard 661 / 860 skipped
Shard 662 / 860 skipped
Shard 663 / 860 skipped
Shard 664 / 860 skipped
Shard 665 / 860 skipped
Shard 666 / 860 skipped
Shard 667 / 860 skipped
Shard 668 / 860 skipped
Shard 669 / 860 skipped
Shard 670 / 860 skipped
Shard 671 / 860 skipped
Shard 672 / 860 skipped
Shard 673 / 860 skipped
Shard 674 / 860 skipped
Shard 675 / 860 skipped
Shard 676 / 860 skipped
Shard 677 / 860 skipped
Shard 678 / 860 skipped
Shard 679 / 860 skipped
Shard 680 / 860 skipped
Shard 681 / 860 skipped
Shard 682 / 860 skipped
Shard 683 / 860 skipped
Shard 684 / 860 skipped
Shard 685 / 860 skipped
Shard 686 / 860 skipped
Shard 687 / 860 skipped
Shard 688 / 860 skipped
Shard 689 / 860 skipped
Shard 690 / 860 skipped
Shard 691 / 860 skipped
Shard 692 / 860 skipped
Shard 693 / 860 skipped
Shard 694 / 860 skipped
Shard 695 / 860 skipped
Shard 696 / 860 skipped
Shard 697 / 860 skipped
Shard 698 / 860 skipped
Shard 699 / 860 skipped
Shard 700 / 860 skipped
Shard 701 / 860 skipped
Shard 702 / 860 skipped
Shard 703 / 860 skipped
Shard 704 / 860 skipped
Shard 705 / 860 skipped
Shard 706 / 860 skipped
Shard 707 / 860 skipped
Shard 708 / 860 skipped
Shard 709 / 860 skipped
Shard 710 / 860 skipped
Shard 711 / 860 skipped
Shard 712 / 860 skipped
Shard 713 / 860 skipped
Shard 714 / 860 skipped
Shard 715 / 860 skipped
Shard 716 / 860 skipped
Shard 717 / 860 skipped
Shard 718 / 860 skipped
Shard 719 / 860 skipped
Shard 720 / 860 skipped
Shard 721 / 860 skipped
Shard 722 / 860 skipped
Shard 723 / 860 skipped
Shard 724 / 860 skipped
Shard 725 / 860 skipped
Shard 726 / 860 skipped
Shard 727 / 860 skipped
Shard 728 / 860 skipped
Shard 729 / 860 skipped
Shard 730 / 860 skipped
Shard 731 / 860 skipped
Shard 732 / 860 skipped
Shard 733 / 860 skipped
Shard 734 / 860 skipped
Shard 735 / 860 skipped
Shard 736 / 860 skipped
Shard 737 / 860 skipped
Shard 738 / 860 skipped
Shard 739 / 860 skipped
Shard 740 / 860 skipped
Shard 741 / 860 skipped
Shard 742 / 860 skipped
Shard 743 / 860 skipped
Shard 744 / 860 skipped
Shard 745 / 860 skipped
Shard 746 / 860 skipped
Shard 747 / 860 skipped
Shard 748 / 860 skipped
Shard 749 / 860 skipped
Shard 750 / 860 skipped
Shard 751 / 860 skipped
Shard 752 / 860 skipped
Shard 753 / 860 skipped
Shard 754 / 860 skipped
Shard 755 / 860 skipped
Shard 756 / 860 skipped
Shard 757 / 860 skipped
Shard 758 / 860 skipped
Shard 759 / 860 skipped
Shard 760 / 860 skipped
Shard 761 / 860 skipped
Shard 762 / 860 skipped
Shard 763 / 860 skipped
Shard 764 / 860 skipped
Shard 765 / 860 skipped
Shard 766 / 860 skipped
Shard 767 / 860 skipped
Shard 768 / 860 skipped
Shard 769 / 860 skipped
Shard 770 / 860 skipped
Shard 771 / 860 skipped
Shard 772 / 860 skipped
Shard 773 / 860 skipped
Shard 774 / 860 skipped
Shard 775 / 860 skipped
Shard 776 / 860 skipped
Shard 777 / 860 skipped
Shard 778 / 860 skipped
Shard 779 / 860 skipped
Shard 780 / 860 skipped
Shard 781 / 860 skipped
Shard 782 / 860 skipped
Shard 783 / 860 skipped
Shard 784 / 860 skipped
Shard 785 / 860 skipped
Shard 786 / 860 skipped
Shard 787 / 860 skipped
Shard 788 / 860 skipped
Shard 789 / 860 skipped
Shard 790 / 860 skipped
Shard 791 / 860 skipped
Shard 792 / 860 skipped
Shard 793 / 860 skipped
Shard 794 / 860 skipped
Shard 795 / 860 skipped
Shard 796 / 860 skipped
Shard 797 / 860 skipped
Shard 798 / 860 skipped
Shard 799 / 860 skipped
Shard 800 / 860 skipped
Shard 801 / 860 skipped
Shard 802 / 860 skipped
Shard 803 / 860 skipped
Shard 804 / 860 skipped
Shard 805 / 860 skipped
Shard 806 / 860 skipped
Shard 807 / 860 skipped
Shard 808 / 860 skipped
Shard 809 / 860 skipped
Shard 810 / 860 skipped
Shard 811 / 860 skipped
Shard 812 / 860 skipped
Shard 813 / 860 skipped
Shard 814 / 860 skipped
Shard 815 / 860 skipped
Shard 816 / 860 skipped
Shard 817 / 860 skipped
Shard 818 / 860 skipped
Shard 819 / 860 skipped
Shard 820 / 860 skipped
Shard 821 / 860 skipped
Shard 822 / 860 skipped
Shard 823 / 860 skipped
Shard 824 / 860 skipped
Shard 825 / 860 skipped
Shard 826 / 860 skipped
Shard 827 / 860 skipped
Shard 828 / 860 skipped
Shard 829 / 860 skipped
Shard 830 / 860 skipped
Shard 831 / 860 skipped
Shard 832 / 860 skipped
Shard 833 / 860 skipped
Shard 834 / 860 skipped
Shard 835 / 860 skipped
Shard 836 / 860 skipped
Shard 837 / 860 skipped
Shard 838 / 860 skipped
Shard 839 / 860 skipped
Shard 840 / 860 skipped
Shard 841 / 860 skipped
Shard 842 / 860 skipped
Shard 843 / 860 skipped
Shard 844 / 860 skipped
Shard 845 / 860 skipped
Shard 846 / 860 skipped
Shard 847 / 860 skipped
Shard 848 / 860 skipped
Shard 849 / 860 skipped
Shard 850 / 860 skipped
Shard 851 / 860 skipped
Shard 852 / 860 skipped
Shard 853 / 860 skipped
Shard 854 / 860 skipped
Shard 855 / 860 skipped
Shard 856 / 860 skipped
Shard 857 / 860 skipped
Shard 858 / 860 skipped
Shard 859 / 860 skipped
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_807
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:04:17,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=853, skipped=0, lr=[1.9264341001723675e-05], mom=[(0.9, 0.999)]
steps: 853 loss: 0.6416 iter time (s): 102.522 samples/sec: 1.249

100%|██████████| 1/1 [02:05<00:00, 125.95s/it][A100%|██████████| 1/1 [02:05<00:00, 125.95s/it]
 17%|█▋        | 861/5198 [02:05<10:34,  6.84it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:09<00:00, 129.72s/it][A100%|██████████| 1/1 [02:09<00:00, 129.72s/it]
 17%|█▋        | 861/5198 [02:09<10:54,  6.63it/s]
100%|██████████| 1/1 [01:49<00:00, 109.89s/it][A100%|██████████| 1/1 [01:49<00:00, 109.89s/it]
 17%|█▋        | 861/5198 [01:49<09:13,  7.83it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.25s/it][A100%|██████████| 1/1 [02:06<00:00, 126.25s/it]
 17%|█▋        | 861/5198 [02:06<10:35,  6.82it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.98s/it][A100%|██████████| 1/1 [01:50<00:00, 110.98s/it]
 17%|█▋        | 861/5198 [01:50<09:19,  7.76it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.14s/it][A100%|██████████| 1/1 [01:52<00:00, 112.14s/it]
 17%|█▋        | 861/5198 [01:52<09:24,  7.68it/s]
100%|██████████| 1/1 [01:52<00:00, 112.52s/it][A100%|██████████| 1/1 [01:52<00:00, 112.52s/it]
 17%|█▋        | 861/5198 [01:52<09:26,  7.65it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.30s/it][A100%|██████████| 1/1 [01:43<00:00, 103.30s/it]
 17%|█▋        | 861/5198 [01:43<08:40,  8.33it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_808
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.24s/it][A100%|██████████| 1/1 [01:26<00:00, 86.24s/it]
 17%|█▋        | 862/5198 [03:32<20:52,  3.46it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:05:48,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=854, skipped=0, lr=[1.9262207321491622e-05], mom=[(0.9, 0.999)]
steps: 854 loss: 0.6669 iter time (s): 87.606 samples/sec: 1.461

100%|██████████| 1/1 [01:28<00:00, 88.43s/it][A100%|██████████| 1/1 [01:28<00:00, 88.43s/it]
 17%|█▋        | 862/5198 [03:38<21:28,  3.37it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.49s/it][A100%|██████████| 1/1 [01:28<00:00, 88.49s/it]
 17%|█▋        | 862/5198 [03:18<19:48,  3.65it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.43s/it][A100%|██████████| 1/1 [01:28<00:00, 88.43s/it]
 17%|█▋        | 862/5198 [03:34<21:09,  3.41it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.44s/it][A100%|██████████| 1/1 [01:28<00:00, 88.44s/it]
 17%|█▋        | 862/5198 [03:19<19:53,  3.63it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.44s/it][A100%|██████████| 1/1 [01:28<00:00, 88.44s/it]
 17%|█▋        | 862/5198 [03:20<19:59,  3.62it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.46s/it][A100%|██████████| 1/1 [01:28<00:00, 88.46s/it]
 17%|█▋        | 862/5198 [03:21<20:01,  3.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.44s/it][A100%|██████████| 1/1 [01:28<00:00, 88.44s/it]
 17%|█▋        | 862/5198 [03:11<19:14,  3.76it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_809
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.47s/it][A100%|██████████| 1/1 [01:21<00:00, 81.47s/it]
 17%|█▋        | 863/5198 [04:53<34:43,  2.08it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:07:10,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=855, skipped=0, lr=[1.926007067003437e-05], mom=[(0.9, 0.999)]
steps: 855 loss: 0.6298 iter time (s): 80.518 samples/sec: 1.590

100%|██████████| 1/1 [01:21<00:00, 81.32s/it][A100%|██████████| 1/1 [01:21<00:00, 81.32s/it]
 17%|█▋        | 863/5198 [04:59<35:17,  2.05it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.29s/it][A100%|██████████| 1/1 [01:21<00:00, 81.29s/it]
 17%|█▋        | 863/5198 [04:39<33:37,  2.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.32s/it][A100%|██████████| 1/1 [01:21<00:00, 81.32s/it]
 17%|█▋        | 863/5198 [04:56<34:59,  2.07it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.33s/it][A100%|██████████| 1/1 [01:21<00:00, 81.33s/it]
 17%|█▋        | 863/5198 [04:40<33:42,  2.14it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.35s/it][A100%|██████████| 1/1 [01:21<00:00, 81.35s/it]
 17%|█▋        | 863/5198 [04:41<33:48,  2.14it/s]
100%|██████████| 1/1 [01:21<00:00, 81.33s/it][A100%|██████████| 1/1 [01:21<00:00, 81.33s/it]
 17%|█▋        | 863/5198 [04:42<33:50,  2.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.33s/it][A100%|██████████| 1/1 [01:21<00:00, 81.33s/it]
 17%|█▋        | 863/5198 [04:33<33:04,  2.18it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_53
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.40s/it][A100%|██████████| 1/1 [02:00<00:00, 120.40s/it]
 17%|█▋        | 864/5198 [06:54<1:03:50,  1.13it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:09:11,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=856, skipped=0, lr=[1.925793104803741e-05], mom=[(0.9, 0.999)]
steps: 856 loss: 0.7991 iter time (s): 120.700 samples/sec: 1.060

100%|██████████| 1/1 [02:01<00:00, 121.46s/it][A100%|██████████| 1/1 [02:01<00:00, 121.46s/it]
 17%|█▋        | 864/5198 [07:01<1:04:38,  1.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.53s/it][A100%|██████████| 1/1 [02:01<00:00, 121.53s/it]
 17%|█▋        | 864/5198 [06:41<1:03:00,  1.15it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.58s/it][A100%|██████████| 1/1 [02:01<00:00, 121.58s/it]
 17%|█▋        | 864/5198 [06:57<1:04:22,  1.12it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.59s/it][A100%|██████████| 1/1 [02:01<00:00, 121.59s/it]
 17%|█▋        | 864/5198 [06:42<1:03:06,  1.14it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.49s/it][A100%|██████████| 1/1 [02:01<00:00, 121.49s/it]
 17%|█▋        | 864/5198 [06:43<1:03:11,  1.14it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.52s/it][A100%|██████████| 1/1 [02:01<00:00, 121.52s/it]
 17%|█▋        | 864/5198 [06:43<1:03:13,  1.14it/s]
100%|██████████| 1/1 [02:01<00:00, 121.50s/it][A100%|██████████| 1/1 [02:01<00:00, 121.50s/it]
 17%|█▋        | 864/5198 [06:34<1:02:27,  1.16it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_810

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.43s/it][A100%|██████████| 1/1 [01:39<00:00, 99.43s/it]
 17%|█▋        | 865/5198 [08:33<1:37:50,  1.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:10:50,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=857, skipped=0, lr=[1.9255788456187185e-05], mom=[(0.9, 0.999)]
steps: 857 loss: 0.6564 iter time (s): 97.996 samples/sec: 1.306

100%|██████████| 1/1 [01:38<00:00, 98.79s/it][A100%|██████████| 1/1 [01:38<00:00, 98.79s/it]
 17%|█▋        | 865/5198 [08:39<1:38:25,  1.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.95s/it][A100%|██████████| 1/1 [01:38<00:00, 98.96s/it]
 17%|█▋        | 865/5198 [08:20<1:36:50,  1.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.76s/it][A100%|██████████| 1/1 [01:38<00:00, 98.76s/it]
 17%|█▋        | 865/5198 [08:36<1:38:08,  1.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.79s/it][A100%|██████████| 1/1 [01:38<00:00, 98.79s/it]
 17%|█▋        | 865/5198 [08:21<1:36:53,  1.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.83s/it][A100%|██████████| 1/1 [01:38<00:00, 98.83s/it]
 17%|█▋        | 865/5198 [08:22<1:36:58,  1.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.83s/it][A100%|██████████| 1/1 [01:38<00:00, 98.83s/it]
 17%|█▋        | 865/5198 [08:22<1:37:01,  1.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.86s/it][A100%|██████████| 1/1 [01:38<00:00, 98.86s/it]
 17%|█▋        | 865/5198 [08:13<1:36:15,  1.33s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_811
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.45s/it][A100%|██████████| 1/1 [01:45<00:00, 105.45s/it]
 17%|█▋        | 866/5198 [10:19<2:28:47,  2.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:12:36,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=858, skipped=0, lr=[1.9253642895171096e-05], mom=[(0.9, 0.999)]
steps: 858 loss: 0.6555 iter time (s): 104.780 samples/sec: 1.222

100%|██████████| 1/1 [01:45<00:00, 105.63s/it][A100%|██████████| 1/1 [01:45<00:00, 105.63s/it]
 17%|█▋        | 866/5198 [10:25<2:29:27,  2.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.46s/it][A100%|██████████| 1/1 [01:45<00:00, 105.46s/it]
 17%|█▋        | 866/5198 [10:05<2:27:47,  2.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.57s/it][A100%|██████████| 1/1 [01:45<00:00, 105.58s/it]
 17%|█▋        | 866/5198 [10:21<2:29:08,  2.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.59s/it][A100%|██████████| 1/1 [01:45<00:00, 105.59s/it]
 17%|█▋        | 866/5198 [10:06<2:27:54,  2.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.56s/it][A100%|██████████| 1/1 [01:45<00:00, 105.56s/it]
 17%|█▋        | 866/5198 [10:07<2:27:58,  2.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.56s/it][A100%|██████████| 1/1 [01:45<00:00, 105.56s/it]
 17%|█▋        | 866/5198 [10:08<2:28:01,  2.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.57s/it][A100%|██████████| 1/1 [01:45<00:00, 105.57s/it]
 17%|█▋        | 866/5198 [09:59<2:27:15,  2.04s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_812
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.75s/it][A100%|██████████| 1/1 [01:39<00:00, 99.75s/it]
 17%|█▋        | 867/5198 [11:58<3:36:25,  3.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:14:15,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=859, skipped=0, lr=[1.9251494365677487e-05], mom=[(0.9, 0.999)]
steps: 859 loss: 0.6471 iter time (s): 98.800 samples/sec: 1.296

100%|██████████| 1/1 [01:39<00:00, 99.57s/it][A100%|██████████| 1/1 [01:39<00:00, 99.57s/it]
 17%|█▋        | 867/5198 [12:05<3:36:56,  3.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.55s/it][A100%|██████████| 1/1 [01:39<00:00, 99.55s/it]
 17%|█▋        | 867/5198 [11:45<3:35:17,  2.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.58s/it][A100%|██████████| 1/1 [01:39<00:00, 99.58s/it]
 17%|█▋        | 867/5198 [12:01<3:36:38,  3.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.55s/it][A100%|██████████| 1/1 [01:39<00:00, 99.55s/it]
 17%|█▋        | 867/5198 [11:46<3:35:23,  2.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.64s/it][A100%|██████████| 1/1 [01:39<00:00, 99.64s/it]
 17%|█▋        | 867/5198 [11:47<3:35:32,  2.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.62s/it][A100%|██████████| 1/1 [01:39<00:00, 99.62s/it]
 17%|█▋        | 867/5198 [11:47<3:35:33,  2.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.61s/it][A100%|██████████| 1/1 [01:39<00:00, 99.61s/it]
 17%|█▋        | 867/5198 [11:38<3:34:48,  2.98s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_813
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.95s/it][A100%|██████████| 1/1 [01:28<00:00, 88.95s/it]
 17%|█▋        | 868/5198 [13:27<5:00:15,  4.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:15:44,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.924934286839566e-05], mom=[(0.9, 0.999)]
steps: 860 loss: 0.6757 iter time (s): 87.835 samples/sec: 1.457

100%|██████████| 1/1 [01:28<00:00, 88.66s/it][A100%|██████████| 1/1 [01:28<00:00, 88.66s/it]
 17%|█▋        | 868/5198 [13:33<5:00:28,  4.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.69s/it][A100%|██████████| 1/1 [01:28<00:00, 88.69s/it]
 17%|█▋        | 868/5198 [13:13<4:58:52,  4.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.79s/it][A100%|██████████| 1/1 [01:28<00:00, 88.79s/it]
 17%|█▋        | 868/5198 [13:30<5:00:17,  4.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.72s/it][A100%|██████████| 1/1 [01:28<00:00, 88.72s/it]
 17%|█▋        | 868/5198 [13:15<4:59:00,  4.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.68s/it][A100%|██████████| 1/1 [01:28<00:00, 88.68s/it]
 17%|█▋        | 868/5198 [13:16<4:59:05,  4.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.72s/it][A100%|██████████| 1/1 [01:28<00:00, 88.72s/it]
 17%|█▋        | 868/5198 [13:16<4:59:09,  4.15s/it]
100%|██████████| 1/1 [01:28<00:00, 88.70s/it][A100%|██████████| 1/1 [01:28<00:00, 88.70s/it]
 17%|█▋        | 868/5198 [13:07<4:58:23,  4.13s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_814

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.94s/it][A100%|██████████| 1/1 [01:51<00:00, 111.94s/it]
 17%|█▋        | 869/5198 [15:19<7:27:33,  6.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:17:37,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=861, skipped=0, lr=[1.9247188404015873e-05], mom=[(0.9, 0.999)]
steps: 861 loss: 0.6410 iter time (s): 111.791 samples/sec: 1.145

100%|██████████| 1/1 [01:52<00:00, 112.69s/it][A100%|██████████| 1/1 [01:52<00:00, 112.69s/it]
 17%|█▋        | 869/5198 [15:26<7:28:46,  6.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.67s/it][A100%|██████████| 1/1 [01:52<00:00, 112.68s/it]
 17%|█▋        | 869/5198 [15:06<7:27:11,  6.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.54s/it][A100%|██████████| 1/1 [01:52<00:00, 112.54s/it]
 17%|█▋        | 869/5198 [15:22<7:28:24,  6.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.69s/it][A100%|██████████| 1/1 [01:52<00:00, 112.69s/it]
 17%|█▋        | 869/5198 [15:07<7:27:20,  6.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.65s/it][A100%|██████████| 1/1 [01:52<00:00, 112.65s/it]
 17%|█▋        | 869/5198 [15:08<7:27:22,  6.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.66s/it][A100%|██████████| 1/1 [01:52<00:00, 112.66s/it]
 17%|█▋        | 869/5198 [15:09<7:27:26,  6.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:52<00:00, 112.67s/it][A100%|██████████| 1/1 [01:52<00:00, 112.67s/it]
 17%|█▋        | 869/5198 [15:00<7:26:42,  6.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_815
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.30s/it][A100%|██████████| 1/1 [01:36<00:00, 96.30s/it]
 17%|█▋        | 870/5198 [16:56<10:18:44,  8.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:19:12,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=862, skipped=0, lr=[1.9245030973229325e-05], mom=[(0.9, 0.999)]
steps: 862 loss: 0.6916 iter time (s): 94.941 samples/sec: 1.348

100%|██████████| 1/1 [01:35<00:00, 95.81s/it][A100%|██████████| 1/1 [01:35<00:00, 95.81s/it]
 17%|█▋        | 870/5198 [17:02<10:19:00,  8.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.82s/it][A100%|██████████| 1/1 [01:35<00:00, 95.82s/it]
 17%|█▋        | 870/5198 [16:42<10:17:29,  8.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.86s/it][A100%|██████████| 1/1 [01:35<00:00, 95.86s/it]
 17%|█▋        | 870/5198 [16:58<10:18:44,  8.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.80s/it][A100%|██████████| 1/1 [01:35<00:00, 95.80s/it]
 17%|█▋        | 870/5198 [16:43<10:17:35,  8.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.79s/it][A100%|██████████| 1/1 [01:35<00:00, 95.79s/it]
 17%|█▋        | 870/5198 [16:44<10:17:38,  8.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.78s/it][A100%|██████████| 1/1 [01:35<00:00, 95.78s/it]
 17%|█▋        | 870/5198 [16:45<10:17:39,  8.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.79s/it][A100%|██████████| 1/1 [01:35<00:00, 95.79s/it]
 17%|█▋        | 870/5198 [16:35<10:16:57,  8.55s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_816
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:55<00:00, 115.90s/it][A100%|██████████| 1/1 [01:55<00:00, 115.90s/it]
 17%|█▋        | 871/5198 [18:51<14:59:29, 12.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:21:09,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=863, skipped=0, lr=[1.924287057672818e-05], mom=[(0.9, 0.999)]
steps: 863 loss: 0.6498 iter time (s): 115.635 samples/sec: 1.107

100%|██████████| 1/1 [01:56<00:00, 116.49s/it][A100%|██████████| 1/1 [01:56<00:00, 116.49s/it]
 17%|█▋        | 871/5198 [18:58<15:01:14, 12.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.62s/it][A100%|██████████| 1/1 [01:56<00:00, 116.62s/it]
 17%|█▋        | 871/5198 [18:39<15:00:08, 12.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.60s/it][A100%|██████████| 1/1 [01:56<00:00, 116.61s/it]
 17%|█▋        | 871/5198 [18:55<15:01:18, 12.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.58s/it][A100%|██████████| 1/1 [01:56<00:00, 116.58s/it]
 17%|█▋        | 871/5198 [18:40<15:00:07, 12.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.61s/it][A100%|██████████| 1/1 [01:56<00:00, 116.61s/it]
 17%|█▋        | 871/5198 [18:41<15:00:14, 12.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.61s/it][A100%|██████████| 1/1 [01:56<00:00, 116.61s/it]
 17%|█▋        | 871/5198 [18:41<15:00:15, 12.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.61s/it][A100%|██████████| 1/1 [01:56<00:00, 116.61s/it]
 17%|█▋        | 871/5198 [18:32<14:59:35, 12.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_817
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.28s/it][A100%|██████████| 1/1 [01:46<00:00, 106.28s/it]
 17%|█▋        | 872/5198 [20:38<20:32:42, 17.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:22:55,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=864, skipped=0, lr=[1.9240707215205542e-05], mom=[(0.9, 0.999)]
steps: 864 loss: 0.6934 iter time (s): 105.088 samples/sec: 1.218

100%|██████████| 1/1 [01:45<00:00, 105.99s/it][A100%|██████████| 1/1 [01:45<00:00, 105.99s/it]
 17%|█▋        | 872/5198 [20:44<20:33:15, 17.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.87s/it][A100%|██████████| 1/1 [01:45<00:00, 105.87s/it]
 17%|█▋        | 872/5198 [20:24<20:31:47, 17.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.92s/it][A100%|██████████| 1/1 [01:45<00:00, 105.92s/it]
 17%|█▋        | 872/5198 [20:41<20:33:03, 17.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.89s/it][A100%|██████████| 1/1 [01:45<00:00, 105.89s/it]
 17%|█▋        | 872/5198 [20:26<20:31:49, 17.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.93s/it][A100%|██████████| 1/1 [01:45<00:00, 105.93s/it]
 17%|█▋        | 872/5198 [20:27<20:32:04, 17.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.93s/it][A100%|██████████| 1/1 [01:45<00:00, 105.93s/it]
 17%|█▋        | 872/5198 [20:27<20:32:04, 17.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.91s/it][A100%|██████████| 1/1 [01:45<00:00, 105.91s/it]
 17%|█▋        | 872/5198 [20:18<20:31:23, 17.08s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_818
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.32s/it][A100%|██████████| 1/1 [01:31<00:00, 91.32s/it]
 17%|█▋        | 873/5198 [22:09<26:24:25, 21.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:24:26,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=865, skipped=0, lr=[1.9238540889355472e-05], mom=[(0.9, 0.999)]
steps: 865 loss: 0.6706 iter time (s): 90.052 samples/sec: 1.421

100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
 17%|█▋        | 873/5198 [22:15<26:23:03, 21.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.98s/it][A100%|██████████| 1/1 [01:30<00:00, 90.98s/it]
 17%|█▋        | 873/5198 [21:55<26:21:52, 21.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
 17%|█▋        | 873/5198 [22:12<26:22:52, 21.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.04s/it][A100%|██████████| 1/1 [01:31<00:00, 91.05s/it]
 17%|█▋        | 873/5198 [21:57<26:22:12, 21.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 17%|█▋        | 873/5198 [21:58<26:22:06, 21.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 17%|█▋        | 873/5198 [21:58<26:22:05, 21.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 17%|█▋        | 873/5198 [21:49<26:21:28, 21.94s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_819
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.08s/it][A100%|██████████| 1/1 [01:44<00:00, 104.08s/it]
 17%|█▋        | 874/5198 [23:53<34:52:18, 29.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:26:10,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=866, skipped=0, lr=[1.9236371599872987e-05], mom=[(0.9, 0.999)]
steps: 866 loss: 0.6416 iter time (s): 103.561 samples/sec: 1.236

100%|██████████| 1/1 [01:44<00:00, 104.43s/it][A100%|██████████| 1/1 [01:44<00:00, 104.43s/it]
 17%|█▋        | 874/5198 [24:00<34:53:09, 29.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.37s/it][A100%|██████████| 1/1 [01:44<00:00, 104.37s/it]
 17%|█▋        | 874/5198 [23:40<34:51:45, 29.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.44s/it][A100%|██████████| 1/1 [01:44<00:00, 104.44s/it]
 17%|█▋        | 874/5198 [23:56<34:53:03, 29.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.34s/it][A100%|██████████| 1/1 [01:44<00:00, 104.34s/it]
 17%|█▋        | 874/5198 [23:41<34:51:49, 29.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.40s/it][A100%|██████████| 1/1 [01:44<00:00, 104.40s/it]
 17%|█▋        | 874/5198 [23:42<34:52:06, 29.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.38s/it][A100%|██████████| 1/1 [01:44<00:00, 104.38s/it]
 17%|█▋        | 874/5198 [23:42<34:52:01, 29.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.39s/it][A100%|██████████| 1/1 [01:44<00:00, 104.39s/it]
 17%|█▋        | 874/5198 [23:33<34:51:28, 29.02s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_820
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.78s/it][A100%|██████████| 1/1 [01:32<00:00, 92.78s/it]
 17%|█▋        | 875/5198 [25:26<43:14:00, 36.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:27:43,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=867, skipped=0, lr=[1.923419934745404e-05], mom=[(0.9, 0.999)]
steps: 867 loss: 0.7130 iter time (s): 91.616 samples/sec: 1.397

100%|██████████| 1/1 [01:32<00:00, 92.43s/it][A100%|██████████| 1/1 [01:32<00:00, 92.43s/it]
 17%|█▋        | 875/5198 [25:32<43:11:50, 35.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.43s/it][A100%|██████████| 1/1 [01:32<00:00, 92.43s/it]
 17%|█▋        | 875/5198 [25:12<43:10:36, 35.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 17%|█▋        | 875/5198 [25:29<43:11:35, 35.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 17%|█▋        | 875/5198 [25:13<43:10:30, 35.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.33s/it][A100%|██████████| 1/1 [01:32<00:00, 92.33s/it]
 17%|█▋        | 875/5198 [25:14<43:10:08, 35.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 17%|█▋        | 875/5198 [25:15<43:10:40, 35.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.41s/it][A100%|██████████| 1/1 [01:32<00:00, 92.41s/it]
 17%|█▋        | 875/5198 [25:06<43:10:09, 35.95s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_821
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.17s/it][A100%|██████████| 1/1 [01:39<00:00, 99.17s/it]
 17%|█▋        | 876/5198 [27:05<53:28:01, 44.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:29:22,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=868, skipped=0, lr=[1.923202413279555e-05], mom=[(0.9, 0.999)]
steps: 868 loss: 0.6173 iter time (s): 98.569 samples/sec: 1.299

100%|██████████| 1/1 [01:39<00:00, 99.44s/it][A100%|██████████| 1/1 [01:39<00:00, 99.44s/it]
 17%|█▋        | 876/5198 [27:12<53:28:43, 44.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.39s/it][A100%|██████████| 1/1 [01:39<00:00, 99.39s/it]
 17%|█▋        | 876/5198 [26:52<53:27:05, 44.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.40s/it][A100%|██████████| 1/1 [01:39<00:00, 99.40s/it]
 17%|█▋        | 876/5198 [27:08<53:28:09, 44.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.39s/it][A100%|██████████| 1/1 [01:39<00:00, 99.39s/it]
 17%|█▋        | 876/5198 [26:53<53:27:00, 44.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.47s/it][A100%|██████████| 1/1 [01:39<00:00, 99.47s/it]
 17%|█▋        | 876/5198 [26:54<53:27:30, 44.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.42s/it][A100%|██████████| 1/1 [01:39<00:00, 99.42s/it]
 17%|█▋        | 876/5198 [26:54<53:27:30, 44.53s/it]
100%|██████████| 1/1 [01:39<00:00, 99.41s/it][A100%|██████████| 1/1 [01:39<00:00, 99.41s/it]
 17%|█▋        | 876/5198 [26:45<53:26:59, 44.52s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_822

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.17s/it][A100%|██████████| 1/1 [02:03<00:00, 123.18s/it]
 17%|█▋        | 877/5198 [29:08<68:43:26, 57.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:31:26,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=869, skipped=0, lr=[1.9229845956595375e-05], mom=[(0.9, 0.999)]
steps: 869 loss: 0.6211 iter time (s): 123.055 samples/sec: 1.040

100%|██████████| 1/1 [02:03<00:00, 123.90s/it][A100%|██████████| 1/1 [02:03<00:00, 123.90s/it]
 17%|█▋        | 877/5198 [29:15<68:52:17, 57.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.97s/it][A100%|██████████| 1/1 [02:03<00:00, 123.97s/it]
 17%|█▋        | 877/5198 [28:56<68:51:45, 57.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.90s/it][A100%|██████████| 1/1 [02:03<00:00, 123.90s/it]
 17%|█▋        | 877/5198 [29:12<68:51:49, 57.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.90s/it][A100%|██████████| 1/1 [02:03<00:00, 123.91s/it]
 17%|█▋        | 877/5198 [28:57<68:50:53, 57.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.91s/it][A100%|██████████| 1/1 [02:03<00:00, 123.91s/it]
 17%|█▋        | 877/5198 [28:58<68:51:22, 57.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.92s/it][A100%|██████████| 1/1 [02:03<00:00, 123.92s/it]
 17%|█▋        | 877/5198 [28:58<68:51:26, 57.37s/it]
100%|██████████| 1/1 [02:03<00:00, 123.92s/it][A100%|██████████| 1/1 [02:03<00:00, 123.92s/it]
 17%|█▋        | 877/5198 [28:49<68:50:58, 57.36s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_823

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.54s/it][A100%|██████████| 1/1 [02:03<00:00, 123.54s/it]
 17%|█▋        | 878/5198 [31:12<83:38:28, 69.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:33:30,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.9227664819552333e-05], mom=[(0.9, 0.999)]
steps: 870 loss: 0.6791 iter time (s): 122.658 samples/sec: 1.044

100%|██████████| 1/1 [02:03<00:00, 123.58s/it][A100%|██████████| 1/1 [02:03<00:00, 123.58s/it]
 17%|█▋        | 878/5198 [31:19<83:45:53, 69.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.54s/it][A100%|██████████| 1/1 [02:03<00:00, 123.54s/it]
 17%|█▋        | 878/5198 [30:59<83:44:55, 69.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.53s/it][A100%|██████████| 1/1 [02:03<00:00, 123.53s/it]
 17%|█▋        | 878/5198 [31:15<83:44:56, 69.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.57s/it][A100%|██████████| 1/1 [02:03<00:00, 123.57s/it]
 17%|█▋        | 878/5198 [31:00<83:44:46, 69.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.54s/it][A100%|██████████| 1/1 [02:03<00:00, 123.54s/it]
 17%|█▋        | 878/5198 [31:01<83:44:40, 69.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.53s/it][A100%|██████████| 1/1 [02:03<00:00, 123.53s/it]
 17%|█▋        | 878/5198 [31:02<83:44:34, 69.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.55s/it][A100%|██████████| 1/1 [02:03<00:00, 123.55s/it]
 17%|█▋        | 878/5198 [30:53<83:44:25, 69.78s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_824
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.48s/it][A100%|██████████| 1/1 [01:22<00:00, 82.48s/it]
 17%|█▋        | 879/5198 [32:34<86:52:06, 72.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:34:51,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=871, skipped=0, lr=[1.922548072236618e-05], mom=[(0.9, 0.999)]
steps: 871 loss: 0.6484 iter time (s): 80.430 samples/sec: 1.591

100%|██████████| 1/1 [01:21<00:00, 81.22s/it][A100%|██████████| 1/1 [01:21<00:00, 81.22s/it]
 17%|█▋        | 879/5198 [32:40<86:38:38, 72.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.25s/it][A100%|██████████| 1/1 [01:21<00:00, 81.25s/it]
 17%|█▋        | 879/5198 [32:20<86:38:21, 72.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.23s/it][A100%|██████████| 1/1 [01:21<00:00, 81.24s/it]
 17%|█▋        | 879/5198 [32:37<86:38:02, 72.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.23s/it][A100%|██████████| 1/1 [01:21<00:00, 81.23s/it]
 17%|█▋        | 879/5198 [32:21<86:37:52, 72.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.23s/it][A100%|██████████| 1/1 [01:21<00:00, 81.23s/it]
 17%|█▋        | 879/5198 [32:23<86:37:48, 72.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.26s/it][A100%|██████████| 1/1 [01:21<00:00, 81.26s/it]
 17%|█▋        | 879/5198 [32:23<86:38:05, 72.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.27s/it][A100%|██████████| 1/1 [01:21<00:00, 81.27s/it]
 17%|█▋        | 879/5198 [32:14<86:38:09, 72.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_54
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:07<00:00, 127.53s/it][A100%|██████████| 1/1 [02:07<00:00, 127.53s/it]
 17%|█▋        | 880/5198 [34:42<102:11:34, 85.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:37:00,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=872, skipped=0, lr=[1.9223293665737637e-05], mom=[(0.9, 0.999)]
steps: 872 loss: 0.8457 iter time (s): 128.045 samples/sec: 1.000

100%|██████████| 1/1 [02:08<00:00, 128.94s/it][A100%|██████████| 1/1 [02:08<00:00, 128.94s/it]
 17%|█▋        | 880/5198 [34:49<102:24:29, 85.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:08<00:00, 128.97s/it][A100%|██████████| 1/1 [02:08<00:00, 128.97s/it]
 17%|█▋        | 880/5198 [34:29<102:24:44, 85.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:08<00:00, 128.94s/it][A100%|██████████| 1/1 [02:08<00:00, 128.94s/it]
 17%|█▋        | 880/5198 [34:46<102:23:56, 85.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:09<00:00, 129.03s/it][A100%|██████████| 1/1 [02:09<00:00, 129.03s/it]
 17%|█▋        | 880/5198 [34:31<102:25:15, 85.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:08<00:00, 128.98s/it][A100%|██████████| 1/1 [02:08<00:00, 128.98s/it]
 17%|█▋        | 880/5198 [34:32<102:24:29, 85.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:08<00:00, 128.94s/it][A
100%|██████████| 1/1 [02:08<00:00, 128.94s/it]
 17%|█▋        | 880/5198 [34:32<102:24:00, 85.37s/it]100%|██████████| 1/1 [02:08<00:00, 128.91s/it][A100%|██████████| 1/1 [02:08<00:00, 128.91s/it]
 17%|█▋        | 880/5198 [34:23<102:23:31, 85.37s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_825
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.17s/it][A100%|██████████| 1/1 [01:37<00:00, 97.17s/it]
 17%|█▋        | 881/5198 [36:19<105:45:04, 88.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:38:36,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=873, skipped=0, lr=[1.922110365036836e-05], mom=[(0.9, 0.999)]
steps: 873 loss: 0.6275 iter time (s): 95.444 samples/sec: 1.341

100%|██████████| 1/1 [01:36<00:00, 96.18s/it][A100%|██████████| 1/1 [01:36<00:00, 96.18s/it]
 17%|█▋        | 881/5198 [36:25<105:36:31, 88.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.31s/it][A100%|██████████| 1/1 [01:36<00:00, 96.31s/it]
 17%|█▋        | 881/5198 [36:06<105:39:09, 88.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.27s/it][A100%|██████████| 1/1 [01:36<00:00, 96.27s/it]
 17%|█▋        | 881/5198 [36:22<105:37:46, 88.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.28s/it][A100%|██████████| 1/1 [01:36<00:00, 96.28s/it]
 17%|█▋        | 881/5198 [36:07<105:39:00, 88.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.28s/it][A100%|██████████| 1/1 [01:36<00:00, 96.28s/it]
 17%|█▋        | 881/5198 [36:08<105:38:24, 88.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.31s/it][A100%|██████████| 1/1 [01:36<00:00, 96.31s/it]
 17%|█▋        | 881/5198 [36:08<105:38:30, 88.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.34s/it][A100%|██████████| 1/1 [01:36<00:00, 96.34s/it]
 17%|█▋        | 881/5198 [35:59<105:38:40, 88.10s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_826
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.55s/it][A100%|██████████| 1/1 [01:53<00:00, 113.55s/it]
 17%|█▋        | 882/5198 [38:13<113:42:22, 94.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:40:30,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=874, skipped=0, lr=[1.9218910676960963e-05], mom=[(0.9, 0.999)]
steps: 874 loss: 0.6356 iter time (s): 113.021 samples/sec: 1.133

100%|██████████| 1/1 [01:53<00:00, 113.98s/it][A100%|██████████| 1/1 [01:53<00:00, 113.98s/it]
 17%|█▋        | 882/5198 [38:19<113:44:10, 94.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.90s/it][A100%|██████████| 1/1 [01:53<00:00, 113.90s/it]
 17%|█▋        | 882/5198 [38:00<113:44:27, 94.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.04s/it][A100%|██████████| 1/1 [01:54<00:00, 114.04s/it]
 17%|█▋        | 882/5198 [38:16<113:46:19, 94.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.91s/it][A100%|██████████| 1/1 [01:53<00:00, 113.91s/it]
 17%|█▋        | 882/5198 [38:01<113:44:33, 94.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.98s/it][A100%|██████████| 1/1 [01:53<00:00, 113.98s/it]
 17%|█▋        | 882/5198 [38:02<113:45:29, 94.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.94s/it][A100%|██████████| 1/1 [01:53<00:00, 113.94s/it]
 17%|█▋        | 882/5198 [38:02<113:44:51, 94.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:53<00:00, 113.95s/it][A100%|██████████| 1/1 [01:53<00:00, 113.95s/it]
 17%|█▋        | 882/5198 [37:53<113:45:03, 94.88s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_827
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.66s/it][A100%|██████████| 1/1 [01:29<00:00, 89.66s/it]
 17%|█▋        | 883/5198 [39:43<111:59:39, 93.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:41:59,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=875, skipped=0, lr=[1.9216714746219005e-05], mom=[(0.9, 0.999)]
steps: 875 loss: 0.6400 iter time (s): 88.193 samples/sec: 1.451

100%|██████████| 1/1 [01:29<00:00, 89.08s/it][A100%|██████████| 1/1 [01:29<00:00, 89.09s/it]
 17%|█▋        | 883/5198 [39:49<111:49:28, 93.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.98s/it][A100%|██████████| 1/1 [01:28<00:00, 88.98s/it]
 17%|█▋        | 883/5198 [39:29<111:47:27, 93.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.88s/it][A100%|██████████| 1/1 [01:28<00:00, 88.89s/it]
 17%|█▋        | 883/5198 [39:45<111:47:01, 93.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.01s/it][A100%|██████████| 1/1 [01:29<00:00, 89.01s/it]
 17%|█▋        | 883/5198 [39:30<111:48:04, 93.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.93s/it][A100%|██████████| 1/1 [01:28<00:00, 88.93s/it]
 17%|█▋        | 883/5198 [39:31<111:47:13, 93.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.95s/it][A100%|██████████| 1/1 [01:28<00:00, 88.95s/it]
 17%|█▋        | 883/5198 [39:31<111:47:15, 93.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.94s/it][A100%|██████████| 1/1 [01:28<00:00, 88.94s/it]
 17%|█▋        | 883/5198 [39:22<111:47:09, 93.26s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_828
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.30s/it][A100%|██████████| 1/1 [01:43<00:00, 103.30s/it]
 17%|█▋        | 884/5198 [41:26<115:17:25, 96.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:43:43,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=876, skipped=0, lr=[1.9214515858847e-05], mom=[(0.9, 0.999)]
steps: 876 loss: 0.6035 iter time (s): 103.000 samples/sec: 1.243

100%|██████████| 1/1 [01:43<00:00, 103.84s/it][A100%|██████████| 1/1 [01:43<00:00, 103.84s/it]
 17%|█▋        | 884/5198 [41:32<115:20:41, 96.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.83s/it][A100%|██████████| 1/1 [01:43<00:00, 103.83s/it]
 17%|█▋        | 884/5198 [41:12<115:18:51, 96.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.82s/it][A100%|██████████| 1/1 [01:43<00:00, 103.82s/it]
 17%|█▋        | 884/5198 [41:29<115:18:26, 96.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.73s/it][A100%|██████████| 1/1 [01:43<00:00, 103.73s/it]
 17%|█▋        | 884/5198 [41:13<115:17:23, 96.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.85s/it][A100%|██████████| 1/1 [01:43<00:00, 103.85s/it]
 17%|█▋        | 884/5198 [41:15<115:19:04, 96.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.85s/it][A100%|██████████| 1/1 [01:43<00:00, 103.85s/it]
 17%|█▋        | 884/5198 [41:15<115:19:12, 96.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.84s/it][A100%|██████████| 1/1 [01:43<00:00, 103.85s/it]
 17%|█▋        | 884/5198 [41:06<115:18:56, 96.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_829
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.27s/it][A100%|██████████| 1/1 [01:21<00:00, 81.27s/it]
 17%|█▋        | 885/5198 [42:47<110:09:20, 91.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:45:04,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=877, skipped=0, lr=[1.9212314015550397e-05], mom=[(0.9, 0.999)]
steps: 877 loss: 0.6214 iter time (s): 79.811 samples/sec: 1.604

100%|██████████| 1/1 [01:20<00:00, 80.48s/it][A100%|██████████| 1/1 [01:20<00:00, 80.48s/it]
 17%|█▋        | 885/5198 [42:53<109:55:04, 91.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.55s/it][A100%|██████████| 1/1 [01:20<00:00, 80.55s/it]
 17%|█▋        | 885/5198 [42:33<109:55:17, 91.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.61s/it][A100%|██████████| 1/1 [01:20<00:00, 80.61s/it]
 17%|█▋        | 885/5198 [42:49<109:56:04, 91.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.64s/it][A100%|██████████| 1/1 [01:20<00:00, 80.64s/it]
 17%|█▋        | 885/5198 [42:34<109:56:02, 91.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.54s/it][A100%|██████████| 1/1 [01:20<00:00, 80.54s/it]
 17%|█▋        | 885/5198 [42:36<109:55:17, 91.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.59s/it][A100%|██████████| 1/1 [01:20<00:00, 80.59s/it]
 17%|█▋        | 885/5198 [42:35<109:56:08, 91.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.57s/it][A100%|██████████| 1/1 [01:20<00:00, 80.57s/it]
 17%|█▋        | 885/5198 [42:26<109:55:40, 91.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_830
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.05s/it][A100%|██████████| 1/1 [01:45<00:00, 105.05s/it]
 17%|█▋        | 886/5198 [44:32<114:41:15, 95.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:46:49,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=878, skipped=0, lr=[1.9210109217035607e-05], mom=[(0.9, 0.999)]
steps: 878 loss: 0.6443 iter time (s): 104.982 samples/sec: 1.219

100%|██████████| 1/1 [01:45<00:00, 105.88s/it][A100%|██████████| 1/1 [01:45<00:00, 105.88s/it]
 17%|█▋        | 886/5198 [44:39<114:48:18, 95.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.82s/it][A100%|██████████| 1/1 [01:45<00:00, 105.82s/it]
 17%|█▋        | 886/5198 [44:19<114:47:04, 95.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.82s/it][A100%|██████████| 1/1 [01:45<00:00, 105.82s/it]
 17%|█▋        | 886/5198 [44:20<114:47:38, 95.84s/it]
100%|██████████| 1/1 [01:45<00:00, 105.91s/it][A100%|██████████| 1/1 [01:45<00:00, 105.91s/it]
 17%|█▋        | 886/5198 [44:35<114:49:37, 95.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.79s/it][A100%|██████████| 1/1 [01:45<00:00, 105.79s/it]
 17%|█▋        | 886/5198 [44:21<114:47:01, 95.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.83s/it][A100%|██████████| 1/1 [01:45<00:00, 105.83s/it]
 17%|█▋        | 886/5198 [44:21<114:47:15, 95.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.81s/it][A100%|██████████| 1/1 [01:45<00:00, 105.81s/it]
 17%|█▋        | 886/5198 [44:12<114:47:15, 95.83s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_831
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.18s/it][A100%|██████████| 1/1 [01:36<00:00, 96.19s/it]
 17%|█▋        | 887/5198 [46:08<114:49:10, 95.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:48:25,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=879, skipped=0, lr=[1.920790146400998e-05], mom=[(0.9, 0.999)]
steps: 879 loss: 0.6134 iter time (s): 95.117 samples/sec: 1.346

100%|██████████| 1/1 [01:35<00:00, 95.88s/it][A100%|██████████| 1/1 [01:35<00:00, 95.88s/it]
 17%|█▋        | 887/5198 [46:15<114:47:27, 95.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.90s/it][A100%|██████████| 1/1 [01:35<00:00, 95.90s/it]
 17%|█▋        | 887/5198 [45:55<114:47:02, 95.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.81s/it][A100%|██████████| 1/1 [01:35<00:00, 95.81s/it]
 17%|█▋        | 887/5198 [46:11<114:47:03, 95.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.90s/it][A100%|██████████| 1/1 [01:35<00:00, 95.90s/it]
 17%|█▋        | 887/5198 [45:56<114:47:32, 95.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.91s/it][A100%|██████████| 1/1 [01:35<00:00, 95.91s/it]
 17%|█▋        | 887/5198 [45:57<114:47:16, 95.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.90s/it][A100%|██████████| 1/1 [01:35<00:00, 95.90s/it]
 17%|█▋        | 887/5198 [45:57<114:47:17, 95.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.90s/it][A100%|██████████| 1/1 [01:35<00:00, 95.90s/it]
 17%|█▋        | 887/5198 [45:48<114:47:07, 95.85s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_832
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.30s/it][A100%|██████████| 1/1 [01:25<00:00, 85.30s/it]
 17%|█▋        | 888/5198 [47:34<111:03:51, 92.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:49:50,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.9205690757181824e-05], mom=[(0.9, 0.999)]
steps: 880 loss: 0.6653 iter time (s): 84.184 samples/sec: 1.520

100%|██████████| 1/1 [01:24<00:00, 85.00s/it][A100%|██████████| 1/1 [01:24<00:00, 85.00s/it]
 17%|█▋        | 888/5198 [47:40<110:55:54, 92.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.94s/it][A100%|██████████| 1/1 [01:24<00:00, 84.94s/it]
 17%|█▋        | 888/5198 [47:20<110:54:19, 92.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.93s/it][A100%|██████████| 1/1 [01:24<00:00, 84.93s/it]
 17%|█▋        | 888/5198 [47:36<110:54:19, 92.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.92s/it][A100%|██████████| 1/1 [01:24<00:00, 84.92s/it]
 17%|█▋        | 888/5198 [47:21<110:54:10, 92.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.01s/it][A100%|██████████| 1/1 [01:25<00:00, 85.01s/it]
 17%|█▋        | 888/5198 [47:22<110:55:55, 92.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.00s/it][A100%|██████████| 1/1 [01:25<00:00, 85.00s/it]
 17%|█▋        | 888/5198 [47:22<110:55:49, 92.66s/it]
100%|██████████| 1/1 [01:24<00:00, 84.99s/it][A100%|██████████| 1/1 [01:24<00:00, 84.99s/it]
 17%|█▋        | 888/5198 [47:13<110:55:24, 92.65s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_833

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.93s/it][A100%|██████████| 1/1 [01:40<00:00, 100.93s/it]
 17%|█▋        | 889/5198 [49:15<113:56:54, 95.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:51:32,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=881, skipped=0, lr=[1.920347709726039e-05], mom=[(0.9, 0.999)]
steps: 881 loss: 0.6727 iter time (s): 100.618 samples/sec: 1.272

100%|██████████| 1/1 [01:41<00:00, 101.33s/it][A100%|██████████| 1/1 [01:41<00:00, 101.33s/it]
 17%|█▋        | 889/5198 [49:21<113:59:13, 95.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.44s/it][A100%|██████████| 1/1 [01:41<00:00, 101.44s/it]
 17%|█▋        | 889/5198 [49:01<114:00:21, 95.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.49s/it][A100%|██████████| 1/1 [01:41<00:00, 101.49s/it]
 17%|█▋        | 889/5198 [49:18<114:01:35, 95.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.48s/it][A100%|██████████| 1/1 [01:41<00:00, 101.48s/it]
 17%|█▋        | 889/5198 [49:02<114:01:13, 95.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.42s/it][A100%|██████████| 1/1 [01:41<00:00, 101.42s/it]
 17%|█▋        | 889/5198 [49:03<114:01:12, 95.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.43s/it][A100%|██████████| 1/1 [01:41<00:00, 101.43s/it]
 17%|█▋        | 889/5198 [49:04<114:01:14, 95.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.46s/it][A100%|██████████| 1/1 [01:41<00:00, 101.46s/it]
 17%|█▋        | 889/5198 [48:55<114:01:28, 95.26s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_834
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.34s/it][A100%|██████████| 1/1 [01:30<00:00, 90.34s/it]
 17%|█▋        | 890/5198 [50:45<112:12:03, 93.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:53:02,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=882, skipped=0, lr=[1.9201260484955865e-05], mom=[(0.9, 0.999)]
steps: 882 loss: 0.6353 iter time (s): 89.229 samples/sec: 1.435

100%|██████████| 1/1 [01:30<00:00, 90.09s/it][A100%|██████████| 1/1 [01:30<00:00, 90.09s/it]
 17%|█▋        | 890/5198 [50:51<112:07:58, 93.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.09s/it][A100%|██████████| 1/1 [01:30<00:00, 90.09s/it]
 17%|█▋        | 890/5198 [50:31<112:08:50, 93.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.02s/it][A100%|██████████| 1/1 [01:30<00:00, 90.02s/it]
 17%|█▋        | 890/5198 [50:48<112:08:07, 93.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.03s/it][A100%|██████████| 1/1 [01:30<00:00, 90.03s/it]
 17%|█▋        | 890/5198 [50:32<112:08:05, 93.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.02s/it][A100%|██████████| 1/1 [01:30<00:00, 90.02s/it]
 17%|█▋        | 890/5198 [50:33<112:08:00, 93.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.04s/it][A100%|██████████| 1/1 [01:30<00:00, 90.04s/it]
 17%|█▋        | 890/5198 [50:34<112:08:19, 93.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.03s/it][A100%|██████████| 1/1 [01:30<00:00, 90.03s/it]
 17%|█▋        | 890/5198 [50:25<112:08:18, 93.71s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_835
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.42s/it][A100%|██████████| 1/1 [01:57<00:00, 117.42s/it]
 17%|█▋        | 891/5198 [52:42<120:37:21, 100.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:55:00,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=883, skipped=0, lr=[1.9199040920979404e-05], mom=[(0.9, 0.999)]
steps: 883 loss: 0.6878 iter time (s): 117.464 samples/sec: 1.090

100%|██████████| 1/1 [01:58<00:00, 118.29s/it][A100%|██████████| 1/1 [01:58<00:00, 118.29s/it]
 17%|█▋        | 891/5198 [52:49<120:53:00, 101.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.27s/it][A100%|██████████| 1/1 [01:58<00:00, 118.27s/it]
 17%|█▋        | 891/5198 [52:30<120:53:17, 101.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.22s/it][A100%|██████████| 1/1 [01:58<00:00, 118.22s/it]
 17%|█▋        | 891/5198 [52:31<120:51:32, 101.02s/it]
100%|██████████| 1/1 [01:58<00:00, 118.28s/it][A100%|██████████| 1/1 [01:58<00:00, 118.28s/it]
 17%|█▋        | 891/5198 [52:46<120:52:57, 101.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.25s/it][A100%|██████████| 1/1 [01:58<00:00, 118.25s/it]
 17%|█▋        | 891/5198 [52:32<120:52:10, 101.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.25s/it][A100%|██████████| 1/1 [01:58<00:00, 118.25s/it]
 17%|█▋        | 891/5198 [52:32<120:52:21, 101.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.25s/it][A100%|██████████| 1/1 [01:58<00:00, 118.25s/it]
 17%|█▋        | 891/5198 [52:23<120:52:14, 101.03s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_836
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.32s/it][A100%|██████████| 1/1 [01:38<00:00, 98.32s/it]
 17%|█▋        | 892/5198 [54:21<119:42:44, 100.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:56:38,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=884, skipped=0, lr=[1.919681840604309e-05], mom=[(0.9, 0.999)]
steps: 884 loss: 0.6523 iter time (s): 96.954 samples/sec: 1.320

100%|██████████| 1/1 [01:37<00:00, 97.82s/it][A100%|██████████| 1/1 [01:37<00:00, 97.82s/it]
 17%|█▋        | 892/5198 [54:27<119:42:36, 100.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.76s/it][A100%|██████████| 1/1 [01:37<00:00, 97.76s/it]
 17%|█▋        | 892/5198 [54:07<119:41:30, 100.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.81s/it][A100%|██████████| 1/1 [01:37<00:00, 97.81s/it]
 17%|█▋        | 892/5198 [54:24<119:42:19, 100.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.85s/it][A100%|██████████| 1/1 [01:37<00:00, 97.85s/it]
 17%|█▋        | 892/5198 [54:08<119:42:06, 100.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.80s/it][A100%|██████████| 1/1 [01:37<00:00, 97.80s/it]
 17%|█▋        | 892/5198 [54:10<119:41:21, 100.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.83s/it][A100%|██████████| 1/1 [01:37<00:00, 97.83s/it]
 17%|█▋        | 892/5198 [54:10<119:42:08, 100.08s/it]
100%|██████████| 1/1 [01:37<00:00, 97.82s/it][A100%|██████████| 1/1 [01:37<00:00, 97.82s/it]
 17%|█▋        | 892/5198 [54:01<119:41:46, 100.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_837

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.64s/it][A100%|██████████| 1/1 [01:23<00:00, 83.64s/it]
 17%|█▋        | 893/5198 [55:45<113:48:48, 95.17s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:58:01,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=885, skipped=0, lr=[1.9194592940859965e-05], mom=[(0.9, 0.999)]
steps: 885 loss: 0.6620 iter time (s): 82.404 samples/sec: 1.553

100%|██████████| 1/1 [01:23<00:00, 83.19s/it][A100%|██████████| 1/1 [01:23<00:00, 83.19s/it]
 17%|█▋        | 893/5198 [55:50<113:38:35, 95.03s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.19s/it][A100%|██████████| 1/1 [01:23<00:00, 83.19s/it]
 17%|█▋        | 893/5198 [55:31<113:37:48, 95.02s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.05s/it][A100%|██████████| 1/1 [01:23<00:00, 83.05s/it]
 17%|█▋        | 893/5198 [55:47<113:35:23, 94.99s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.15s/it][A100%|██████████| 1/1 [01:23<00:00, 83.15s/it]
 17%|█▋        | 893/5198 [55:32<113:37:11, 95.01s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.19s/it][A100%|██████████| 1/1 [01:23<00:00, 83.20s/it]
 17%|█▋        | 893/5198 [55:33<113:37:43, 95.02s/it] 
100%|██████████| 1/1 [01:23<00:00, 83.13s/it][A100%|██████████| 1/1 [01:23<00:00, 83.13s/it]
 17%|█▋        | 893/5198 [55:33<113:36:55, 95.01s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.16s/it][A100%|██████████| 1/1 [01:23<00:00, 83.16s/it]
 17%|█▋        | 893/5198 [55:24<113:37:11, 95.01s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_838
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.99s/it][A100%|██████████| 1/1 [01:41<00:00, 101.99s/it]
 17%|█▋        | 894/5198 [57:27<116:14:03, 97.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 01:59:44,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=886, skipped=0, lr=[1.9192364526144013e-05], mom=[(0.9, 0.999)]
steps: 886 loss: 0.6811 iter time (s): 101.780 samples/sec: 1.258

100%|██████████| 1/1 [01:42<00:00, 102.46s/it][A100%|██████████| 1/1 [01:42<00:00, 102.46s/it]
 17%|█▋        | 894/5198 [57:33<116:16:38, 97.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.60s/it][A100%|██████████| 1/1 [01:42<00:00, 102.60s/it]
 17%|█▋        | 894/5198 [57:13<116:19:09, 97.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.60s/it][A100%|██████████| 1/1 [01:42<00:00, 102.60s/it]
 17%|█▋        | 894/5198 [57:29<116:17:35, 97.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.55s/it][A100%|██████████| 1/1 [01:42<00:00, 102.55s/it]
 17%|█▋        | 894/5198 [57:14<116:17:36, 97.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.53s/it][A100%|██████████| 1/1 [01:42<00:00, 102.53s/it]
 17%|█▋        | 894/5198 [57:15<116:17:34, 97.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.58s/it][A100%|██████████| 1/1 [01:42<00:00, 102.58s/it]
 17%|█▋        | 894/5198 [57:16<116:18:03, 97.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.58s/it][A100%|██████████| 1/1 [01:42<00:00, 102.58s/it]
 17%|█▋        | 894/5198 [57:06<116:18:13, 97.28s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_839
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.39s/it][A100%|██████████| 1/1 [01:27<00:00, 87.39s/it]
 17%|█▋        | 895/5198 [58:54<112:41:38, 94.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:01:11,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=887, skipped=0, lr=[1.9190133162610168e-05], mom=[(0.9, 0.999)]
steps: 887 loss: 0.6488 iter time (s): 86.160 samples/sec: 1.486

100%|██████████| 1/1 [01:27<00:00, 87.01s/it][A100%|██████████| 1/1 [01:27<00:00, 87.01s/it]
 17%|█▋        | 895/5198 [59:00<112:34:57, 94.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.97s/it][A100%|██████████| 1/1 [01:26<00:00, 86.97s/it]
 17%|█▋        | 895/5198 [58:40<112:35:55, 94.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.02s/it][A100%|██████████| 1/1 [01:27<00:00, 87.02s/it]
 17%|█▋        | 895/5198 [58:56<112:35:56, 94.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.05s/it][A100%|██████████| 1/1 [01:27<00:00, 87.05s/it]
 17%|█▋        | 895/5198 [58:41<112:36:35, 94.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.02s/it][A100%|██████████| 1/1 [01:27<00:00, 87.02s/it]
 17%|█▋        | 895/5198 [58:42<112:35:45, 94.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.99s/it][A100%|██████████| 1/1 [01:26<00:00, 86.99s/it]
 17%|█▋        | 895/5198 [58:43<112:35:31, 94.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.98s/it][A100%|██████████| 1/1 [01:26<00:00, 86.99s/it]
 17%|█▋        | 895/5198 [58:33<112:35:31, 94.20s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_55
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.82s/it][A100%|██████████| 1/1 [02:01<00:00, 121.82s/it]
 17%|█▋        | 896/5198 [1:00:56<122:32:24, 102.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:03:13,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=888, skipped=0, lr=[1.91878988509743e-05], mom=[(0.9, 0.999)]
steps: 888 loss: 0.8072 iter time (s): 122.043 samples/sec: 1.049

100%|██████████| 1/1 [02:02<00:00, 122.93s/it][A100%|██████████| 1/1 [02:02<00:00, 122.93s/it]
 17%|█▋        | 896/5198 [1:01:03<122:51:02, 102.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.91s/it][A100%|██████████| 1/1 [02:02<00:00, 122.91s/it]
 17%|█▋        | 896/5198 [1:00:43<122:51:22, 102.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.85s/it][A100%|██████████| 1/1 [02:02<00:00, 122.85s/it]
 17%|█▋        | 896/5198 [1:00:59<122:50:01, 102.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.93s/it][A100%|██████████| 1/1 [02:02<00:00, 122.93s/it]
 17%|█▋        | 896/5198 [1:00:44<122:52:13, 102.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.90s/it][A100%|██████████| 1/1 [02:02<00:00, 122.90s/it]
 17%|█▋        | 896/5198 [1:00:45<122:51:03, 102.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.89s/it][A100%|██████████| 1/1 [02:02<00:00, 122.89s/it]
 17%|█▋        | 896/5198 [1:00:36<122:50:40, 102.80s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_840
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.92s/it][A100%|██████████| 1/1 [02:02<00:00, 122.92s/it]
 17%|█▋        | 896/5198 [1:00:46<122:51:22, 102.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.45s/it][A100%|██████████| 1/1 [01:39<00:00, 99.45s/it]
 17%|█▋        | 897/5198 [1:02:35<121:24:41, 101.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:04:52,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=889, skipped=0, lr=[1.9185661591953236e-05], mom=[(0.9, 0.999)]
steps: 889 loss: 0.6619 iter time (s): 97.899 samples/sec: 1.307

100%|██████████| 1/1 [01:38<00:00, 98.67s/it][A100%|██████████| 1/1 [01:38<00:00, 98.67s/it]
 17%|█▋        | 897/5198 [1:02:42<121:20:38, 101.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.68s/it][A100%|██████████| 1/1 [01:38<00:00, 98.68s/it]
 17%|█▋        | 897/5198 [1:02:22<121:21:10, 101.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.82s/it][A100%|██████████| 1/1 [01:38<00:00, 98.82s/it]
 17%|█▋        | 897/5198 [1:02:38<121:23:04, 101.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.74s/it][A100%|██████████| 1/1 [01:38<00:00, 98.74s/it]
 17%|█▋        | 897/5198 [1:02:23<121:22:53, 101.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.75s/it][A100%|██████████| 1/1 [01:38<00:00, 98.75s/it]
 17%|█▋        | 897/5198 [1:02:24<121:22:27, 101.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A100%|██████████| 1/1 [01:38<00:00, 98.71s/it][A100%|██████████| 1/1 [01:38<00:00, 98.71s/it]
 17%|█▋        | 897/5198 [1:02:24<121:21:48, 101.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.75s/it][A100%|██████████| 1/1 [01:38<00:00, 98.75s/it]
 17%|█▋        | 897/5198 [1:02:15<121:21:59, 101.59s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_841
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.62s/it][A100%|██████████| 1/1 [01:35<00:00, 95.62s/it]
 17%|█▋        | 898/5198 [1:04:11<119:14:31, 99.83s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:06:28,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.918342138626474e-05], mom=[(0.9, 0.999)]
steps: 890 loss: 0.6627 iter time (s): 94.710 samples/sec: 1.351

100%|██████████| 1/1 [01:35<00:00, 95.67s/it][A100%|██████████| 1/1 [01:35<00:00, 95.67s/it]
 17%|█▋        | 898/5198 [1:04:17<119:12:26, 99.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.50s/it][A100%|██████████| 1/1 [01:35<00:00, 95.50s/it]
 17%|█▋        | 898/5198 [1:03:58<119:10:14, 99.77s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.83s/it][A100%|██████████| 1/1 [01:35<00:00, 95.83s/it]
 17%|█▋        | 898/5198 [1:03:58<119:16:07, 99.85s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.74s/it][A100%|██████████| 1/1 [01:35<00:00, 95.74s/it]
 17%|█▋        | 898/5198 [1:04:14<119:15:32, 99.84s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.72s/it][A100%|██████████| 1/1 [01:35<00:00, 95.72s/it]
 17%|█▋        | 898/5198 [1:04:00<119:14:42, 99.83s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.74s/it][A100%|██████████| 1/1 [01:35<00:00, 95.74s/it]
 17%|█▋        | 898/5198 [1:04:00<119:14:45, 99.83s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.28s/it][A100%|██████████| 1/1 [01:36<00:00, 96.28s/it]
 17%|█▋        | 898/5198 [1:03:51<119:26:22, 100.00s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_842
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.69s/it][A100%|██████████| 1/1 [01:40<00:00, 100.69s/it]
 17%|█▋        | 899/5198 [1:05:52<119:31:57, 100.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:08:09,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=891, skipped=0, lr=[1.9181178234627532e-05], mom=[(0.9, 0.999)]
steps: 891 loss: 0.6906 iter time (s): 99.346 samples/sec: 1.288

100%|██████████| 1/1 [01:40<00:00, 100.88s/it][A100%|██████████| 1/1 [01:40<00:00, 100.88s/it]
 17%|█▋        | 899/5198 [1:05:58<119:34:16, 100.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.59s/it][A100%|██████████| 1/1 [01:40<00:00, 100.59s/it]
 17%|█▋        | 899/5198 [1:05:38<119:30:31, 100.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.60s/it][A100%|██████████| 1/1 [01:40<00:00, 100.60s/it]
 17%|█▋        | 899/5198 [1:05:54<119:30:37, 100.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.89s/it][A100%|██████████| 1/1 [01:40<00:00, 100.89s/it]
 17%|█▋        | 899/5198 [1:05:39<119:32:51, 100.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.67s/it][A100%|██████████| 1/1 [01:40<00:00, 100.67s/it]
 17%|█▋        | 899/5198 [1:05:40<119:31:14, 100.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.70s/it][A100%|██████████| 1/1 [01:40<00:00, 100.70s/it]
 17%|█▋        | 899/5198 [1:05:41<119:31:54, 100.10s/it]
100%|██████████| 1/1 [01:40<00:00, 100.16s/it][A100%|██████████| 1/1 [01:40<00:00, 100.16s/it]
 17%|█▋        | 899/5198 [1:05:32<119:28:19, 100.05s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_843

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.11s/it][A100%|██████████| 1/1 [02:00<00:00, 120.11s/it]
[2024-08-18 02:10:09,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=892, skipped=0, lr=[1.9178932137761274e-05], mom=[(0.9, 0.999)]
steps: 892 loss: 0.6780 iter time (s): 119.854 samples/sec: 1.068

100%|██████████| 1/1 [02:00<00:00, 120.59s/it][A100%|██████████| 1/1 [02:00<00:00, 120.59s/it]

100%|██████████| 1/1 [02:00<00:00, 120.72s/it][A100%|██████████| 1/1 [02:00<00:00, 120.72s/it]

100%|██████████| 1/1 [02:00<00:00, 120.77s/it][A100%|██████████| 1/1 [02:00<00:00, 120.77s/it]

100%|██████████| 1/1 [02:00<00:00, 120.71s/it][A100%|██████████| 1/1 [02:00<00:00, 120.71s/it]

100%|██████████| 1/1 [02:00<00:00, 120.73s/it][A100%|██████████| 1/1 [02:00<00:00, 120.73s/it]

100%|██████████| 1/1 [02:00<00:00, 120.71s/it][A100%|██████████| 1/1 [02:00<00:00, 120.71s/it]

100%|██████████| 1/1 [02:00<00:00, 120.71s/it][A100%|██████████| 1/1 [02:00<00:00, 120.71s/it]
Checkpointing at shard 899
[2024-08-18 02:10:13,369] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step892 is about to be saved!
[2024-08-18 02:10:13,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_00-model_states.pt...
[2024-08-18 02:10:20,136] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_00-model_states.pt.
[2024-08-18 02:10:21,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_02-model_states.pt...
[2024-08-18 02:10:23,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_03-model_states.pt...
[2024-08-18 02:10:24,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_04-model_states.pt...
[2024-08-18 02:10:24,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_01-model_states.pt...
[2024-08-18 02:10:28,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_07-model_states.pt...
[2024-08-18 02:10:29,328] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_08-model_states.pt...
[2024-08-18 02:10:34,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_06-model_states.pt...
[2024-08-18 02:10:34,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_05-model_states.pt...
[2024-08-18 02:12:01,472] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_03-model_states.pt.
[2024-08-18 02:12:01,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_01-model_states.pt.
[2024-08-18 02:12:01,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_02-model_states.pt.
[2024-08-18 02:12:01,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_04-model_states.pt.
[2024-08-18 02:12:01,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_02_model_states.pt...
[2024-08-18 02:12:01,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_05-model_states.pt.
[2024-08-18 02:12:01,606] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_00_model_states.pt
[2024-08-18 02:12:01,606] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_00_model_states.pt...
[2024-08-18 02:12:01,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_04_model_states.pt...
[2024-08-18 02:12:01,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_02_model_states.pt.
[2024-08-18 02:12:01,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
[2024-08-18 02:12:01,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_03_model_states.pt...
[2024-08-18 02:12:01,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_04_model_states.pt.
[2024-08-18 02:12:01,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
[2024-08-18 02:12:01,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_03_model_states.pt.
[2024-08-18 02:12:01,880] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
[2024-08-18 02:12:02,067] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_01_model_states.pt
[2024-08-18 02:12:02,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_01_model_states.pt...
[2024-08-18 02:12:02,222] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_01_model_states.pt.
[2024-08-18 02:12:02,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
[2024-08-18 02:12:02,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_00_model_states.pt.
[2024-08-18 02:12:02,408] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
[2024-08-18 02:12:02,788] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_06-model_states.pt.
[2024-08-18 02:12:02,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_05_model_states.pt...
[2024-08-18 02:12:02,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_08-model_states.pt.
[2024-08-18 02:12:03,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_05_model_states.pt.
[2024-08-18 02:12:03,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
[2024-08-18 02:12:03,730] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_09-model_states.pt...
[2024-08-18 02:12:04,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_09-model_states.pt.
[2024-08-18 02:12:04,758] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_07_model_states.pt...
[2024-08-18 02:12:04,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_07_model_states.pt.
[2024-08-18 02:12:04,871] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
[2024-08-18 02:12:06,133] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/layer_07-model_states.pt.
[2024-08-18 02:12:06,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_06_model_states.pt...
[2024-08-18 02:12:06,757] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_lora_128_256_checkpoint/global_step892/mp_rank_06_model_states.pt.
[2024-08-18 02:12:06,759] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step892 is ready now!
Checkpoint saved using --- 116.09513068199158 seconds ---
 17%|█▋        | 900/5198 [1:09:52<169:35:53, 142.06s/it] 17%|█▋        | 900/5198 [1:09:35<168:34:49, 141.20s/it] 17%|█▋        | 900/5198 [1:09:52<168:32:05, 141.16s/it] 17%|█▋        | 900/5198 [1:09:38<168:27:43, 141.10s/it] 17%|█▋        | 900/5198 [1:09:28<168:25:02, 141.07s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_844
 17%|█▋        | 900/5198 [1:09:37<168:28:31, 141.11s/it] 17%|█▋        | 900/5198 [1:09:36<168:30:46, 141.15s/it] 17%|█▋        | 900/5198 [1:09:55<168:37:54, 141.25s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.32s/it][A100%|██████████| 1/1 [01:16<00:00, 76.33s/it]
 17%|█▋        | 901/5198 [1:11:08<146:01:59, 122.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:13:24,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=893, skipped=0, lr=[1.9176683096386558e-05], mom=[(0.9, 0.999)]
steps: 893 loss: 0.6397 iter time (s): 78.090 samples/sec: 1.639

100%|██████████| 1/1 [01:18<00:00, 78.39s/it][A
100%|██████████| 1/1 [01:18<00:00, 78.39s/it]
 17%|█▋        | 901/5198 [1:11:14<146:08:34, 122.44s/it]100%|██████████| 1/1 [01:18<00:00, 78.53s/it][A100%|██████████| 1/1 [01:18<00:00, 78.53s/it]
 17%|█▋        | 901/5198 [1:10:54<146:06:32, 122.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.65s/it][A100%|██████████| 1/1 [01:18<00:00, 78.66s/it]
 17%|█▋        | 901/5198 [1:11:10<146:07:21, 122.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.69s/it][A100%|██████████| 1/1 [01:18<00:00, 78.69s/it]
 17%|█▋        | 901/5198 [1:10:55<146:09:40, 122.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.83s/it][A100%|██████████| 1/1 [01:18<00:00, 78.83s/it]
 17%|█▋        | 901/5198 [1:10:56<146:08:47, 122.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.89s/it][A100%|██████████| 1/1 [01:18<00:00, 78.89s/it]
 17%|█▋        | 901/5198 [1:10:57<146:09:19, 122.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.90s/it][A100%|██████████| 1/1 [01:18<00:00, 78.90s/it]
 17%|█▋        | 901/5198 [1:10:47<146:07:44, 122.43s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_845
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.62s/it][A100%|██████████| 1/1 [01:31<00:00, 91.62s/it]
 17%|█▋        | 902/5198 [1:12:40<135:00:25, 113.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:14:56,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=894, skipped=0, lr=[1.9174431111224935e-05], mom=[(0.9, 0.999)]
steps: 894 loss: 0.6703 iter time (s): 91.137 samples/sec: 1.404

100%|██████████| 1/1 [01:31<00:00, 91.86s/it][A100%|██████████| 1/1 [01:31<00:00, 91.86s/it]
 17%|█▋        | 902/5198 [1:12:46<135:10:03, 113.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.93s/it][A100%|██████████| 1/1 [01:31<00:00, 91.93s/it]
 17%|█▋        | 902/5198 [1:12:26<135:10:02, 113.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.03s/it][A100%|██████████| 1/1 [01:32<00:00, 92.03s/it]
 17%|█▋        | 902/5198 [1:12:42<135:12:41, 113.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.94s/it][A100%|██████████| 1/1 [01:31<00:00, 91.94s/it]
 17%|█▋        | 902/5198 [1:12:27<135:12:32, 113.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.96s/it][A100%|██████████| 1/1 [01:31<00:00, 91.96s/it]
 17%|█▋        | 902/5198 [1:12:28<135:12:14, 113.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.00s/it][A100%|██████████| 1/1 [01:32<00:00, 92.00s/it]
 17%|█▋        | 902/5198 [1:12:29<135:13:30, 113.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.00s/it][A100%|██████████| 1/1 [01:32<00:00, 92.00s/it]
 17%|█▋        | 902/5198 [1:12:19<135:12:24, 113.30s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_846
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.60s/it][A100%|██████████| 1/1 [01:28<00:00, 88.60s/it]
 17%|█▋        | 903/5198 [1:14:08<126:12:11, 105.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:16:25,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=895, skipped=0, lr=[1.91721761829989e-05], mom=[(0.9, 0.999)]
steps: 895 loss: 0.6945 iter time (s): 87.669 samples/sec: 1.460

100%|██████████| 1/1 [01:28<00:00, 88.46s/it][A100%|██████████| 1/1 [01:28<00:00, 88.46s/it]
 17%|█▋        | 903/5198 [1:14:14<126:15:39, 105.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.56s/it][A100%|██████████| 1/1 [01:28<00:00, 88.56s/it]
 17%|█▋        | 903/5198 [1:13:54<126:17:59, 105.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.45s/it][A100%|██████████| 1/1 [01:28<00:00, 88.45s/it]
 17%|█▋        | 903/5198 [1:14:11<126:17:07, 105.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.56s/it][A100%|██████████| 1/1 [01:28<00:00, 88.56s/it]
 17%|█▋        | 903/5198 [1:13:56<126:19:31, 105.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.55s/it][A100%|██████████| 1/1 [01:28<00:00, 88.55s/it]
 17%|█▋        | 903/5198 [1:13:57<126:18:58, 105.88s/it]
100%|██████████| 1/1 [01:28<00:00, 88.46s/it][A100%|██████████| 1/1 [01:28<00:00, 88.46s/it]
 17%|█▋        | 903/5198 [1:13:57<126:17:55, 105.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.48s/it][A100%|██████████| 1/1 [01:28<00:00, 88.48s/it]
 17%|█▋        | 903/5198 [1:13:48<126:17:36, 105.86s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_847
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.85s/it][A100%|██████████| 1/1 [01:31<00:00, 91.85s/it]
 17%|█▋        | 904/5198 [1:15:40<121:11:50, 101.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:17:57,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=896, skipped=0, lr=[1.9169918312431894e-05], mom=[(0.9, 0.999)]
steps: 896 loss: 0.6228 iter time (s): 91.105 samples/sec: 1.405

100%|██████████| 1/1 [01:31<00:00, 91.97s/it][A100%|██████████| 1/1 [01:31<00:00, 91.97s/it]
 17%|█▋        | 904/5198 [1:15:46<121:16:27, 101.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.00s/it][A100%|██████████| 1/1 [01:32<00:00, 92.00s/it]
 17%|█▋        | 904/5198 [1:15:26<121:18:49, 101.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.85s/it][A100%|██████████| 1/1 [01:31<00:00, 91.85s/it]
 17%|█▋        | 904/5198 [1:15:27<121:16:38, 101.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.06s/it][A100%|██████████| 1/1 [01:32<00:00, 92.06s/it]
 17%|█▋        | 904/5198 [1:15:43<121:19:23, 101.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.96s/it][A100%|██████████| 1/1 [01:31<00:00, 91.96s/it]
 17%|█▋        | 904/5198 [1:15:29<121:18:29, 101.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.00s/it][A100%|██████████| 1/1 [01:32<00:00, 92.00s/it]
 17%|█▋        | 904/5198 [1:15:29<121:18:44, 101.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.98s/it][A100%|██████████| 1/1 [01:31<00:00, 91.98s/it]
 17%|█▋        | 904/5198 [1:15:20<121:18:03, 101.70s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_848
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.45s/it][A100%|██████████| 1/1 [01:50<00:00, 110.45s/it]
 17%|█▋        | 905/5198 [1:17:31<124:20:27, 104.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:19:48,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=897, skipped=0, lr=[1.9167657500248292e-05], mom=[(0.9, 0.999)]
steps: 897 loss: 0.6278 iter time (s): 110.139 samples/sec: 1.162

100%|██████████| 1/1 [01:51<00:00, 111.07s/it][A100%|██████████| 1/1 [01:51<00:00, 111.07s/it]
 17%|█▋        | 905/5198 [1:17:37<124:36:44, 104.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.06s/it][A100%|██████████| 1/1 [01:51<00:00, 111.06s/it]
 17%|█▋        | 905/5198 [1:17:17<124:37:55, 104.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.00s/it][A100%|██████████| 1/1 [01:51<00:00, 111.00s/it]
 17%|█▋        | 905/5198 [1:17:34<124:37:10, 104.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.15s/it][A100%|██████████| 1/1 [01:51<00:00, 111.15s/it]
 17%|█▋        | 905/5198 [1:17:19<124:38:31, 104.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.04s/it][A100%|██████████| 1/1 [01:51<00:00, 111.04s/it]
 17%|█▋        | 905/5198 [1:17:20<124:37:29, 104.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.10s/it][A100%|██████████| 1/1 [01:51<00:00, 111.10s/it]
 17%|█▋        | 905/5198 [1:17:20<124:38:45, 104.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.06s/it][A100%|██████████| 1/1 [01:51<00:00, 111.06s/it]
 17%|█▋        | 905/5198 [1:17:11<124:37:25, 104.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_849
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.42s/it][A100%|██████████| 1/1 [01:36<00:00, 96.42s/it]
 17%|█▋        | 906/5198 [1:19:07<121:30:50, 101.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:21:24,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=898, skipped=0, lr=[1.9165393747173423e-05], mom=[(0.9, 0.999)]
steps: 898 loss: 0.5970 iter time (s): 95.109 samples/sec: 1.346

100%|██████████| 1/1 [01:35<00:00, 95.94s/it][A100%|██████████| 1/1 [01:35<00:00, 95.94s/it]
 17%|█▋        | 906/5198 [1:19:13<121:31:42, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.91s/it][A100%|██████████| 1/1 [01:35<00:00, 95.91s/it]
 17%|█▋        | 906/5198 [1:18:53<121:31:44, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.93s/it][A100%|██████████| 1/1 [01:35<00:00, 95.94s/it]
 17%|█▋        | 906/5198 [1:19:10<121:31:43, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.89s/it][A100%|██████████| 1/1 [01:35<00:00, 95.89s/it]
 17%|█▋        | 906/5198 [1:18:54<121:31:38, 101.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.85s/it][A100%|██████████| 1/1 [01:35<00:00, 95.85s/it]
 17%|█▋        | 906/5198 [1:18:56<121:30:59, 101.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.93s/it][A100%|██████████| 1/1 [01:35<00:00, 95.93s/it]
 17%|█▋        | 906/5198 [1:18:56<121:31:45, 101.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.92s/it][A100%|██████████| 1/1 [01:35<00:00, 95.92s/it]
 17%|█▋        | 906/5198 [1:18:47<121:31:36, 101.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_850
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.36s/it][A100%|██████████| 1/1 [01:54<00:00, 114.36s/it]
 17%|█▋        | 907/5198 [1:21:01<125:56:26, 105.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:23:19,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=899, skipped=0, lr=[1.916312705393355e-05], mom=[(0.9, 0.999)]
steps: 899 loss: 0.6083 iter time (s): 114.101 samples/sec: 1.122

100%|██████████| 1/1 [01:54<00:00, 114.93s/it][A100%|██████████| 1/1 [01:54<00:00, 114.93s/it]
 17%|█▋        | 907/5198 [1:21:08<126:08:56, 105.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.91s/it][A100%|██████████| 1/1 [01:54<00:00, 114.91s/it]
 17%|█▋        | 907/5198 [1:20:48<126:08:33, 105.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.98s/it][A100%|██████████| 1/1 [01:54<00:00, 114.98s/it]
 17%|█▋        | 907/5198 [1:21:05<126:10:00, 105.85s/it]
100%|██████████| 1/1 [01:54<00:00, 114.89s/it][A100%|██████████| 1/1 [01:54<00:00, 114.89s/it]
 17%|█▋        | 907/5198 [1:20:49<126:08:13, 105.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.94s/it][A100%|██████████| 1/1 [01:54<00:00, 114.94s/it]
 17%|█▋        | 907/5198 [1:20:51<126:08:44, 105.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.92s/it][A100%|██████████| 1/1 [01:54<00:00, 114.92s/it]
 17%|█▋        | 907/5198 [1:20:51<126:08:41, 105.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:54<00:00, 114.90s/it][A100%|██████████| 1/1 [01:54<00:00, 114.90s/it]
 17%|█▋        | 907/5198 [1:20:42<126:08:12, 105.82s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_851
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.87s/it][A100%|██████████| 1/1 [01:40<00:00, 100.87s/it]
 17%|█▋        | 908/5198 [1:22:42<124:12:11, 104.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:24:59,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.9160857421255892e-05], mom=[(0.9, 0.999)]
steps: 900 loss: 0.5901 iter time (s): 99.691 samples/sec: 1.284

100%|██████████| 1/1 [01:40<00:00, 100.48s/it][A100%|██████████| 1/1 [01:40<00:00, 100.48s/it]
 17%|█▋        | 908/5198 [1:22:49<124:12:26, 104.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.44s/it][A100%|██████████| 1/1 [01:40<00:00, 100.44s/it]
 17%|█▋        | 908/5198 [1:22:29<124:11:25, 104.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.42s/it][A100%|██████████| 1/1 [01:40<00:00, 100.42s/it]
 17%|█▋        | 908/5198 [1:22:45<124:11:56, 104.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.50s/it][A100%|██████████| 1/1 [01:40<00:00, 100.50s/it]
 17%|█▋        | 908/5198 [1:22:30<124:12:34, 104.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.51s/it][A100%|██████████| 1/1 [01:40<00:00, 100.51s/it]
 17%|█▋        | 908/5198 [1:22:31<124:12:58, 104.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.52s/it][A100%|██████████| 1/1 [01:40<00:00, 100.52s/it]
 17%|█▋        | 908/5198 [1:22:22<124:12:41, 104.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_852

100%|██████████| 1/1 [01:40<00:00, 100.53s/it][A100%|██████████| 1/1 [01:40<00:00, 100.53s/it]
 17%|█▋        | 908/5198 [1:22:31<124:13:18, 104.24s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.28s/it][A100%|██████████| 1/1 [01:31<00:00, 91.28s/it]
 17%|█▋        | 909/5198 [1:24:14<119:33:14, 100.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:26:30,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=901, skipped=0, lr=[1.9158584849868596e-05], mom=[(0.9, 0.999)]
steps: 901 loss: 0.6625 iter time (s): 89.953 samples/sec: 1.423

100%|██████████| 1/1 [01:30<00:00, 90.79s/it][A100%|██████████| 1/1 [01:30<00:00, 90.79s/it]
 17%|█▋        | 909/5198 [1:24:19<119:22:40, 100.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.80s/it][A100%|██████████| 1/1 [01:30<00:00, 90.80s/it]
 17%|█▋        | 909/5198 [1:24:00<119:22:03, 100.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.84s/it][A100%|██████████| 1/1 [01:30<00:00, 90.84s/it]
 17%|█▋        | 909/5198 [1:24:16<119:23:19, 100.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.85s/it][A100%|██████████| 1/1 [01:30<00:00, 90.85s/it]
 17%|█▋        | 909/5198 [1:24:01<119:24:00, 100.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.78s/it][A100%|██████████| 1/1 [01:30<00:00, 90.78s/it]
 17%|█▋        | 909/5198 [1:24:02<119:23:00, 100.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.84s/it][A100%|██████████| 1/1 [01:30<00:00, 90.84s/it]
 17%|█▋        | 909/5198 [1:24:02<119:24:06, 100.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.82s/it][A100%|██████████| 1/1 [01:30<00:00, 90.82s/it]
 17%|█▋        | 909/5198 [1:23:53<119:23:25, 100.21s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_853
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.04s/it][A100%|██████████| 1/1 [01:29<00:00, 89.04s/it]
 18%|█▊        | 910/5198 [1:25:43<115:29:47, 96.97s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:27:59,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=902, skipped=0, lr=[1.9156309340500763e-05], mom=[(0.9, 0.999)]
steps: 902 loss: 0.6407 iter time (s): 88.370 samples/sec: 1.448

100%|██████████| 1/1 [01:29<00:00, 89.28s/it][A100%|██████████| 1/1 [01:29<00:00, 89.28s/it]
 18%|█▊        | 910/5198 [1:25:49<115:26:55, 96.93s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.30s/it][A100%|██████████| 1/1 [01:29<00:00, 89.30s/it]
 18%|█▊        | 910/5198 [1:25:29<115:27:09, 96.93s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.14s/it][A100%|██████████| 1/1 [01:29<00:00, 89.14s/it]
 18%|█▊        | 910/5198 [1:25:30<115:24:57, 96.90s/it] 
100%|██████████| 1/1 [01:29<00:00, 89.24s/it][A100%|██████████| 1/1 [01:29<00:00, 89.24s/it]
 18%|█▊        | 910/5198 [1:25:45<115:26:44, 96.92s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.18s/it][A100%|██████████| 1/1 [01:29<00:00, 89.18s/it]
 18%|█▊        | 910/5198 [1:25:31<115:25:46, 96.91s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.20s/it][A100%|██████████| 1/1 [01:29<00:00, 89.20s/it]
 18%|█▊        | 910/5198 [1:25:31<115:25:30, 96.91s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.19s/it][A100%|██████████| 1/1 [01:29<00:00, 89.19s/it]
 18%|█▊        | 910/5198 [1:25:22<115:25:31, 96.91s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_854
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.27s/it][A100%|██████████| 1/1 [01:31<00:00, 91.27s/it]
 18%|█▊        | 911/5198 [1:27:14<113:26:12, 95.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:29:30,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=903, skipped=0, lr=[1.9154030893882433e-05], mom=[(0.9, 0.999)]
steps: 903 loss: 0.6161 iter time (s): 90.238 samples/sec: 1.418

100%|██████████| 1/1 [01:30<00:00, 90.98s/it][A100%|██████████| 1/1 [01:30<00:00, 90.98s/it]
 18%|█▊        | 911/5198 [1:27:20<113:18:12, 95.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 18%|█▊        | 911/5198 [1:27:00<113:18:04, 95.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.97s/it][A100%|██████████| 1/1 [01:30<00:00, 90.97s/it]
 18%|█▊        | 911/5198 [1:27:16<113:17:35, 95.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.02s/it][A100%|██████████| 1/1 [01:31<00:00, 91.02s/it]
 18%|█▊        | 911/5198 [1:27:01<113:17:33, 95.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.05s/it][A100%|██████████| 1/1 [01:31<00:00, 91.05s/it]
 18%|█▊        | 911/5198 [1:27:02<113:18:47, 95.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.05s/it][A100%|██████████| 1/1 [01:31<00:00, 91.05s/it]
 18%|█▊        | 911/5198 [1:27:03<113:18:35, 95.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.04s/it][A100%|██████████| 1/1 [01:31<00:00, 91.04s/it]
 18%|█▊        | 911/5198 [1:26:53<113:18:17, 95.15s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_56
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.96s/it][A100%|██████████| 1/1 [01:48<00:00, 108.96s/it]
 18%|█▊        | 912/5198 [1:29:03<118:18:32, 99.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:31:20,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=904, skipped=0, lr=[1.9151749510744583e-05], mom=[(0.9, 0.999)]
steps: 904 loss: 0.8152 iter time (s): 109.046 samples/sec: 1.174

100%|██████████| 1/1 [01:49<00:00, 109.98s/it][A100%|██████████| 1/1 [01:49<00:00, 109.98s/it]
 18%|█▊        | 912/5198 [1:29:10<118:34:48, 99.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.15s/it][A100%|██████████| 1/1 [01:50<00:00, 110.15s/it]
 18%|█▊        | 912/5198 [1:28:50<118:38:16, 99.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.10s/it][A100%|██████████| 1/1 [01:50<00:00, 110.10s/it]
 18%|█▊        | 912/5198 [1:29:06<118:36:43, 99.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 110.00s/it][A100%|██████████| 1/1 [01:50<00:00, 110.00s/it]
 18%|█▊        | 912/5198 [1:28:52<118:35:27, 99.61s/it]
100%|██████████| 1/1 [01:50<00:00, 110.15s/it][A100%|██████████| 1/1 [01:50<00:00, 110.15s/it]
 18%|█▊        | 912/5198 [1:28:51<118:37:54, 99.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.00s/it][A100%|██████████| 1/1 [01:50<00:00, 110.00s/it]
 18%|█▊        | 912/5198 [1:28:53<118:35:25, 99.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.00s/it][A100%|██████████| 1/1 [01:50<00:00, 110.00s/it]
 18%|█▊        | 912/5198 [1:28:43<118:35:11, 99.61s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_855
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.96s/it][A100%|██████████| 1/1 [01:25<00:00, 85.96s/it]
 18%|█▊        | 913/5198 [1:30:29<113:30:02, 95.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:32:45,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=905, skipped=0, lr=[1.9149465191819147e-05], mom=[(0.9, 0.999)]
steps: 905 loss: 0.6270 iter time (s): 84.320 samples/sec: 1.518

100%|██████████| 1/1 [01:24<00:00, 84.87s/it][A100%|██████████| 1/1 [01:24<00:00, 84.87s/it]
 18%|█▊        | 913/5198 [1:30:15<113:20:06, 95.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.19s/it][A100%|██████████| 1/1 [01:25<00:00, 85.19s/it]
 18%|█▊        | 913/5198 [1:30:35<113:24:40, 95.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.03s/it][A100%|██████████| 1/1 [01:25<00:00, 85.03s/it]
 18%|█▊        | 913/5198 [1:30:31<113:22:22, 95.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.00s/it][A100%|██████████| 1/1 [01:25<00:00, 85.00s/it]
 18%|█▊        | 913/5198 [1:30:16<113:22:44, 95.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.03s/it][A100%|██████████| 1/1 [01:25<00:00, 85.03s/it]
 18%|█▊        | 913/5198 [1:30:17<113:21:28, 95.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.05s/it][A100%|██████████| 1/1 [01:25<00:00, 85.05s/it]
 18%|█▊        | 913/5198 [1:30:18<113:22:02, 95.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.07s/it][A100%|██████████| 1/1 [01:25<00:00, 85.07s/it]
 18%|█▊        | 913/5198 [1:30:08<113:22:12, 95.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_856
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.57s/it][A100%|██████████| 1/1 [02:10<00:00, 130.57s/it]
 18%|█▊        | 914/5198 [1:32:39<126:03:20, 105.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:34:57,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=906, skipped=0, lr=[1.9147177937838985e-05], mom=[(0.9, 0.999)]
steps: 906 loss: 0.6692 iter time (s): 131.084 samples/sec: 0.976

100%|██████████| 1/1 [02:11<00:00, 131.79s/it][A100%|██████████| 1/1 [02:11<00:00, 131.79s/it]
 18%|█▊        | 914/5198 [1:32:47<126:25:23, 106.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.99s/it][A100%|██████████| 1/1 [02:11<00:00, 131.99s/it]
 18%|█▊        | 914/5198 [1:32:27<126:26:23, 106.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.87s/it][A100%|██████████| 1/1 [02:11<00:00, 131.87s/it]
 18%|█▊        | 914/5198 [1:32:43<126:25:18, 106.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.88s/it][A100%|██████████| 1/1 [02:11<00:00, 131.88s/it]
 18%|█▊        | 914/5198 [1:32:28<126:25:51, 106.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.95s/it][A100%|██████████| 1/1 [02:11<00:00, 131.95s/it]
 18%|█▊        | 914/5198 [1:32:29<126:26:29, 106.25s/it]
100%|██████████| 1/1 [02:11<00:00, 131.92s/it][A100%|██████████| 1/1 [02:11<00:00, 131.92s/it]
 18%|█▊        | 914/5198 [1:32:29<126:26:05, 106.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.91s/it][A100%|██████████| 1/1 [02:11<00:00, 131.91s/it]
 18%|█▊        | 914/5198 [1:32:20<126:26:04, 106.25s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_857
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.78s/it][A100%|██████████| 1/1 [02:10<00:00, 130.79s/it]
 18%|█▊        | 915/5198 [1:34:50<134:54:21, 113.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:37:08,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=907, skipped=0, lr=[1.9144887749537905e-05], mom=[(0.9, 0.999)]
steps: 907 loss: 0.6536 iter time (s): 129.948 samples/sec: 0.985

100%|██████████| 1/1 [02:10<00:00, 130.81s/it][A100%|██████████| 1/1 [02:10<00:00, 130.81s/it]
 18%|█▊        | 915/5198 [1:34:58<135:10:09, 113.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.74s/it][A100%|██████████| 1/1 [02:10<00:00, 130.74s/it]
 18%|█▊        | 915/5198 [1:34:38<135:09:05, 113.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.68s/it][A100%|██████████| 1/1 [02:10<00:00, 130.68s/it]
 18%|█▊        | 915/5198 [1:34:39<135:07:31, 113.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.80s/it][A100%|██████████| 1/1 [02:10<00:00, 130.80s/it]
 18%|█▊        | 915/5198 [1:34:54<135:09:53, 113.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.67s/it][A100%|██████████| 1/1 [02:10<00:00, 130.67s/it]
 18%|█▊        | 915/5198 [1:34:40<135:07:49, 113.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.70s/it][A100%|██████████| 1/1 [02:10<00:00, 130.70s/it]
 18%|█▊        | 915/5198 [1:34:40<135:08:00, 113.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.70s/it][A100%|██████████| 1/1 [02:10<00:00, 130.70s/it]
 18%|█▊        | 915/5198 [1:34:31<135:08:02, 113.58s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_858
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.55s/it][A100%|██████████| 1/1 [01:49<00:00, 109.55s/it]
 18%|█▊        | 916/5198 [1:36:40<133:30:39, 112.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:38:57,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=908, skipped=0, lr=[1.9142594627650657e-05], mom=[(0.9, 0.999)]
steps: 908 loss: 0.6762 iter time (s): 108.247 samples/sec: 1.182

100%|██████████| 1/1 [01:48<00:00, 108.94s/it][A100%|██████████| 1/1 [01:48<00:00, 108.94s/it]
 18%|█▊        | 916/5198 [1:36:46<133:28:23, 112.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 109.00s/it][A100%|██████████| 1/1 [01:48<00:00, 109.00s/it]
 18%|█▊        | 916/5198 [1:36:27<133:28:46, 112.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:48<00:00, 108.97s/it][A100%|██████████| 1/1 [01:48<00:00, 108.97s/it]
 18%|█▊        | 916/5198 [1:36:43<133:28:53, 112.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.03s/it][A100%|██████████| 1/1 [01:49<00:00, 109.03s/it]
 18%|█▊        | 916/5198 [1:36:28<133:28:34, 112.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.07s/it][A100%|██████████| 1/1 [01:49<00:00, 109.07s/it]
 18%|█▊        | 916/5198 [1:36:29<133:29:34, 112.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.08s/it][A100%|██████████| 1/1 [01:49<00:00, 109.08s/it]
 18%|█▊        | 916/5198 [1:36:29<133:29:54, 112.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:49<00:00, 109.09s/it][A100%|██████████| 1/1 [01:49<00:00, 109.09s/it]
 18%|█▊        | 916/5198 [1:36:20<133:30:00, 112.24s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_859
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.11s/it][A100%|██████████| 1/1 [01:46<00:00, 106.11s/it]
 18%|█▊        | 917/5198 [1:38:26<131:17:53, 110.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:40:43,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=909, skipped=0, lr=[1.914029857291293e-05], mom=[(0.9, 0.999)]
steps: 909 loss: 0.6560 iter time (s): 105.192 samples/sec: 1.217

100%|██████████| 1/1 [01:45<00:00, 105.98s/it][A100%|██████████| 1/1 [01:45<00:00, 105.98s/it]
 18%|█▊        | 917/5198 [1:38:32<131:13:16, 110.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.98s/it][A100%|██████████| 1/1 [01:45<00:00, 105.98s/it]
 18%|█▊        | 917/5198 [1:38:13<131:13:23, 110.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.98s/it][A100%|██████████| 1/1 [01:45<00:00, 105.98s/it]
 18%|█▊        | 917/5198 [1:38:29<131:13:35, 110.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.97s/it][A100%|██████████| 1/1 [01:45<00:00, 105.97s/it]
 18%|█▊        | 917/5198 [1:38:14<131:13:02, 110.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.97s/it][A100%|██████████| 1/1 [01:45<00:00, 105.97s/it]
 18%|█▊        | 917/5198 [1:38:15<131:13:50, 110.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.99s/it][A100%|██████████| 1/1 [01:45<00:00, 105.99s/it]
 18%|█▊        | 917/5198 [1:38:15<131:14:35, 110.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.98s/it][A100%|██████████| 1/1 [01:45<00:00, 105.98s/it]
 18%|█▊        | 917/5198 [1:38:06<131:14:18, 110.36s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_860
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.20s/it][A100%|██████████| 1/1 [01:23<00:00, 83.21s/it]
 18%|█▊        | 918/5198 [1:39:49<121:34:12, 102.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:42:06,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.9137999586061364e-05], mom=[(0.9, 0.999)]
steps: 910 loss: 0.6143 iter time (s): 81.770 samples/sec: 1.565

100%|██████████| 1/1 [01:22<00:00, 82.59s/it][A100%|██████████| 1/1 [01:22<00:00, 82.59s/it]
 18%|█▊        | 918/5198 [1:39:55<121:17:41, 102.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.61s/it][A100%|██████████| 1/1 [01:22<00:00, 82.62s/it]
 18%|█▊        | 918/5198 [1:39:35<121:18:10, 102.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.56s/it][A100%|██████████| 1/1 [01:22<00:00, 82.56s/it]
 18%|█▊        | 918/5198 [1:39:52<121:17:12, 102.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.69s/it][A100%|██████████| 1/1 [01:22<00:00, 82.69s/it]
 18%|█▊        | 918/5198 [1:39:36<121:19:37, 102.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.62s/it][A100%|██████████| 1/1 [01:22<00:00, 82.62s/it]
 18%|█▊        | 918/5198 [1:39:37<121:18:36, 102.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.58s/it][A100%|██████████| 1/1 [01:22<00:00, 82.58s/it]
 18%|█▊        | 918/5198 [1:39:38<121:18:17, 102.03s/it]
100%|██████████| 1/1 [01:22<00:00, 82.58s/it][A100%|██████████| 1/1 [01:22<00:00, 82.58s/it]
 18%|█▊        | 918/5198 [1:39:29<121:18:02, 102.03s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_861

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:59<00:00, 119.50s/it][A100%|██████████| 1/1 [01:59<00:00, 119.50s/it]
 18%|█▊        | 919/5198 [1:41:49<127:41:50, 107.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:44:06,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=911, skipped=0, lr=[1.9135697667833517e-05], mom=[(0.9, 0.999)]
steps: 911 loss: 0.7041 iter time (s): 119.759 samples/sec: 1.069

100%|██████████| 1/1 [02:00<00:00, 120.53s/it][A100%|██████████| 1/1 [02:00<00:00, 120.53s/it]
 18%|█▊        | 919/5198 [1:41:56<127:52:00, 107.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.54s/it][A100%|██████████| 1/1 [02:00<00:00, 120.54s/it]
 18%|█▊        | 919/5198 [1:41:36<127:52:37, 107.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.58s/it][A100%|██████████| 1/1 [02:00<00:00, 120.59s/it]
 18%|█▊        | 919/5198 [1:41:52<127:53:02, 107.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.51s/it][A100%|██████████| 1/1 [02:00<00:00, 120.51s/it]
 18%|█▊        | 919/5198 [1:41:37<127:52:55, 107.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.57s/it][A100%|██████████| 1/1 [02:00<00:00, 120.57s/it]
 18%|█▊        | 919/5198 [1:41:38<127:53:38, 107.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.58s/it][A100%|██████████| 1/1 [02:00<00:00, 120.58s/it]
 18%|█▊        | 919/5198 [1:41:38<127:53:31, 107.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.59s/it][A100%|██████████| 1/1 [02:00<00:00, 120.59s/it]
 18%|█▊        | 919/5198 [1:41:29<127:53:40, 107.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_862
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.99s/it][A100%|██████████| 1/1 [01:19<00:00, 79.99s/it]
 18%|█▊        | 920/5198 [1:43:09<117:53:36, 99.21s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:45:25,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=912, skipped=0, lr=[1.913339281896791e-05], mom=[(0.9, 0.999)]
steps: 912 loss: 0.6658 iter time (s): 78.023 samples/sec: 1.641

100%|██████████| 1/1 [01:18<00:00, 78.93s/it][A100%|██████████| 1/1 [01:18<00:00, 78.93s/it]
 18%|█▊        | 920/5198 [1:43:15<117:37:41, 98.99s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.89s/it][A100%|██████████| 1/1 [01:18<00:00, 78.89s/it]
 18%|█▊        | 920/5198 [1:42:55<117:37:12, 98.98s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.89s/it][A100%|██████████| 1/1 [01:18<00:00, 78.89s/it]
 18%|█▊        | 920/5198 [1:43:11<117:37:30, 98.98s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.86s/it][A100%|██████████| 1/1 [01:18<00:00, 78.86s/it]
 18%|█▊        | 920/5198 [1:42:56<117:36:49, 98.97s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.81s/it][A100%|██████████| 1/1 [01:18<00:00, 78.81s/it]
 18%|█▊        | 920/5198 [1:42:57<117:36:11, 98.96s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.83s/it][A100%|██████████| 1/1 [01:18<00:00, 78.83s/it]
 18%|█▊        | 920/5198 [1:42:57<117:36:37, 98.97s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.82s/it][A100%|██████████| 1/1 [01:18<00:00, 78.82s/it]
 18%|█▊        | 920/5198 [1:42:48<117:36:28, 98.97s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_863
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.72s/it][A100%|██████████| 1/1 [01:35<00:00, 95.72s/it]
 18%|█▊        | 921/5198 [1:44:44<116:37:53, 98.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:47:01,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=913, skipped=0, lr=[1.9131085040203995e-05], mom=[(0.9, 0.999)]
steps: 913 loss: 0.6538 iter time (s): 95.387 samples/sec: 1.342

100%|██████████| 1/1 [01:36<00:00, 96.21s/it][A100%|██████████| 1/1 [01:36<00:00, 96.22s/it]
 18%|█▊        | 921/5198 [1:44:51<116:36:55, 98.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.17s/it][A100%|██████████| 1/1 [01:36<00:00, 96.17s/it]
 18%|█▊        | 921/5198 [1:44:31<116:35:46, 98.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.18s/it][A100%|██████████| 1/1 [01:36<00:00, 96.18s/it]
 18%|█▊        | 921/5198 [1:44:47<116:36:02, 98.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.22s/it][A100%|██████████| 1/1 [01:36<00:00, 96.23s/it]
 18%|█▊        | 921/5198 [1:44:32<116:36:32, 98.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.22s/it][A100%|██████████| 1/1 [01:36<00:00, 96.22s/it]
 18%|█▊        | 921/5198 [1:44:33<116:36:04, 98.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.26s/it][A100%|██████████| 1/1 [01:36<00:00, 96.26s/it]
 18%|█▊        | 921/5198 [1:44:34<116:37:04, 98.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.26s/it][A100%|██████████| 1/1 [01:36<00:00, 96.26s/it]
 18%|█▊        | 921/5198 [1:44:24<116:37:06, 98.16s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_864
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:45<00:00, 105.80s/it][A100%|██████████| 1/1 [01:45<00:00, 105.80s/it]
 18%|█▊        | 922/5198 [1:46:30<119:19:39, 100.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:48:47,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=914, skipped=0, lr=[1.9128774332282163e-05], mom=[(0.9, 0.999)]
steps: 914 loss: 0.6272 iter time (s): 105.233 samples/sec: 1.216

100%|██████████| 1/1 [01:46<00:00, 106.13s/it][A100%|██████████| 1/1 [01:46<00:00, 106.13s/it]
 18%|█▊        | 922/5198 [1:46:37<119:26:02, 100.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.12s/it][A100%|██████████| 1/1 [01:46<00:00, 106.12s/it]
 18%|█▊        | 922/5198 [1:46:17<119:24:58, 100.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.11s/it][A100%|██████████| 1/1 [01:46<00:00, 106.11s/it]
 18%|█▊        | 922/5198 [1:46:33<119:24:52, 100.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.04s/it][A100%|██████████| 1/1 [01:46<00:00, 106.04s/it]
 18%|█▊        | 922/5198 [1:46:18<119:23:45, 100.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.09s/it][A100%|██████████| 1/1 [01:46<00:00, 106.10s/it]
 18%|█▊        | 922/5198 [1:46:19<119:24:34, 100.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.06s/it][A100%|██████████| 1/1 [01:46<00:00, 106.06s/it]
 18%|█▊        | 922/5198 [1:46:20<119:24:37, 100.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:46<00:00, 106.07s/it][A100%|██████████| 1/1 [01:46<00:00, 106.07s/it]
 18%|█▊        | 922/5198 [1:46:10<119:24:46, 100.53s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_865
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.10s/it][A100%|██████████| 1/1 [01:39<00:00, 99.10s/it]
 18%|█▊        | 923/5198 [1:48:09<118:49:20, 100.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:50:26,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=915, skipped=0, lr=[1.9126460695943748e-05], mom=[(0.9, 0.999)]
steps: 915 loss: 0.6567 iter time (s): 98.051 samples/sec: 1.305

100%|██████████| 1/1 [01:38<00:00, 98.89s/it][A100%|██████████| 1/1 [01:38<00:00, 98.89s/it]
 18%|█▊        | 923/5198 [1:48:16<118:48:56, 100.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.97s/it][A100%|██████████| 1/1 [01:38<00:00, 98.97s/it]
 18%|█▊        | 923/5198 [1:47:56<118:50:01, 100.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.92s/it][A100%|██████████| 1/1 [01:38<00:00, 98.92s/it]
 18%|█▊        | 923/5198 [1:48:12<118:48:47, 100.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.97s/it][A100%|██████████| 1/1 [01:38<00:00, 98.97s/it]
 18%|█▊        | 923/5198 [1:47:57<118:49:04, 100.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.96s/it][A100%|██████████| 1/1 [01:38<00:00, 98.96s/it]
 18%|█▊        | 923/5198 [1:47:58<118:49:23, 100.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.94s/it][A100%|██████████| 1/1 [01:38<00:00, 98.94s/it]
 18%|█▊        | 923/5198 [1:47:59<118:49:02, 100.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.26s/it][A100%|██████████| 1/1 [01:39<00:00, 99.26s/it]
 18%|█▊        | 923/5198 [1:47:50<118:56:02, 100.15s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_866
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.35s/it][A100%|██████████| 1/1 [01:43<00:00, 103.35s/it]
 18%|█▊        | 924/5198 [1:49:53<119:58:19, 101.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-08-18 02:52:10,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=916, skipped=0, lr=[1.912414413193102e-05], mom=[(0.9, 0.999)]
steps: 916 loss: 0.6770 iter time (s): 102.276 samples/sec: 1.252

100%|██████████| 1/1 [01:43<00:00, 103.41s/it][A100%|██████████| 1/1 [01:43<00:00, 103.41s/it]
 18%|█▊        | 924/5198 [1:49:59<119:59:11, 101.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.29s/it][A100%|██████████| 1/1 [01:43<00:00, 103.29s/it]
 18%|█▊        | 924/5198 [1:49:39<119:57:19, 101.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.43s/it][A100%|██████████| 1/1 [01:43<00:00, 103.43s/it]
 18%|█▊        | 924/5198 [1:49:56<119:59:40, 101.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.35s/it][A100%|██████████| 1/1 [01:43<00:00, 103.36s/it]
 18%|█▊        | 924/5198 [1:49:42<119:58:21, 101.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.48s/it][A100%|██████████| 1/1 [01:43<00:00, 103.48s/it]
 18%|█▊        | 924/5198 [1:49:40<120:00:37, 101.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:43<00:00, 103.40s/it][A100%|██████████| 1/1 [01:43<00:00, 103.40s/it]
 18%|█▊        | 924/5198 [1:49:42<119:59:07, 101.06s/it]
100%|██████████| 1/1 [01:43<00:00, 103.06s/it][A100%|██████████| 1/1 [01:43<00:00, 103.06s/it]
 18%|█▊        | 924/5198 [1:49:33<119:56:42, 101.03s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_867

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A