[2024-07-02 12:02:19,010] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:31,720] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-02 12:02:31,720] [INFO] [runner.py:568:main] cmd = /home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train_parallel_deepspeed_mixtral_adapter.py --num_stages=8 --shared_adapter_num=1 --shared_adapter_type=LoRA --lora_r=32 --lora_alpha=64 --save_model_shard=20 --skip_shard=2900 --checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint
[2024-07-02 12:02:34,663] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:36,056] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2024-07-02 12:02:36,056] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2024-07-02 12:02:36,056] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2024-07-02 12:02:36,056] [INFO] [launch.py:163:main] dist_world_size=8
[2024-07-02 12:02:36,056] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2024-07-02 12:02:36,068] [INFO] [launch.py:253:main] process 3953877 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=0', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:36,077] [INFO] [launch.py:253:main] process 3953878 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=1', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:36,087] [INFO] [launch.py:253:main] process 3953879 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=2', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:36,104] [INFO] [launch.py:253:main] process 3953880 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=3', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:36,121] [INFO] [launch.py:253:main] process 3953881 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=4', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:36,137] [INFO] [launch.py:253:main] process 3953883 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=5', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:36,153] [INFO] [launch.py:253:main] process 3953884 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=6', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:36,163] [INFO] [launch.py:253:main] process 3953885 spawned with command: ['/home/hpc/b207dd/b207dd11/miniconda3/envs/py38/bin/python', '-u', 'llava/train_parallel_deepspeed_mixtral_adapter.py', '--local_rank=7', '--num_stages=8', '--shared_adapter_num=1', '--shared_adapter_type=LoRA', '--lora_r=32', '--lora_alpha=64', '--save_model_shard=20', '--skip_shard=2900', '--checkpoint_dir=/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint']
[2024-07-02 12:02:50,088] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:51,358] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 12:02:52,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:52,470] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:52,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:52,638] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:52,742] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 12:02:53,107] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 12:02:53,231] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 12:02:53,309] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 12:02:57,061] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:57,100] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:02:57,688] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 12:02:57,721] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-02 12:02:57,721] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s][2024-07-02 12:03:11,013] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-02 12:03:12,143] [INFO] [comm.py:637:init_distributed] cdb=None
Loading checkpoint shards:   5%|▌         | 1/20 [00:09<03:08,  9.92s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:10<03:12, 10.11s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:10<03:11, 10.06s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:10<03:15, 10.27s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:10<03:17, 10.40s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:10<03:25, 10.83s/it]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/20 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/20 [00:02<00:45,  2.37s/it]Loading checkpoint shards:   5%|▌         | 1/20 [00:02<00:54,  2.85s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:17<02:37,  8.76s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:18<02:40,  8.94s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:18<02:39,  8.85s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:18<02:39,  8.88s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:06<00:59,  3.29s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:18<02:40,  8.92s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:18<02:41,  8.95s/it]Loading checkpoint shards:  10%|█         | 2/20 [00:06<01:01,  3.43s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:13<01:27,  5.16s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:25<02:20,  8.24s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:25<02:20,  8.26s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:25<02:19,  8.24s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:25<02:20,  8.24s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:14<01:29,  5.25s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:25<02:20,  8.27s/it]Loading checkpoint shards:  15%|█▌        | 3/20 [00:25<02:20,  8.29s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:33<02:06,  7.91s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:21<01:38,  6.13s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:21<01:38,  6.14s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:33<02:08,  8.01s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:33<02:07,  7.99s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:33<02:07,  7.99s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:33<02:07,  7.99s/it]Loading checkpoint shards:  20%|██        | 4/20 [00:33<02:08,  8.00s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:28<01:37,  6.52s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:40<01:55,  7.70s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:40<01:56,  7.74s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:40<01:55,  7.73s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:40<01:56,  7.77s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:28<01:38,  6.59s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:40<01:56,  7.79s/it]Loading checkpoint shards:  25%|██▌       | 5/20 [00:40<01:56,  7.77s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:47<01:46,  7.59s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:36<01:35,  6.85s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:36<01:36,  6.89s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:48<01:47,  7.66s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:48<01:46,  7.62s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:47<01:47,  7.65s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:48<01:47,  7.65s/it]Loading checkpoint shards:  30%|███       | 6/20 [00:48<01:47,  7.66s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:56<01:40,  7.74s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:56<01:40,  7.72s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:44<01:34,  7.25s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:56<01:40,  7.75s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:55<01:41,  7.77s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:55<01:40,  7.76s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:44<01:34,  7.27s/it]Loading checkpoint shards:  35%|███▌      | 7/20 [00:56<01:41,  7.79s/it]Loading checkpoint shards:  40%|████      | 8/20 [01:02<01:30,  7.52s/it]Loading checkpoint shards:  40%|████      | 8/20 [01:02<01:30,  7.52s/it]Loading checkpoint shards:  40%|████      | 8/20 [01:03<01:30,  7.51s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:51<01:26,  7.20s/it]Loading checkpoint shards:  40%|████      | 8/20 [00:51<01:26,  7.18s/it]Loading checkpoint shards:  40%|████      | 8/20 [01:03<01:30,  7.55s/it]Loading checkpoint shards:  40%|████      | 8/20 [01:03<01:30,  7.55s/it]Loading checkpoint shards:  40%|████      | 8/20 [01:03<01:31,  7.60s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [01:09<01:19,  7.25s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:57<01:17,  7.01s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [01:09<01:20,  7.28s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [01:09<01:20,  7.28s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [01:09<01:20,  7.29s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [01:09<01:20,  7.28s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [00:58<01:17,  7.08s/it]Loading checkpoint shards:  45%|████▌     | 9/20 [01:10<01:20,  7.29s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:06<01:14,  7.46s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:18<01:16,  7.64s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:06<01:14,  7.48s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:18<01:16,  7.66s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:18<01:16,  7.66s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:18<01:16,  7.68s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:18<01:16,  7.65s/it]Loading checkpoint shards:  50%|█████     | 10/20 [01:18<01:16,  7.69s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:25<01:07,  7.49s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:25<01:07,  7.52s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:13<01:06,  7.43s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:25<01:08,  7.56s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:25<01:07,  7.55s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:25<01:07,  7.54s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:13<01:07,  7.46s/it]Loading checkpoint shards:  55%|█████▌    | 11/20 [01:25<01:08,  7.58s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:31<00:56,  7.07s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:31<00:56,  7.09s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:31<00:56,  7.08s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:20<00:56,  7.02s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:31<00:56,  7.12s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:32<00:57,  7.24s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:20<00:58,  7.32s/it]Loading checkpoint shards:  60%|██████    | 12/20 [01:32<00:59,  7.47s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:21<00:38,  5.47s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:33<00:38,  5.49s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:33<00:38,  5.54s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:33<00:38,  5.51s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:33<00:39,  5.59s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:33<00:39,  5.63s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:22<00:39,  5.65s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:23<00:25,  4.28s/it]Loading checkpoint shards:  65%|██████▌   | 13/20 [01:35<00:41,  5.89s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:35<00:26,  4.34s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:35<00:26,  4.37s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:35<00:26,  4.47s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:35<00:27,  4.52s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:35<00:27,  4.56s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:24<00:27,  4.60s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:25<00:18,  3.63s/it]Loading checkpoint shards:  70%|███████   | 14/20 [01:37<00:28,  4.73s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:37<00:18,  3.62s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:37<00:18,  3.61s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:37<00:18,  3.63s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:37<00:18,  3.68s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:38<00:20,  4.01s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:27<00:19,  3.93s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:27<00:12,  3.10s/it]Loading checkpoint shards:  75%|███████▌  | 15/20 [01:39<00:19,  3.93s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:39<00:12,  3.16s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:39<00:12,  3.16s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:39<00:12,  3.19s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:40<00:13,  3.36s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:41<00:14,  3.52s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:29<00:13,  3.40s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:29<00:08,  2.81s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:42<00:08,  2.99s/it]Loading checkpoint shards:  80%|████████  | 16/20 [01:41<00:14,  3.55s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:41<00:09,  3.05s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:42<00:09,  3.12s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:42<00:09,  3.20s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:43<00:09,  3.21s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [01:32<00:05,  2.76s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:32<00:10,  3.39s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [01:44<00:05,  2.89s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [01:44<00:05,  2.98s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [01:44<00:05,  2.97s/it]Loading checkpoint shards:  85%|████████▌ | 17/20 [01:45<00:10,  3.40s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [01:45<00:05,  2.80s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [01:33<00:02,  2.44s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [01:45<00:06,  3.16s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:35<00:00,  2.08s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:35<00:00,  4.76s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
Loading checkpoint shards:  95%|█████████▌| 19/20 [01:47<00:02,  2.78s/it]Loading checkpoint shards:  90%|█████████ | 18/20 [01:35<00:06,  3.16s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [01:47<00:02,  2.77s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [01:47<00:02,  2.77s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [01:47<00:02,  2.57s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:47<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:47<00:00,  5.40s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
Loading checkpoint shards: 100%|██████████| 20/20 [01:47<00:00,  2.11s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:47<00:00,  5.39s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
Loading checkpoint shards: 100%|██████████| 20/20 [01:47<00:00,  2.15s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:47<00:00,  5.39s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:  90%|█████████ | 18/20 [01:48<00:06,  3.27s/it]Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
Loading checkpoint shards: 100%|██████████| 20/20 [01:48<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:48<00:00,  5.41s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
Loading checkpoint shards:  95%|█████████▌| 19/20 [01:48<00:02,  2.95s/it]Loading checkpoint shards:  95%|█████████▌| 19/20 [01:36<00:02,  2.69s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:48<00:00,  2.18s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:48<00:00,  5.44s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
Loading checkpoint shards: 100%|██████████| 20/20 [01:37<00:00,  2.01s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:37<00:00,  4.87s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:  95%|█████████▌| 19/20 [01:49<00:02,  2.66s/it]Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=1, data=0): 1, ProcessCoord(pipe=2, data=0): 2, ProcessCoord(pipe=3, data=0): 3, ProcessCoord(pipe=4, data=0): 4, ProcessCoord(pipe=5, data=0): 5, ProcessCoord(pipe=6, data=0): 6, ProcessCoord(pipe=7, data=0): 7}
[2024-07-02 12:04:55,819] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:LanguageModelLayerWrapper
stage=0 layers=2
     0: LlavaMultiModalModuleWrapper
     1: LanguageModelLayerWrapper
stage=1 layers=1
     2: LanguageModelLayerWrapper
stage=2 layers=1
     3: LanguageModelLayerWrapper
stage=3 layers=1
     4: LanguageModelLayerWrapper
stage=4 layers=1
     5: LanguageModelLayerWrapper
stage=5 layers=1
     6: LanguageModelLayerWrapper
stage=6 layers=1
     7: LanguageModelLayerWrapper
stage=7 layers=2
     8: LanguageModelLayerWrapper
     9: LanguageModelFinalWrapper
  loss: loss_fn
Loading checkpoint shards: 100%|██████████| 20/20 [01:49<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 20/20 [01:49<00:00,  5.47s/it]
Some weights of LlavaForConditionalGeneration were not initialized from the model checkpoint at /home/vault/b207dd/b207dd11/llava-mixtral/llava-mixtral-pretrained-2/ and are newly initialized: ['language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.0.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.1.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.10.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.11.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.12.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.13.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.14.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.15.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.16.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.17.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.18.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.19.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.2.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.20.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.21.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.22.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.23.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.24.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.25.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.26.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.27.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.28.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.29.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.3.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.30.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.31.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.4.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.5.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.6.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.7.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.8.block_sparse_moe.shared_adapter.0.unit.lora_B.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_A.weight', 'language_model.model.layers.9.block_sparse_moe.shared_adapter.0.unit.lora_B.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Convert trainable params: 8388608 || all params: 47035684864 || trainable%: 0.02
Print trainable params: 29368320 || all params: 47035684864 || trainable%: 0.06
Rank 3 initialized with CUDA_MEM (60893691904, 85100068864)
Deepspeed engine initializing at --- RANK 3 --- ...
Rank 6 initialized with CUDA_MEM (60893691904, 85100068864)
Deepspeed engine initializing at --- RANK 6 --- ...
Rank 7 initialized with CUDA_MEM (60367306752, 85100068864)
Deepspeed engine initializing at --- RANK 7 --- ...
Rank 5 initialized with CUDA_MEM (60893691904, 85100068864)
Deepspeed engine initializing at --- RANK 5 --- ...
Rank 4 initialized with CUDA_MEM (60893691904, 85100068864)
Deepspeed engine initializing at --- RANK 4 --- ...
Rank 2 initialized with CUDA_MEM (60893691904, 85100068864)
Deepspeed engine initializing at --- RANK 2 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 1 initialized with CUDA_MEM (60893691904, 85100068864)
Deepspeed engine initializing at --- RANK 1 --- ...
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Rank 0 initialized with CUDA_MEM (59054489600, 85100068864)
Deepspeed engine initializing at --- RANK 0 --- ...
[2024-07-02 12:05:04,099] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
ninja: no work to do.
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 2.731959104537964 seconds
Time to load fused_adam op: 3.108731985092163 seconds
Time to load fused_adam op: 2.7258617877960205 seconds
[2024-07-02 12:05:04,308] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-07-02 12:05:04,309] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-07-02 12:05:04,310] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 1.5060105323791504 seconds
[2024-07-02 12:05:04,316] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Time to load fused_adam op: 2.107499361038208 seconds
[2024-07-02 12:05:04,337] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
Loading extension module fused_adam...
Loading extension module fused_adam...
Time to load fused_adam op: 3.1631195545196533 seconds
Time to load fused_adam op: 3.162412405014038 seconds
[2024-07-02 12:05:04,359] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-07-02 12:05:04,359] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-07-02 12:05:04,376] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Using /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/hpc/b207dd/b207dd11/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 4.789077997207642 seconds
[2024-07-02 12:05:10,209] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-07-02 12:05:10,209] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-02 12:05:10,218] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-07-02 12:05:10,218] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam
[2024-07-02 12:05:10,218] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupCosineLR
[2024-07-02 12:05:10,218] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupCosineLR object at 0x150e918425b0>
[2024-07-02 12:05:10,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-07-02 12:05:10,220] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-07-02 12:05:10,220] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-02 12:05:10,220] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-02 12:05:10,220] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-07-02 12:05:10,220] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x150e91bc8340>
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   fp16_enabled ................. False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 128
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0
[2024-07-02 12:05:10,221] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=True, output_path='/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint', job_name='deepspeed_monitor_logs') enabled=True
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   optimizer_name ............... adam
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 2e-05}
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True, 'use_reentrant': False}
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupCosineLR
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   scheduler_params ............. {'total_num_steps': 5718, 'warmup_min_ratio': 0.1, 'warmup_num_steps': 171.54}
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   steps_per_print .............. 1
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   train_batch_size ............. 128
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   world_size ................... 1
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   zero_enabled ................. False
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-02 12:05:10,222] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 0
[2024-07-02 12:05:10,222] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 128, 
    "steps_per_print": 1, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 2e-05
        }
    }, 
    "scheduler": {
        "type": "WarmupCosineLR", 
        "params": {
            "total_num_steps": 5.718000e+03, 
            "warmup_min_ratio": 0.1, 
            "warmup_num_steps": 171.54
        }
    }, 
    "pipeline": {
        "use_reentrant": false
    }, 
    "csv_monitor": {
        "enabled": true, 
        "output_path": "/home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint", 
        "job_name": "deepspeed_monitor_logs"
    }
}
[2024-07-02 12:05:10,222] [INFO] [engine.py:101:__init__] CONFIG: micro_batches=128 micro_batch_size=1
[2024-07-02 12:05:10,222] [INFO] [engine.py:141:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2024-07-02 12:05:14,682] [INFO] [engine.py:160:__init__] RANK=0 STAGE=0 LAYERS=2 [0, 2) STAGE_PARAMS=22028288 (22.028M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
[2024-07-02 12:05:14,682] [INFO] [engine.py:160:__init__] RANK=4 STAGE=4 LAYERS=1 [5, 6) STAGE_PARAMS=1048576 (1.049M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
[2024-07-02 12:05:14,682] [INFO] [engine.py:160:__init__] RANK=3 STAGE=3 LAYERS=1 [4, 5) STAGE_PARAMS=1048576 (1.049M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
[2024-07-02 12:05:14,682] [INFO] [engine.py:160:__init__] RANK=6 STAGE=6 LAYERS=1 [7, 8) STAGE_PARAMS=1048576 (1.049M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
[2024-07-02 12:05:14,682] [INFO] [engine.py:160:__init__] RANK=5 STAGE=5 LAYERS=1 [6, 7) STAGE_PARAMS=1048576 (1.049M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
[2024-07-02 12:05:14,682] [INFO] [engine.py:160:__init__] RANK=7 STAGE=7 LAYERS=2 [8, 10) STAGE_PARAMS=1048576 (1.049M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
[2024-07-02 12:05:14,682] [INFO] [engine.py:160:__init__] RANK=2 STAGE=2 LAYERS=1 [3, 4) STAGE_PARAMS=1048576 (1.049M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
[2024-07-02 12:05:14,684] [INFO] [engine.py:160:__init__] RANK=1 STAGE=1 LAYERS=1 [2, 3) STAGE_PARAMS=1048576 (1.049M) TOTAL_PARAMS=29368320 (29.368M) UNIQUE_PARAMS=29368320 (29.368M)
Deepspeed engine successfully initialized at --- RANK 0 --- hosting 12 of 68 trainable parameters
Loading latest model checkpoint at shard 2900
[2024-07-02 12:05:17,223] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:17,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:17,287] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:17,345] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:17,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_00-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 7 --- hosting 8 of 68 trainable parameters
[2024-07-02 12:05:17,468] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:17,531] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:17,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_07_model_states.pt...
[2024-07-02 12:05:17,535] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_07_model_states.pt.
[2024-07-02 12:05:17,536] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_08-model_states.pt...
[2024-07-02 12:05:17,776] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_00-model_states.pt.
[2024-07-02 12:05:17,777] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_00-model_states.pt...
[2024-07-02 12:05:18,132] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_00-model_states.pt.
[2024-07-02 12:05:18,298] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_01-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 2 --- hosting 8 of 68 trainable parameters
Deepspeed engine successfully initialized at --- RANK 1 --- hosting 8 of 68 trainable parameters
[2024-07-02 12:05:19,257] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:19,258] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
Deepspeed engine successfully initialized at --- RANK 3 --- hosting 8 of 68 trainable parameters
Deepspeed engine successfully initialized at --- RANK 4 --- hosting 8 of 68 trainable parameters
[2024-07-02 12:05:19,273] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:19,273] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:19,304] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:19,304] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_02_model_states.pt...
[2024-07-02 12:05:19,308] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_02_model_states.pt.
[2024-07-02 12:05:19,309] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_03-model_states.pt...
[2024-07-02 12:05:19,320] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:19,321] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_01_model_states.pt...
[2024-07-02 12:05:19,330] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_01_model_states.pt.
[2024-07-02 12:05:19,331] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_02-model_states.pt...
[2024-07-02 12:05:19,334] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:19,334] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_04_model_states.pt...
[2024-07-02 12:05:19,338] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_04_model_states.pt.
[2024-07-02 12:05:19,339] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:19,339] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_05-model_states.pt...
[2024-07-02 12:05:19,339] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_03_model_states.pt...
[2024-07-02 12:05:19,343] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_03_model_states.pt.
[2024-07-02 12:05:19,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_04-model_states.pt...
Deepspeed engine successfully initialized at --- RANK 5 --- hosting 8 of 68 trainable parameters
Deepspeed engine successfully initialized at --- RANK 6 --- hosting 8 of 68 trainable parameters
[2024-07-02 12:05:19,367] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:19,367] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt...
[2024-07-02 12:05:19,409] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:19,409] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_06_model_states.pt...
[2024-07-02 12:05:19,417] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_06_model_states.pt.
[2024-07-02 12:05:19,418] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_07-model_states.pt...
[2024-07-02 12:05:19,434] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_00_model_states.pt.
[2024-07-02 12:05:19,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_05_model_states.pt...
[2024-07-02 12:05:19,441] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/mp_rank_05_model_states.pt.
[2024-07-02 12:05:19,441] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_06-model_states.pt...
[2024-07-02 12:05:21,851] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_08-model_states.pt.
[2024-07-02 12:05:21,994] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_08-model_states.pt...
[2024-07-02 12:05:24,132] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_03-model_states.pt.
[2024-07-02 12:05:24,201] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_02-model_states.pt.
[2024-07-02 12:05:24,251] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_02-model_states.pt...
[2024-07-02 12:05:24,325] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_03-model_states.pt...
[2024-07-02 12:05:24,410] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_01-model_states.pt.
[2024-07-02 12:05:24,605] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_01-model_states.pt...
[2024-07-02 12:05:24,972] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_04-model_states.pt.
[2024-07-02 12:05:25,142] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_05-model_states.pt.
[2024-07-02 12:05:25,210] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_04-model_states.pt...
[2024-07-02 12:05:25,428] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_05-model_states.pt...
[2024-07-02 12:05:26,077] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_06-model_states.pt.
[2024-07-02 12:05:26,317] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_07-model_states.pt.
[2024-07-02 12:05:26,530] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_06-model_states.pt...
[2024-07-02 12:05:26,670] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_07-model_states.pt...
[2024-07-02 12:05:26,893] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_08-model_states.pt.
[2024-07-02 12:05:28,759] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_09-model_states.pt...
[2024-07-02 12:05:28,931] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_09-model_states.pt.
[2024-07-02 12:05:28,946] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_09-model_states.pt...
[2024-07-02 12:05:29,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_09-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:05:30,127] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_02-model_states.pt.
[2024-07-02 12:05:30,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_03-model_states.pt.
[2024-07-02 12:05:30,243] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_05-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:05:32,466] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_04-model_states.pt.
[2024-07-02 12:05:32,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_01-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:05:34,315] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_06-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/5198 [00:00<?, ?it/s]Shard 0 / 2900 skipped
Shard 1 / 2900 skipped
Shard 2 / 2900 skipped
Shard 3 / 2900 skipped
Shard 4 / 2900 skipped
Shard 5 / 2900 skipped
Shard 6 / 2900 skipped
Shard 7 / 2900 skipped
Shard 8 / 2900 skipped
Shard 9 / 2900 skipped
Shard 10 / 2900 skipped
Shard 11 / 2900 skipped
Shard 12 / 2900 skipped
Shard 13 / 2900 skipped
Shard 14 / 2900 skipped
Shard 15 / 2900 skipped
Shard 16 / 2900 skipped
Shard 17 / 2900 skipped
Shard 18 / 2900 skipped
Shard 19 / 2900 skipped
Shard 20 / 2900 skipped
Shard 21 / 2900 skipped
Shard 22 / 2900 skipped
Shard 23 / 2900 skipped
Shard 24 / 2900 skipped
Shard 25 / 2900 skipped
Shard 26 / 2900 skipped
Shard 27 / 2900 skipped
Shard 28 / 2900 skipped
Shard 29 / 2900 skipped
Shard 30 / 2900 skipped
Shard 31 / 2900 skipped
Shard 32 / 2900 skipped
Shard 33 / 2900 skipped
Shard 34 / 2900 skipped
Shard 35 / 2900 skipped
Shard 36 / 2900 skipped
Shard 37 / 2900 skipped
Shard 38 / 2900 skipped
Shard 39 / 2900 skipped
Shard 40 / 2900 skipped
Shard 41 / 2900 skipped
Shard 42 / 2900 skipped
Shard 43 / 2900 skipped
Shard 44 / 2900 skipped
Shard 45 / 2900 skipped
Shard 46 / 2900 skipped
Shard 47 / 2900 skipped
Shard 48 / 2900 skipped
Shard 49 / 2900 skipped
Shard 50 / 2900 skipped
Shard 51 / 2900 skipped
Shard 52 / 2900 skipped
Shard 53 / 2900 skipped
Shard 54 / 2900 skipped
Shard 55 / 2900 skipped
Shard 56 / 2900 skipped
Shard 57 / 2900 skipped
Shard 58 / 2900 skipped
Shard 59 / 2900 skipped
Shard 60 / 2900 skipped
Shard 61 / 2900 skipped
Shard 62 / 2900 skipped
Shard 63 / 2900 skipped
Shard 64 / 2900 skipped
Shard 65 / 2900 skipped
Shard 66 / 2900 skipped
Shard 67 / 2900 skipped
Shard 68 / 2900 skipped
Shard 69 / 2900 skipped
Shard 70 / 2900 skipped
Shard 71 / 2900 skipped
Shard 72 / 2900 skipped
Shard 73 / 2900 skipped
Shard 74 / 2900 skipped
Shard 75 / 2900 skipped
Shard 76 / 2900 skipped
Shard 77 / 2900 skipped
Shard 78 / 2900 skipped
Shard 79 / 2900 skipped
Shard 80 / 2900 skipped
Shard 81 / 2900 skipped
Shard 82 / 2900 skipped
Shard 83 / 2900 skipped
Shard 84 / 2900 skipped
Shard 85 / 2900 skipped
Shard 86 / 2900 skipped
Shard 87 / 2900 skipped
Shard 88 / 2900 skipped
Shard 89 / 2900 skipped
Shard 90 / 2900 skipped
Shard 91 / 2900 skipped
Shard 92 / 2900 skipped
Shard 93 / 2900 skipped
Shard 94 / 2900 skipped
Shard 95 / 2900 skipped
Shard 96 / 2900 skipped
Shard 97 / 2900 skipped
Shard 98 / 2900 skipped
Shard 99 / 2900 skipped
Shard 100 / 2900 skipped
Shard 101 / 2900 skipped
Shard 102 / 2900 skipped
Shard 103 / 2900 skipped
Shard 104 / 2900 skipped
Shard 105 / 2900 skipped
Shard 106 / 2900 skipped
Shard 107 / 2900 skipped
Shard 108 / 2900 skipped
Shard 109 / 2900 skipped
Shard 110 / 2900 skipped
Shard 111 / 2900 skipped
Shard 112 / 2900 skipped
Shard 113 / 2900 skipped
Shard 114 / 2900 skipped
Shard 115 / 2900 skipped
Shard 116 / 2900 skipped
Shard 117 / 2900 skipped
Shard 118 / 2900 skipped
Shard 119 / 2900 skipped
Shard 120 / 2900 skipped
Shard 121 / 2900 skipped
Shard 122 / 2900 skipped
Shard 123 / 2900 skipped
Shard 124 / 2900 skipped
Shard 125 / 2900 skipped
Shard 126 / 2900 skipped
Shard 127 / 2900 skipped
Shard 128 / 2900 skipped
Shard 129 / 2900 skipped
Shard 130 / 2900 skipped
Shard 131 / 2900 skipped
Shard 132 / 2900 skipped
Shard 133 / 2900 skipped
Shard 134 / 2900 skipped
Shard 135 / 2900 skipped
Shard 136 / 2900 skipped
Shard 137 / 2900 skipped
Shard 138 / 2900 skipped
Shard 139 / 2900 skipped
Shard 140 / 2900 skipped
Shard 141 / 2900 skipped
Shard 142 / 2900 skipped
Shard 143 / 2900 skipped
Shard 144 / 2900 skipped
Shard 145 / 2900 skipped
Shard 146 / 2900 skipped
Shard 147 / 2900 skipped
Shard 148 / 2900 skipped
Shard 149 / 2900 skipped
Shard 150 / 2900 skipped
Shard 151 / 2900 skipped
Shard 152 / 2900 skipped
Shard 153 / 2900 skipped
Shard 154 / 2900 skipped
Shard 155 / 2900 skipped
Shard 156 / 2900 skipped
Shard 157 / 2900 skipped
Shard 158 / 2900 skipped
Shard 159 / 2900 skipped
Shard 160 / 2900 skipped
Shard 161 / 2900 skipped
Shard 162 / 2900 skipped
Shard 163 / 2900 skipped
Shard 164 / 2900 skipped
Shard 165 / 2900 skipped
Shard 166 / 2900 skipped
Shard 167 / 2900 skipped
Shard 168 / 2900 skipped
Shard 169 / 2900 skipped
Shard 170 / 2900 skipped
Shard 171 / 2900 skipped
Shard 172 / 2900 skipped
Shard 173 / 2900 skipped
Shard 174 / 2900 skipped
Shard 175 / 2900 skipped
Shard 176 / 2900 skipped
Shard 177 / 2900 skipped
Shard 178 / 2900 skipped
Shard 179 / 2900 skipped
Shard 180 / 2900 skipped
Shard 181 / 2900 skipped
Shard 182 / 2900 skipped
Shard 183 / 2900 skipped
Shard 184 / 2900 skipped
Shard 185 / 2900 skipped
Shard 186 / 2900 skipped
Shard 187 / 2900 skipped
Shard 188 / 2900 skipped
Shard 189 / 2900 skipped
Shard 190 / 2900 skipped
Shard 191 / 2900 skipped
Shard 192 / 2900 skipped
Shard 193 / 2900 skipped
Shard 194 / 2900 skipped
Shard 195 / 2900 skipped
Shard 196 / 2900 skipped
Shard 197 / 2900 skipped
Shard 198 / 2900 skipped
Shard 199 / 2900 skipped
Shard 200 / 2900 skipped
Shard 201 / 2900 skipped
Shard 202 / 2900 skipped
Shard 203 / 2900 skipped
Shard 204 / 2900 skipped
Shard 205 / 2900 skipped
Shard 206 / 2900 skipped
Shard 207 / 2900 skipped
Shard 208 / 2900 skipped
Shard 209 / 2900 skipped
Shard 210 / 2900 skipped
Shard 211 / 2900 skipped
Shard 212 / 2900 skipped
Shard 213 / 2900 skipped
Shard 214 / 2900 skipped
Shard 215 / 2900 skipped
Shard 216 / 2900 skipped
Shard 217 / 2900 skipped
Shard 218 / 2900 skipped
Shard 219 / 2900 skipped
Shard 220 / 2900 skipped
Shard 221 / 2900 skipped
Shard 222 / 2900 skipped
Shard 223 / 2900 skipped
Shard 224 / 2900 skipped
Shard 225 / 2900 skipped
Shard 226 / 2900 skipped
Shard 227 / 2900 skipped
Shard 228 / 2900 skipped
Shard 229 / 2900 skipped
Shard 230 / 2900 skipped
Shard 231 / 2900 skipped
Shard 232 / 2900 skipped
Shard 233 / 2900 skipped
Shard 234 / 2900 skipped
Shard 235 / 2900 skipped
Shard 236 / 2900 skipped
Shard 237 / 2900 skipped
Shard 238 / 2900 skipped
Shard 239 / 2900 skipped
Shard 240 / 2900 skipped
Shard 241 / 2900 skipped
Shard 242 / 2900 skipped
Shard 243 / 2900 skipped
Shard 244 / 2900 skipped
Shard 245 / 2900 skipped
Shard 246 / 2900 skipped
Shard 247 / 2900 skipped
Shard 248 / 2900 skipped
Shard 249 / 2900 skipped
Shard 250 / 2900 skipped
Shard 251 / 2900 skipped
Shard 252 / 2900 skipped
Shard 253 / 2900 skipped
Shard 254 / 2900 skipped
Shard 255 / 2900 skipped
Shard 256 / 2900 skipped
Shard 257 / 2900 skipped
Shard 258 / 2900 skipped
Shard 259 / 2900 skipped
Shard 260 / 2900 skipped
Shard 261 / 2900 skipped
Shard 262 / 2900 skipped
Shard 263 / 2900 skipped
Shard 264 / 2900 skipped
Shard 265 / 2900 skipped
Shard 266 / 2900 skipped
Shard 267 / 2900 skipped
Shard 268 / 2900 skipped
Shard 269 / 2900 skipped
Shard 270 / 2900 skipped
Shard 271 / 2900 skipped
Shard 272 / 2900 skipped
Shard 273 / 2900 skipped
Shard 274 / 2900 skipped
Shard 275 / 2900 skipped
Shard 276 / 2900 skipped
Shard 277 / 2900 skipped
Shard 278 / 2900 skipped
Shard 279 / 2900 skipped
Shard 280 / 2900 skipped
Shard 281 / 2900 skipped
Shard 282 / 2900 skipped
Shard 283 / 2900 skipped
Shard 284 / 2900 skipped
Shard 285 / 2900 skipped
Shard 286 / 2900 skipped
Shard 287 / 2900 skipped
Shard 288 / 2900 skipped
Shard 289 / 2900 skipped
Shard 290 / 2900 skipped
Shard 291 / 2900 skipped
Shard 292 / 2900 skipped
Shard 293 / 2900 skipped
Shard 294 / 2900 skipped
Shard 295 / 2900 skipped
Shard 296 / 2900 skipped
Shard 297 / 2900 skipped
Shard 298 / 2900 skipped
Shard 299 / 2900 skipped
Shard 300 / 2900 skipped
Shard 301 / 2900 skipped
Shard 302 / 2900 skipped
Shard 303 / 2900 skipped
Shard 304 / 2900 skipped
Shard 305 / 2900 skipped
Shard 306 / 2900 skipped
Shard 307 / 2900 skipped
Shard 308 / 2900 skipped
Shard 309 / 2900 skipped
Shard 310 / 2900 skipped
Shard 311 / 2900 skipped
Shard 312 / 2900 skipped
Shard 313 / 2900 skipped
Shard 314 / 2900 skipped
Shard 315 / 2900 skipped
Shard 316 / 2900 skipped
Shard 317 / 2900 skipped
Shard 318 / 2900 skipped
Shard 319 / 2900 skipped
Shard 320 / 2900 skipped
Shard 321 / 2900 skipped
Shard 322 / 2900 skipped
Shard 323 / 2900 skipped
Shard 324 / 2900 skipped
Shard 325 / 2900 skipped
Shard 326 / 2900 skipped
Shard 327 / 2900 skipped
Shard 328 / 2900 skipped
Shard 329 / 2900 skipped
Shard 330 / 2900 skipped
Shard 331 / 2900 skipped
Shard 332 / 2900 skipped
Shard 333 / 2900 skipped
Shard 334 / 2900 skipped
Shard 335 / 2900 skipped
Shard 336 / 2900 skipped
Shard 337 / 2900 skipped
Shard 338 / 2900 skipped
Shard 339 / 2900 skipped
Shard 340 / 2900 skipped
Shard 341 / 2900 skipped
Shard 342 / 2900 skipped
Shard 343 / 2900 skipped
Shard 344 / 2900 skipped
Shard 345 / 2900 skipped
Shard 346 / 2900 skipped
Shard 347 / 2900 skipped
Shard 348 / 2900 skipped
Shard 349 / 2900 skipped
Shard 350 / 2900 skipped
Shard 351 / 2900 skipped
Shard 352 / 2900 skipped
Shard 353 / 2900 skipped
Shard 354 / 2900 skipped
Shard 355 / 2900 skipped
Shard 356 / 2900 skipped
Shard 357 / 2900 skipped
Shard 358 / 2900 skipped
Shard 359 / 2900 skipped
Shard 360 / 2900 skipped
Shard 361 / 2900 skipped
Shard 362 / 2900 skipped
Shard 363 / 2900 skipped
Shard 364 / 2900 skipped
Shard 365 / 2900 skipped
Shard 366 / 2900 skipped
Shard 367 / 2900 skipped
Shard 368 / 2900 skipped
Shard 369 / 2900 skipped
Shard 370 / 2900 skipped
Shard 371 / 2900 skipped
Shard 372 / 2900 skipped
Shard 373 / 2900 skipped
Shard 374 / 2900 skipped
Shard 375 / 2900 skipped
Shard 376 / 2900 skipped
Shard 377 / 2900 skipped
Shard 378 / 2900 skipped
Shard 379 / 2900 skipped
Shard 380 / 2900 skipped
Shard 381 / 2900 skipped
Shard 382 / 2900 skipped
Shard 383 / 2900 skipped
Shard 384 / 2900 skipped
Shard 385 / 2900 skipped
Shard 386 / 2900 skipped
Shard 387 / 2900 skipped
Shard 388 / 2900 skipped
Shard 389 / 2900 skipped
Shard 390 / 2900 skipped
Shard 391 / 2900 skipped
Shard 392 / 2900 skipped
Shard 393 / 2900 skipped
Shard 394 / 2900 skipped
Shard 395 / 2900 skipped
Shard 396 / 2900 skipped
Shard 397 / 2900 skipped
Shard 398 / 2900 skipped
Shard 399 / 2900 skipped
Shard 400 / 2900 skipped
Shard 401 / 2900 skipped
Shard 402 / 2900 skipped
Shard 403 / 2900 skipped
Shard 404 / 2900 skipped
Shard 405 / 2900 skipped
Shard 406 / 2900 skipped
Shard 407 / 2900 skipped
Shard 408 / 2900 skipped
Shard 409 / 2900 skipped
Shard 410 / 2900 skipped
Shard 411 / 2900 skipped
Shard 412 / 2900 skipped
Shard 413 / 2900 skipped
Shard 414 / 2900 skipped
Shard 415 / 2900 skipped
Shard 416 / 2900 skipped
Shard 417 / 2900 skipped
Shard 418 / 2900 skipped
Shard 419 / 2900 skipped
Shard 420 / 2900 skipped
Shard 421 / 2900 skipped
Shard 422 / 2900 skipped
Shard 423 / 2900 skipped
Shard 424 / 2900 skipped
Shard 425 / 2900 skipped
Shard 426 / 2900 skipped
Shard 427 / 2900 skipped
Shard 428 / 2900 skipped
Shard 429 / 2900 skipped
Shard 430 / 2900 skipped
Shard 431 / 2900 skipped
Shard 432 / 2900 skipped
Shard 433 / 2900 skipped
Shard 434 / 2900 skipped
Shard 435 / 2900 skipped
Shard 436 / 2900 skipped
Shard 437 / 2900 skipped
Shard 438 / 2900 skipped
Shard 439 / 2900 skipped
Shard 440 / 2900 skipped
Shard 441 / 2900 skipped
Shard 442 / 2900 skipped
Shard 443 / 2900 skipped
Shard 444 / 2900 skipped
Shard 445 / 2900 skipped
Shard 446 / 2900 skipped
Shard 447 / 2900 skipped
Shard 448 / 2900 skipped
Shard 449 / 2900 skipped
Shard 450 / 2900 skipped
Shard 451 / 2900 skipped
Shard 452 / 2900 skipped
Shard 453 / 2900 skipped
Shard 454 / 2900 skipped
Shard 455 / 2900 skipped
Shard 456 / 2900 skipped
Shard 457 / 2900 skipped
Shard 458 / 2900 skipped
Shard 459 / 2900 skipped
Shard 460 / 2900 skipped
Shard 461 / 2900 skipped
Shard 462 / 2900 skipped
Shard 463 / 2900 skipped
Shard 464 / 2900 skipped
Shard 465 / 2900 skipped
Shard 466 / 2900 skipped
Shard 467 / 2900 skipped
Shard 468 / 2900 skipped
Shard 469 / 2900 skipped
Shard 470 / 2900 skipped
Shard 471 / 2900 skipped
Shard 472 / 2900 skipped
Shard 473 / 2900 skipped
Shard 474 / 2900 skipped
Shard 475 / 2900 skipped
Shard 476 / 2900 skipped
Shard 477 / 2900 skipped
Shard 478 / 2900 skipped
Shard 479 / 2900 skipped
Shard 480 / 2900 skipped
Shard 481 / 2900 skipped
Shard 482 / 2900 skipped
Shard 483 / 2900 skipped
Shard 484 / 2900 skipped
Shard 485 / 2900 skipped
Shard 486 / 2900 skipped
Shard 487 / 2900 skipped
Shard 488 / 2900 skipped
Shard 489 / 2900 skipped
Shard 490 / 2900 skipped
Shard 491 / 2900 skipped
Shard 492 / 2900 skipped
Shard 493 / 2900 skipped
Shard 494 / 2900 skipped
Shard 495 / 2900 skipped
Shard 496 / 2900 skipped
Shard 497 / 2900 skipped
Shard 498 / 2900 skipped
Shard 499 / 2900 skipped
Shard 500 / 2900 skipped
Shard 501 / 2900 skipped
Shard 502 / 2900 skipped
Shard 503 / 2900 skipped
Shard 504 / 2900 skipped
Shard 505 / 2900 skipped
Shard 506 / 2900 skipped
Shard 507 / 2900 skipped
Shard 508 / 2900 skipped
Shard 509 / 2900 skipped
Shard 510 / 2900 skipped
Shard 511 / 2900 skipped
Shard 512 / 2900 skipped
Shard 513 / 2900 skipped
Shard 514 / 2900 skipped
Shard 515 / 2900 skipped
Shard 516 / 2900 skipped
Shard 517 / 2900 skipped
Shard 518 / 2900 skipped
Shard 519 / 2900 skipped
Shard 520 / 2900 skipped
Shard 521 / 2900 skipped
Shard 522 / 2900 skipped
Shard 523 / 2900 skipped
Shard 524 / 2900 skipped
Shard 525 / 2900 skipped
Shard 526 / 2900 skipped
Shard 527 / 2900 skipped
Shard 528 / 2900 skipped
Shard 529 / 2900 skipped
Shard 530 / 2900 skipped
Shard 531 / 2900 skipped
Shard 532 / 2900 skipped
Shard 533 / 2900 skipped
Shard 534 / 2900 skipped
Shard 535 / 2900 skipped
Shard 536 / 2900 skipped
Shard 537 / 2900 skipped
Shard 538 / 2900 skipped
Shard 539 / 2900 skipped
Shard 540 / 2900 skipped
Shard 541 / 2900 skipped
Shard 542 / 2900 skipped
Shard 543 / 2900 skipped
Shard 544 / 2900 skipped
Shard 545 / 2900 skipped
Shard 546 / 2900 skipped
Shard 547 / 2900 skipped
Shard 548 / 2900 skipped
Shard 549 / 2900 skipped
Shard 550 / 2900 skipped
Shard 551 / 2900 skipped
Shard 552 / 2900 skipped
Shard 553 / 2900 skipped
Shard 554 / 2900 skipped
Shard 555 / 2900 skipped
Shard 556 / 2900 skipped
Shard 557 / 2900 skipped
Shard 558 / 2900 skipped
Shard 559 / 2900 skipped
Shard 560 / 2900 skipped
Shard 561 / 2900 skipped
Shard 562 / 2900 skipped
Shard 563 / 2900 skipped
Shard 564 / 2900 skipped
Shard 565 / 2900 skipped
Shard 566 / 2900 skipped
Shard 567 / 2900 skipped
Shard 568 / 2900 skipped
Shard 569 / 2900 skipped
Shard 570 / 2900 skipped
Shard 571 / 2900 skipped
Shard 572 / 2900 skipped
Shard 573 / 2900 skipped
Shard 574 / 2900 skipped
Shard 575 / 2900 skipped
Shard 576 / 2900 skipped
Shard 577 / 2900 skipped
Shard 578 / 2900 skipped
Shard 579 / 2900 skipped
Shard 580 / 2900 skipped
Shard 581 / 2900 skipped
Shard 582 / 2900 skipped
Shard 583 / 2900 skipped
Shard 584 / 2900 skipped
Shard 585 / 2900 skipped
Shard 586 / 2900 skipped
Shard 587 / 2900 skipped
Shard 588 / 2900 skipped
Shard 589 / 2900 skipped
Shard 590 / 2900 skipped
Shard 591 / 2900 skipped
Shard 592 / 2900 skipped
Shard 593 / 2900 skipped
Shard 594 / 2900 skipped
Shard 595 / 2900 skipped
Shard 596 / 2900 skipped
Shard 597 / 2900 skipped
Shard 598 / 2900 skipped
Shard 599 / 2900 skipped
Shard 600 / 2900 skipped
Shard 601 / 2900 skipped
Shard 602 / 2900 skipped
Shard 603 / 2900 skipped
Shard 604 / 2900 skipped
Shard 605 / 2900 skipped
Shard 606 / 2900 skipped
Shard 607 / 2900 skipped
Shard 608 / 2900 skipped
Shard 609 / 2900 skipped
Shard 610 / 2900 skipped
Shard 611 / 2900 skipped
Shard 612 / 2900 skipped
Shard 613 / 2900 skipped
Shard 614 / 2900 skipped
Shard 615 / 2900 skipped
Shard 616 / 2900 skipped
Shard 617 / 2900 skipped
Shard 618 / 2900 skipped
Shard 619 / 2900 skipped
Shard 620 / 2900 skipped
Shard 621 / 2900 skipped
Shard 622 / 2900 skipped
Shard 623 / 2900 skipped
Shard 624 / 2900 skipped
Shard 625 / 2900 skipped
Shard 626 / 2900 skipped
Shard 627 / 2900 skipped
Shard 628 / 2900 skipped
Shard 629 / 2900 skipped
Shard 630 / 2900 skipped
Shard 631 / 2900 skipped
Shard 632 / 2900 skipped
Shard 633 / 2900 skipped
Shard 634 / 2900 skipped
Shard 635 / 2900 skipped
Shard 636 / 2900 skipped
Shard 637 / 2900 skipped
Shard 638 / 2900 skipped
Shard 639 / 2900 skipped
Shard 640 / 2900 skipped
Shard 641 / 2900 skipped
Shard 642 / 2900 skipped
Shard 643 / 2900 skipped
Shard 644 / 2900 skipped
Shard 645 / 2900 skipped
Shard 646 / 2900 skipped
Shard 647 / 2900 skipped
Shard 648 / 2900 skipped
Shard 649 / 2900 skipped
Shard 650 / 2900 skipped
Shard 651 / 2900 skipped
Shard 652 / 2900 skipped
Shard 653 / 2900 skipped
Shard 654 / 2900 skipped
Shard 655 / 2900 skipped
Shard 656 / 2900 skipped
Shard 657 / 2900 skipped
Shard 658 / 2900 skipped
Shard 659 / 2900 skipped
Shard 660 / 2900 skipped
Shard 661 / 2900 skipped
Shard 662 / 2900 skipped
Shard 663 / 2900 skipped
Shard 664 / 2900 skipped
Shard 665 / 2900 skipped
Shard 666 / 2900 skipped
Shard 667 / 2900 skipped
Shard 668 / 2900 skipped
Shard 669 / 2900 skipped
Shard 670 / 2900 skipped
Shard 671 / 2900 skipped
Shard 672 / 2900 skipped
Shard 673 / 2900 skipped
Shard 674 / 2900 skipped
Shard 675 / 2900 skipped
Shard 676 / 2900 skipped
Shard 677 / 2900 skipped
Shard 678 / 2900 skipped
Shard 679 / 2900 skipped
Shard 680 / 2900 skipped
Shard 681 / 2900 skipped
Shard 682 / 2900 skipped
Shard 683 / 2900 skipped
Shard 684 / 2900 skipped
Shard 685 / 2900 skipped
Shard 686 / 2900 skipped
Shard 687 / 2900 skipped
Shard 688 / 2900 skipped
Shard 689 / 2900 skipped
Shard 690 / 2900 skipped
Shard 691 / 2900 skipped
Shard 692 / 2900 skipped
Shard 693 / 2900 skipped
Shard 694 / 2900 skipped
Shard 695 / 2900 skipped
Shard 696 / 2900 skipped
Shard 697 / 2900 skipped
Shard 698 / 2900 skipped
Shard 699 / 2900 skipped
Shard 700 / 2900 skipped
Shard 701 / 2900 skipped
Shard 702 / 2900 skipped
Shard 703 / 2900 skipped
Shard 704 / 2900 skipped
Shard 705 / 2900 skipped
Shard 706 / 2900 skipped
Shard 707 / 2900 skipped
Shard 708 / 2900 skipped
Shard 709 / 2900 skipped
Shard 710 / 2900 skipped
Shard 711 / 2900 skipped
Shard 712 / 2900 skipped
Shard 713 / 2900 skipped
Shard 714 / 2900 skipped
Shard 715 / 2900 skipped
Shard 716 / 2900 skipped
Shard 717 / 2900 skipped
Shard 718 / 2900 skipped
Shard 719 / 2900 skipped
Shard 720 / 2900 skipped
Shard 721 / 2900 skipped
Shard 722 / 2900 skipped
Shard 723 / 2900 skipped
Shard 724 / 2900 skipped
Shard 725 / 2900 skipped
Shard 726 / 2900 skipped
Shard 727 / 2900 skipped
Shard 728 / 2900 skipped
Shard 729 / 2900 skipped
Shard 730 / 2900 skipped
Shard 731 / 2900 skipped
Shard 732 / 2900 skipped
Shard 733 / 2900 skipped
Shard 734 / 2900 skipped
Shard 735 / 2900 skipped
Shard 736 / 2900 skipped
Shard 737 / 2900 skipped
Shard 738 / 2900 skipped
Shard 739 / 2900 skipped
Shard 740 / 2900 skipped
Shard 741 / 2900 skipped
Shard 742 / 2900 skipped
Shard 743 / 2900 skipped
Shard 744 / 2900 skipped
Shard 745 / 2900 skipped
Shard 746 / 2900 skipped
Shard 747 / 2900 skipped
Shard 748 / 2900 skipped
Shard 749 / 2900 skipped
Shard 750 / 2900 skipped
Shard 751 / 2900 skipped
Shard 752 / 2900 skipped
Shard 753 / 2900 skipped
Shard 754 / 2900 skipped
Shard 755 / 2900 skipped
Shard 756 / 2900 skipped
Shard 757 / 2900 skipped
Shard 758 / 2900 skipped
Shard 759 / 2900 skipped
Shard 760 / 2900 skipped
Shard 761 / 2900 skipped
Shard 762 / 2900 skipped
Shard 763 / 2900 skipped
Shard 764 / 2900 skipped
Shard 765 / 2900 skipped
Shard 766 / 2900 skipped
Shard 767 / 2900 skipped
Shard 768 / 2900 skipped
Shard 769 / 2900 skipped
Shard 770 / 2900 skipped
Shard 771 / 2900 skipped
Shard 772 / 2900 skipped
Shard 773 / 2900 skipped
Shard 774 / 2900 skipped
Shard 775 / 2900 skipped
Shard 776 / 2900 skipped
Shard 777 / 2900 skipped
Shard 778 / 2900 skipped
Shard 779 / 2900 skipped
Shard 780 / 2900 skipped
Shard 781 / 2900 skipped
Shard 782 / 2900 skipped
Shard 783 / 2900 skipped
Shard 784 / 2900 skipped
Shard 785 / 2900 skipped
Shard 786 / 2900 skipped
Shard 787 / 2900 skipped
Shard 788 / 2900 skipped
Shard 789 / 2900 skipped
Shard 790 / 2900 skipped
Shard 791 / 2900 skipped
Shard 792 / 2900 skipped
Shard 793 / 2900 skipped
Shard 794 / 2900 skipped
Shard 795 / 2900 skipped
Shard 796 / 2900 skipped
Shard 797 / 2900 skipped
Shard 798 / 2900 skipped
Shard 799 / 2900 skipped
Shard 800 / 2900 skipped
Shard 801 / 2900 skipped
Shard 802 / 2900 skipped
Shard 803 / 2900 skipped
Shard 804 / 2900 skipped
Shard 805 / 2900 skipped
Shard 806 / 2900 skipped
Shard 807 / 2900 skipped
Shard 808 / 2900 skipped
Shard 809 / 2900 skipped
Shard 810 / 2900 skipped
Shard 811 / 2900 skipped
Shard 812 / 2900 skipped
Shard 813 / 2900 skipped
Shard 814 / 2900 skipped
Shard 815 / 2900 skipped
Shard 816 / 2900 skipped
Shard 817 / 2900 skipped
Shard 818 / 2900 skipped
Shard 819 / 2900 skipped
Shard 820 / 2900 skipped
Shard 821 / 2900 skipped
Shard 822 / 2900 skipped
Shard 823 / 2900 skipped
Shard 824 / 2900 skipped
Shard 825 / 2900 skipped
Shard 826 / 2900 skipped
Shard 827 / 2900 skipped
Shard 828 / 2900 skipped
Shard 829 / 2900 skipped
Shard 830 / 2900 skipped
Shard 831 / 2900 skipped
Shard 832 / 2900 skipped
Shard 833 / 2900 skipped
Shard 834 / 2900 skipped
Shard 835 / 2900 skipped
Shard 836 / 2900 skipped
Shard 837 / 2900 skipped
Shard 838 / 2900 skipped
Shard 839 / 2900 skipped
Shard 840 / 2900 skipped
Shard 841 / 2900 skipped
Shard 842 / 2900 skipped
Shard 843 / 2900 skipped
Shard 844 / 2900 skipped
Shard 845 / 2900 skipped
Shard 846 / 2900 skipped
Shard 847 / 2900 skipped
Shard 848 / 2900 skipped
Shard 849 / 2900 skipped
Shard 850 / 2900 skipped
Shard 851 / 2900 skipped
Shard 852 / 2900 skipped
Shard 853 / 2900 skipped
Shard 854 / 2900 skipped
Shard 855 / 2900 skipped
Shard 856 / 2900 skipped
Shard 857 / 2900 skipped
Shard 858 / 2900 skipped
Shard 859 / 2900 skipped
Shard 860 / 2900 skipped
Shard 861 / 2900 skipped
Shard 862 / 2900 skipped
Shard 863 / 2900 skipped
Shard 864 / 2900 skipped
Shard 865 / 2900 skipped
Shard 866 / 2900 skipped
Shard 867 / 2900 skipped
Shard 868 / 2900 skipped
Shard 869 / 2900 skipped
Shard 870 / 2900 skipped
Shard 871 / 2900 skipped
Shard 872 / 2900 skipped
Shard 873 / 2900 skipped
Shard 874 / 2900 skipped
Shard 875 / 2900 skipped
Shard 876 / 2900 skipped
Shard 877 / 2900 skipped
Shard 878 / 2900 skipped
Shard 879 / 2900 skipped
Shard 880 / 2900 skipped
Shard 881 / 2900 skipped
Shard 882 / 2900 skipped
Shard 883 / 2900 skipped
Shard 884 / 2900 skipped
Shard 885 / 2900 skipped
Shard 886 / 2900 skipped
Shard 887 / 2900 skipped
Shard 888 / 2900 skipped
Shard 889 / 2900 skipped
Shard 890 / 2900 skipped
Shard 891 / 2900 skipped
Shard 892 / 2900 skipped
Shard 893 / 2900 skipped
Shard 894 / 2900 skipped
Shard 895 / 2900 skipped
Shard 896 / 2900 skipped
Shard 897 / 2900 skipped
Shard 898 / 2900 skipped
Shard 899 / 2900 skipped
Shard 900 / 2900 skipped
Shard 901 / 2900 skipped
Shard 902 / 2900 skipped
Shard 903 / 2900 skipped
Shard 904 / 2900 skipped
Shard 905 / 2900 skipped
Shard 906 / 2900 skipped
Shard 907 / 2900 skipped
Shard 908 / 2900 skipped
Shard 909 / 2900 skipped
Shard 910 / 2900 skipped
Shard 911 / 2900 skipped
Shard 912 / 2900 skipped
Shard 913 / 2900 skipped
Shard 914 / 2900 skipped
Shard 915 / 2900 skipped
Shard 916 / 2900 skipped
Shard 917 / 2900 skipped
Shard 918 / 2900 skipped
Shard 919 / 2900 skipped
Shard 920 / 2900 skipped
Shard 921 / 2900 skipped
Shard 922 / 2900 skipped
Shard 923 / 2900 skipped
Shard 924 / 2900 skipped
Shard 925 / 2900 skipped
Shard 926 / 2900 skipped
Shard 927 / 2900 skipped
Shard 928 / 2900 skipped
Shard 929 / 2900 skipped
Shard 930 / 2900 skipped
Shard 931 / 2900 skipped
Shard 932 / 2900 skipped
Shard 933 / 2900 skipped
Shard 934 / 2900 skipped
Shard 935 / 2900 skipped
Shard 936 / 2900 skipped
Shard 937 / 2900 skipped
Shard 938 / 2900 skipped
Shard 939 / 2900 skipped
Shard 940 / 2900 skipped
Shard 941 / 2900 skipped
Shard 942 / 2900 skipped
Shard 943 / 2900 skipped
Shard 944 / 2900 skipped
Shard 945 / 2900 skipped
Shard 946 / 2900 skipped
Shard 947 / 2900 skipped
Shard 948 / 2900 skipped
Shard 949 / 2900 skipped
Shard 950 / 2900 skipped
Shard 951 / 2900 skipped
Shard 952 / 2900 skipped
Shard 953 / 2900 skipped
Shard 954 / 2900 skipped
Shard 955 / 2900 skipped
Shard 956 / 2900 skipped
Shard 957 / 2900 skipped
Shard 958 / 2900 skipped
Shard 959 / 2900 skipped
Shard 960 / 2900 skipped
Shard 961 / 2900 skipped
Shard 962 / 2900 skipped
Shard 963 / 2900 skipped
Shard 964 / 2900 skipped
Shard 965 / 2900 skipped
Shard 966 / 2900 skipped
Shard 967 / 2900 skipped
Shard 968 / 2900 skipped
Shard 969 / 2900 skipped
Shard 970 / 2900 skipped
Shard 971 / 2900 skipped
Shard 972 / 2900 skipped
Shard 973 / 2900 skipped
Shard 974 / 2900 skipped
Shard 975 / 2900 skipped
Shard 976 / 2900 skipped
Shard 977 / 2900 skipped
Shard 978 / 2900 skipped
Shard 979 / 2900 skipped
Shard 980 / 2900 skipped
Shard 981 / 2900 skipped
Shard 982 / 2900 skipped
Shard 983 / 2900 skipped
Shard 984 / 2900 skipped
Shard 985 / 2900 skipped
Shard 986 / 2900 skipped
Shard 987 / 2900 skipped
Shard 988 / 2900 skipped
Shard 989 / 2900 skipped
Shard 990 / 2900 skipped
Shard 991 / 2900 skipped
Shard 992 / 2900 skipped
Shard 993 / 2900 skipped
Shard 994 / 2900 skipped
Shard 995 / 2900 skipped
Shard 996 / 2900 skipped
Shard 997 / 2900 skipped
Shard 998 / 2900 skipped
Shard 999 / 2900 skipped
Shard 1000 / 2900 skipped
Shard 1001 / 2900 skipped
Shard 1002 / 2900 skipped
Shard 1003 / 2900 skipped
Shard 1004 / 2900 skipped
Shard 1005 / 2900 skipped
Shard 1006 / 2900 skipped
Shard 1007 / 2900 skipped
Shard 1008 / 2900 skipped
Shard 1009 / 2900 skipped
Shard 1010 / 2900 skipped
Shard 1011 / 2900 skipped
Shard 1012 / 2900 skipped
Shard 1013 / 2900 skipped
Shard 1014 / 2900 skipped
Shard 1015 / 2900 skipped
Shard 1016 / 2900 skipped
Shard 1017 / 2900 skipped
Shard 1018 / 2900 skipped
Shard 1019 / 2900 skipped
Shard 1020 / 2900 skipped
Shard 1021 / 2900 skipped
Shard 1022 / 2900 skipped
Shard 1023 / 2900 skipped
Shard 1024 / 2900 skipped
Shard 1025 / 2900 skipped
Shard 1026 / 2900 skipped
Shard 1027 / 2900 skipped
Shard 1028 / 2900 skipped
Shard 1029 / 2900 skipped
Shard 1030 / 2900 skipped
Shard 1031 / 2900 skipped
Shard 1032 / 2900 skipped
Shard 1033 / 2900 skipped
Shard 1034 / 2900 skipped
Shard 1035 / 2900 skipped
Shard 1036 / 2900 skipped
Shard 1037 / 2900 skipped
Shard 1038 / 2900 skipped
Shard 1039 / 2900 skipped
Shard 1040 / 2900 skipped
Shard 1041 / 2900 skipped
Shard 1042 / 2900 skipped
Shard 1043 / 2900 skipped
Shard 1044 / 2900 skipped
Shard 1045 / 2900 skipped
Shard 1046 / 2900 skipped
Shard 1047 / 2900 skipped
Shard 1048 / 2900 skipped
Shard 1049 / 2900 skipped
Shard 1050 / 2900 skipped
Shard 1051 / 2900 skipped
Shard 1052 / 2900 skipped
Shard 1053 / 2900 skipped
Shard 1054 / 2900 skipped
Shard 1055 / 2900 skipped
Shard 1056 / 2900 skipped
Shard 1057 / 2900 skipped
Shard 1058 / 2900 skipped
Shard 1059 / 2900 skipped
Shard 1060 / 2900 skipped
Shard 1061 / 2900 skipped
Shard 1062 / 2900 skipped
Shard 1063 / 2900 skipped
Shard 1064 / 2900 skipped
Shard 1065 / 2900 skipped
Shard 1066 / 2900 skipped
Shard 1067 / 2900 skipped
Shard 1068 / 2900 skipped
Shard 1069 / 2900 skipped
Shard 1070 / 2900 skipped
Shard 1071 / 2900 skipped
Shard 1072 / 2900 skipped
Shard 1073 / 2900 skipped
Shard 1074 / 2900 skipped
Shard 1075 / 2900 skipped
Shard 1076 / 2900 skipped
Shard 1077 / 2900 skipped
Shard 1078 / 2900 skipped
Shard 1079 / 2900 skipped
Shard 1080 / 2900 skipped
Shard 1081 / 2900 skipped
Shard 1082 / 2900 skipped
Shard 1083 / 2900 skipped
Shard 1084 / 2900 skipped
Shard 1085 / 2900 skipped
Shard 1086 / 2900 skipped
Shard 1087 / 2900 skipped
Shard 1088 / 2900 skipped
Shard 1089 / 2900 skipped
Shard 1090 / 2900 skipped
Shard 1091 / 2900 skipped
Shard 1092 / 2900 skipped
Shard 1093 / 2900 skipped
Shard 1094 / 2900 skipped
Shard 1095 / 2900 skipped
Shard 1096 / 2900 skipped
Shard 1097 / 2900 skipped
Shard 1098 / 2900 skipped
Shard 1099 / 2900 skipped
Shard 1100 / 2900 skipped
Shard 1101 / 2900 skipped
Shard 1102 / 2900 skipped
Shard 1103 / 2900 skipped
Shard 1104 / 2900 skipped
Shard 1105 / 2900 skipped
Shard 1106 / 2900 skipped
Shard 1107 / 2900 skipped
Shard 1108 / 2900 skipped
Shard 1109 / 2900 skipped
Shard 1110 / 2900 skipped
Shard 1111 / 2900 skipped
Shard 1112 / 2900 skipped
Shard 1113 / 2900 skipped
Shard 1114 / 2900 skipped
Shard 1115 / 2900 skipped
Shard 1116 / 2900 skipped
Shard 1117 / 2900 skipped
Shard 1118 / 2900 skipped
Shard 1119 / 2900 skipped
Shard 1120 / 2900 skipped
Shard 1121 / 2900 skipped
Shard 1122 / 2900 skipped
Shard 1123 / 2900 skipped
Shard 1124 / 2900 skipped
Shard 1125 / 2900 skipped
Shard 1126 / 2900 skipped
Shard 1127 / 2900 skipped
Shard 1128 / 2900 skipped
Shard 1129 / 2900 skipped
Shard 1130 / 2900 skipped
Shard 1131 / 2900 skipped
Shard 1132 / 2900 skipped
Shard 1133 / 2900 skipped
Shard 1134 / 2900 skipped
Shard 1135 / 2900 skipped
Shard 1136 / 2900 skipped
Shard 1137 / 2900 skipped
Shard 1138 / 2900 skipped
Shard 1139 / 2900 skipped
Shard 1140 / 2900 skipped
Shard 1141 / 2900 skipped
Shard 1142 / 2900 skipped
Shard 1143 / 2900 skipped
Shard 1144 / 2900 skipped
Shard 1145 / 2900 skipped
Shard 1146 / 2900 skipped
Shard 1147 / 2900 skipped
Shard 1148 / 2900 skipped
Shard 1149 / 2900 skipped
Shard 1150 / 2900 skipped
Shard 1151 / 2900 skipped
Shard 1152 / 2900 skipped
Shard 1153 / 2900 skipped
Shard 1154 / 2900 skipped
Shard 1155 / 2900 skipped
Shard 1156 / 2900 skipped
Shard 1157 / 2900 skipped
Shard 1158 / 2900 skipped
Shard 1159 / 2900 skipped
Shard 1160 / 2900 skipped
Shard 1161 / 2900 skipped
Shard 1162 / 2900 skipped
Shard 1163 / 2900 skipped
Shard 1164 / 2900 skipped
Shard 1165 / 2900 skipped
Shard 1166 / 2900 skipped
Shard 1167 / 2900 skipped
Shard 1168 / 2900 skipped
Shard 1169 / 2900 skipped
Shard 1170 / 2900 skipped
Shard 1171 / 2900 skipped
Shard 1172 / 2900 skipped
Shard 1173 / 2900 skipped
Shard 1174 / 2900 skipped
Shard 1175 / 2900 skipped
Shard 1176 / 2900 skipped
Shard 1177 / 2900 skipped
Shard 1178 / 2900 skipped
Shard 1179 / 2900 skipped
Shard 1180 / 2900 skipped
Shard 1181 / 2900 skipped
Shard 1182 / 2900 skipped
Shard 1183 / 2900 skipped
Shard 1184 / 2900 skipped
Shard 1185 / 2900 skipped
Shard 1186 / 2900 skipped
Shard 1187 / 2900 skipped
Shard 1188 / 2900 skipped
Shard 1189 / 2900 skipped
Shard 1190 / 2900 skipped
Shard 1191 / 2900 skipped
Shard 1192 / 2900 skipped
Shard 1193 / 2900 skipped
Shard 1194 / 2900 skipped
Shard 1195 / 2900 skipped
Shard 1196 / 2900 skipped
Shard 1197 / 2900 skipped
Shard 1198 / 2900 skipped
Shard 1199 / 2900 skipped
Shard 1200 / 2900 skipped
Shard 1201 / 2900 skipped
Shard 1202 / 2900 skipped
Shard 1203 / 2900 skipped
Shard 1204 / 2900 skipped
Shard 1205 / 2900 skipped
Shard 1206 / 2900 skipped
Shard 1207 / 2900 skipped
Shard 1208 / 2900 skipped
Shard 1209 / 2900 skipped
Shard 1210 / 2900 skipped
Shard 1211 / 2900 skipped
Shard 1212 / 2900 skipped
Shard 1213 / 2900 skipped
Shard 1214 / 2900 skipped
Shard 1215 / 2900 skipped
Shard 1216 / 2900 skipped
Shard 1217 / 2900 skipped
Shard 1218 / 2900 skipped
Shard 1219 / 2900 skipped
Shard 1220 / 2900 skipped
Shard 1221 / 2900 skipped
Shard 1222 / 2900 skipped
Shard 1223 / 2900 skipped
Shard 1224 / 2900 skipped
Shard 1225 / 2900 skipped
Shard 1226 / 2900 skipped
Shard 1227 / 2900 skipped
Shard 1228 / 2900 skipped
Shard 1229 / 2900 skipped
Shard 1230 / 2900 skipped
Shard 1231 / 2900 skipped
Shard 1232 / 2900 skipped
Shard 1233 / 2900 skipped
Shard 1234 / 2900 skipped
Shard 1235 / 2900 skipped
Shard 1236 / 2900 skipped
Shard 1237 / 2900 skipped
Shard 1238 / 2900 skipped
Shard 1239 / 2900 skipped
Shard 1240 / 2900 skipped
Shard 1241 / 2900 skipped
Shard 1242 / 2900 skipped
Shard 1243 / 2900 skipped
Shard 1244 / 2900 skipped
Shard 1245 / 2900 skipped
Shard 1246 / 2900 skipped
Shard 1247 / 2900 skipped
Shard 1248 / 2900 skipped
Shard 1249 / 2900 skipped
Shard 1250 / 2900 skipped
Shard 1251 / 2900 skipped
Shard 1252 / 2900 skipped
Shard 1253 / 2900 skipped
Shard 1254 / 2900 skipped
Shard 1255 / 2900 skipped
Shard 1256 / 2900 skipped
Shard 1257 / 2900 skipped
Shard 1258 / 2900 skipped
Shard 1259 / 2900 skipped
Shard 1260 / 2900 skipped
Shard 1261 / 2900 skipped
Shard 1262 / 2900 skipped
Shard 1263 / 2900 skipped
Shard 1264 / 2900 skipped
Shard 1265 / 2900 skipped
Shard 1266 / 2900 skipped
Shard 1267 / 2900 skipped
Shard 1268 / 2900 skipped
Shard 1269 / 2900 skipped
Shard 1270 / 2900 skipped
Shard 1271 / 2900 skipped
Shard 1272 / 2900 skipped
Shard 1273 / 2900 skipped
Shard 1274 / 2900 skipped
Shard 1275 / 2900 skipped
Shard 1276 / 2900 skipped
Shard 1277 / 2900 skipped
Shard 1278 / 2900 skipped
Shard 1279 / 2900 skipped
Shard 1280 / 2900 skipped
Shard 1281 / 2900 skipped
Shard 1282 / 2900 skipped
Shard 1283 / 2900 skipped
Shard 1284 / 2900 skipped
Shard 1285 / 2900 skipped
Shard 1286 / 2900 skipped
Shard 1287 / 2900 skipped
Shard 1288 / 2900 skipped
Shard 1289 / 2900 skipped
Shard 1290 / 2900 skipped
Shard 1291 / 2900 skipped
Shard 1292 / 2900 skipped
Shard 1293 / 2900 skipped
Shard 1294 / 2900 skipped
Shard 1295 / 2900 skipped
Shard 1296 / 2900 skipped
Shard 1297 / 2900 skipped
Shard 1298 / 2900 skipped
Shard 1299 / 2900 skipped
Shard 1300 / 2900 skipped
Shard 1301 / 2900 skipped
Shard 1302 / 2900 skipped
Shard 1303 / 2900 skipped
Shard 1304 / 2900 skipped
Shard 1305 / 2900 skipped
Shard 1306 / 2900 skipped
Shard 1307 / 2900 skipped
Shard 1308 / 2900 skipped
Shard 1309 / 2900 skipped
Shard 1310 / 2900 skipped
Shard 1311 / 2900 skipped
Shard 1312 / 2900 skipped
Shard 1313 / 2900 skipped
Shard 1314 / 2900 skipped
Shard 1315 / 2900 skipped
Shard 1316 / 2900 skipped
Shard 1317 / 2900 skipped
Shard 1318 / 2900 skipped
Shard 1319 / 2900 skipped
Shard 1320 / 2900 skipped
Shard 1321 / 2900 skipped
Shard 1322 / 2900 skipped
Shard 1323 / 2900 skipped
Shard 1324 / 2900 skipped
Shard 1325 / 2900 skipped
Shard 1326 / 2900 skipped
Shard 1327 / 2900 skipped
Shard 1328 / 2900 skipped
Shard 1329 / 2900 skipped
Shard 1330 / 2900 skipped
Shard 1331 / 2900 skipped
Shard 1332 / 2900 skipped
Shard 1333 / 2900 skipped
Shard 1334 / 2900 skipped
Shard 1335 / 2900 skipped
Shard 1336 / 2900 skipped
Shard 1337 / 2900 skipped
Shard 1338 / 2900 skipped
Shard 1339 / 2900 skipped
Shard 1340 / 2900 skipped
Shard 1341 / 2900 skipped
Shard 1342 / 2900 skipped
Shard 1343 / 2900 skipped
Shard 1344 / 2900 skipped
Shard 1345 / 2900 skipped
Shard 1346 / 2900 skipped
Shard 1347 / 2900 skipped
Shard 1348 / 2900 skipped
Shard 1349 / 2900 skipped
Shard 1350 / 2900 skipped
Shard 1351 / 2900 skipped
Shard 1352 / 2900 skipped
Shard 1353 / 2900 skipped
Shard 1354 / 2900 skipped
Shard 1355 / 2900 skipped
Shard 1356 / 2900 skipped
Shard 1357 / 2900 skipped
Shard 1358 / 2900 skipped
Shard 1359 / 2900 skipped
Shard 1360 / 2900 skipped
Shard 1361 / 2900 skipped
Shard 1362 / 2900 skipped
Shard 1363 / 2900 skipped
Shard 1364 / 2900 skipped
Shard 1365 / 2900 skipped
Shard 1366 / 2900 skipped
Shard 1367 / 2900 skipped
Shard 1368 / 2900 skipped
Shard 1369 / 2900 skipped
Shard 1370 / 2900 skipped
Shard 1371 / 2900 skipped
Shard 1372 / 2900 skipped
Shard 1373 / 2900 skipped
Shard 1374 / 2900 skipped
Shard 1375 / 2900 skipped
Shard 1376 / 2900 skipped
Shard 1377 / 2900 skipped
Shard 1378 / 2900 skipped
Shard 1379 / 2900 skipped
Shard 1380 / 2900 skipped
Shard 1381 / 2900 skipped
Shard 1382 / 2900 skipped
Shard 1383 / 2900 skipped
Shard 1384 / 2900 skipped
Shard 1385 / 2900 skipped
Shard 1386 / 2900 skipped
Shard 1387 / 2900 skipped
Shard 1388 / 2900 skipped
Shard 1389 / 2900 skipped
Shard 1390 / 2900 skipped
Shard 1391 / 2900 skipped
Shard 1392 / 2900 skipped
Shard 1393 / 2900 skipped
Shard 1394 / 2900 skipped
Shard 1395 / 2900 skipped
Shard 1396 / 2900 skipped
Shard 1397 / 2900 skipped
Shard 1398 / 2900 skipped
Shard 1399 / 2900 skipped
Shard 1400 / 2900 skipped
Shard 1401 / 2900 skipped
Shard 1402 / 2900 skipped
Shard 1403 / 2900 skipped
Shard 1404 / 2900 skipped
Shard 1405 / 2900 skipped
Shard 1406 / 2900 skipped
Shard 1407 / 2900 skipped
Shard 1408 / 2900 skipped
Shard 1409 / 2900 skipped
Shard 1410 / 2900 skipped
Shard 1411 / 2900 skipped
Shard 1412 / 2900 skipped
Shard 1413 / 2900 skipped
Shard 1414 / 2900 skipped
Shard 1415 / 2900 skipped
Shard 1416 / 2900 skipped
Shard 1417 / 2900 skipped
Shard 1418 / 2900 skipped
Shard 1419 / 2900 skipped
Shard 1420 / 2900 skipped
Shard 1421 / 2900 skipped
Shard 1422 / 2900 skipped
Shard 1423 / 2900 skipped
Shard 1424 / 2900 skipped
Shard 1425 / 2900 skipped
Shard 1426 / 2900 skipped
Shard 1427 / 2900 skipped
Shard 1428 / 2900 skipped
Shard 1429 / 2900 skipped
Shard 1430 / 2900 skipped
Shard 1431 / 2900 skipped
Shard 1432 / 2900 skipped
Shard 1433 / 2900 skipped
Shard 1434 / 2900 skipped
Shard 1435 / 2900 skipped
Shard 1436 / 2900 skipped
Shard 1437 / 2900 skipped
Shard 1438 / 2900 skipped
Shard 1439 / 2900 skipped
Shard 1440 / 2900 skipped
Shard 1441 / 2900 skipped
Shard 1442 / 2900 skipped
Shard 1443 / 2900 skipped
Shard 1444 / 2900 skipped
Shard 1445 / 2900 skipped
Shard 1446 / 2900 skipped
Shard 1447 / 2900 skipped
Shard 1448 / 2900 skipped
Shard 1449 / 2900 skipped
Shard 1450 / 2900 skipped
Shard 1451 / 2900 skipped
Shard 1452 / 2900 skipped
Shard 1453 / 2900 skipped
Shard 1454 / 2900 skipped
Shard 1455 / 2900 skipped
Shard 1456 / 2900 skipped
Shard 1457 / 2900 skipped
Shard 1458 / 2900 skipped
Shard 1459 / 2900 skipped
Shard 1460 / 2900 skipped
Shard 1461 / 2900 skipped
Shard 1462 / 2900 skipped
Shard 1463 / 2900 skipped
Shard 1464 / 2900 skipped
Shard 1465 / 2900 skipped
Shard 1466 / 2900 skipped
Shard 1467 / 2900 skipped
Shard 1468 / 2900 skipped
Shard 1469 / 2900 skipped
Shard 1470 / 2900 skipped
Shard 1471 / 2900 skipped
Shard 1472 / 2900 skipped
Shard 1473 / 2900 skipped
Shard 1474 / 2900 skipped
Shard 1475 / 2900 skipped
Shard 1476 / 2900 skipped
Shard 1477 / 2900 skipped
Shard 1478 / 2900 skipped
Shard 1479 / 2900 skipped
Shard 1480 / 2900 skipped
Shard 1481 / 2900 skipped
Shard 1482 / 2900 skipped
Shard 1483 / 2900 skipped
Shard 1484 / 2900 skipped
Shard 1485 / 2900 skipped
Shard 1486 / 2900 skipped
Shard 1487 / 2900 skipped
Shard 1488 / 2900 skipped
Shard 1489 / 2900 skipped
Shard 1490 / 2900 skipped
Shard 1491 / 2900 skipped
Shard 1492 / 2900 skipped
Shard 1493 / 2900 skipped
Shard 1494 / 2900 skipped
Shard 1495 / 2900 skipped
Shard 1496 / 2900 skipped
Shard 1497 / 2900 skipped
Shard 1498 / 2900 skipped
Shard 1499 / 2900 skipped
Shard 1500 / 2900 skipped
Shard 1501 / 2900 skipped
Shard 1502 / 2900 skipped
Shard 1503 / 2900 skipped
Shard 1504 / 2900 skipped
Shard 1505 / 2900 skipped
Shard 1506 / 2900 skipped
Shard 1507 / 2900 skipped
Shard 1508 / 2900 skipped
Shard 1509 / 2900 skipped
Shard 1510 / 2900 skipped
Shard 1511 / 2900 skipped
Shard 1512 / 2900 skipped
Shard 1513 / 2900 skipped
Shard 1514 / 2900 skipped
Shard 1515 / 2900 skipped
Shard 1516 / 2900 skipped
Shard 1517 / 2900 skipped
Shard 1518 / 2900 skipped
Shard 1519 / 2900 skipped
Shard 1520 / 2900 skipped
Shard 1521 / 2900 skipped
Shard 1522 / 2900 skipped
Shard 1523 / 2900 skipped
Shard 1524 / 2900 skipped
Shard 1525 / 2900 skipped
Shard 1526 / 2900 skipped
Shard 1527 / 2900 skipped
Shard 1528 / 2900 skipped
Shard 1529 / 2900 skipped
Shard 1530 / 2900 skipped
Shard 1531 / 2900 skipped
Shard 1532 / 2900 skipped
Shard 1533 / 2900 skipped
Shard 1534 / 2900 skipped
Shard 1535 / 2900 skipped
Shard 1536 / 2900 skipped
Shard 1537 / 2900 skipped
Shard 1538 / 2900 skipped
Shard 1539 / 2900 skipped
Shard 1540 / 2900 skipped
Shard 1541 / 2900 skipped
Shard 1542 / 2900 skipped
Shard 1543 / 2900 skipped
Shard 1544 / 2900 skipped
Shard 1545 / 2900 skipped
Shard 1546 / 2900 skipped
Shard 1547 / 2900 skipped
Shard 1548 / 2900 skipped
Shard 1549 / 2900 skipped
Shard 1550 / 2900 skipped
Shard 1551 / 2900 skipped
Shard 1552 / 2900 skipped
Shard 1553 / 2900 skipped
Shard 1554 / 2900 skipped
Shard 1555 / 2900 skipped
Shard 1556 / 2900 skipped
Shard 1557 / 2900 skipped
Shard 1558 / 2900 skipped
Shard 1559 / 2900 skipped
Shard 1560 / 2900 skipped
Shard 1561 / 2900 skipped
Shard 1562 / 2900 skipped
Shard 1563 / 2900 skipped
Shard 1564 / 2900 skipped
Shard 1565 / 2900 skipped
Shard 1566 / 2900 skipped
Shard 1567 / 2900 skipped
Shard 1568 / 2900 skipped
Shard 1569 / 2900 skipped
Shard 1570 / 2900 skipped
Shard 1571 / 2900 skipped
Shard 1572 / 2900 skipped
Shard 1573 / 2900 skipped
Shard 1574 / 2900 skipped
Shard 1575 / 2900 skipped
Shard 1576 / 2900 skipped
Shard 1577 / 2900 skipped
Shard 1578 / 2900 skipped
Shard 1579 / 2900 skipped
Shard 1580 / 2900 skipped
Shard 1581 / 2900 skipped
Shard 1582 / 2900 skipped
Shard 1583 / 2900 skipped
Shard 1584 / 2900 skipped
Shard 1585 / 2900 skipped
Shard 1586 / 2900 skipped
Shard 1587 / 2900 skipped
Shard 1588 / 2900 skipped
Shard 1589 / 2900 skipped
Shard 1590 / 2900 skipped
Shard 1591 / 2900 skipped
Shard 1592 / 2900 skipped
Shard 1593 / 2900 skipped
Shard 1594 / 2900 skipped
Shard 1595 / 2900 skipped
Shard 1596 / 2900 skipped
Shard 1597 / 2900 skipped
Shard 1598 / 2900 skipped
Shard 1599 / 2900 skipped
Shard 1600 / 2900 skipped
Shard 1601 / 2900 skipped
Shard 1602 / 2900 skipped
Shard 1603 / 2900 skipped
Shard 1604 / 2900 skipped
Shard 1605 / 2900 skipped
Shard 1606 / 2900 skipped
Shard 1607 / 2900 skipped
Shard 1608 / 2900 skipped
Shard 1609 / 2900 skipped
Shard 1610 / 2900 skipped
Shard 1611 / 2900 skipped
Shard 1612 / 2900 skipped
Shard 1613 / 2900 skipped
Shard 1614 / 2900 skipped
Shard 1615 / 2900 skipped
Shard 1616 / 2900 skipped
Shard 1617 / 2900 skipped
Shard 1618 / 2900 skipped
Shard 1619 / 2900 skipped
Shard 1620 / 2900 skipped
Shard 1621 / 2900 skipped
Shard 1622 / 2900 skipped
Shard 1623 / 2900 skipped
Shard 1624 / 2900 skipped
Shard 1625 / 2900 skipped
Shard 1626 / 2900 skipped
Shard 1627 / 2900 skipped
Shard 1628 / 2900 skipped
Shard 1629 / 2900 skipped
Shard 1630 / 2900 skipped
Shard 1631 / 2900 skipped
Shard 1632 / 2900 skipped
Shard 1633 / 2900 skipped
Shard 1634 / 2900 skipped
Shard 1635 / 2900 skipped
Shard 1636 / 2900 skipped
Shard 1637 / 2900 skipped
Shard 1638 / 2900 skipped
Shard 1639 / 2900 skipped
Shard 1640 / 2900 skipped
Shard 1641 / 2900 skipped
Shard 1642 / 2900 skipped
Shard 1643 / 2900 skipped
Shard 1644 / 2900 skipped
Shard 1645 / 2900 skipped
Shard 1646 / 2900 skipped
Shard 1647 / 2900 skipped
Shard 1648 / 2900 skipped
Shard 1649 / 2900 skipped
Shard 1650 / 2900 skipped
Shard 1651 / 2900 skipped
Shard 1652 / 2900 skipped
Shard 1653 / 2900 skipped
Shard 1654 / 2900 skipped
Shard 1655 / 2900 skipped
Shard 1656 / 2900 skipped
Shard 1657 / 2900 skipped
Shard 1658 / 2900 skipped
Shard 1659 / 2900 skipped
Shard 1660 / 2900 skipped
Shard 1661 / 2900 skipped
Shard 1662 / 2900 skipped
Shard 1663 / 2900 skipped
Shard 1664 / 2900 skipped
Shard 1665 / 2900 skipped
Shard 1666 / 2900 skipped
Shard 1667 / 2900 skipped
Shard 1668 / 2900 skipped
Shard 1669 / 2900 skipped
Shard 1670 / 2900 skipped
Shard 1671 / 2900 skipped
Shard 1672 / 2900 skipped
Shard 1673 / 2900 skipped
Shard 1674 / 2900 skipped
Shard 1675 / 2900 skipped
Shard 1676 / 2900 skipped
Shard 1677 / 2900 skipped
Shard 1678 / 2900 skipped
Shard 1679 / 2900 skipped
Shard 1680 / 2900 skipped
Shard 1681 / 2900 skipped
Shard 1682 / 2900 skipped
Shard 1683 / 2900 skipped
Shard 1684 / 2900 skipped
Shard 1685 / 2900 skipped
Shard 1686 / 2900 skipped
Shard 1687 / 2900 skipped
Shard 1688 / 2900 skipped
Shard 1689 / 2900 skipped
Shard 1690 / 2900 skipped
Shard 1691 / 2900 skipped
Shard 1692 / 2900 skipped
Shard 1693 / 2900 skipped
Shard 1694 / 2900 skipped
Shard 1695 / 2900 skipped
Shard 1696 / 2900 skipped
Shard 1697 / 2900 skipped
Shard 1698 / 2900 skipped
Shard 1699 / 2900 skipped
Shard 1700 / 2900 skipped
Shard 1701 / 2900 skipped
Shard 1702 / 2900 skipped
Shard 1703 / 2900 skipped
Shard 1704 / 2900 skipped
Shard 1705 / 2900 skipped
Shard 1706 / 2900 skipped
Shard 1707 / 2900 skipped
Shard 1708 / 2900 skipped
Shard 1709 / 2900 skipped
Shard 1710 / 2900 skipped
Shard 1711 / 2900 skipped
Shard 1712 / 2900 skipped
Shard 1713 / 2900 skipped
Shard 1714 / 2900 skipped
Shard 1715 / 2900 skipped
Shard 1716 / 2900 skipped
Shard 1717 / 2900 skipped
Shard 1718 / 2900 skipped
Shard 1719 / 2900 skipped
Shard 1720 / 2900 skipped
Shard 1721 / 2900 skipped
Shard 1722 / 2900 skipped
Shard 1723 / 2900 skipped
Shard 1724 / 2900 skipped
Shard 1725 / 2900 skipped
Shard 1726 / 2900 skipped
Shard 1727 / 2900 skipped
Shard 1728 / 2900 skipped
Shard 1729 / 2900 skipped
Shard 1730 / 2900 skipped
Shard 1731 / 2900 skipped
Shard 1732 / 2900 skipped
Shard 1733 / 2900 skipped
Shard 1734 / 2900 skipped
Shard 1735 / 2900 skipped
Shard 1736 / 2900 skipped
Shard 1737 / 2900 skipped
Shard 1738 / 2900 skipped
Shard 1739 / 2900 skipped
Shard 1740 / 2900 skipped
Shard 1741 / 2900 skipped
Shard 1742 / 2900 skipped
Shard 1743 / 2900 skipped
Shard 1744 / 2900 skipped
Shard 1745 / 2900 skipped
Shard 1746 / 2900 skipped
Shard 1747 / 2900 skipped
Shard 1748 / 2900 skipped
Shard 1749 / 2900 skipped
Shard 1750 / 2900 skipped
Shard 1751 / 2900 skipped
Shard 1752 / 2900 skipped
Shard 1753 / 2900 skipped
Shard 1754 / 2900 skipped
Shard 1755 / 2900 skipped
Shard 1756 / 2900 skipped
Shard 1757 / 2900 skipped
Shard 1758 / 2900 skipped
Shard 1759 / 2900 skipped
Shard 1760 / 2900 skipped
Shard 1761 / 2900 skipped
Shard 1762 / 2900 skipped
Shard 1763 / 2900 skipped
Shard 1764 / 2900 skipped
Shard 1765 / 2900 skipped
Shard 1766 / 2900 skipped
Shard 1767 / 2900 skipped
Shard 1768 / 2900 skipped
Shard 1769 / 2900 skipped
Shard 1770 / 2900 skipped
Shard 1771 / 2900 skipped
Shard 1772 / 2900 skipped
Shard 1773 / 2900 skipped
Shard 1774 / 2900 skipped
Shard 1775 / 2900 skipped
Shard 1776 / 2900 skipped
Shard 1777 / 2900 skipped
Shard 1778 / 2900 skipped
Shard 1779 / 2900 skipped
Shard 1780 / 2900 skipped
Shard 1781 / 2900 skipped
Shard 1782 / 2900 skipped
Shard 1783 / 2900 skipped
Shard 1784 / 2900 skipped
Shard 1785 / 2900 skipped
Shard 1786 / 2900 skipped
Shard 1787 / 2900 skipped
Shard 1788 / 2900 skipped
Shard 1789 / 2900 skipped
Shard 1790 / 2900 skipped
Shard 1791 / 2900 skipped
Shard 1792 / 2900 skipped
Shard 1793 / 2900 skipped
Shard 1794 / 2900 skipped
Shard 1795 / 2900 skipped
Shard 1796 / 2900 skipped
Shard 1797 / 2900 skipped
Shard 1798 / 2900 skipped
Shard 1799 / 2900 skipped
Shard 1800 / 2900 skipped
Shard 1801 / 2900 skipped
Shard 1802 / 2900 skipped
Shard 1803 / 2900 skipped
Shard 1804 / 2900 skipped
Shard 1805 / 2900 skipped
Shard 1806 / 2900 skipped
Shard 1807 / 2900 skipped
Shard 1808 / 2900 skipped
Shard 1809 / 2900 skipped
Shard 1810 / 2900 skipped
Shard 1811 / 2900 skipped
Shard 1812 / 2900 skipped
Shard 1813 / 2900 skipped
Shard 1814 / 2900 skipped
Shard 1815 / 2900 skipped
Shard 1816 / 2900 skipped
Shard 1817 / 2900 skipped
Shard 1818 / 2900 skipped
Shard 1819 / 2900 skipped
Shard 1820 / 2900 skipped
Shard 1821 / 2900 skipped
Shard 1822 / 2900 skipped
Shard 1823 / 2900 skipped
Shard 1824 / 2900 skipped
Shard 1825 / 2900 skipped
Shard 1826 / 2900 skipped
Shard 1827 / 2900 skipped
Shard 1828 / 2900 skipped
Shard 1829 / 2900 skipped
Shard 1830 / 2900 skipped
Shard 1831 / 2900 skipped
Shard 1832 / 2900 skipped
Shard 1833 / 2900 skipped
Shard 1834 / 2900 skipped
Shard 1835 / 2900 skipped
Shard 1836 / 2900 skipped
Shard 1837 / 2900 skipped
Shard 1838 / 2900 skipped
Shard 1839 / 2900 skipped
Shard 1840 / 2900 skipped
Shard 1841 / 2900 skipped
Shard 1842 / 2900 skipped
Shard 1843 / 2900 skipped
Shard 1844 / 2900 skipped
Shard 1845 / 2900 skipped
Shard 1846 / 2900 skipped
Shard 1847 / 2900 skipped
Shard 1848 / 2900 skipped
Shard 1849 / 2900 skipped
Shard 1850 / 2900 skipped
Shard 1851 / 2900 skipped
Shard 1852 / 2900 skipped
Shard 1853 / 2900 skipped
Shard 1854 / 2900 skipped
Shard 1855 / 2900 skipped
Shard 1856 / 2900 skipped
Shard 1857 / 2900 skipped
Shard 1858 / 2900 skipped
Shard 1859 / 2900 skipped
Shard 1860 / 2900 skipped
Shard 1861 / 2900 skipped
Shard 1862 / 2900 skipped
Shard 1863 / 2900 skipped
Shard 1864 / 2900 skipped
Shard 1865 / 2900 skipped
Shard 1866 / 2900 skipped
Shard 1867 / 2900 skipped
Shard 1868 / 2900 skipped
Shard 1869 / 2900 skipped
Shard 1870 / 2900 skipped
Shard 1871 / 2900 skipped
Shard 1872 / 2900 skipped
Shard 1873 / 2900 skipped
Shard 1874 / 2900 skipped
Shard 1875 / 2900 skipped
Shard 1876 / 2900 skipped
Shard 1877 / 2900 skipped
Shard 1878 / 2900 skipped
Shard 1879 / 2900 skipped
Shard 1880 / 2900 skipped
Shard 1881 / 2900 skipped
Shard 1882 / 2900 skipped
Shard 1883 / 2900 skipped
Shard 1884 / 2900 skipped
Shard 1885 / 2900 skipped
Shard 1886 / 2900 skipped
Shard 1887 / 2900 skipped
Shard 1888 / 2900 skipped
Shard 1889 / 2900 skipped
Shard 1890 / 2900 skipped
Shard 1891 / 2900 skipped
Shard 1892 / 2900 skipped
Shard 1893 / 2900 skipped
Shard 1894 / 2900 skipped
Shard 1895 / 2900 skipped
Shard 1896 / 2900 skipped
Shard 1897 / 2900 skipped
Shard 1898 / 2900 skipped
Shard 1899 / 2900 skipped
Shard 1900 / 2900 skipped
Shard 1901 / 2900 skipped
Shard 1902 / 2900 skipped
Shard 1903 / 2900 skipped
Shard 1904 / 2900 skipped
Shard 1905 / 2900 skipped
Shard 1906 / 2900 skipped
Shard 1907 / 2900 skipped
Shard 1908 / 2900 skipped
Shard 1909 / 2900 skipped
Shard 1910 / 2900 skipped
Shard 1911 / 2900 skipped
Shard 1912 / 2900 skipped
Shard 1913 / 2900 skipped
Shard 1914 / 2900 skipped
Shard 1915 / 2900 skipped
Shard 1916 / 2900 skipped
Shard 1917 / 2900 skipped
Shard 1918 / 2900 skipped
Shard 1919 / 2900 skipped
Shard 1920 / 2900 skipped
Shard 1921 / 2900 skipped
Shard 1922 / 2900 skipped
Shard 1923 / 2900 skipped
Shard 1924 / 2900 skipped
Shard 1925 / 2900 skipped
Shard 1926 / 2900 skipped
Shard 1927 / 2900 skipped
Shard 1928 / 2900 skipped
Shard 1929 / 2900 skipped
Shard 1930 / 2900 skipped
Shard 1931 / 2900 skipped
Shard 1932 / 2900 skipped
Shard 1933 / 2900 skipped
Shard 1934 / 2900 skipped
Shard 1935 / 2900 skipped
Shard 1936 / 2900 skipped
Shard 1937 / 2900 skipped
Shard 1938 / 2900 skipped
Shard 1939 / 2900 skipped
Shard 1940 / 2900 skipped
Shard 1941 / 2900 skipped
Shard 1942 / 2900 skipped
Shard 1943 / 2900 skipped
Shard 1944 / 2900 skipped
Shard 1945 / 2900 skipped
Shard 1946 / 2900 skipped
Shard 1947 / 2900 skipped
Shard 1948 / 2900 skipped
Shard 1949 / 2900 skipped
Shard 1950 / 2900 skipped
Shard 1951 / 2900 skipped
Shard 1952 / 2900 skipped
Shard 1953 / 2900 skipped
Shard 1954 / 2900 skipped
Shard 1955 / 2900 skipped
Shard 1956 / 2900 skipped
Shard 1957 / 2900 skipped
Shard 1958 / 2900 skipped
Shard 1959 / 2900 skipped
Shard 1960 / 2900 skipped
Shard 1961 / 2900 skipped
Shard 1962 / 2900 skipped
Shard 1963 / 2900 skipped
Shard 1964 / 2900 skipped
Shard 1965 / 2900 skipped
Shard 1966 / 2900 skipped
Shard 1967 / 2900 skipped
Shard 1968 / 2900 skipped
Shard 1969 / 2900 skipped
Shard 1970 / 2900 skipped
Shard 1971 / 2900 skipped
Shard 1972 / 2900 skipped
Shard 1973 / 2900 skipped
Shard 1974 / 2900 skipped
Shard 1975 / 2900 skipped
Shard 1976 / 2900 skipped
Shard 1977 / 2900 skipped
Shard 1978 / 2900 skipped
Shard 1979 / 2900 skipped
Shard 1980 / 2900 skipped
Shard 1981 / 2900 skipped
Shard 1982 / 2900 skipped
Shard 1983 / 2900 skipped
Shard 1984 / 2900 skipped
Shard 1985 / 2900 skipped
Shard 1986 / 2900 skipped
Shard 1987 / 2900 skipped
Shard 1988 / 2900 skipped
Shard 1989 / 2900 skipped
Shard 1990 / 2900 skipped
Shard 1991 / 2900 skipped
Shard 1992 / 2900 skipped
Shard 1993 / 2900 skipped
Shard 1994 / 2900 skipped
Shard 1995 / 2900 skipped
Shard 1996 / 2900 skipped
Shard 1997 / 2900 skipped
Shard 1998 / 2900 skipped
Shard 1999 / 2900 skipped
Shard 2000 / 2900 skipped
Shard 2001 / 2900 skipped
Shard 2002 / 2900 skipped
Shard 2003 / 2900 skipped
Shard 2004 / 2900 skipped
Shard 2005 / 2900 skipped
Shard 2006 / 2900 skipped
Shard 2007 / 2900 skipped
Shard 2008 / 2900 skipped
Shard 2009 / 2900 skipped
Shard 2010 / 2900 skipped
Shard 2011 / 2900 skipped
Shard 2012 / 2900 skipped
Shard 2013 / 2900 skipped
Shard 2014 / 2900 skipped
Shard 2015 / 2900 skipped
Shard 2016 / 2900 skipped
Shard 2017 / 2900 skipped
Shard 2018 / 2900 skipped
Shard 2019 / 2900 skipped
Shard 2020 / 2900 skipped
Shard 2021 / 2900 skipped
Shard 2022 / 2900 skipped
Shard 2023 / 2900 skipped
Shard 2024 / 2900 skipped
Shard 2025 / 2900 skipped
Shard 2026 / 2900 skipped
Shard 2027 / 2900 skipped
Shard 2028 / 2900 skipped
Shard 2029 / 2900 skipped
Shard 2030 / 2900 skipped
Shard 2031 / 2900 skipped
Shard 2032 / 2900 skipped
Shard 2033 / 2900 skipped
Shard 2034 / 2900 skipped
Shard 2035 / 2900 skipped
Shard 2036 / 2900 skipped
Shard 2037 / 2900 skipped
Shard 2038 / 2900 skipped
Shard 2039 / 2900 skipped
Shard 2040 / 2900 skipped
Shard 2041 / 2900 skipped
Shard 2042 / 2900 skipped
Shard 2043 / 2900 skipped
Shard 2044 / 2900 skipped
Shard 2045 / 2900 skipped
Shard 2046 / 2900 skipped
Shard 2047 / 2900 skipped
Shard 2048 / 2900 skipped
Shard 2049 / 2900 skipped
Shard 2050 / 2900 skipped
Shard 2051 / 2900 skipped
Shard 2052 / 2900 skipped
Shard 2053 / 2900 skipped
Shard 2054 / 2900 skipped
Shard 2055 / 2900 skipped
Shard 2056 / 2900 skipped
Shard 2057 / 2900 skipped
Shard 2058 / 2900 skipped
Shard 2059 / 2900 skipped
Shard 2060 / 2900 skipped
Shard 2061 / 2900 skipped
Shard 2062 / 2900 skipped
Shard 2063 / 2900 skipped
Shard 2064 / 2900 skipped
Shard 2065 / 2900 skipped
Shard 2066 / 2900 skipped
Shard 2067 / 2900 skipped
Shard 2068 / 2900 skipped
Shard 2069 / 2900 skipped
Shard 2070 / 2900 skipped
Shard 2071 / 2900 skipped
Shard 2072 / 2900 skipped
Shard 2073 / 2900 skipped
Shard 2074 / 2900 skipped
Shard 2075 / 2900 skipped
Shard 2076 / 2900 skipped
Shard 2077 / 2900 skipped
Shard 2078 / 2900 skipped
Shard 2079 / 2900 skipped
Shard 2080 / 2900 skipped
Shard 2081 / 2900 skipped
Shard 2082 / 2900 skipped
Shard 2083 / 2900 skipped
Shard 2084 / 2900 skipped
Shard 2085 / 2900 skipped
Shard 2086 / 2900 skipped
Shard 2087 / 2900 skipped
Shard 2088 / 2900 skipped
Shard 2089 / 2900 skipped
Shard 2090 / 2900 skipped
Shard 2091 / 2900 skipped
Shard 2092 / 2900 skipped
Shard 2093 / 2900 skipped
Shard 2094 / 2900 skipped
Shard 2095 / 2900 skipped
Shard 2096 / 2900 skipped
Shard 2097 / 2900 skipped
Shard 2098 / 2900 skipped
Shard 2099 / 2900 skipped
Shard 2100 / 2900 skipped
Shard 2101 / 2900 skipped
Shard 2102 / 2900 skipped
Shard 2103 / 2900 skipped
Shard 2104 / 2900 skipped
Shard 2105 / 2900 skipped
Shard 2106 / 2900 skipped
Shard 2107 / 2900 skipped
Shard 2108 / 2900 skipped
Shard 2109 / 2900 skipped
Shard 2110 / 2900 skipped
Shard 2111 / 2900 skipped
Shard 2112 / 2900 skipped
Shard 2113 / 2900 skipped
Shard 2114 / 2900 skipped
Shard 2115 / 2900 skipped
Shard 2116 / 2900 skipped
Shard 2117 / 2900 skipped
Shard 2118 / 2900 skipped
Shard 2119 / 2900 skipped
Shard 2120 / 2900 skipped
Shard 2121 / 2900 skipped
Shard 2122 / 2900 skipped
Shard 2123 / 2900 skipped
Shard 2124 / 2900 skipped
Shard 2125 / 2900 skipped
Shard 2126 / 2900 skipped
Shard 2127 / 2900 skipped
Shard 2128 / 2900 skipped
Shard 2129 / 2900 skipped
Shard 2130 / 2900 skipped
Shard 2131 / 2900 skipped
Shard 2132 / 2900 skipped
Shard 2133 / 2900 skipped
Shard 2134 / 2900 skipped
Shard 2135 / 2900 skipped
Shard 2136 / 2900 skipped
Shard 2137 / 2900 skipped
Shard 2138 / 2900 skipped
Shard 2139 / 2900 skipped
Shard 2140 / 2900 skipped
Shard 2141 / 2900 skipped
Shard 2142 / 2900 skipped
Shard 2143 / 2900 skipped
Shard 2144 / 2900 skipped
Shard 2145 / 2900 skipped
Shard 2146 / 2900 skipped
Shard 2147 / 2900 skipped
Shard 2148 / 2900 skipped
Shard 2149 / 2900 skipped
Shard 2150 / 2900 skipped
Shard 2151 / 2900 skipped
Shard 2152 / 2900 skipped
Shard 2153 / 2900 skipped
Shard 2154 / 2900 skipped
Shard 2155 / 2900 skipped
Shard 2156 / 2900 skipped
Shard 2157 / 2900 skipped
Shard 2158 / 2900 skipped
Shard 2159 / 2900 skipped
Shard 2160 / 2900 skipped
Shard 2161 / 2900 skipped
Shard 2162 / 2900 skipped
Shard 2163 / 2900 skipped
Shard 2164 / 2900 skipped
Shard 2165 / 2900 skipped
Shard 2166 / 2900 skipped
Shard 2167 / 2900 skipped
Shard 2168 / 2900 skipped
Shard 2169 / 2900 skipped
Shard 2170 / 2900 skipped
Shard 2171 / 2900 skipped
Shard 2172 / 2900 skipped
Shard 2173 / 2900 skipped
Shard 2174 / 2900 skipped
Shard 2175 / 2900 skipped
Shard 2176 / 2900 skipped
Shard 2177 / 2900 skipped
Shard 2178 / 2900 skipped
Shard 2179 / 2900 skipped
Shard 2180 / 2900 skipped
Shard 2181 / 2900 skipped
Shard 2182 / 2900 skipped
Shard 2183 / 2900 skipped
Shard 2184 / 2900 skipped
Shard 2185 / 2900 skipped
Shard 2186 / 2900 skipped
Shard 2187 / 2900 skipped
Shard 2188 / 2900 skipped
Shard 2189 / 2900 skipped
Shard 2190 / 2900 skipped
Shard 2191 / 2900 skipped
Shard 2192 / 2900 skipped
Shard 2193 / 2900 skipped
Shard 2194 / 2900 skipped
Shard 2195 / 2900 skipped
Shard 2196 / 2900 skipped
Shard 2197 / 2900 skipped
Shard 2198 / 2900 skipped
Shard 2199 / 2900 skipped
Shard 2200 / 2900 skipped
Shard 2201 / 2900 skipped
Shard 2202 / 2900 skipped
Shard 2203 / 2900 skipped
Shard 2204 / 2900 skipped
Shard 2205 / 2900 skipped
Shard 2206 / 2900 skipped
Shard 2207 / 2900 skipped
Shard 2208 / 2900 skipped
Shard 2209 / 2900 skipped
Shard 2210 / 2900 skipped
Shard 2211 / 2900 skipped
Shard 2212 / 2900 skipped
Shard 2213 / 2900 skipped
Shard 2214 / 2900 skipped
Shard 2215 / 2900 skipped
Shard 2216 / 2900 skipped
Shard 2217 / 2900 skipped
Shard 2218 / 2900 skipped
Shard 2219 / 2900 skipped
Shard 2220 / 2900 skipped
Shard 2221 / 2900 skipped
Shard 2222 / 2900 skipped
Shard 2223 / 2900 skipped
Shard 2224 / 2900 skipped
Shard 2225 / 2900 skipped
Shard 2226 / 2900 skipped
Shard 2227 / 2900 skipped
Shard 2228 / 2900 skipped
Shard 2229 / 2900 skipped
Shard 2230 / 2900 skipped
Shard 2231 / 2900 skipped
Shard 2232 / 2900 skipped
Shard 2233 / 2900 skipped
Shard 2234 / 2900 skipped
Shard 2235 / 2900 skipped
Shard 2236 / 2900 skipped
Shard 2237 / 2900 skipped
Shard 2238 / 2900 skipped
Shard 2239 / 2900 skipped
Shard 2240 / 2900 skipped
Shard 2241 / 2900 skipped
Shard 2242 / 2900 skipped
Shard 2243 / 2900 skipped
Shard 2244 / 2900 skipped
Shard 2245 / 2900 skipped
Shard 2246 / 2900 skipped
Shard 2247 / 2900 skipped
Shard 2248 / 2900 skipped
Shard 2249 / 2900 skipped
Shard 2250 / 2900 skipped
Shard 2251 / 2900 skipped
Shard 2252 / 2900 skipped
Shard 2253 / 2900 skipped
Shard 2254 / 2900 skipped
Shard 2255 / 2900 skipped
Shard 2256 / 2900 skipped
Shard 2257 / 2900 skipped
Shard 2258 / 2900 skipped
Shard 2259 / 2900 skipped
Shard 2260 / 2900 skipped
Shard 2261 / 2900 skipped
Shard 2262 / 2900 skipped
Shard 2263 / 2900 skipped
Shard 2264 / 2900 skipped
Shard 2265 / 2900 skipped
Shard 2266 / 2900 skipped
Shard 2267 / 2900 skipped
Shard 2268 / 2900 skipped
Shard 2269 / 2900 skipped
Shard 2270 / 2900 skipped
Shard 2271 / 2900 skipped
Shard 2272 / 2900 skipped
Shard 2273 / 2900 skipped
Shard 2274 / 2900 skipped
Shard 2275 / 2900 skipped
Shard 2276 / 2900 skipped
Shard 2277 / 2900 skipped
Shard 2278 / 2900 skipped
Shard 2279 / 2900 skipped
Shard 2280 / 2900 skipped
Shard 2281 / 2900 skipped
Shard 2282 / 2900 skipped
Shard 2283 / 2900 skipped
Shard 2284 / 2900 skipped
Shard 2285 / 2900 skipped
Shard 2286 / 2900 skipped
Shard 2287 / 2900 skipped
Shard 2288 / 2900 skipped
Shard 2289 / 2900 skipped
Shard 2290 / 2900 skipped
Shard 2291 / 2900 skipped
Shard 2292 / 2900 skipped
Shard 2293 / 2900 skipped
Shard 2294 / 2900 skipped
Shard 2295 / 2900 skipped
Shard 2296 / 2900 skipped
Shard 2297 / 2900 skipped
Shard 2298 / 2900 skipped
Shard 2299 / 2900 skipped
Shard 2300 / 2900 skipped
Shard 2301 / 2900 skipped
Shard 2302 / 2900 skipped
Shard 2303 / 2900 skipped
Shard 2304 / 2900 skipped
Shard 2305 / 2900 skipped
Shard 2306 / 2900 skipped
Shard 2307 / 2900 skipped
Shard 2308 / 2900 skipped
Shard 2309 / 2900 skipped
Shard 2310 / 2900 skipped
Shard 2311 / 2900 skipped
Shard 2312 / 2900 skipped
Shard 2313 / 2900 skipped
Shard 2314 / 2900 skipped
Shard 2315 / 2900 skipped
Shard 2316 / 2900 skipped
Shard 2317 / 2900 skipped
Shard 2318 / 2900 skipped
Shard 2319 / 2900 skipped
Shard 2320 / 2900 skipped
Shard 2321 / 2900 skipped
Shard 2322 / 2900 skipped
Shard 2323 / 2900 skipped
Shard 2324 / 2900 skipped
Shard 2325 / 2900 skipped
Shard 2326 / 2900 skipped
Shard 2327 / 2900 skipped
Shard 2328 / 2900 skipped
Shard 2329 / 2900 skipped
Shard 2330 / 2900 skipped
Shard 2331 / 2900 skipped
Shard 2332 / 2900 skipped
Shard 2333 / 2900 skipped
Shard 2334 / 2900 skipped
Shard 2335 / 2900 skipped
Shard 2336 / 2900 skipped
Shard 2337 / 2900 skipped
Shard 2338 / 2900 skipped
Shard 2339 / 2900 skipped
Shard 2340 / 2900 skipped
Shard 2341 / 2900 skipped
Shard 2342 / 2900 skipped
Shard 2343 / 2900 skipped
Shard 2344 / 2900 skipped
Shard 2345 / 2900 skipped
Shard 2346 / 2900 skipped
Shard 2347 / 2900 skipped
Shard 2348 / 2900 skipped
Shard 2349 / 2900 skipped
Shard 2350 / 2900 skipped
Shard 2351 / 2900 skipped
Shard 2352 / 2900 skipped
Shard 2353 / 2900 skipped
Shard 2354 / 2900 skipped
Shard 2355 / 2900 skipped
Shard 2356 / 2900 skipped
Shard 2357 / 2900 skipped
Shard 2358 / 2900 skipped
Shard 2359 / 2900 skipped
Shard 2360 / 2900 skipped
Shard 2361 / 2900 skipped
Shard 2362 / 2900 skipped
Shard 2363 / 2900 skipped
Shard 2364 / 2900 skipped
Shard 2365 / 2900 skipped
Shard 2366 / 2900 skipped
Shard 2367 / 2900 skipped
Shard 2368 / 2900 skipped
Shard 2369 / 2900 skipped
Shard 2370 / 2900 skipped
Shard 2371 / 2900 skipped
Shard 2372 / 2900 skipped
Shard 2373 / 2900 skipped
Shard 2374 / 2900 skipped
Shard 2375 / 2900 skipped
Shard 2376 / 2900 skipped
Shard 2377 / 2900 skipped
Shard 2378 / 2900 skipped
Shard 2379 / 2900 skipped
Shard 2380 / 2900 skipped
Shard 2381 / 2900 skipped
Shard 2382 / 2900 skipped
Shard 2383 / 2900 skipped
Shard 2384 / 2900 skipped
Shard 2385 / 2900 skipped
Shard 2386 / 2900 skipped
Shard 2387 / 2900 skipped
Shard 2388 / 2900 skipped
Shard 2389 / 2900 skipped
Shard 2390 / 2900 skipped
Shard 2391 / 2900 skipped
Shard 2392 / 2900 skipped
Shard 2393 / 2900 skipped
Shard 2394 / 2900 skipped
Shard 2395 / 2900 skipped
Shard 2396 / 2900 skipped
Shard 2397 / 2900 skipped
Shard 2398 / 2900 skipped
Shard 2399 / 2900 skipped
Shard 2400 / 2900 skipped
Shard 2401 / 2900 skipped
Shard 2402 / 2900 skipped
Shard 2403 / 2900 skipped
Shard 2404 / 2900 skipped
Shard 2405 / 2900 skipped
Shard 2406 / 2900 skipped
Shard 2407 / 2900 skipped
Shard 2408 / 2900 skipped
Shard 2409 / 2900 skipped
Shard 2410 / 2900 skipped
Shard 2411 / 2900 skipped
Shard 2412 / 2900 skipped
Shard 2413 / 2900 skipped
Shard 2414 / 2900 skipped
Shard 2415 / 2900 skipped
Shard 2416 / 2900 skipped
Shard 2417 / 2900 skipped
Shard 2418 / 2900 skipped
Shard 2419 / 2900 skipped
Shard 2420 / 2900 skipped
Shard 2421 / 2900 skipped
Shard 2422 / 2900 skipped
Shard 2423 / 2900 skipped
Shard 2424 / 2900 skipped
Shard 2425 / 2900 skipped
Shard 2426 / 2900 skipped
Shard 2427 / 2900 skipped
Shard 2428 / 2900 skipped
Shard 2429 / 2900 skipped
Shard 2430 / 2900 skipped
Shard 2431 / 2900 skipped
Shard 2432 / 2900 skipped
Shard 2433 / 2900 skipped
Shard 2434 / 2900 skipped
Shard 2435 / 2900 skipped
Shard 2436 / 2900 skipped
Shard 2437 / 2900 skipped
Shard 2438 / 2900 skipped
Shard 2439 / 2900 skipped
Shard 2440 / 2900 skipped
Shard 2441 / 2900 skipped
Shard 2442 / 2900 skipped
Shard 2443 / 2900 skipped
Shard 2444 / 2900 skipped
Shard 2445 / 2900 skipped
Shard 2446 / 2900 skipped
Shard 2447 / 2900 skipped
Shard 2448 / 2900 skipped
Shard 2449 / 2900 skipped
Shard 2450 / 2900 skipped
Shard 2451 / 2900 skipped
Shard 2452 / 2900 skipped
Shard 2453 / 2900 skipped
Shard 2454 / 2900 skipped
Shard 2455 / 2900 skipped
Shard 2456 / 2900 skipped
Shard 2457 / 2900 skipped
Shard 2458 / 2900 skipped
Shard 2459 / 2900 skipped
Shard 2460 / 2900 skipped
Shard 2461 / 2900 skipped
Shard 2462 / 2900 skipped
Shard 2463 / 2900 skipped
Shard 2464 / 2900 skipped
Shard 2465 / 2900 skipped
Shard 2466 / 2900 skipped
Shard 2467 / 2900 skipped
Shard 2468 / 2900 skipped
Shard 2469 / 2900 skipped
Shard 2470 / 2900 skipped
Shard 2471 / 2900 skipped
Shard 2472 / 2900 skipped
Shard 2473 / 2900 skipped
Shard 2474 / 2900 skipped
Shard 2475 / 2900 skipped
Shard 2476 / 2900 skipped
Shard 2477 / 2900 skipped
Shard 2478 / 2900 skipped
Shard 2479 / 2900 skipped
Shard 2480 / 2900 skipped
Shard 2481 / 2900 skipped
Shard 2482 / 2900 skipped
Shard 2483 / 2900 skipped
Shard 2484 / 2900 skipped
Shard 2485 / 2900 skipped
Shard 2486 / 2900 skipped
Shard 2487 / 2900 skipped
Shard 2488 / 2900 skipped
Shard 2489 / 2900 skipped
Shard 2490 / 2900 skipped
Shard 2491 / 2900 skipped
Shard 2492 / 2900 skipped
Shard 2493 / 2900 skipped
Shard 2494 / 2900 skipped
Shard 2495 / 2900 skipped
Shard 2496 / 2900 skipped
Shard 2497 / 2900 skipped
Shard 2498 / 2900 skipped
Shard 2499 / 2900 skipped
Shard 2500 / 2900 skipped
Shard 2501 / 2900 skipped
Shard 2502 / 2900 skipped
Shard 2503 / 2900 skipped
Shard 2504 / 2900 skipped
Shard 2505 / 2900 skipped
Shard 2506 / 2900 skipped
Shard 2507 / 2900 skipped
Shard 2508 / 2900 skipped
Shard 2509 / 2900 skipped
Shard 2510 / 2900 skipped
Shard 2511 / 2900 skipped
Shard 2512 / 2900 skipped
Shard 2513 / 2900 skipped
Shard 2514 / 2900 skipped
Shard 2515 / 2900 skipped
Shard 2516 / 2900 skipped
Shard 2517 / 2900 skipped
Shard 2518 / 2900 skipped
Shard 2519 / 2900 skipped
Shard 2520 / 2900 skipped
Shard 2521 / 2900 skipped
Shard 2522 / 2900 skipped
Shard 2523 / 2900 skipped
Shard 2524 / 2900 skipped
Shard 2525 / 2900 skipped
Shard 2526 / 2900 skipped
Shard 2527 / 2900 skipped
Shard 2528 / 2900 skipped
Shard 2529 / 2900 skipped
Shard 2530 / 2900 skipped
Shard 2531 / 2900 skipped
Shard 2532 / 2900 skipped
Shard 2533 / 2900 skipped
Shard 2534 / 2900 skipped
Shard 2535 / 2900 skipped
Shard 2536 / 2900 skipped
Shard 2537 / 2900 skipped
Shard 2538 / 2900 skipped
Shard 2539 / 2900 skipped
Shard 2540 / 2900 skipped
Shard 2541 / 2900 skipped
Shard 2542 / 2900 skipped
Shard 2543 / 2900 skipped
Shard 2544 / 2900 skipped
Shard 2545 / 2900 skipped
Shard 2546 / 2900 skipped
Shard 2547 / 2900 skipped
Shard 2548 / 2900 skipped
Shard 2549 / 2900 skipped
Shard 2550 / 2900 skipped
Shard 2551 / 2900 skipped
Shard 2552 / 2900 skipped
Shard 2553 / 2900 skipped
Shard 2554 / 2900 skipped
Shard 2555 / 2900 skipped
Shard 2556 / 2900 skipped
Shard 2557 / 2900 skipped
Shard 2558 / 2900 skipped
Shard 2559 / 2900 skipped
Shard 2560 / 2900 skipped
Shard 2561 / 2900 skipped
Shard 2562 / 2900 skipped
Shard 2563 / 2900 skipped
Shard 2564 / 2900 skipped
Shard 2565 / 2900 skipped
Shard 2566 / 2900 skipped
Shard 2567 / 2900 skipped
Shard 2568 / 2900 skipped
Shard 2569 / 2900 skipped
Shard 2570 / 2900 skipped
Shard 2571 / 2900 skipped
Shard 2572 / 2900 skipped
Shard 2573 / 2900 skipped
Shard 2574 / 2900 skipped
Shard 2575 / 2900 skipped
Shard 2576 / 2900 skipped
Shard 2577 / 2900 skipped
Shard 2578 / 2900 skipped
Shard 2579 / 2900 skipped
Shard 2580 / 2900 skipped
Shard 2581 / 2900 skipped
Shard 2582 / 2900 skipped
Shard 2583 / 2900 skipped
Shard 2584 / 2900 skipped
Shard 2585 / 2900 skipped
Shard 2586 / 2900 skipped
Shard 2587 / 2900 skipped
Shard 2588 / 2900 skipped
Shard 2589 / 2900 skipped
Shard 2590 / 2900 skipped
Shard 2591 / 2900 skipped
Shard 2592 / 2900 skipped
Shard 2593 / 2900 skipped
Shard 2594 / 2900 skipped
Shard 2595 / 2900 skipped
Shard 2596 / 2900 skipped
Shard 2597 / 2900 skipped
Shard 2598 / 2900 skipped
Shard 2599 / 2900 skipped
Shard 2600 / 2900 skipped
Shard 2601 / 2900 skipped
Shard 2602 / 2900 skipped
Shard 2603 / 2900 skipped
Shard 2604 / 2900 skipped
Shard 2605 / 2900 skipped
Shard 2606 / 2900 skipped
Shard 2607 / 2900 skipped
Shard 2608 / 2900 skipped
Shard 2609 / 2900 skipped
Shard 2610 / 2900 skipped
Shard 2611 / 2900 skipped
Shard 2612 / 2900 skipped
Shard 2613 / 2900 skipped
Shard 2614 / 2900 skipped
Shard 2615 / 2900 skipped
Shard 2616 / 2900 skipped
Shard 2617 / 2900 skipped
Shard 2618 / 2900 skipped
Shard 2619 / 2900 skipped
Shard 2620 / 2900 skipped
Shard 2621 / 2900 skipped
Shard 2622 / 2900 skipped
Shard 2623 / 2900 skipped
Shard 2624 / 2900 skipped
Shard 2625 / 2900 skipped
Shard 2626 / 2900 skipped
Shard 2627 / 2900 skipped
Shard 2628 / 2900 skipped
Shard 2629 / 2900 skipped
Shard 2630 / 2900 skipped
Shard 2631 / 2900 skipped
Shard 2632 / 2900 skipped
Shard 2633 / 2900 skipped
Shard 2634 / 2900 skipped
Shard 2635 / 2900 skipped
Shard 2636 / 2900 skipped
Shard 2637 / 2900 skipped
Shard 2638 / 2900 skipped
Shard 2639 / 2900 skipped
Shard 2640 / 2900 skipped
Shard 2641 / 2900 skipped
Shard 2642 / 2900 skipped
Shard 2643 / 2900 skipped
Shard 2644 / 2900 skipped
Shard 2645 / 2900 skipped
Shard 2646 / 2900 skipped
Shard 2647 / 2900 skipped
Shard 2648 / 2900 skipped
Shard 2649 / 2900 skipped
Shard 2650 / 2900 skipped
Shard 2651 / 2900 skipped
Shard 2652 / 2900 skipped
Shard 2653 / 2900 skipped
Shard 2654 / 2900 skipped
Shard 2655 / 2900 skipped
Shard 2656 / 2900 skipped
Shard 2657 / 2900 skipped
Shard 2658 / 2900 skipped
Shard 2659 / 2900 skipped
Shard 2660 / 2900 skipped
Shard 2661 / 2900 skipped
Shard 2662 / 2900 skipped
Shard 2663 / 2900 skipped
Shard 2664 / 2900 skipped
Shard 2665 / 2900 skipped
Shard 2666 / 2900 skipped
Shard 2667 / 2900 skipped
Shard 2668 / 2900 skipped
Shard 2669 / 2900 skipped
Shard 2670 / 2900 skipped
Shard 2671 / 2900 skipped
Shard 2672 / 2900 skipped
Shard 2673 / 2900 skipped
Shard 2674 / 2900 skipped
Shard 2675 / 2900 skipped
Shard 2676 / 2900 skipped
Shard 2677 / 2900 skipped
Shard 2678 / 2900 skipped
Shard 2679 / 2900 skipped
Shard 2680 / 2900 skipped
Shard 2681 / 2900 skipped
Shard 2682 / 2900 skipped
Shard 2683 / 2900 skipped
Shard 2684 / 2900 skipped
Shard 2685 / 2900 skipped
Shard 2686 / 2900 skipped
Shard 2687 / 2900 skipped
Shard 2688 / 2900 skipped
Shard 2689 / 2900 skipped
Shard 2690 / 2900 skipped
Shard 2691 / 2900 skipped
Shard 2692 / 2900 skipped
Shard 2693 / 2900 skipped
Shard 2694 / 2900 skipped
Shard 2695 / 2900 skipped
Shard 2696 / 2900 skipped
Shard 2697 / 2900 skipped
Shard 2698 / 2900 skipped
Shard 2699 / 2900 skipped
Shard 2700 / 2900 skipped
Shard 2701 / 2900 skipped
Shard 2702 / 2900 skipped
Shard 2703 / 2900 skipped
Shard 2704 / 2900 skipped
Shard 2705 / 2900 skipped
Shard 2706 / 2900 skipped
Shard 2707 / 2900 skipped
Shard 2708 / 2900 skipped
Shard 2709 / 2900 skipped
Shard 2710 / 2900 skipped
Shard 2711 / 2900 skipped
Shard 2712 / 2900 skipped
Shard 2713 / 2900 skipped
Shard 2714 / 2900 skipped
Shard 2715 / 2900 skipped
Shard 2716 / 2900 skipped
Shard 2717 / 2900 skipped
Shard 2718 / 2900 skipped
Shard 2719 / 2900 skipped
Shard 2720 / 2900 skipped
Shard 2721 / 2900 skipped
Shard 2722 / 2900 skipped
Shard 2723 / 2900 skipped
Shard 2724 / 2900 skipped
Shard 2725 / 2900 skipped
Shard 2726 / 2900 skipped
Shard 2727 / 2900 skipped
Shard 2728 / 2900 skipped
Shard 2729 / 2900 skipped
Shard 2730 / 2900 skipped
Shard 2731 / 2900 skipped
Shard 2732 / 2900 skipped
Shard 2733 / 2900 skipped
Shard 2734 / 2900 skipped
Shard 2735 / 2900 skipped
Shard 2736 / 2900 skipped
Shard 2737 / 2900 skipped
Shard 2738 / 2900 skipped
Shard 2739 / 2900 skipped
Shard 2740 / 2900 skipped
Shard 2741 / 2900 skipped
Shard 2742 / 2900 skipped
Shard 2743 / 2900 skipped
Shard 2744 / 2900 skipped
Shard 2745 / 2900 skipped
Shard 2746 / 2900 skipped
Shard 2747 / 2900 skipped
Shard 2748 / 2900 skipped
Shard 2749 / 2900 skipped
Shard 2750 / 2900 skipped
Shard 2751 / 2900 skipped
Shard 2752 / 2900 skipped
Shard 2753 / 2900 skipped
Shard 2754 / 2900 skipped
Shard 2755 / 2900 skipped
Shard 2756 / 2900 skipped
Shard 2757 / 2900 skipped
Shard 2758 / 2900 skipped
Shard 2759 / 2900 skipped
Shard 2760 / 2900 skipped
Shard 2761 / 2900 skipped
Shard 2762 / 2900 skipped
Shard 2763 / 2900 skipped
Shard 2764 / 2900 skipped
Shard 2765 / 2900 skipped
Shard 2766 / 2900 skipped
Shard 2767 / 2900 skipped
Shard 2768 / 2900 skipped
Shard 2769 / 2900 skipped
Shard 2770 / 2900 skipped
Shard 2771 / 2900 skipped
Shard 2772 / 2900 skipped
Shard 2773 / 2900 skipped
Shard 2774 / 2900 skipped
Shard 2775 / 2900 skipped
Shard 2776 / 2900 skipped
Shard 2777 / 2900 skipped
Shard 2778 / 2900 skipped
Shard 2779 / 2900 skipped
Shard 2780 / 2900 skipped
Shard 2781 / 2900 skipped
Shard 2782 / 2900 skipped
Shard 2783 / 2900 skipped
Shard 2784 / 2900 skipped
Shard 2785 / 2900 skipped
Shard 2786 / 2900 skipped
Shard 2787 / 2900 skipped
Shard 2788 / 2900 skipped
Shard 2789 / 2900 skipped
Shard 2790 / 2900 skipped
Shard 2791 / 2900 skipped
Shard 2792 / 2900 skipped
Shard 2793 / 2900 skipped
Shard 2794 / 2900 skipped
Shard 2795 / 2900 skipped
Shard 2796 / 2900 skipped
Shard 2797 / 2900 skipped
Shard 2798 / 2900 skipped
Shard 2799 / 2900 skipped
Shard 2800 / 2900 skipped
Shard 2801 / 2900 skipped
Shard 2802 / 2900 skipped
Shard 2803 / 2900 skipped
Shard 2804 / 2900 skipped
Shard 2805 / 2900 skipped
Shard 2806 / 2900 skipped
Shard 2807 / 2900 skipped
Shard 2808 / 2900 skipped
Shard 2809 / 2900 skipped
Shard 2810 / 2900 skipped
Shard 2811 / 2900 skipped
Shard 2812 / 2900 skipped
Shard 2813 / 2900 skipped
Shard 2814 / 2900 skipped
Shard 2815 / 2900 skipped
Shard 2816 / 2900 skipped
Shard 2817 / 2900 skipped
Shard 2818 / 2900 skipped
Shard 2819 / 2900 skipped
Shard 2820 / 2900 skipped
Shard 2821 / 2900 skipped
Shard 2822 / 2900 skipped
Shard 2823 / 2900 skipped
Shard 2824 / 2900 skipped
Shard 2825 / 2900 skipped
Shard 2826 / 2900 skipped
Shard 2827 / 2900 skipped
Shard 2828 / 2900 skipped
Shard 2829 / 2900 skipped
Shard 2830 / 2900 skipped
Shard 2831 / 2900 skipped
Shard 2832 / 2900 skipped
Shard 2833 / 2900 skipped
Shard 2834 / 2900 skipped
Shard 2835 / 2900 skipped
Shard 2836 / 2900 skipped
Shard 2837 / 2900 skipped
Shard 2838 / 2900 skipped
Shard 2839 / 2900 skipped
Shard 2840 / 2900 skipped
Shard 2841 / 2900 skipped
Shard 2842 / 2900 skipped
Shard 2843 / 2900 skipped
Shard 2844 / 2900 skipped
Shard 2845 / 2900 skipped
Shard 2846 / 2900 skipped
Shard 2847 / 2900 skipped
Shard 2848 / 2900 skipped
Shard 2849 / 2900 skipped
Shard 2850 / 2900 skipped
Shard 2851 / 2900 skipped
Shard 2852 / 2900 skipped
Shard 2853 / 2900 skipped
Shard 2854 / 2900 skipped
Shard 2855 / 2900 skipped
Shard 2856 / 2900 skipped
Shard 2857 / 2900 skipped
Shard 2858 / 2900 skipped
Shard 2859 / 2900 skipped
Shard 2860 / 2900 skipped
Shard 2861 / 2900 skipped
Shard 2862 / 2900 skipped
Shard 2863 / 2900 skipped
Shard 2864 / 2900 skipped
Shard 2865 / 2900 skipped
Shard 2866 / 2900 skipped
Shard 2867 / 2900 skipped
Shard 2868 / 2900 skipped
Shard 2869 / 2900 skipped
Shard 2870 / 2900 skipped
Shard 2871 / 2900 skipped
Shard 2872 / 2900 skipped
Shard 2873 / 2900 skipped
Shard 2874 / 2900 skipped
Shard 2875 / 2900 skipped
Shard 2876 / 2900 skipped
Shard 2877 / 2900 skipped
Shard 2878 / 2900 skipped
Shard 2879 / 2900 skipped
Shard 2880 / 2900 skipped
Shard 2881 / 2900 skipped
Shard 2882 / 2900 skipped
Shard 2883 / 2900 skipped
Shard 2884 / 2900 skipped
Shard 2885 / 2900 skipped
Shard 2886 / 2900 skipped
Shard 2887 / 2900 skipped
Shard 2888 / 2900 skipped
Shard 2889 / 2900 skipped
Shard 2890 / 2900 skipped
Shard 2891 / 2900 skipped
Shard 2892 / 2900 skipped
Shard 2893 / 2900 skipped
Shard 2894 / 2900 skipped
Shard 2895 / 2900 skipped
Shard 2896 / 2900 skipped
Shard 2897 / 2900 skipped
Shard 2898 / 2900 skipped
Shard 2899 / 2900 skipped
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2719
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:05:36,929] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2879/layer_07-model_states.pt.
  0%|          | 0/5198 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:07:03,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=0, lr=[1.0367747387037008e-05], mom=[(0.9, 0.999)]
steps: 2880 loss: 0.5542 iter time (s): 91.003 samples/sec: 1.407

100%|██████████| 1/1 [01:38<00:00, 98.49s/it][A100%|██████████| 1/1 [01:38<00:00, 98.49s/it]
 56%|█████▌    | 2901/5198 [01:38<01:18, 29.44it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.38s/it][A100%|██████████| 1/1 [01:26<00:00, 86.38s/it]
 56%|█████▌    | 2901/5198 [01:26<01:08, 33.58it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.97s/it][A100%|██████████| 1/1 [01:31<00:00, 91.97s/it]
 56%|█████▌    | 2901/5198 [01:31<01:12, 31.54it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.96s/it][A100%|██████████| 1/1 [01:35<00:00, 95.96s/it]
 56%|█████▌    | 2901/5198 [01:35<01:15, 30.23it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.78s/it][A100%|██████████| 1/1 [01:31<00:00, 91.78s/it]
 56%|█████▌    | 2901/5198 [01:31<01:12, 31.61it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.26s/it][A100%|██████████| 1/1 [01:34<00:00, 94.26s/it]
 56%|█████▌    | 2901/5198 [01:34<01:14, 30.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.01s/it][A100%|██████████| 1/1 [01:37<00:00, 97.01s/it]
 56%|█████▌    | 2901/5198 [01:37<01:16, 29.90it/s]
100%|██████████| 1/1 [01:31<00:00, 91.72s/it][A100%|██████████| 1/1 [01:31<00:00, 91.72s/it]
 56%|█████▌    | 2901/5198 [01:31<01:12, 31.63it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2720
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A 56%|█████▌    | 2901/5198 [01:42<01:12, 31.63it/s] 56%|█████▌    | 2901/5198 [01:50<01:18, 29.44it/s] 56%|█████▌    | 2901/5198 [01:50<01:16, 29.90it/s] 56%|█████▌    | 2901/5198 [01:40<01:08, 33.58it/s] 56%|█████▌    | 2901/5198 [01:50<01:15, 30.23it/s] 56%|█████▌    | 2901/5198 [01:50<01:14, 30.77it/s] 56%|█████▌    | 2901/5198 [01:50<01:12, 31.54it/s] 56%|█████▌    | 2901/5198 [01:50<01:12, 31.61it/s]
100%|██████████| 1/1 [01:29<00:00, 89.53s/it][A100%|██████████| 1/1 [01:29<00:00, 89.54s/it]
 56%|█████▌    | 2902/5198 [03:08<02:59, 12.80it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:08:39,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=2881, skipped=0, lr=[1.0362087565716749e-05], mom=[(0.9, 0.999)]
steps: 2881 loss: 0.5686 iter time (s): 91.180 samples/sec: 1.404

100%|██████████| 1/1 [01:31<00:00, 91.95s/it][A100%|██████████| 1/1 [01:31<00:00, 91.95s/it]
 56%|█████▌    | 2902/5198 [02:58<02:52, 13.33it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.96s/it][A100%|██████████| 1/1 [01:31<00:00, 91.96s/it]
 56%|█████▌    | 2902/5198 [03:03<02:56, 12.99it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.89s/it][A100%|██████████| 1/1 [01:31<00:00, 91.89s/it]
 56%|█████▌    | 2902/5198 [03:07<02:59, 12.77it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.94s/it][A100%|██████████| 1/1 [01:31<00:00, 91.94s/it]
 56%|█████▌    | 2902/5198 [03:03<02:56, 13.01it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.96s/it][A100%|██████████| 1/1 [01:31<00:00, 91.96s/it]
 56%|█████▌    | 2902/5198 [03:06<02:58, 12.86it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.98s/it][A100%|██████████| 1/1 [01:31<00:00, 91.98s/it]
 56%|█████▌    | 2902/5198 [03:09<03:00, 12.70it/s]
100%|██████████| 1/1 [01:31<00:00, 91.98s/it][A100%|██████████| 1/1 [01:31<00:00, 91.98s/it]
 56%|█████▌    | 2902/5198 [03:03<02:56, 13.01it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2721
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.61s/it][A100%|██████████| 1/1 [01:24<00:00, 84.61s/it]
 56%|█████▌    | 2903/5198 [04:32<05:15,  7.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:10:04,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=2882, skipped=0, lr=[1.0356427628550636e-05], mom=[(0.9, 0.999)]
steps: 2882 loss: 0.6201 iter time (s): 83.654 samples/sec: 1.530

100%|██████████| 1/1 [01:24<00:00, 84.33s/it][A100%|██████████| 1/1 [01:24<00:00, 84.33s/it]
 56%|█████▌    | 2903/5198 [04:22<05:08,  7.45it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.33s/it][A100%|██████████| 1/1 [01:24<00:00, 84.33s/it]
 56%|█████▌    | 2903/5198 [04:28<05:12,  7.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.44s/it][A100%|██████████| 1/1 [01:24<00:00, 84.44s/it]
 56%|█████▌    | 2903/5198 [04:32<05:15,  7.27it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.41s/it][A100%|██████████| 1/1 [01:24<00:00, 84.41s/it]
 56%|█████▌    | 2903/5198 [04:28<05:12,  7.34it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.42s/it][A100%|██████████| 1/1 [01:24<00:00, 84.42s/it]
 56%|█████▌    | 2903/5198 [04:30<05:14,  7.30it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.38s/it][A100%|██████████| 1/1 [01:24<00:00, 84.38s/it]
 56%|█████▌    | 2903/5198 [04:33<05:16,  7.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.40s/it][A100%|██████████| 1/1 [01:24<00:00, 84.40s/it]
 56%|█████▌    | 2903/5198 [04:28<05:12,  7.35it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2722
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.36s/it][A100%|██████████| 1/1 [01:24<00:00, 84.36s/it]
 56%|█████▌    | 2904/5198 [05:57<08:29,  4.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:11:28,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=2883, skipped=0, lr=[1.0350767577354517e-05], mom=[(0.9, 0.999)]
steps: 2883 loss: 0.5519 iter time (s): 83.630 samples/sec: 1.531

100%|██████████| 1/1 [01:24<00:00, 84.35s/it][A100%|██████████| 1/1 [01:24<00:00, 84.35s/it]
 56%|█████▌    | 2904/5198 [05:47<08:21,  4.57it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.35s/it][A100%|██████████| 1/1 [01:24<00:00, 84.35s/it]
 56%|█████▌    | 2904/5198 [05:52<08:26,  4.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.39s/it][A100%|██████████| 1/1 [01:24<00:00, 84.39s/it]
 56%|█████▌    | 2904/5198 [05:56<08:29,  4.50it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.40s/it][A100%|██████████| 1/1 [01:24<00:00, 84.40s/it]
 56%|█████▌    | 2904/5198 [05:52<08:26,  4.53it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.35s/it][A100%|██████████| 1/1 [01:24<00:00, 84.35s/it]
 56%|█████▌    | 2904/5198 [05:55<08:28,  4.52it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.36s/it][A100%|██████████| 1/1 [01:24<00:00, 84.36s/it]
 56%|█████▌    | 2904/5198 [05:52<08:26,  4.53it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2723

100%|██████████| 1/1 [01:24<00:00, 84.38s/it][A100%|██████████| 1/1 [01:24<00:00, 84.38s/it]
 56%|█████▌    | 2904/5198 [05:57<08:30,  4.50it/s]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.63s/it][A100%|██████████| 1/1 [01:32<00:00, 92.63s/it]
 56%|█████▌    | 2905/5198 [07:29<13:32,  2.82it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:13:01,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=2884, skipped=0, lr=[1.0345107413944277e-05], mom=[(0.9, 0.999)]
steps: 2884 loss: 0.5987 iter time (s): 92.113 samples/sec: 1.390

100%|██████████| 1/1 [01:32<00:00, 92.86s/it][A100%|██████████| 1/1 [01:32<00:00, 92.86s/it]
 56%|█████▌    | 2905/5198 [07:19<13:25,  2.85it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.89s/it][A100%|██████████| 1/1 [01:32<00:00, 92.89s/it]
 56%|█████▌    | 2905/5198 [07:25<13:29,  2.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.90s/it][A100%|██████████| 1/1 [01:32<00:00, 92.90s/it]
 56%|█████▌    | 2905/5198 [07:29<13:33,  2.82it/s]
100%|██████████| 1/1 [01:32<00:00, 92.82s/it][A100%|██████████| 1/1 [01:32<00:00, 92.82s/it]
 56%|█████▌    | 2905/5198 [07:25<13:29,  2.83it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.90s/it][A100%|██████████| 1/1 [01:32<00:00, 92.90s/it]
 56%|█████▌    | 2905/5198 [07:27<13:31,  2.82it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.87s/it][A100%|██████████| 1/1 [01:32<00:00, 92.87s/it]
 56%|█████▌    | 2905/5198 [07:25<13:29,  2.83it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2724
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.88s/it][A100%|██████████| 1/1 [01:32<00:00, 92.88s/it]
 56%|█████▌    | 2905/5198 [07:30<13:34,  2.82it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.58s/it][A100%|██████████| 1/1 [01:27<00:00, 87.58s/it]
 56%|█████▌    | 2906/5198 [08:57<20:19,  1.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:14:28,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=2885, skipped=0, lr=[1.0339447140135836e-05], mom=[(0.9, 0.999)]
steps: 2885 loss: 0.5519 iter time (s): 86.690 samples/sec: 1.477

100%|██████████| 1/1 [01:27<00:00, 87.38s/it][A100%|██████████| 1/1 [01:27<00:00, 87.39s/it]
 56%|█████▌    | 2906/5198 [08:47<20:11,  1.89it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.50s/it][A100%|██████████| 1/1 [01:27<00:00, 87.50s/it]
 56%|█████▌    | 2906/5198 [08:53<20:16,  1.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.42s/it][A100%|██████████| 1/1 [01:27<00:00, 87.42s/it]
 56%|█████▌    | 2906/5198 [08:57<20:19,  1.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.44s/it][A100%|██████████| 1/1 [01:27<00:00, 87.44s/it]
 56%|█████▌    | 2906/5198 [08:52<20:16,  1.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.43s/it][A100%|██████████| 1/1 [01:27<00:00, 87.43s/it]
 56%|█████▌    | 2906/5198 [08:55<20:18,  1.88it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.44s/it][A100%|██████████| 1/1 [01:27<00:00, 87.44s/it]
 56%|█████▌    | 2906/5198 [08:58<20:20,  1.88it/s]
100%|██████████| 1/1 [01:27<00:00, 87.46s/it][A100%|██████████| 1/1 [01:27<00:00, 87.46s/it]
 56%|█████▌    | 2906/5198 [08:52<20:16,  1.88it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2725
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.23s/it][A100%|██████████| 1/1 [01:32<00:00, 92.23s/it]
 56%|█████▌    | 2907/5198 [10:29<30:29,  1.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:16:01,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=2886, skipped=0, lr=[1.0333786757745157e-05], mom=[(0.9, 0.999)]
steps: 2886 loss: 0.5619 iter time (s): 91.554 samples/sec: 1.398

100%|██████████| 1/1 [01:32<00:00, 92.43s/it][A100%|██████████| 1/1 [01:32<00:00, 92.43s/it]
 56%|█████▌    | 2907/5198 [10:19<30:23,  1.26it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.28s/it][A100%|██████████| 1/1 [01:32<00:00, 92.28s/it]
 56%|█████▌    | 2907/5198 [10:25<30:27,  1.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.23s/it][A100%|██████████| 1/1 [01:32<00:00, 92.23s/it]
 56%|█████▌    | 2907/5198 [10:29<30:29,  1.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.34s/it][A100%|██████████| 1/1 [01:32<00:00, 92.35s/it]
 56%|█████▌    | 2907/5198 [10:25<30:27,  1.25it/s]
100%|██████████| 1/1 [01:32<00:00, 92.25s/it][A100%|██████████| 1/1 [01:32<00:00, 92.25s/it]
 56%|█████▌    | 2907/5198 [10:27<30:28,  1.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.26s/it][A100%|██████████| 1/1 [01:32<00:00, 92.26s/it]
 56%|█████▌    | 2907/5198 [10:30<30:30,  1.25it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.28s/it][A100%|██████████| 1/1 [01:32<00:00, 92.28s/it]
 56%|█████▌    | 2907/5198 [10:25<30:26,  1.25it/s]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2726
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.01s/it][A100%|██████████| 1/1 [01:44<00:00, 104.01s/it]
 56%|█████▌    | 2908/5198 [12:13<46:45,  1.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:17:45,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=2887, skipped=0, lr=[1.0328126268588218e-05], mom=[(0.9, 0.999)]
steps: 2887 loss: 0.5495 iter time (s): 103.708 samples/sec: 1.234

100%|██████████| 1/1 [01:44<00:00, 104.55s/it][A100%|██████████| 1/1 [01:44<00:00, 104.55s/it]
 56%|█████▌    | 2908/5198 [12:04<46:44,  1.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.60s/it][A100%|██████████| 1/1 [01:44<00:00, 104.60s/it]
 56%|█████▌    | 2908/5198 [12:09<46:48,  1.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.67s/it][A100%|██████████| 1/1 [01:44<00:00, 104.67s/it]
 56%|█████▌    | 2908/5198 [12:13<46:52,  1.23s/it]
100%|██████████| 1/1 [01:44<00:00, 104.54s/it][A100%|██████████| 1/1 [01:44<00:00, 104.54s/it]
 56%|█████▌    | 2908/5198 [12:09<46:48,  1.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.62s/it][A100%|██████████| 1/1 [01:44<00:00, 104.62s/it]
 56%|█████▌    | 2908/5198 [12:12<46:50,  1.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.60s/it][A100%|██████████| 1/1 [01:44<00:00, 104.60s/it]
 56%|█████▌    | 2908/5198 [12:09<46:48,  1.23s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2727

100%|██████████| 1/1 [01:44<00:00, 104.62s/it][ATraining on 128 of 128 sentences.
100%|██████████| 1/1 [01:44<00:00, 104.62s/it]
 56%|█████▌    | 2908/5198 [12:15<46:52,  1.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.91s/it][A100%|██████████| 1/1 [01:32<00:00, 92.91s/it]
 56%|█████▌    | 2909/5198 [13:46<1:07:16,  1.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:19:18,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=2888, skipped=0, lr=[1.0322465674481054e-05], mom=[(0.9, 0.999)]
steps: 2888 loss: 0.5450 iter time (s): 91.621 samples/sec: 1.397

100%|██████████| 1/1 [01:32<00:00, 92.38s/it][A100%|██████████| 1/1 [01:32<00:00, 92.38s/it]
 56%|█████▌    | 2909/5198 [13:36<1:07:08,  1.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.49s/it][A100%|██████████| 1/1 [01:32<00:00, 92.49s/it]
 56%|█████▌    | 2909/5198 [13:42<1:07:13,  1.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.42s/it][A100%|██████████| 1/1 [01:32<00:00, 92.42s/it]
 56%|█████▌    | 2909/5198 [13:46<1:07:16,  1.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.54s/it][A100%|██████████| 1/1 [01:32<00:00, 92.54s/it]
 56%|█████▌    | 2909/5198 [13:42<1:07:13,  1.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.47s/it][A100%|██████████| 1/1 [01:32<00:00, 92.47s/it]
 56%|█████▌    | 2909/5198 [13:44<1:07:15,  1.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.45s/it][A100%|██████████| 1/1 [01:32<00:00, 92.45s/it]
 56%|█████▌    | 2909/5198 [13:47<1:07:17,  1.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.48s/it][A100%|██████████| 1/1 [01:32<00:00, 92.48s/it]
 56%|█████▌    | 2909/5198 [13:42<1:07:13,  1.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2728
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.13s/it][A100%|██████████| 1/1 [01:28<00:00, 88.13s/it]
 56%|█████▌    | 2910/5198 [15:14<1:34:38,  2.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:20:46,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=2889, skipped=0, lr=[1.0316804977239723e-05], mom=[(0.9, 0.999)]
steps: 2889 loss: 0.5122 iter time (s): 87.241 samples/sec: 1.467

100%|██████████| 1/1 [01:28<00:00, 88.22s/it][A100%|██████████| 1/1 [01:28<00:00, 88.22s/it]
 56%|█████▌    | 2910/5198 [15:04<1:34:31,  2.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.21s/it][A100%|██████████| 1/1 [01:28<00:00, 88.21s/it]
 56%|█████▌    | 2910/5198 [15:10<1:34:36,  2.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.19s/it][A100%|██████████| 1/1 [01:28<00:00, 88.19s/it]
 56%|█████▌    | 2910/5198 [15:14<1:34:38,  2.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.18s/it][A100%|██████████| 1/1 [01:28<00:00, 88.18s/it]
 56%|█████▌    | 2910/5198 [15:10<1:34:36,  2.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.23s/it][A100%|██████████| 1/1 [01:28<00:00, 88.23s/it]
 56%|█████▌    | 2910/5198 [15:13<1:34:38,  2.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [01:28<00:00, 88.23s/it][A100%|██████████| 1/1 [01:28<00:00, 88.21s/it][A100%|██████████| 1/1 [01:28<00:00, 88.23s/it]
 56%|█████▌    | 2910/5198 [15:15<1:34:40,  2.48s/it]100%|██████████| 1/1 [01:28<00:00, 88.21s/it]
 56%|█████▌    | 2910/5198 [15:10<1:34:36,  2.48s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2729
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.37s/it][A100%|██████████| 1/1 [01:29<00:00, 89.37s/it]
 56%|█████▌    | 2911/5198 [16:44<2:13:28,  3.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:22:15,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=0, lr=[1.0311144178680315e-05], mom=[(0.9, 0.999)]
steps: 2890 loss: 0.5782 iter time (s): 88.394 samples/sec: 1.448

100%|██████████| 1/1 [01:29<00:00, 89.24s/it][A100%|██████████| 1/1 [01:29<00:00, 89.24s/it]
 56%|█████▌    | 2911/5198 [16:34<2:13:18,  3.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.12s/it][A100%|██████████| 1/1 [01:29<00:00, 89.12s/it]
 56%|█████▌    | 2911/5198 [16:39<2:13:21,  3.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.16s/it][A100%|██████████| 1/1 [01:29<00:00, 89.16s/it]
 56%|█████▌    | 2911/5198 [16:43<2:13:23,  3.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.15s/it][A100%|██████████| 1/1 [01:29<00:00, 89.15s/it]
 56%|█████▌    | 2911/5198 [16:39<2:13:20,  3.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.17s/it][A100%|██████████| 1/1 [01:29<00:00, 89.17s/it]
 56%|█████▌    | 2911/5198 [16:42<2:13:23,  3.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.15s/it][A100%|██████████| 1/1 [01:29<00:00, 89.15s/it]
 56%|█████▌    | 2911/5198 [16:44<2:13:24,  3.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.17s/it][A100%|██████████| 1/1 [01:29<00:00, 89.17s/it]
 56%|█████▌    | 2911/5198 [16:39<2:13:21,  3.50s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_181
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.00s/it][A100%|██████████| 1/1 [01:57<00:00, 117.00s/it]
 56%|█████▌    | 2912/5198 [18:41<3:24:45,  5.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:24:13,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=2891, skipped=0, lr=[1.0305483280618958e-05], mom=[(0.9, 0.999)]
steps: 2891 loss: 0.7323 iter time (s): 116.997 samples/sec: 1.094

100%|██████████| 1/1 [01:57<00:00, 117.78s/it][A100%|██████████| 1/1 [01:57<00:00, 117.78s/it]
 56%|█████▌    | 2912/5198 [18:32<3:25:04,  5.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.87s/it][A100%|██████████| 1/1 [01:57<00:00, 117.87s/it]
 56%|█████▌    | 2912/5198 [18:37<3:25:10,  5.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.92s/it][A100%|██████████| 1/1 [01:57<00:00, 117.92s/it]
 56%|█████▌    | 2912/5198 [18:41<3:25:14,  5.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.83s/it][A100%|██████████| 1/1 [01:57<00:00, 117.83s/it]
 56%|█████▌    | 2912/5198 [18:40<3:25:11,  5.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.93s/it][A100%|██████████| 1/1 [01:57<00:00, 117.93s/it]
 56%|█████▌    | 2912/5198 [18:37<3:25:12,  5.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:57<00:00, 117.88s/it][A100%|██████████| 1/1 [01:57<00:00, 117.88s/it]
 56%|█████▌    | 2912/5198 [18:42<3:25:14,  5.39s/it]
100%|██████████| 1/1 [01:57<00:00, 117.86s/it][A100%|██████████| 1/1 [01:57<00:00, 117.86s/it]
 56%|█████▌    | 2912/5198 [18:37<3:25:10,  5.39s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2730
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.05s/it][A100%|██████████| 1/1 [01:42<00:00, 102.05s/it]
 56%|█████▌    | 2913/5198 [20:23<4:49:26,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:25:55,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=2892, skipped=0, lr=[1.0299822284871801e-05], mom=[(0.9, 0.999)]
steps: 2892 loss: 0.5269 iter time (s): 100.861 samples/sec: 1.269

100%|██████████| 1/1 [01:41<00:00, 101.66s/it][A100%|██████████| 1/1 [01:41<00:00, 101.66s/it]
 56%|█████▌    | 2913/5198 [20:13<4:49:24,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.67s/it][A100%|██████████| 1/1 [01:41<00:00, 101.67s/it]
 56%|█████▌    | 2913/5198 [20:19<4:49:30,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.62s/it][A100%|██████████| 1/1 [01:41<00:00, 101.62s/it]
 56%|█████▌    | 2913/5198 [20:23<4:49:32,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.55s/it][A100%|██████████| 1/1 [01:41<00:00, 101.55s/it]
 56%|█████▌    | 2913/5198 [20:19<4:49:26,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.67s/it][A100%|██████████| 1/1 [01:41<00:00, 101.68s/it]
 56%|█████▌    | 2913/5198 [20:21<4:49:31,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.66s/it][A100%|██████████| 1/1 [01:41<00:00, 101.66s/it]
 56%|█████▌    | 2913/5198 [20:19<4:49:29,  7.60s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2731
Training on 128 of 128 sentences.

100%|██████████| 1/1 [01:41<00:00, 101.67s/it][A100%|██████████| 1/1 [01:41<00:00, 101.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][A 56%|█████▌    | 2913/5198 [20:24<4:49:33,  7.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.44s/it][A100%|██████████| 1/1 [01:20<00:00, 80.44s/it]
 56%|█████▌    | 2914/5198 [21:43<6:17:36,  9.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:27:14,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=2893, skipped=0, lr=[1.0294161193255041e-05], mom=[(0.9, 0.999)]
steps: 2893 loss: 0.5592 iter time (s): 78.982 samples/sec: 1.621

100%|██████████| 1/1 [01:19<00:00, 79.73s/it][A100%|██████████| 1/1 [01:19<00:00, 79.73s/it]
 56%|█████▌    | 2914/5198 [21:33<6:16:43,  9.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.76s/it][A100%|██████████| 1/1 [01:19<00:00, 79.76s/it]
 56%|█████▌    | 2914/5198 [21:39<6:16:50,  9.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.80s/it][A100%|██████████| 1/1 [01:19<00:00, 79.80s/it]
 56%|█████▌    | 2914/5198 [21:43<6:16:55,  9.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.83s/it][A100%|██████████| 1/1 [01:19<00:00, 79.83s/it]
 56%|█████▌    | 2914/5198 [21:39<6:16:51,  9.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.78s/it][A100%|██████████| 1/1 [01:19<00:00, 79.78s/it]
 56%|█████▌    | 2914/5198 [21:41<6:16:53,  9.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.73s/it][A100%|██████████| 1/1 [01:19<00:00, 79.73s/it]
 56%|█████▌    | 2914/5198 [21:44<6:16:52,  9.90s/it]
100%|██████████| 1/1 [01:19<00:00, 79.76s/it][A100%|██████████| 1/1 [01:19<00:00, 79.76s/it]
 56%|█████▌    | 2914/5198 [21:38<6:16:49,  9.90s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2732

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.06s/it][A100%|██████████| 1/1 [01:22<00:00, 82.06s/it]
 56%|█████▌    | 2915/5198 [23:05<8:16:54, 13.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:28:37,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=2894, skipped=0, lr=[1.0288500007584894e-05], mom=[(0.9, 0.999)]
steps: 2894 loss: 0.5461 iter time (s): 81.351 samples/sec: 1.573

100%|██████████| 1/1 [01:22<00:00, 82.18s/it][A100%|██████████| 1/1 [01:22<00:00, 82.18s/it]
 56%|█████▌    | 2915/5198 [22:55<8:16:13, 13.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.19s/it][A100%|██████████| 1/1 [01:22<00:00, 82.19s/it]
 56%|█████▌    | 2915/5198 [23:01<8:16:22, 13.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.13s/it][A100%|██████████| 1/1 [01:22<00:00, 82.13s/it]
 56%|█████▌    | 2915/5198 [23:05<8:16:19, 13.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.21s/it][A100%|██████████| 1/1 [01:22<00:00, 82.21s/it]
 56%|█████▌    | 2915/5198 [23:01<8:16:24, 13.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.19s/it][A100%|██████████| 1/1 [01:22<00:00, 82.19s/it]
 56%|█████▌    | 2915/5198 [23:03<8:16:24, 13.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.25s/it][A100%|██████████| 1/1 [01:22<00:00, 82.25s/it]
 56%|█████▌    | 2915/5198 [23:06<8:16:29, 13.05s/it]
100%|██████████| 1/1 [01:22<00:00, 82.25s/it][A100%|██████████| 1/1 [01:22<00:00, 82.25s/it]
 56%|█████▌    | 2915/5198 [23:01<8:16:26, 13.05s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2733

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.40s/it][A100%|██████████| 1/1 [01:40<00:00, 100.40s/it]
 56%|█████▌    | 2916/5198 [24:46<11:31:07, 18.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:30:18,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=2895, skipped=0, lr=[1.0282838729677606e-05], mom=[(0.9, 0.999)]
steps: 2895 loss: 0.5863 iter time (s): 100.071 samples/sec: 1.279

100%|██████████| 1/1 [01:40<00:00, 100.91s/it][A100%|██████████| 1/1 [01:40<00:00, 100.91s/it]
 56%|█████▌    | 2916/5198 [24:36<11:31:36, 18.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.89s/it][A100%|██████████| 1/1 [01:40<00:00, 100.89s/it]
 56%|█████▌    | 2916/5198 [24:42<11:31:40, 18.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.98s/it][A100%|██████████| 1/1 [01:40<00:00, 100.98s/it]
 56%|█████▌    | 2916/5198 [24:46<11:31:50, 18.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.89s/it][A100%|██████████| 1/1 [01:40<00:00, 100.89s/it]
 56%|█████▌    | 2916/5198 [24:42<11:31:41, 18.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.92s/it][A100%|██████████| 1/1 [01:40<00:00, 100.92s/it]
 56%|█████▌    | 2916/5198 [24:44<11:31:45, 18.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.87s/it][A100%|██████████| 1/1 [01:40<00:00, 100.87s/it]
 56%|█████▌    | 2916/5198 [24:42<11:31:41, 18.19s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2734

100%|██████████| 1/1 [01:40<00:00, 100.89s/it][A100%|██████████| 1/1 [01:40<00:00, 100.89s/it]
 56%|█████▌    | 2916/5198 [24:47<11:31:46, 18.19s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.14s/it][A100%|██████████| 1/1 [01:35<00:00, 95.14s/it]
 56%|█████▌    | 2917/5198 [26:21<15:16:36, 24.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:31:53,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=2896, skipped=0, lr=[1.027717736134946e-05], mom=[(0.9, 0.999)]
steps: 2896 loss: 0.5733 iter time (s): 94.181 samples/sec: 1.359

100%|██████████| 1/1 [01:34<00:00, 94.96s/it][A100%|██████████| 1/1 [01:34<00:00, 94.96s/it]
 56%|█████▌    | 2917/5198 [26:11<15:16:30, 24.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.97s/it][A100%|██████████| 1/1 [01:34<00:00, 94.97s/it]
 56%|█████▌    | 2917/5198 [26:17<15:16:35, 24.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.91s/it][A100%|██████████| 1/1 [01:34<00:00, 94.92s/it]
 56%|█████▌    | 2917/5198 [26:21<15:16:34, 24.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.98s/it][A100%|██████████| 1/1 [01:34<00:00, 94.98s/it]
 56%|█████▌    | 2917/5198 [26:17<15:16:38, 24.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.94s/it][A100%|██████████| 1/1 [01:34<00:00, 94.94s/it]
 56%|█████▌    | 2917/5198 [26:19<15:16:34, 24.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.96s/it][A100%|██████████| 1/1 [01:34<00:00, 94.96s/it]
 56%|█████▌    | 2917/5198 [26:22<15:16:38, 24.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.98s/it][A100%|██████████| 1/1 [01:34<00:00, 94.98s/it]
 56%|█████▌    | 2917/5198 [26:17<15:16:38, 24.11s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2735
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.75s/it][A100%|██████████| 1/1 [01:34<00:00, 94.75s/it]
 56%|█████▌    | 2918/5198 [27:56<19:43:00, 31.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:33:27,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=2897, skipped=0, lr=[1.0271515904416762e-05], mom=[(0.9, 0.999)]
steps: 2897 loss: 0.5527 iter time (s): 94.033 samples/sec: 1.361

100%|██████████| 1/1 [01:34<00:00, 94.93s/it][A100%|██████████| 1/1 [01:34<00:00, 94.93s/it]
 56%|█████▌    | 2918/5198 [27:46<19:43:16, 31.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.88s/it][A100%|██████████| 1/1 [01:34<00:00, 94.88s/it]
 56%|█████▌    | 2918/5198 [27:52<19:43:12, 31.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.89s/it][A100%|██████████| 1/1 [01:34<00:00, 94.89s/it]
 56%|█████▌    | 2918/5198 [27:52<19:43:14, 31.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.03s/it][A100%|██████████| 1/1 [01:35<00:00, 95.03s/it]
 56%|█████▌    | 2918/5198 [27:56<19:43:44, 31.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.96s/it][A100%|██████████| 1/1 [01:34<00:00, 94.96s/it]
 56%|█████▌    | 2918/5198 [27:54<19:43:27, 31.14s/it]
100%|██████████| 1/1 [01:34<00:00, 94.91s/it][A100%|██████████| 1/1 [01:34<00:00, 94.91s/it]
 56%|█████▌    | 2918/5198 [27:57<19:43:18, 31.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:34<00:00, 94.90s/it][A100%|██████████| 1/1 [01:34<00:00, 94.90s/it]
 56%|█████▌    | 2918/5198 [27:51<19:43:16, 31.14s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2736

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.05s/it][A100%|██████████| 1/1 [01:44<00:00, 104.06s/it]
 56%|█████▌    | 2919/5198 [29:40<25:26:36, 40.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:35:12,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=2898, skipped=0, lr=[1.0265854360695842e-05], mom=[(0.9, 0.999)]
steps: 2898 loss: 0.5586 iter time (s): 103.456 samples/sec: 1.237

100%|██████████| 1/1 [01:44<00:00, 104.21s/it][A100%|██████████| 1/1 [01:44<00:00, 104.21s/it]
 56%|█████▌    | 2919/5198 [29:30<25:27:28, 40.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.27s/it][A100%|██████████| 1/1 [01:44<00:00, 104.27s/it]
 56%|█████▌    | 2919/5198 [29:36<25:27:42, 40.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.14s/it][A100%|██████████| 1/1 [01:44<00:00, 104.14s/it]
 56%|█████▌    | 2919/5198 [29:40<25:27:38, 40.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.24s/it][A100%|██████████| 1/1 [01:44<00:00, 104.24s/it]
 56%|█████▌    | 2919/5198 [29:36<25:27:37, 40.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.23s/it][A100%|██████████| 1/1 [01:44<00:00, 104.23s/it]
 56%|█████▌    | 2919/5198 [29:38<25:27:44, 40.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.26s/it][A100%|██████████| 1/1 [01:44<00:00, 104.26s/it]
 56%|█████▌    | 2919/5198 [29:36<25:27:43, 40.22s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2737

100%|██████████| 1/1 [01:44<00:00, 104.27s/it][A100%|██████████| 1/1 [01:44<00:00, 104.27s/it]
 56%|█████▌    | 2919/5198 [29:41<25:27:48, 40.22s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.69s/it][A100%|██████████| 1/1 [01:23<00:00, 83.69s/it]
[2024-07-02 12:36:34,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=2899, skipped=0, lr=[1.0260192732003071e-05], mom=[(0.9, 0.999)]
steps: 2899 loss: 0.5883 iter time (s): 81.784 samples/sec: 1.565

100%|██████████| 1/1 [01:22<00:00, 82.64s/it][A100%|██████████| 1/1 [01:22<00:00, 82.64s/it]

100%|██████████| 1/1 [01:22<00:00, 82.59s/it][A100%|██████████| 1/1 [01:22<00:00, 82.59s/it]

100%|██████████| 1/1 [01:22<00:00, 82.53s/it][A100%|██████████| 1/1 [01:22<00:00, 82.53s/it]

100%|██████████| 1/1 [01:22<00:00, 82.60s/it][A100%|██████████| 1/1 [01:22<00:00, 82.60s/it]

100%|██████████| 1/1 [01:22<00:00, 82.53s/it][A100%|██████████| 1/1 [01:22<00:00, 82.53s/it]

100%|██████████| 1/1 [01:22<00:00, 82.54s/it][A100%|██████████| 1/1 [01:22<00:00, 82.54s/it]

100%|██████████| 1/1 [01:22<00:00, 82.56s/it][A100%|██████████| 1/1 [01:22<00:00, 82.56s/it]
Checkpointing at shard 2919
[2024-07-02 12:36:40,198] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2899 is about to be saved!
[2024-07-02 12:36:41,606] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_00-model_states.pt...
[2024-07-02 12:36:44,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_00-model_states.pt.
[2024-07-02 12:36:45,404] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_04-model_states.pt...
[2024-07-02 12:36:47,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_02-model_states.pt...
[2024-07-02 12:36:48,125] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_03-model_states.pt...
[2024-07-02 12:36:51,763] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_06-model_states.pt...
[2024-07-02 12:36:54,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_08-model_states.pt...
[2024-07-02 12:36:55,258] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_05-model_states.pt...
[2024-07-02 12:36:55,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_01-model_states.pt...
[2024-07-02 12:36:55,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_07-model_states.pt...
[2024-07-02 12:40:10,478] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_05-model_states.pt.
[2024-07-02 12:40:10,710] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_07-model_states.pt.
[2024-07-02 12:40:11,116] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_01-model_states.pt.
[2024-07-02 12:40:11,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_06-model_states.pt.
[2024-07-02 12:40:11,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_04_model_states.pt...
[2024-07-02 12:40:11,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_04-model_states.pt.
[2024-07-02 12:40:11,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_02-model_states.pt.
[2024-07-02 12:40:11,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_03_model_states.pt...
[2024-07-02 12:40:11,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_04_model_states.pt.
[2024-07-02 12:40:11,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
[2024-07-02 12:40:11,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_08-model_states.pt.
[2024-07-02 12:40:11,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_06_model_states.pt...
[2024-07-02 12:40:11,385] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_03_model_states.pt.
[2024-07-02 12:40:11,385] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
[2024-07-02 12:40:11,417] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_06_model_states.pt.
[2024-07-02 12:40:11,417] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
[2024-07-02 12:40:11,420] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_01_model_states.pt
[2024-07-02 12:40:11,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_01_model_states.pt...
[2024-07-02 12:40:11,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_01_model_states.pt.
[2024-07-02 12:40:11,494] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
[2024-07-02 12:40:11,563] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_00_model_states.pt
[2024-07-02 12:40:11,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_00_model_states.pt...
[2024-07-02 12:40:11,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_03-model_states.pt.
[2024-07-02 12:40:11,641] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_05_model_states.pt...
[2024-07-02 12:40:11,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_05_model_states.pt.
[2024-07-02 12:40:11,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
[2024-07-02 12:40:11,858] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_00_model_states.pt.
[2024-07-02 12:40:11,858] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
[2024-07-02 12:40:11,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_09-model_states.pt...
[2024-07-02 12:40:11,962] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_02_model_states.pt...
[2024-07-02 12:40:11,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_02_model_states.pt.
[2024-07-02 12:40:11,982] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
[2024-07-02 12:40:12,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/layer_09-model_states.pt.
[2024-07-02 12:40:12,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_07_model_states.pt...
[2024-07-02 12:40:12,427] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2899/mp_rank_07_model_states.pt.
[2024-07-02 12:40:12,427] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2899 is ready now!
Checkpoint saved using --- 216.84958744049072 seconds ---
 56%|█████▌    | 2920/5198 [34:35<50:09:55, 79.28s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2738
 56%|█████▌    | 2920/5198 [34:38<50:10:11, 79.29s/it] 56%|█████▌    | 2920/5198 [34:35<50:10:38, 79.30s/it] 56%|█████▌    | 2920/5198 [34:40<50:10:57, 79.31s/it] 56%|█████▌    | 2920/5198 [34:40<50:09:59, 79.28s/it] 56%|█████▌    | 2920/5198 [34:43<50:27:45, 79.75s/it] 56%|█████▌    | 2920/5198 [34:30<50:12:42, 79.35s/it] 56%|█████▌    | 2920/5198 [34:36<50:11:48, 79.33s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.10s/it][A100%|██████████| 1/1 [01:26<00:00, 86.10s/it]
 56%|█████▌    | 2921/5198 [36:09<51:10:06, 80.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:41:40,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=0, lr=[1.0254531020154839e-05], mom=[(0.9, 0.999)]
steps: 2900 loss: 0.5641 iter time (s): 88.073 samples/sec: 1.453

100%|██████████| 1/1 [01:28<00:00, 88.35s/it][A100%|██████████| 1/1 [01:28<00:00, 88.35s/it]
 56%|█████▌    | 2921/5198 [35:59<51:12:48, 80.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.43s/it][A100%|██████████| 1/1 [01:28<00:00, 88.44s/it]
 56%|█████▌    | 2921/5198 [36:04<51:12:40, 80.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.65s/it][A100%|██████████| 1/1 [01:28<00:00, 88.65s/it]
 56%|█████▌    | 2921/5198 [36:08<51:13:25, 80.99s/it]
100%|██████████| 1/1 [01:28<00:00, 88.66s/it][A100%|██████████| 1/1 [01:28<00:00, 88.66s/it]
 56%|█████▌    | 2921/5198 [36:04<51:13:14, 80.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.75s/it][A100%|██████████| 1/1 [01:28<00:00, 88.75s/it]
 56%|█████▌    | 2921/5198 [36:07<51:13:28, 80.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.82s/it][A100%|██████████| 1/1 [01:28<00:00, 88.82s/it]
 56%|█████▌    | 2921/5198 [36:09<51:13:46, 81.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.84s/it][A100%|██████████| 1/1 [01:28<00:00, 88.84s/it]
 56%|█████▌    | 2921/5198 [36:04<51:13:49, 81.00s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2739
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.63s/it][A100%|██████████| 1/1 [01:35<00:00, 95.63s/it]
 56%|█████▌    | 2922/5198 [37:44<53:01:44, 83.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:43:16,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=2901, skipped=0, lr=[1.024886922696756e-05], mom=[(0.9, 0.999)]
steps: 2901 loss: 0.5294 iter time (s): 95.072 samples/sec: 1.346

100%|██████████| 1/1 [01:36<00:00, 96.05s/it][A100%|██████████| 1/1 [01:36<00:00, 96.05s/it]
 56%|█████▌    | 2922/5198 [37:35<53:07:05, 84.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:36<00:00, 96.01s/it][A100%|██████████| 1/1 [01:36<00:00, 96.01s/it]
 56%|█████▌    | 2922/5198 [37:40<53:06:39, 84.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.90s/it][A100%|██████████| 1/1 [01:35<00:00, 95.90s/it]
 56%|█████▌    | 2922/5198 [37:44<53:06:27, 84.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.93s/it][A100%|██████████| 1/1 [01:35<00:00, 95.93s/it]
 56%|█████▌    | 2922/5198 [37:40<53:06:27, 84.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.96s/it][A100%|██████████| 1/1 [01:35<00:00, 95.96s/it]
 56%|█████▌    | 2922/5198 [37:43<53:06:54, 84.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:35<00:00, 95.95s/it][A100%|██████████| 1/1 [01:35<00:00, 95.95s/it]
 56%|█████▌    | 2922/5198 [37:45<53:07:02, 84.02s/it]
100%|██████████| 1/1 [01:35<00:00, 95.94s/it][A100%|██████████| 1/1 [01:35<00:00, 95.94s/it]
 56%|█████▌    | 2922/5198 [37:40<53:07:00, 84.02s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2740

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.36s/it][A100%|██████████| 1/1 [01:40<00:00, 100.36s/it]
 56%|█████▌    | 2923/5198 [39:25<55:22:06, 87.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:44:57,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=2902, skipped=0, lr=[1.0243207354257684e-05], mom=[(0.9, 0.999)]
steps: 2902 loss: 0.5483 iter time (s): 99.883 samples/sec: 1.281

100%|██████████| 1/1 [01:40<00:00, 100.67s/it][A100%|██████████| 1/1 [01:40<00:00, 100.67s/it]
 56%|█████▌    | 2923/5198 [39:15<55:27:10, 87.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.70s/it][A100%|██████████| 1/1 [01:40<00:00, 100.71s/it]
 56%|█████▌    | 2923/5198 [39:21<55:27:05, 87.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.74s/it][A100%|██████████| 1/1 [01:40<00:00, 100.74s/it]
 56%|█████▌    | 2923/5198 [39:25<55:27:20, 87.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.84s/it][A100%|██████████| 1/1 [01:40<00:00, 100.84s/it]
 56%|█████▌    | 2923/5198 [39:21<55:28:04, 87.77s/it]
100%|██████████| 1/1 [01:40<00:00, 100.73s/it][A100%|██████████| 1/1 [01:40<00:00, 100.73s/it]
 56%|█████▌    | 2923/5198 [39:23<55:27:28, 87.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.71s/it][A100%|██████████| 1/1 [01:40<00:00, 100.71s/it]
 56%|█████▌    | 2923/5198 [39:26<55:27:27, 87.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.72s/it][A100%|██████████| 1/1 [01:40<00:00, 100.72s/it]
 56%|█████▌    | 2923/5198 [39:21<55:27:29, 87.76s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2741
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.21s/it][A100%|██████████| 1/1 [01:39<00:00, 99.22s/it]
 56%|█████▋    | 2924/5198 [41:04<57:08:31, 90.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:46:36,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=2903, skipped=0, lr=[1.0237545403841673e-05], mom=[(0.9, 0.999)]
steps: 2903 loss: 0.5153 iter time (s): 98.293 samples/sec: 1.302

100%|██████████| 1/1 [01:39<00:00, 99.05s/it][A100%|██████████| 1/1 [01:39<00:00, 99.05s/it]
 56%|█████▋    | 2924/5198 [40:54<57:09:37, 90.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.11s/it][A100%|██████████| 1/1 [01:39<00:00, 99.11s/it]
 56%|█████▋    | 2924/5198 [41:00<57:10:08, 90.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.08s/it][A100%|██████████| 1/1 [01:39<00:00, 99.08s/it]
 56%|█████▋    | 2924/5198 [41:04<57:10:00, 90.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.05s/it][A100%|██████████| 1/1 [01:39<00:00, 99.06s/it]
 56%|█████▋    | 2924/5198 [41:00<57:10:20, 90.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.12s/it][A100%|██████████| 1/1 [01:39<00:00, 99.12s/it]
 56%|█████▋    | 2924/5198 [41:02<57:10:29, 90.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.13s/it][A100%|██████████| 1/1 [01:39<00:00, 99.13s/it]
 56%|█████▋    | 2924/5198 [41:05<57:10:32, 90.52s/it]
100%|██████████| 1/1 [01:39<00:00, 99.13s/it][A100%|██████████| 1/1 [01:39<00:00, 99.13s/it]
 56%|█████▋    | 2924/5198 [41:00<57:10:29, 90.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2742

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.40s/it][A100%|██████████| 1/1 [01:40<00:00, 100.40s/it]
 56%|█████▋    | 2925/5198 [42:45<58:45:09, 93.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:48:17,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=2904, skipped=0, lr=[1.0231883377536025e-05], mom=[(0.9, 0.999)]
steps: 2904 loss: 0.5605 iter time (s): 99.858 samples/sec: 1.282

100%|██████████| 1/1 [01:40<00:00, 100.67s/it][A100%|██████████| 1/1 [01:40<00:00, 100.67s/it]
 56%|█████▋    | 2925/5198 [42:35<58:47:21, 93.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.57s/it][A100%|██████████| 1/1 [01:40<00:00, 100.57s/it]
 56%|█████▋    | 2925/5198 [42:41<58:46:51, 93.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.71s/it][A100%|██████████| 1/1 [01:40<00:00, 100.71s/it]
 56%|█████▋    | 2925/5198 [42:45<58:48:04, 93.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.69s/it][A100%|██████████| 1/1 [01:40<00:00, 100.69s/it]
 56%|█████▋    | 2925/5198 [42:41<58:48:06, 93.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.69s/it][A100%|██████████| 1/1 [01:40<00:00, 100.69s/it]
 56%|█████▋    | 2925/5198 [42:43<58:48:10, 93.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.66s/it][A100%|██████████| 1/1 [01:40<00:00, 100.66s/it]
 56%|█████▋    | 2925/5198 [42:46<58:47:55, 93.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.67s/it][A100%|██████████| 1/1 [01:40<00:00, 100.67s/it]
 56%|█████▋    | 2925/5198 [42:41<58:47:59, 93.13s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2743
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
 56%|█████▋    | 2926/5198 [44:16<58:24:29, 92.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:49:48,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=2905, skipped=0, lr=[1.0226221277157257e-05], mom=[(0.9, 0.999)]
steps: 2905 loss: 0.5788 iter time (s): 90.119 samples/sec: 1.420

100%|██████████| 1/1 [01:30<00:00, 90.98s/it][A100%|██████████| 1/1 [01:30<00:00, 90.98s/it]
 56%|█████▋    | 2926/5198 [44:06<58:24:16, 92.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.04s/it][A100%|██████████| 1/1 [01:31<00:00, 91.04s/it]
 56%|█████▋    | 2926/5198 [44:12<58:24:30, 92.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.87s/it][A100%|██████████| 1/1 [01:30<00:00, 90.87s/it]
 56%|█████▋    | 2926/5198 [44:12<58:23:43, 92.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.96s/it][A100%|██████████| 1/1 [01:30<00:00, 90.96s/it]
 56%|█████▋    | 2926/5198 [44:16<58:24:34, 92.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.88s/it][A100%|██████████| 1/1 [01:30<00:00, 90.88s/it]
 56%|█████▋    | 2926/5198 [44:14<58:23:49, 92.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.93s/it][A100%|██████████| 1/1 [01:30<00:00, 90.93s/it]
 56%|█████▋    | 2926/5198 [44:17<58:24:09, 92.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.94s/it][A100%|██████████| 1/1 [01:30<00:00, 90.94s/it]
 56%|█████▋    | 2926/5198 [44:12<58:24:13, 92.54s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2744
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.60s/it][A100%|██████████| 1/1 [01:26<00:00, 86.60s/it]
 56%|█████▋    | 2927/5198 [45:43<57:22:59, 90.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:51:14,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=2906, skipped=0, lr=[1.0220559104521914e-05], mom=[(0.9, 0.999)]
steps: 2906 loss: 0.5722 iter time (s): 85.910 samples/sec: 1.490

100%|██████████| 1/1 [01:26<00:00, 86.68s/it][A100%|██████████| 1/1 [01:26<00:00, 86.69s/it]
 56%|█████▋    | 2927/5198 [45:33<57:21:22, 90.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.72s/it][A100%|██████████| 1/1 [01:26<00:00, 86.72s/it]
 56%|█████▋    | 2927/5198 [45:39<57:21:55, 90.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.69s/it][A100%|██████████| 1/1 [01:26<00:00, 86.69s/it]
 56%|█████▋    | 2927/5198 [45:43<57:21:37, 90.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.75s/it][A100%|██████████| 1/1 [01:26<00:00, 86.75s/it]
 56%|█████▋    | 2927/5198 [45:38<57:21:38, 90.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.74s/it][A100%|██████████| 1/1 [01:26<00:00, 86.74s/it]
 56%|█████▋    | 2927/5198 [45:41<57:21:36, 90.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.73s/it][A100%|██████████| 1/1 [01:26<00:00, 86.73s/it]
 56%|█████▋    | 2927/5198 [45:44<57:21:41, 90.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.74s/it][A100%|██████████| 1/1 [01:26<00:00, 86.74s/it]
 56%|█████▋    | 2927/5198 [45:38<57:21:47, 90.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_182
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:10<00:00, 130.20s/it][A100%|██████████| 1/1 [02:10<00:00, 130.20s/it]
 56%|█████▋    | 2928/5198 [47:53<64:24:46, 102.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:53:26,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=2907, skipped=0, lr=[1.0214896861446561e-05], mom=[(0.9, 0.999)]
steps: 2907 loss: 0.7825 iter time (s): 130.798 samples/sec: 0.979

100%|██████████| 1/1 [02:11<00:00, 131.65s/it][A100%|██████████| 1/1 [02:11<00:00, 131.65s/it]
 56%|█████▋    | 2928/5198 [47:44<64:37:11, 102.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.59s/it][A100%|██████████| 1/1 [02:11<00:00, 131.59s/it]
 56%|█████▋    | 2928/5198 [47:50<64:36:57, 102.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.67s/it][A100%|██████████| 1/1 [02:11<00:00, 131.67s/it]
 56%|█████▋    | 2928/5198 [47:54<64:37:37, 102.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.70s/it][A100%|██████████| 1/1 [02:11<00:00, 131.70s/it]
 56%|█████▋    | 2928/5198 [47:50<64:38:01, 102.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.68s/it][A100%|██████████| 1/1 [02:11<00:00, 131.68s/it]
 56%|█████▋    | 2928/5198 [47:53<64:37:43, 102.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.66s/it][A100%|██████████| 1/1 [02:11<00:00, 131.66s/it]
 56%|█████▋    | 2928/5198 [47:55<64:37:36, 102.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:11<00:00, 131.67s/it][A100%|██████████| 1/1 [02:11<00:00, 131.67s/it]
 56%|█████▋    | 2928/5198 [47:50<64:37:44, 102.50s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2745
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:42<00:00, 102.53s/it][A100%|██████████| 1/1 [01:42<00:00, 102.53s/it]
 56%|█████▋    | 2929/5198 [49:36<64:28:57, 102.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:55:08,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=2908, skipped=0, lr=[1.0209234549747786e-05], mom=[(0.9, 0.999)]
steps: 2908 loss: 0.5562 iter time (s): 100.985 samples/sec: 1.268

100%|██████████| 1/1 [01:41<00:00, 101.89s/it][A100%|██████████| 1/1 [01:41<00:00, 101.89s/it]
 56%|█████▋    | 2929/5198 [49:26<64:29:11, 102.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.95s/it][A100%|██████████| 1/1 [01:41<00:00, 101.95s/it]
 56%|█████▋    | 2929/5198 [49:32<64:29:42, 102.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.86s/it][A100%|██████████| 1/1 [01:41<00:00, 101.86s/it]
 56%|█████▋    | 2929/5198 [49:36<64:29:09, 102.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.84s/it][A100%|██████████| 1/1 [01:41<00:00, 101.84s/it]
 56%|█████▋    | 2929/5198 [49:32<64:29:09, 102.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.82s/it][A100%|██████████| 1/1 [01:41<00:00, 101.82s/it]
 56%|█████▋    | 2929/5198 [49:34<64:28:47, 102.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.86s/it][A100%|██████████| 1/1 [01:41<00:00, 101.86s/it]
 56%|█████▋    | 2929/5198 [49:37<64:29:05, 102.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:41<00:00, 101.84s/it][A100%|██████████| 1/1 [01:41<00:00, 101.84s/it]
 56%|█████▋    | 2929/5198 [49:32<64:28:57, 102.31s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2746
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.64s/it][A100%|██████████| 1/1 [01:29<00:00, 89.64s/it]
 56%|█████▋    | 2930/5198 [51:06<62:09:41, 98.67s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:56:37,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=2909, skipped=0, lr=[1.0203572171242198e-05], mom=[(0.9, 0.999)]
steps: 2909 loss: 0.5564 iter time (s): 88.634 samples/sec: 1.444

100%|██████████| 1/1 [01:29<00:00, 89.41s/it][A100%|██████████| 1/1 [01:29<00:00, 89.41s/it]
 56%|█████▋    | 2930/5198 [50:56<62:05:13, 98.55s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.40s/it][A100%|██████████| 1/1 [01:29<00:00, 89.40s/it]
 56%|█████▋    | 2930/5198 [51:01<62:05:30, 98.56s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.42s/it][A100%|██████████| 1/1 [01:29<00:00, 89.42s/it]
 56%|█████▋    | 2930/5198 [51:05<62:05:20, 98.55s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.41s/it][A100%|██████████| 1/1 [01:29<00:00, 89.41s/it]
 56%|█████▋    | 2930/5198 [51:01<62:05:15, 98.55s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.46s/it][A100%|██████████| 1/1 [01:29<00:00, 89.46s/it]
 56%|█████▋    | 2930/5198 [51:04<62:05:37, 98.56s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.44s/it][A100%|██████████| 1/1 [01:29<00:00, 89.44s/it]
 56%|█████▋    | 2930/5198 [51:07<62:05:25, 98.56s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.43s/it][A100%|██████████| 1/1 [01:29<00:00, 89.43s/it]
 56%|█████▋    | 2930/5198 [51:01<62:05:22, 98.55s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2747
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.86s/it][A100%|██████████| 1/1 [01:37<00:00, 97.86s/it]
 56%|█████▋    | 2931/5198 [52:44<62:00:54, 98.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:58:16,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=0, lr=[1.0197909727746426e-05], mom=[(0.9, 0.999)]
steps: 2910 loss: 0.6070 iter time (s): 97.458 samples/sec: 1.313

100%|██████████| 1/1 [01:38<00:00, 98.32s/it][A100%|██████████| 1/1 [01:38<00:00, 98.32s/it]
 56%|█████▋    | 2931/5198 [52:34<62:01:09, 98.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.28s/it][A100%|██████████| 1/1 [01:38<00:00, 98.28s/it]
 56%|█████▋    | 2931/5198 [52:40<62:00:53, 98.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.21s/it][A100%|██████████| 1/1 [01:38<00:00, 98.21s/it]
 56%|█████▋    | 2931/5198 [52:44<62:00:03, 98.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.29s/it][A100%|██████████| 1/1 [01:38<00:00, 98.29s/it]
 56%|█████▋    | 2931/5198 [52:40<62:00:47, 98.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.24s/it][A100%|██████████| 1/1 [01:38<00:00, 98.24s/it]
 56%|█████▋    | 2931/5198 [52:42<62:00:32, 98.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.27s/it][A100%|██████████| 1/1 [01:38<00:00, 98.27s/it]
 56%|█████▋    | 2931/5198 [52:45<62:00:41, 98.47s/it]
100%|██████████| 1/1 [01:38<00:00, 98.26s/it][A100%|██████████| 1/1 [01:38<00:00, 98.26s/it]
 56%|█████▋    | 2931/5198 [52:40<62:00:34, 98.47s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2748

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.24s/it][A100%|██████████| 1/1 [01:25<00:00, 85.24s/it]
 56%|█████▋    | 2932/5198 [54:09<59:33:45, 94.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 12:59:41,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=2911, skipped=0, lr=[1.0192247221077128e-05], mom=[(0.9, 0.999)]
steps: 2911 loss: 0.5967 iter time (s): 84.237 samples/sec: 1.520

100%|██████████| 1/1 [01:24<00:00, 84.97s/it][A100%|██████████| 1/1 [01:24<00:00, 84.97s/it]
 56%|█████▋    | 2932/5198 [53:59<59:28:32, 94.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.99s/it][A100%|██████████| 1/1 [01:24<00:00, 84.99s/it]
 56%|█████▋    | 2932/5198 [54:05<59:28:35, 94.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.05s/it][A100%|██████████| 1/1 [01:25<00:00, 85.05s/it]
 56%|█████▋    | 2932/5198 [54:09<59:28:46, 94.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.06s/it][A100%|██████████| 1/1 [01:25<00:00, 85.06s/it]
 56%|█████▋    | 2932/5198 [54:05<59:29:17, 94.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.09s/it][A100%|██████████| 1/1 [01:25<00:00, 85.09s/it]
 56%|█████▋    | 2932/5198 [54:07<59:29:26, 94.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.05s/it][A100%|██████████| 1/1 [01:25<00:00, 85.05s/it]
 56%|█████▋    | 2932/5198 [54:10<59:29:08, 94.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:25<00:00, 85.08s/it][A100%|██████████| 1/1 [01:25<00:00, 85.08s/it]
 56%|█████▋    | 2932/5198 [54:05<59:29:17, 94.51s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2749
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.65s/it][A100%|██████████| 1/1 [02:00<00:00, 120.65s/it]
 56%|█████▋    | 2933/5198 [56:10<64:26:28, 102.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:01:43,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=2912, skipped=0, lr=[1.0186584653050972e-05], mom=[(0.9, 0.999)]
steps: 2912 loss: 0.5335 iter time (s): 121.108 samples/sec: 1.057

100%|██████████| 1/1 [02:01<00:00, 121.96s/it][A100%|██████████| 1/1 [02:01<00:00, 121.96s/it]
 56%|█████▋    | 2933/5198 [56:01<64:35:15, 102.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.02s/it][A100%|██████████| 1/1 [02:02<00:00, 122.02s/it]
 56%|█████▋    | 2933/5198 [56:07<64:36:01, 102.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.90s/it][A100%|██████████| 1/1 [02:01<00:00, 121.90s/it]
 56%|█████▋    | 2933/5198 [56:07<64:35:00, 102.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.03s/it][A100%|██████████| 1/1 [02:02<00:00, 122.03s/it]
 56%|█████▋    | 2933/5198 [56:11<64:36:08, 102.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.92s/it][A100%|██████████| 1/1 [02:01<00:00, 121.92s/it]
 56%|█████▋    | 2933/5198 [56:09<64:35:25, 102.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.93s/it][A100%|██████████| 1/1 [02:01<00:00, 121.93s/it]
 56%|█████▋    | 2933/5198 [56:12<64:35:18, 102.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.92s/it][A100%|██████████| 1/1 [02:01<00:00, 121.92s/it]
 56%|█████▋    | 2933/5198 [56:07<64:35:20, 102.66s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2750
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.89s/it][A100%|██████████| 1/1 [01:37<00:00, 97.89s/it]
 56%|█████▋    | 2934/5198 [57:48<63:37:24, 101.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:03:20,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=2913, skipped=0, lr=[1.0180922025484658e-05], mom=[(0.9, 0.999)]
steps: 2913 loss: 0.5911 iter time (s): 96.712 samples/sec: 1.324

100%|██████████| 1/1 [01:37<00:00, 97.46s/it][A100%|██████████| 1/1 [01:37<00:00, 97.46s/it]
 56%|█████▋    | 2934/5198 [57:39<63:35:17, 101.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.37s/it][A100%|██████████| 1/1 [01:37<00:00, 97.37s/it]
 56%|█████▋    | 2934/5198 [57:44<63:34:47, 101.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.47s/it][A100%|██████████| 1/1 [01:37<00:00, 97.47s/it]
 56%|█████▋    | 2934/5198 [57:48<63:35:59, 101.13s/it]
100%|██████████| 1/1 [01:37<00:00, 97.49s/it][A100%|██████████| 1/1 [01:37<00:00, 97.49s/it]
 56%|█████▋    | 2934/5198 [57:44<63:35:26, 101.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.49s/it][A100%|██████████| 1/1 [01:37<00:00, 97.49s/it]
 56%|█████▋    | 2934/5198 [57:47<63:35:40, 101.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.52s/it][A100%|██████████| 1/1 [01:37<00:00, 97.52s/it]
 56%|█████▋    | 2934/5198 [57:44<63:35:56, 101.13s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2751
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.56s/it][A100%|██████████| 1/1 [01:37<00:00, 97.56s/it]
 56%|█████▋    | 2934/5198 [57:49<63:36:22, 101.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.82s/it][A100%|██████████| 1/1 [01:24<00:00, 84.82s/it]
 56%|█████▋    | 2935/5198 [59:14<60:34:32, 96.36s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:04:45,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=2914, skipped=0, lr=[1.017525934019489e-05], mom=[(0.9, 0.999)]
steps: 2914 loss: 0.5682 iter time (s): 83.854 samples/sec: 1.526

100%|██████████| 1/1 [01:24<00:00, 84.70s/it][A100%|██████████| 1/1 [01:24<00:00, 84.70s/it]
 56%|█████▋    | 2935/5198 [59:03<60:28:54, 96.21s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.75s/it][A100%|██████████| 1/1 [01:24<00:00, 84.75s/it]
 56%|█████▋    | 2935/5198 [59:09<60:29:01, 96.22s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.60s/it][A100%|██████████| 1/1 [01:24<00:00, 84.60s/it]
 56%|█████▋    | 2935/5198 [59:13<60:28:13, 96.20s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.70s/it][A100%|██████████| 1/1 [01:24<00:00, 84.70s/it]
 56%|█████▋    | 2935/5198 [59:09<60:28:58, 96.22s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.65s/it][A100%|██████████| 1/1 [01:24<00:00, 84.65s/it]
 56%|█████▋    | 2935/5198 [59:11<60:28:41, 96.21s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.61s/it][A100%|██████████| 1/1 [01:24<00:00, 84.61s/it]
 56%|█████▋    | 2935/5198 [59:14<60:28:36, 96.21s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.65s/it][A100%|██████████| 1/1 [01:24<00:00, 84.65s/it]
 56%|█████▋    | 2935/5198 [59:09<60:28:44, 96.21s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2752
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:56<00:00, 116.87s/it][A100%|██████████| 1/1 [01:56<00:00, 116.87s/it]
 56%|█████▋    | 2936/5198 [1:01:11<64:27:41, 102.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:06:43,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=2915, skipped=0, lr=[1.0169596598998394e-05], mom=[(0.9, 0.999)]
steps: 2915 loss: 0.5889 iter time (s): 117.386 samples/sec: 1.090

100%|██████████| 1/1 [01:58<00:00, 118.19s/it][A100%|██████████| 1/1 [01:58<00:00, 118.19s/it]
 56%|█████▋    | 2936/5198 [1:01:01<64:35:11, 102.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.19s/it][A100%|██████████| 1/1 [01:58<00:00, 118.19s/it]
 56%|█████▋    | 2936/5198 [1:01:07<64:35:14, 102.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.21s/it][A100%|██████████| 1/1 [01:58<00:00, 118.21s/it]
 56%|█████▋    | 2936/5198 [1:01:11<64:34:49, 102.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.19s/it][A100%|██████████| 1/1 [01:58<00:00, 118.19s/it]
 56%|█████▋    | 2936/5198 [1:01:07<64:35:06, 102.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.17s/it][A100%|██████████| 1/1 [01:58<00:00, 118.17s/it]
 56%|█████▋    | 2936/5198 [1:01:09<64:34:47, 102.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:58<00:00, 118.17s/it][A100%|██████████| 1/1 [01:58<00:00, 118.17s/it]
 56%|█████▋    | 2936/5198 [1:01:07<64:34:46, 102.78s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2753

100%|██████████| 1/1 [01:58<00:00, 118.19s/it][A100%|██████████| 1/1 [01:58<00:00, 118.20s/it]
 56%|█████▋    | 2936/5198 [1:01:12<64:34:57, 102.78s/it]Training on 128 of 128 sentences.


  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:24<00:00, 84.44s/it][A100%|██████████| 1/1 [01:24<00:00, 84.44s/it]
 57%|█████▋    | 2937/5198 [1:02:35<61:03:23, 97.22s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:08:07,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=2916, skipped=0, lr=[1.016393380371193e-05], mom=[(0.9, 0.999)]
steps: 2916 loss: 0.5883 iter time (s): 82.795 samples/sec: 1.546

100%|██████████| 1/1 [01:23<00:00, 83.59s/it][A100%|██████████| 1/1 [01:23<00:00, 83.59s/it]
 57%|█████▋    | 2937/5198 [1:02:25<60:57:09, 97.05s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.57s/it][A100%|██████████| 1/1 [01:23<00:00, 83.57s/it]
 57%|█████▋    | 2937/5198 [1:02:31<60:56:50, 97.04s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.59s/it][A100%|██████████| 1/1 [01:23<00:00, 83.59s/it]
 57%|█████▋    | 2937/5198 [1:02:35<60:56:44, 97.04s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.56s/it][A100%|██████████| 1/1 [01:23<00:00, 83.56s/it]
 57%|█████▋    | 2937/5198 [1:02:31<60:56:41, 97.04s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.64s/it][A100%|██████████| 1/1 [01:23<00:00, 83.64s/it]
 57%|█████▋    | 2937/5198 [1:02:33<60:57:17, 97.05s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:23<00:00, 83.60s/it][A100%|██████████| 1/1 [01:23<00:00, 83.60s/it]
 57%|█████▋    | 2937/5198 [1:02:36<60:57:01, 97.05s/it] 
100%|██████████| 1/1 [01:23<00:00, 83.61s/it][A100%|██████████| 1/1 [01:23<00:00, 83.61s/it]
 57%|█████▋    | 2937/5198 [1:02:31<60:57:04, 97.05s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2754

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.51s/it][A100%|██████████| 1/1 [01:38<00:00, 98.51s/it]
 57%|█████▋    | 2938/5198 [1:04:14<61:19:04, 97.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:09:46,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=2917, skipped=0, lr=[1.0158270956152257e-05], mom=[(0.9, 0.999)]
steps: 2917 loss: 0.5499 iter time (s): 98.416 samples/sec: 1.301

100%|██████████| 1/1 [01:39<00:00, 99.28s/it][A100%|██████████| 1/1 [01:39<00:00, 99.28s/it]
 57%|█████▋    | 2938/5198 [1:04:10<61:20:37, 97.72s/it]
100%|██████████| 1/1 [01:39<00:00, 99.38s/it][A100%|██████████| 1/1 [01:39<00:00, 99.38s/it]
 57%|█████▋    | 2938/5198 [1:04:04<61:21:58, 97.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.33s/it][A100%|██████████| 1/1 [01:39<00:00, 99.33s/it]
 57%|█████▋    | 2938/5198 [1:04:14<61:21:08, 97.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.31s/it][A100%|██████████| 1/1 [01:39<00:00, 99.31s/it]
 57%|█████▋    | 2938/5198 [1:04:10<61:20:50, 97.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.28s/it][A100%|██████████| 1/1 [01:39<00:00, 99.28s/it]
 57%|█████▋    | 2938/5198 [1:04:12<61:20:56, 97.72s/it]
100%|██████████| 1/1 [01:39<00:00, 99.25s/it][A100%|██████████| 1/1 [01:39<00:00, 99.25s/it]
 57%|█████▋    | 2938/5198 [1:04:15<61:20:26, 97.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:39<00:00, 99.29s/it][A100%|██████████| 1/1 [01:39<00:00, 99.29s/it]
 57%|█████▋    | 2938/5198 [1:04:10<61:20:51, 97.72s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2755
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.76s/it][A100%|██████████| 1/1 [01:40<00:00, 100.76s/it]
 57%|█████▋    | 2939/5198 [1:05:55<61:54:37, 98.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:11:27,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=2918, skipped=0, lr=[1.0152608058136152e-05], mom=[(0.9, 0.999)]
steps: 2918 loss: 0.5692 iter time (s): 100.066 samples/sec: 1.279

100%|██████████| 1/1 [01:40<00:00, 100.78s/it][A100%|██████████| 1/1 [01:40<00:00, 100.78s/it]
 57%|█████▋    | 2939/5198 [1:05:45<61:54:39, 98.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.85s/it][A100%|██████████| 1/1 [01:40<00:00, 100.86s/it]
 57%|█████▋    | 2939/5198 [1:05:51<61:54:32, 98.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.80s/it][A100%|██████████| 1/1 [01:40<00:00, 100.80s/it]
 57%|█████▋    | 2939/5198 [1:05:55<61:54:15, 98.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.88s/it][A100%|██████████| 1/1 [01:40<00:00, 100.88s/it]
 57%|█████▋    | 2939/5198 [1:05:51<61:55:00, 98.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.88s/it][A100%|██████████| 1/1 [01:40<00:00, 100.88s/it]
 57%|█████▋    | 2939/5198 [1:05:53<61:54:59, 98.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.91s/it][A100%|██████████| 1/1 [01:40<00:00, 100.91s/it]
 57%|█████▋    | 2939/5198 [1:05:56<61:55:00, 98.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:40<00:00, 100.88s/it][A100%|██████████| 1/1 [01:40<00:00, 100.88s/it]
 57%|█████▋    | 2939/5198 [1:05:51<61:55:00, 98.67s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2756
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.38s/it][A100%|██████████| 1/1 [01:28<00:00, 88.39s/it]
[2024-07-02 13:12:55,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=2919, skipped=0, lr=[1.0146945111480426e-05], mom=[(0.9, 0.999)]
steps: 2919 loss: 0.5162 iter time (s): 87.385 samples/sec: 1.465

100%|██████████| 1/1 [01:28<00:00, 88.24s/it][A100%|██████████| 1/1 [01:28<00:00, 88.24s/it]

100%|██████████| 1/1 [01:28<00:00, 88.28s/it][A100%|██████████| 1/1 [01:28<00:00, 88.28s/it]

100%|██████████| 1/1 [01:28<00:00, 88.23s/it][A100%|██████████| 1/1 [01:28<00:00, 88.23s/it]

100%|██████████| 1/1 [01:28<00:00, 88.21s/it][A100%|██████████| 1/1 [01:28<00:00, 88.21s/it]

100%|██████████| 1/1 [01:28<00:00, 88.20s/it][A100%|██████████| 1/1 [01:28<00:00, 88.20s/it]

100%|██████████| 1/1 [01:28<00:00, 88.20s/it][A100%|██████████| 1/1 [01:28<00:00, 88.20s/it]

100%|██████████| 1/1 [01:28<00:00, 88.20s/it][A100%|██████████| 1/1 [01:28<00:00, 88.20s/it]
Checkpointing at shard 2939
[2024-07-02 13:12:56,231] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2919 is about to be saved!
[2024-07-02 13:12:58,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_00-model_states.pt...
[2024-07-02 13:13:00,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_00-model_states.pt.
[2024-07-02 13:13:01,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_04-model_states.pt...
[2024-07-02 13:13:02,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_03-model_states.pt...
[2024-07-02 13:13:04,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_02-model_states.pt...
[2024-07-02 13:13:07,704] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_08-model_states.pt...
[2024-07-02 13:13:07,809] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_07-model_states.pt...
[2024-07-02 13:13:08,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_06-model_states.pt...
[2024-07-02 13:13:09,277] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_05-model_states.pt...
[2024-07-02 13:13:12,256] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_01-model_states.pt...
[2024-07-02 13:17:36,373] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_02-model_states.pt.
[2024-07-02 13:17:36,552] [INFO] [logging.py:96:log_dist] [Rank 1] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_01_model_states.pt
[2024-07-02 13:17:36,552] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_01_model_states.pt...
[2024-07-02 13:17:36,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_01_model_states.pt.
[2024-07-02 13:17:36,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
[2024-07-02 13:17:36,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_04-model_states.pt.
[2024-07-02 13:17:36,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_03-model_states.pt.
[2024-07-02 13:17:37,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_03_model_states.pt...
[2024-07-02 13:17:37,031] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_07-model_states.pt.
[2024-07-02 13:17:37,032] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_06-model_states.pt.
[2024-07-02 13:17:37,059] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_03_model_states.pt.
[2024-07-02 13:17:37,059] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
[2024-07-02 13:17:37,065] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_08-model_states.pt.
[2024-07-02 13:17:37,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_02_model_states.pt...
[2024-07-02 13:17:37,275] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_06_model_states.pt...
[2024-07-02 13:17:37,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_02_model_states.pt.
[2024-07-02 13:17:37,301] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
[2024-07-02 13:17:37,311] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_06_model_states.pt.
[2024-07-02 13:17:37,312] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
[2024-07-02 13:17:37,516] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_05_model_states.pt...
[2024-07-02 13:17:37,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_05_model_states.pt.
[2024-07-02 13:17:37,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
[2024-07-02 13:17:38,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_09-model_states.pt...
[2024-07-02 13:17:46,664] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_09-model_states.pt.
[2024-07-02 13:17:46,673] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_07_model_states.pt...
[2024-07-02 13:17:46,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_07_model_states.pt.
[2024-07-02 13:17:46,763] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
[2024-07-02 13:18:06,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_01-model_states.pt.
[2024-07-02 13:18:07,214] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_00_model_states.pt
[2024-07-02 13:18:07,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_00_model_states.pt...
[2024-07-02 13:18:09,841] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_00_model_states.pt.
[2024-07-02 13:18:09,841] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
[2024-07-02 13:18:10,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/layer_05-model_states.pt.
[2024-07-02 13:18:11,451] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_04_model_states.pt...
[2024-07-02 13:18:11,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/atuin/b207dd/b207dd11/LLaVA-PEFT_adapter_lora_32_64_checkpoint/global_step2919/mp_rank_04_model_states.pt.
[2024-07-02 13:18:11,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2919 is ready now!
Checkpoint saved using --- 315.2516553401947 seconds ---
 57%|█████▋    | 2940/5198 [1:12:42<119:46:36, 190.96s/it] 57%|█████▋    | 2940/5198 [1:12:34<119:11:43, 190.04s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2757
 57%|█████▋    | 2940/5198 [1:12:39<119:14:09, 190.10s/it] 57%|█████▋    | 2940/5198 [1:12:34<119:13:15, 190.08s/it] 57%|█████▋    | 2940/5198 [1:12:40<119:11:54, 190.04s/it] 57%|█████▋    | 2940/5198 [1:12:29<119:16:47, 190.17s/it] 57%|█████▋    | 2940/5198 [1:12:37<119:12:22, 190.05s/it] 57%|█████▋    | 2940/5198 [1:12:35<119:15:54, 190.15s/it]Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A

  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:25<00:00, 145.43s/it][A100%|██████████| 1/1 [02:25<00:00, 145.43s/it]
 57%|█████▋    | 2941/5198 [1:15:07<111:11:55, 177.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:20:40,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=0, lr=[1.0141282118001882e-05], mom=[(0.9, 0.999)]
steps: 2920 loss: 0.5707 iter time (s): 149.185 samples/sec: 0.858

100%|██████████| 1/1 [02:29<00:00, 149.47s/it][A100%|██████████| 1/1 [02:29<00:00, 149.47s/it]
 57%|█████▋    | 2941/5198 [1:14:59<111:36:30, 178.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:29<00:00, 149.61s/it][A100%|██████████| 1/1 [02:29<00:00, 149.61s/it]
 57%|█████▋    | 2941/5198 [1:15:05<111:37:30, 178.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:29<00:00, 149.73s/it][A100%|██████████| 1/1 [02:29<00:00, 149.73s/it]
 57%|█████▋    | 2941/5198 [1:15:09<111:37:40, 178.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:29<00:00, 149.80s/it][A100%|██████████| 1/1 [02:29<00:00, 149.80s/it]
 57%|█████▋    | 2941/5198 [1:15:04<111:37:47, 178.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:29<00:00, 149.88s/it][A100%|██████████| 1/1 [02:29<00:00, 149.88s/it]
 57%|█████▋    | 2941/5198 [1:15:07<111:38:02, 178.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:29<00:00, 149.91s/it][A100%|██████████| 1/1 [02:29<00:00, 149.91s/it]
 57%|█████▋    | 2941/5198 [1:15:04<111:37:55, 178.06s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2758
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:29<00:00, 149.91s/it][A100%|██████████| 1/1 [02:29<00:00, 149.91s/it]
 57%|█████▋    | 2941/5198 [1:15:10<111:38:10, 178.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.57s/it][A100%|██████████| 1/1 [01:31<00:00, 91.57s/it]
 57%|█████▋    | 2942/5198 [1:16:39<95:04:24, 151.71s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:22:10,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=2921, skipped=0, lr=[1.0135619079517351e-05], mom=[(0.9, 0.999)]
steps: 2921 loss: 0.5829 iter time (s): 89.364 samples/sec: 1.432

100%|██████████| 1/1 [01:30<00:00, 90.18s/it][A100%|██████████| 1/1 [01:30<00:00, 90.19s/it]
 57%|█████▋    | 2942/5198 [1:16:29<95:03:19, 151.68s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.18s/it][A100%|██████████| 1/1 [01:30<00:00, 90.19s/it]
 57%|█████▋    | 2942/5198 [1:16:35<95:04:01, 151.70s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.20s/it][A100%|██████████| 1/1 [01:30<00:00, 90.20s/it]
 57%|█████▋    | 2942/5198 [1:16:39<95:04:16, 151.71s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.22s/it][A100%|██████████| 1/1 [01:30<00:00, 90.22s/it]
 57%|█████▋    | 2942/5198 [1:16:35<95:04:33, 151.72s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.23s/it][A100%|██████████| 1/1 [01:30<00:00, 90.23s/it]
 57%|█████▋    | 2942/5198 [1:16:37<95:04:48, 151.72s/it] 
100%|██████████| 1/1 [01:30<00:00, 90.21s/it][A100%|██████████| 1/1 [01:30<00:00, 90.21s/it]
 57%|█████▋    | 2942/5198 [1:16:35<95:04:26, 151.71s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2759

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:30<00:00, 90.20s/it][A100%|██████████| 1/1 [01:30<00:00, 90.20s/it]
 57%|█████▋    | 2942/5198 [1:16:40<95:04:38, 151.72s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.53s/it][A100%|██████████| 1/1 [01:26<00:00, 86.53s/it]
 57%|█████▋    | 2943/5198 [1:18:06<82:49:14, 132.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:23:37,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=2922, skipped=0, lr=[1.0129955997843679e-05], mom=[(0.9, 0.999)]
steps: 2922 loss: 0.6136 iter time (s): 85.810 samples/sec: 1.492

100%|██████████| 1/1 [01:26<00:00, 86.69s/it][A100%|██████████| 1/1 [01:26<00:00, 86.69s/it]
 57%|█████▋    | 2943/5198 [1:17:56<82:48:14, 132.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.68s/it][A100%|██████████| 1/1 [01:26<00:00, 86.68s/it]
 57%|█████▋    | 2943/5198 [1:18:01<82:48:41, 132.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.63s/it][A100%|██████████| 1/1 [01:26<00:00, 86.63s/it]
 57%|█████▋    | 2943/5198 [1:18:05<82:48:14, 132.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.67s/it][A100%|██████████| 1/1 [01:26<00:00, 86.67s/it]
 57%|█████▋    | 2943/5198 [1:18:01<82:48:58, 132.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.64s/it][A100%|██████████| 1/1 [01:26<00:00, 86.64s/it]
 57%|█████▋    | 2943/5198 [1:18:04<82:48:43, 132.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.66s/it][A100%|██████████| 1/1 [01:26<00:00, 86.66s/it]
 57%|█████▋    | 2943/5198 [1:18:06<82:48:52, 132.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.69s/it][A100%|██████████| 1/1 [01:26<00:00, 86.69s/it]
 57%|█████▋    | 2943/5198 [1:18:01<82:49:04, 132.22s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_plain_texts/shard_183
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:00<00:00, 120.45s/it][A100%|██████████| 1/1 [02:00<00:00, 120.45s/it]
 57%|█████▋    | 2944/5198 [1:20:06<80:36:23, 128.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:25:38,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=2923, skipped=0, lr=[1.0124292874797724e-05], mom=[(0.9, 0.999)]
steps: 2923 loss: 0.7946 iter time (s): 120.665 samples/sec: 1.061

100%|██████████| 1/1 [02:01<00:00, 121.62s/it][A100%|██████████| 1/1 [02:01<00:00, 121.62s/it]
 57%|█████▋    | 2944/5198 [1:19:57<80:47:01, 129.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.68s/it][A100%|██████████| 1/1 [02:01<00:00, 121.68s/it]
 57%|█████▋    | 2944/5198 [1:20:03<80:47:59, 129.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.70s/it][A100%|██████████| 1/1 [02:01<00:00, 121.70s/it]
 57%|█████▋    | 2944/5198 [1:20:07<80:47:55, 129.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.64s/it][A100%|██████████| 1/1 [02:01<00:00, 121.64s/it]
 57%|█████▋    | 2944/5198 [1:20:05<80:47:39, 129.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.73s/it][A100%|██████████| 1/1 [02:01<00:00, 121.73s/it]
 57%|█████▋    | 2944/5198 [1:20:03<80:48:45, 129.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:01<00:00, 121.68s/it][A100%|██████████| 1/1 [02:01<00:00, 121.68s/it]
 57%|█████▋    | 2944/5198 [1:20:08<80:48:11, 129.06s/it]
100%|██████████| 1/1 [02:01<00:00, 121.68s/it][A100%|██████████| 1/1 [02:01<00:00, 121.68s/it]
 57%|█████▋    | 2944/5198 [1:20:03<80:48:14, 129.06s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2760

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.35s/it][A100%|██████████| 1/1 [01:20<00:00, 80.35s/it]
 57%|█████▋    | 2945/5198 [1:21:27<71:31:13, 114.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:26:58,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=2924, skipped=0, lr=[1.0118629712196355e-05], mom=[(0.9, 0.999)]
steps: 2924 loss: 0.5071 iter time (s): 78.362 samples/sec: 1.633

100%|██████████| 1/1 [01:19<00:00, 79.15s/it][A100%|██████████| 1/1 [01:19<00:00, 79.15s/it]
 57%|█████▋    | 2945/5198 [1:21:17<71:23:15, 114.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.10s/it][A100%|██████████| 1/1 [01:19<00:00, 79.10s/it]
 57%|█████▋    | 2945/5198 [1:21:22<71:23:19, 114.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.11s/it][A100%|██████████| 1/1 [01:19<00:00, 79.11s/it]
 57%|█████▋    | 2945/5198 [1:21:26<71:23:25, 114.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.15s/it][A100%|██████████| 1/1 [01:19<00:00, 79.15s/it]
 57%|█████▋    | 2945/5198 [1:21:25<71:23:38, 114.08s/it]
100%|██████████| 1/1 [01:19<00:00, 79.12s/it][A100%|██████████| 1/1 [01:19<00:00, 79.12s/it]
 57%|█████▋    | 2945/5198 [1:21:22<71:24:05, 114.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.15s/it][A100%|██████████| 1/1 [01:19<00:00, 79.15s/it]
 57%|█████▋    | 2945/5198 [1:21:27<71:24:03, 114.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.16s/it][A100%|██████████| 1/1 [01:19<00:00, 79.16s/it]
 57%|█████▋    | 2945/5198 [1:21:22<71:24:08, 114.09s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2761
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.51s/it][A100%|██████████| 1/1 [01:27<00:00, 87.51s/it]
 57%|█████▋    | 2946/5198 [1:22:55<66:30:04, 106.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:28:26,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=2925, skipped=0, lr=[1.0112966511856448e-05], mom=[(0.9, 0.999)]
steps: 2925 loss: 0.5887 iter time (s): 87.132 samples/sec: 1.469

100%|██████████| 1/1 [01:27<00:00, 87.96s/it][A100%|██████████| 1/1 [01:27<00:00, 87.96s/it]
 57%|█████▋    | 2946/5198 [1:22:45<66:27:37, 106.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.03s/it][A100%|██████████| 1/1 [01:28<00:00, 88.03s/it]
 57%|█████▋    | 2946/5198 [1:22:50<66:28:24, 106.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.04s/it][A100%|██████████| 1/1 [01:28<00:00, 88.04s/it]
 57%|█████▋    | 2946/5198 [1:22:54<66:28:33, 106.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.88s/it][A100%|██████████| 1/1 [01:27<00:00, 87.88s/it]
 57%|█████▋    | 2946/5198 [1:22:50<66:27:14, 106.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.00s/it][A100%|██████████| 1/1 [01:28<00:00, 88.00s/it]
 57%|█████▋    | 2946/5198 [1:22:53<66:28:19, 106.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:27<00:00, 87.95s/it][A100%|██████████| 1/1 [01:27<00:00, 87.95s/it]
 57%|█████▋    | 2946/5198 [1:22:55<66:27:55, 106.25s/it]
100%|██████████| 1/1 [01:27<00:00, 87.94s/it][A100%|██████████| 1/1 [01:27<00:00, 87.94s/it]
 57%|█████▋    | 2946/5198 [1:22:50<66:27:55, 106.25s/it]
Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2762
  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.69s/it][A100%|██████████| 1/1 [01:26<00:00, 86.69s/it]
 57%|█████▋    | 2947/5198 [1:24:22<62:49:34, 100.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:29:53,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=2926, skipped=0, lr=[1.010730327559491e-05], mom=[(0.9, 0.999)]
steps: 2926 loss: 0.5206 iter time (s): 86.010 samples/sec: 1.488

100%|██████████| 1/1 [01:26<00:00, 86.85s/it][A100%|██████████| 1/1 [01:26<00:00, 86.85s/it]
 57%|█████▋    | 2947/5198 [1:24:11<62:47:42, 100.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.72s/it][A100%|██████████| 1/1 [01:26<00:00, 86.72s/it]
 57%|█████▋    | 2947/5198 [1:24:17<62:46:49, 100.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.78s/it][A100%|██████████| 1/1 [01:26<00:00, 86.78s/it]
 57%|█████▋    | 2947/5198 [1:24:21<62:47:31, 100.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.85s/it][A100%|██████████| 1/1 [01:26<00:00, 86.86s/it]
 57%|█████▋    | 2947/5198 [1:24:17<62:47:29, 100.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.78s/it][A100%|██████████| 1/1 [01:26<00:00, 86.78s/it]
 57%|█████▋    | 2947/5198 [1:24:19<62:47:22, 100.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.80s/it][A100%|██████████| 1/1 [01:26<00:00, 86.80s/it]
 57%|█████▋    | 2947/5198 [1:24:22<62:47:23, 100.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.81s/it][A100%|██████████| 1/1 [01:26<00:00, 86.81s/it]
 57%|█████▋    | 2947/5198 [1:24:17<62:47:27, 100.42s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2763
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:37<00:00, 97.85s/it][A100%|██████████| 1/1 [01:37<00:00, 97.85s/it]
 57%|█████▋    | 2948/5198 [1:26:00<62:20:08, 99.74s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:31:31,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=2927, skipped=0, lr=[1.0101640005228639e-05], mom=[(0.9, 0.999)]
steps: 2927 loss: 0.5085 iter time (s): 97.494 samples/sec: 1.313

100%|██████████| 1/1 [01:38<00:00, 98.30s/it][A100%|██████████| 1/1 [01:38<00:00, 98.30s/it]
 57%|█████▋    | 2948/5198 [1:25:50<62:22:13, 99.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.41s/it][A100%|██████████| 1/1 [01:38<00:00, 98.41s/it]
 57%|█████▋    | 2948/5198 [1:25:55<62:22:53, 99.81s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.32s/it][A100%|██████████| 1/1 [01:38<00:00, 98.32s/it]
 57%|█████▋    | 2948/5198 [1:25:59<62:22:23, 99.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.29s/it][A100%|██████████| 1/1 [01:38<00:00, 98.29s/it]
 57%|█████▋    | 2948/5198 [1:25:55<62:21:57, 99.79s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.36s/it][A100%|██████████| 1/1 [01:38<00:00, 98.36s/it]
 57%|█████▋    | 2948/5198 [1:25:58<62:22:36, 99.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:38<00:00, 98.34s/it][A100%|██████████| 1/1 [01:38<00:00, 98.34s/it]
 57%|█████▋    | 2948/5198 [1:26:00<62:22:28, 99.80s/it] 
100%|██████████| 1/1 [01:38<00:00, 98.34s/it][A100%|██████████| 1/1 [01:38<00:00, 98.34s/it]
 57%|█████▋    | 2948/5198 [1:25:55<62:22:25, 99.80s/it] Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2764

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:29<00:00, 89.06s/it][A100%|██████████| 1/1 [01:29<00:00, 89.06s/it]
 57%|█████▋    | 2949/5198 [1:27:29<60:20:05, 96.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:33:00,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=2928, skipped=0, lr=[1.0095976702574557e-05], mom=[(0.9, 0.999)]
steps: 2928 loss: 0.5794 iter time (s): 88.093 samples/sec: 1.453

100%|██████████| 1/1 [01:28<00:00, 88.99s/it][A100%|██████████| 1/1 [01:28<00:00, 88.99s/it]
 57%|█████▋    | 2949/5198 [1:27:19<60:19:11, 96.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.82s/it][A100%|██████████| 1/1 [01:28<00:00, 88.82s/it]
 57%|█████▋    | 2949/5198 [1:27:24<60:17:47, 96.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.89s/it][A100%|██████████| 1/1 [01:28<00:00, 88.89s/it]
 57%|█████▋    | 2949/5198 [1:27:28<60:18:11, 96.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.91s/it][A100%|██████████| 1/1 [01:28<00:00, 88.91s/it]
 57%|█████▋    | 2949/5198 [1:27:24<60:18:09, 96.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.90s/it][A100%|██████████| 1/1 [01:28<00:00, 88.90s/it]
 57%|█████▋    | 2949/5198 [1:27:27<60:18:28, 96.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.88s/it][A100%|██████████| 1/1 [01:28<00:00, 88.88s/it]
 57%|█████▋    | 2949/5198 [1:27:29<60:18:08, 96.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.90s/it][A100%|██████████| 1/1 [01:28<00:00, 88.90s/it]
 57%|█████▋    | 2949/5198 [1:27:24<60:18:18, 96.53s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2765
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.13s/it][A100%|██████████| 1/1 [01:26<00:00, 86.13s/it]
 57%|█████▋    | 2950/5198 [1:28:55<58:22:54, 93.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:34:26,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=2929, skipped=0, lr=[1.0090313369449591e-05], mom=[(0.9, 0.999)]
steps: 2929 loss: 0.5638 iter time (s): 85.435 samples/sec: 1.498

100%|██████████| 1/1 [01:26<00:00, 86.22s/it][A100%|██████████| 1/1 [01:26<00:00, 86.22s/it]
 57%|█████▋    | 2950/5198 [1:28:45<58:21:32, 93.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.26s/it][A100%|██████████| 1/1 [01:26<00:00, 86.26s/it]
 57%|█████▋    | 2950/5198 [1:28:51<58:21:06, 93.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.39s/it][A100%|██████████| 1/1 [01:26<00:00, 86.39s/it]
 57%|█████▋    | 2950/5198 [1:28:55<58:22:42, 93.49s/it]
100%|██████████| 1/1 [01:26<00:00, 86.31s/it][A100%|██████████| 1/1 [01:26<00:00, 86.31s/it]
 57%|█████▋    | 2950/5198 [1:28:50<58:21:51, 93.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.29s/it][A100%|██████████| 1/1 [01:26<00:00, 86.29s/it]
 57%|█████▋    | 2950/5198 [1:28:56<58:21:36, 93.46s/it]
100%|██████████| 1/1 [01:26<00:00, 86.31s/it][A100%|██████████| 1/1 [01:26<00:00, 86.31s/it]
 57%|█████▋    | 2950/5198 [1:28:53<58:22:03, 93.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.30s/it][A100%|██████████| 1/1 [01:26<00:00, 86.30s/it]
 57%|█████▋    | 2950/5198 [1:28:50<58:21:47, 93.46s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2766
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.19s/it][A100%|██████████| 1/1 [01:28<00:00, 88.19s/it]
 57%|█████▋    | 2951/5198 [1:30:23<57:24:02, 91.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:35:55,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=0, lr=[1.0084650007670681e-05], mom=[(0.9, 0.999)]
steps: 2930 loss: 0.5957 iter time (s): 87.549 samples/sec: 1.462

100%|██████████| 1/1 [01:28<00:00, 88.36s/it][A100%|██████████| 1/1 [01:28<00:00, 88.36s/it]
 57%|█████▋    | 2951/5198 [1:30:13<57:22:47, 91.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.43s/it][A100%|██████████| 1/1 [01:28<00:00, 88.43s/it]
 57%|█████▋    | 2951/5198 [1:30:19<57:23:21, 91.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 57%|█████▋    | 2951/5198 [1:30:19<57:22:28, 91.92s/it]
100%|██████████| 1/1 [01:28<00:00, 88.32s/it][A100%|██████████| 1/1 [01:28<00:00, 88.32s/it]
 57%|█████▋    | 2951/5198 [1:30:23<57:23:10, 91.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.31s/it][A100%|██████████| 1/1 [01:28<00:00, 88.31s/it]
 57%|█████▋    | 2951/5198 [1:30:21<57:22:38, 91.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.33s/it][A100%|██████████| 1/1 [01:28<00:00, 88.33s/it]
 57%|█████▋    | 2951/5198 [1:30:24<57:22:34, 91.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:28<00:00, 88.34s/it][A100%|██████████| 1/1 [01:28<00:00, 88.34s/it]
 57%|█████▋    | 2951/5198 [1:30:19<57:22:42, 91.93s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2767
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.26s/it][A100%|██████████| 1/1 [01:50<00:00, 110.26s/it]
 57%|█████▋    | 2952/5198 [1:32:14<60:49:48, 97.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:37:46,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=2931, skipped=0, lr=[1.0078986619054773e-05], mom=[(0.9, 0.999)]
steps: 2931 loss: 0.5518 iter time (s): 110.271 samples/sec: 1.161

100%|██████████| 1/1 [01:51<00:00, 111.10s/it][A100%|██████████| 1/1 [01:51<00:00, 111.10s/it]
 57%|█████▋    | 2952/5198 [1:32:04<60:56:43, 97.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 110.98s/it][A100%|██████████| 1/1 [01:50<00:00, 110.98s/it]
 57%|█████▋    | 2952/5198 [1:32:10<60:55:42, 97.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:50<00:00, 111.00s/it][A100%|██████████| 1/1 [01:50<00:00, 111.00s/it]
 57%|█████▋    | 2952/5198 [1:32:14<60:55:49, 97.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.08s/it][A100%|██████████| 1/1 [01:51<00:00, 111.08s/it]
 57%|█████▋    | 2952/5198 [1:32:10<60:56:10, 97.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.04s/it][A100%|██████████| 1/1 [01:51<00:00, 111.04s/it]
 57%|█████▋    | 2952/5198 [1:32:12<60:55:53, 97.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.06s/it][A100%|██████████| 1/1 [01:51<00:00, 111.06s/it]
 57%|█████▋    | 2952/5198 [1:32:15<60:56:01, 97.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:51<00:00, 111.05s/it][A100%|██████████| 1/1 [01:51<00:00, 111.05s/it]
 57%|█████▋    | 2952/5198 [1:32:10<60:55:57, 97.67s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2768
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.60s/it][A100%|██████████| 1/1 [01:44<00:00, 104.60s/it]
 57%|█████▋    | 2953/5198 [1:33:59<62:10:07, 99.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:39:30,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=2932, skipped=0, lr=[1.0073323205418819e-05], mom=[(0.9, 0.999)]
steps: 2932 loss: 0.5668 iter time (s): 103.843 samples/sec: 1.233

100%|██████████| 1/1 [01:44<00:00, 104.62s/it][A100%|██████████| 1/1 [01:44<00:00, 104.62s/it]
 57%|█████▋    | 2953/5198 [1:33:49<62:13:04, 99.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.70s/it][A100%|██████████| 1/1 [01:44<00:00, 104.70s/it]
 57%|█████▋    | 2953/5198 [1:33:55<62:13:09, 99.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.69s/it][A100%|██████████| 1/1 [01:44<00:00, 104.69s/it]
 57%|█████▋    | 2953/5198 [1:33:59<62:13:25, 99.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.65s/it][A100%|██████████| 1/1 [01:44<00:00, 104.65s/it]
 57%|█████▋    | 2953/5198 [1:33:55<62:13:04, 99.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.72s/it][A100%|██████████| 1/1 [01:44<00:00, 104.72s/it]
 57%|█████▋    | 2953/5198 [1:33:57<62:13:30, 99.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:44<00:00, 104.74s/it][A100%|██████████| 1/1 [01:44<00:00, 104.74s/it]
 57%|█████▋    | 2953/5198 [1:34:00<62:13:51, 99.79s/it]
100%|██████████| 1/1 [01:44<00:00, 104.73s/it][A100%|██████████| 1/1 [01:44<00:00, 104.73s/it]
 57%|█████▋    | 2953/5198 [1:33:55<62:13:41, 99.79s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2769

  0%|          | 0/1 [00:00<?, ?it/s][ATraining on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:05<00:00, 125.46s/it][A100%|██████████| 1/1 [02:05<00:00, 125.46s/it]
 57%|█████▋    | 2954/5198 [1:36:04<66:59:49, 107.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:41:37,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=2933, skipped=0, lr=[1.0067659768579788e-05], mom=[(0.9, 0.999)]
steps: 2933 loss: 0.5218 iter time (s): 125.380 samples/sec: 1.021

100%|██████████| 1/1 [02:06<00:00, 126.12s/it][A100%|██████████| 1/1 [02:06<00:00, 126.12s/it]
 57%|█████▋    | 2954/5198 [1:35:55<67:07:11, 107.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.21s/it][A100%|██████████| 1/1 [02:06<00:00, 126.21s/it]
 57%|█████▋    | 2954/5198 [1:36:01<67:08:15, 107.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.20s/it][A100%|██████████| 1/1 [02:06<00:00, 126.20s/it]
 57%|█████▋    | 2954/5198 [1:36:01<67:08:01, 107.70s/it]
100%|██████████| 1/1 [02:06<00:00, 126.22s/it][A100%|██████████| 1/1 [02:06<00:00, 126.22s/it]
 57%|█████▋    | 2954/5198 [1:36:05<67:08:37, 107.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.20s/it][A100%|██████████| 1/1 [02:06<00:00, 126.20s/it]
 57%|█████▋    | 2954/5198 [1:36:03<67:08:22, 107.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.16s/it][A100%|██████████| 1/1 [02:06<00:00, 126.16s/it]
 57%|█████▋    | 2954/5198 [1:36:06<67:08:10, 107.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:06<00:00, 126.18s/it][A100%|██████████| 1/1 [02:06<00:00, 126.18s/it]
 57%|█████▋    | 2954/5198 [1:36:01<67:08:15, 107.71s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2770
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:32<00:00, 92.40s/it][A100%|██████████| 1/1 [01:32<00:00, 92.40s/it]
 57%|█████▋    | 2955/5198 [1:37:37<64:10:50, 103.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:43:08,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=2934, skipped=0, lr=[1.0061996310354644e-05], mom=[(0.9, 0.999)]
steps: 2934 loss: 0.5695 iter time (s): 90.848 samples/sec: 1.409

100%|██████████| 1/1 [01:31<00:00, 91.70s/it][A100%|██████████| 1/1 [01:31<00:00, 91.71s/it]
 57%|█████▋    | 2955/5198 [1:37:27<64:06:22, 102.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.64s/it][A100%|██████████| 1/1 [01:31<00:00, 91.64s/it]
 57%|█████▋    | 2955/5198 [1:37:33<64:06:18, 102.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.54s/it][A100%|██████████| 1/1 [01:31<00:00, 91.54s/it]
 57%|█████▋    | 2955/5198 [1:37:37<64:05:28, 102.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.66s/it][A100%|██████████| 1/1 [01:31<00:00, 91.66s/it]
 57%|█████▋    | 2955/5198 [1:37:32<64:06:30, 102.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.59s/it][A100%|██████████| 1/1 [01:31<00:00, 91.59s/it]
 57%|█████▋    | 2955/5198 [1:37:35<64:05:55, 102.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.63s/it][A100%|██████████| 1/1 [01:31<00:00, 91.63s/it]
 57%|█████▋    | 2955/5198 [1:37:38<64:06:13, 102.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:31<00:00, 91.64s/it][A100%|██████████| 1/1 [01:31<00:00, 91.64s/it]
 57%|█████▋    | 2955/5198 [1:37:32<64:06:21, 102.89s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2771
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:02<00:00, 122.52s/it][A100%|██████████| 1/1 [02:02<00:00, 122.52s/it]
 57%|█████▋    | 2956/5198 [1:39:40<67:49:20, 108.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A[2024-07-02 13:45:12,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=2935, skipped=0, lr=[1.0056332832560376e-05], mom=[(0.9, 0.999)]
steps: 2935 loss: 0.5167 iter time (s): 122.734 samples/sec: 1.043

100%|██████████| 1/1 [02:03<00:00, 123.58s/it][A100%|██████████| 1/1 [02:03<00:00, 123.58s/it]
 57%|█████▋    | 2956/5198 [1:39:30<67:56:45, 109.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.56s/it][A100%|██████████| 1/1 [02:03<00:00, 123.56s/it]
 57%|█████▋    | 2956/5198 [1:39:36<67:56:28, 109.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.58s/it][A100%|██████████| 1/1 [02:03<00:00, 123.58s/it]
 57%|█████▋    | 2956/5198 [1:39:40<67:56:02, 109.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.59s/it][A100%|██████████| 1/1 [02:03<00:00, 123.59s/it]
 57%|█████▋    | 2956/5198 [1:39:36<67:56:56, 109.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.57s/it][A100%|██████████| 1/1 [02:03<00:00, 123.57s/it]
 57%|█████▋    | 2956/5198 [1:39:38<67:56:14, 109.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.58s/it][A100%|██████████| 1/1 [02:03<00:00, 123.58s/it]
 57%|█████▋    | 2956/5198 [1:39:41<67:56:39, 109.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [02:03<00:00, 123.57s/it][A100%|██████████| 1/1 [02:03<00:00, 123.57s/it]
 57%|█████▋    | 2956/5198 [1:39:36<67:56:32, 109.10s/it]Loading dataset from /home/atuin/b207dd/b207dd11/LLaVA-PEFT_processed_dataset_with_images/shard_2772
Training on 128 of 128 sentences.

  0%|          | 0/1 [00:00<?, ?it/s][A